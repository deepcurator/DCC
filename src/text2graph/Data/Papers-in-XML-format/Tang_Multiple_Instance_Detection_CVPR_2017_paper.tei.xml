<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Instance Detection Network with Online Instance Classifier Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
							<email>pengtang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Instance Detection Network with Online Instance Classifier Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the development of Convolutional Neural Network (CNN) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, great improvements have been achieved on object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, due to the availability of large scale datasets with accurate boundingbox-level annotations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. However, collecting such accurate annotations can be very labor-intensive and time-consuming, whereas achieving only image-level annotations (i.e., image tags) is much easier, as these annotations are often available at the Internet (e.g., image search queries <ref type="bibr" target="#b20">[21]</ref>). In this paper, we aim at the Weakly Supervised Object Detection (WSOD) problem, i.e., only image tags are available during * Corresponding author. In the left, the top ranking proposal A does not correctly localize the object. After instance classifier refinement, in the right, the correct proposal D is detected and more discriminative performance of instance classifier is shown.</p><p>training to indicate whether an object exists in an image.</p><p>Most of previous methods follow the Multiple Instance Learning (MIL) pipeline for WSOD <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. They treat images as bags and image regions generated by object proposal methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref> as instances to train instance classifiers (object detectors) under the MIL constraints <ref type="bibr" target="#b9">[10]</ref>. Meanwhile, recent efforts tend to combine MIL and CNN by either using CNN as an off-the-shelf feature extractor <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> or training an end-to-end MIL network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Here we are also along the MIL line for WSOD, and train an end-to-end network.</p><p>Though many promising results have been achieved in WSOD, they are still far from comparable to fully supervised ones <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. Weakly supervised object detection only requires supervision at image category level. Bilen and Vedaldi <ref type="bibr" target="#b3">[4]</ref> presents an end-to-end deep network for WSOD, in which final image classification score is the weighted sum of proposal scores, that is, each proposal contributes a percentage to the final image classification. The deep network can correctly classify image even only "see" a part of object, and as a result, the top ranking proposal may fail to meet the standard object detection requirement (IoU&gt;0.5 between ground truths and predicted boxes). As shown in <ref type="figure" target="#fig_0">Fig. 1 (left)</ref>, the top-ranking proposal A is too small. Meanwhile, proposals B, C, and D have similar detection scores. This shows that the WSOD network is not discriminative enough to correctly localize object. This is a core problem of end-to-end deep network based WSOD. To address this problem, we put forward two improvements in this paper: 1) Instead of estimating instance weights through weighted sum pooling, we propose to add some blocks in the network for learning more discriminative instance classifiers by explicitly assigning binary instance labels; 2) We propose to refine instance classifier online using spatial relation.</p><p>Our motivation is that, though some detectors only capture objects partially, proposals having high spatial overlaps with detected parts may cover the whole object, or at least contain larger portion of the object. In <ref type="bibr" target="#b3">[4]</ref>, Bilen and Vedaldi propose a spatial regulariser via forcing features of highest scoring region and its adjacent regions to be the same, which significantly improves WSOD performance. Nevertheless, forcing spatially overlapped proposals to have the same features seems too rigorous. Rather than taking the rigorous constraint, we think the features of spatially overlapped proposals are in the same manifold. Then these overlapped proposals could share similar label information. As shown in <ref type="figure" target="#fig_0">Fig. 1 (right)</ref>, we except the label information of A can propagate to B and C which has large overlap with A, and then the label information of B and C can propagate to D to correctly localize object. To implement this idea, we design some instance classifiers in the network of <ref type="bibr" target="#b3">[4]</ref>. The labels of instance could be refined by their spatially overlapped instances. We name this new network structure Multiple Instance Detection Network (MIDN) with instance classifier.</p><p>In practice, there are two important issues. 1) How to initialize instance labels, since there is no instance-level supervision in this task. 2) How to train the network with instance classifier efficiently. A natural way for classifier refinement is the alternative strategy, that is, alternatively relabelling instance and training instance classifier, while this procedure is very time-consuming, especially considering training deep networks with a huge number of Stochastic Gradient Descent (SGD) iterations. To overcome these difficulties, we propose a novel Online Instance Classifier Refinement (OICR) algorithm to train the network online.</p><p>Our method has multiple output streams for different stages: the first is the MIDN to train a basic instance classifier and others refine the classifier. To refine instance classifier online, after the forward process of SGD, we can obtain a set of proposal scores. According to these scores, for each stage, we can label the top-scoring proposal along with its spatially overlapped proposals to the image label. Then these proposal labels can be used as the supervision to train instance classifier in the next stage. Though the top-scoring proposal may only contain a part of an object, its adjacent proposals will cover larger portion of the object. Thus the instance classifier can be refined. After implementing the refinement procedure multiple times, the detector can discover the whole object instead of parts gradually, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. But in the beginning of training, all classifiers are almost non-trained, which will result in very noisy supervision of refined classifiers, and then the training will deviate from correct solutions a lot. To solve this problem, we design a weighted loss further by assigning different weights to different proposals in different training iterations. Using this strategy, all classifier refinement procedures can thus be integrated into a single network and trained end-to-end. It can improve the performance benefiting from the classifier refinement procedure. Meanwhile, the multi-stage strategy and online refinement algorithm is very computational efficient in both training and testing. Moreover, performance can be improved by sharing representations among different training stages.</p><p>We elaborately conduct many experiments on the challenging PASCAL VOC dataset to confirm the effectiveness of our method. Our method achieves 47.0% mAP and 64.3% CorLoc on VOC 2007 that outperforms previous best performed methods by a large margin.</p><p>In summary, the main contributions of our work are listed as follows.</p><p>• We propose a framework for weakly supervised learning that combines MIDN with multi-stage instance classifiers. With only supervision of the outputs from its preceding stage, the discriminatory power of the instance classifier can be enhanced iteratively.</p><p>• We further design a novel OICR algorithm that integrates the basic detection network and the multi-stage instance-level classifier into a single network. The proposed network is end-to-end trainable. Compared with the alternatively training strategy, we demonstrate that our method can not only reduce the training time, but also boost the performance.</p><p>• Our method achieves significantly better results over previous state-of-the-art methods on the challenging PASCAL VOC 2007 and 2012 benchmarks for weakly supervised object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>MIL is a classical weakly supervised learning problem and was first proposed in <ref type="bibr" target="#b9">[10]</ref> for drug activity prediction. After that, many solutions have been proposed for MIL <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. In MIL, a set of bags are given, and each bag is associated with a collection of instances. MIL has two constraints: 1) If a bag is positive, at least one instance in the bag is positive; 2) If a bag is negative, all instances in the bag are negative. It is natural to treat WSOD as a MIL problem. Then the problem turns into finding an instance classifier only given bag labels. Our method also follows the MIL line, and the classifier refinement is inspired by the classifier updating procedure in mi-SVM <ref type="bibr" target="#b0">[1]</ref> to some extent. The differences are that, in mi-SVM, it uses an alternative strategy to relabel instances and retrain a classifier, while we adopt an online refinement algorithm; the mi-SVM relabel instances according to the instance score predicted by the classifier, while we select instances according to the spatial relation.</p><p>Most of the existing methods solve the WSOD problem based on MIL <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. For example, Wang et al. <ref type="bibr" target="#b30">[31]</ref> relaxed the MIL restraints into a differentiable loss function and optimized it by SGD to speed up training and improve results. Cibis et al. <ref type="bibr" target="#b6">[7]</ref> trained a multi-fold MIL detector by alternatively relabelling instances and retraining classifier. Recently, some researchers combined CNN and MIL to train an end-to-end network for WSOD <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. Oquab et al. <ref type="bibr" target="#b23">[24]</ref> trained a CNN network using the maxpooing MIL strategy to localize objects. But their methods can only coarsely localize objects regardless of their sizes and aspect ratios, our method can detect objects more accurately. Bilen and Vedaldi <ref type="bibr" target="#b3">[4]</ref> proposed a Weakly Supervised Deep Detection Network (WSDDN), which presents a novel weighted MIL pooling strategy and combines with the proposal objectness and spatial regulariser for better performance. Based on the WSDDN, Kantorov et al. <ref type="bibr" target="#b15">[16]</ref> used a contrastive model to consider the context information for improvement. We also choose the WSDDN as our basic network, but we combine it with multi-stage classifier refinement, and propose a novel OICR algorithm to train our network effectively and efficiently, which can boost the performance significantly. Different from the spatial regulariser in WSDDN <ref type="bibr" target="#b3">[4]</ref> that forces features of highest scoring proposal and its spatially overlapped proposals to be the same, our OICR assumes features of spatially overlapped proposals are in the same manifold, which is more reasonable. Experiments on Section 4 demonstrate that our strategy can obtain more superior results.</p><p>The proposal labelling procedure is also related to the semi-supervised label propagation method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>. But in label propagation, it labels data according to the similarity among labelled and unlabelled data, while we use spatial overlap as the metric; and there are no available labelled instances for propagation, which is quite different from semisupervised methods. Meanwhile, the sharing representation strategy in our network is similar to multi-task learning <ref type="bibr" target="#b4">[5]</ref>. Unlike the multi-task learning that each output stream has their own relatively independent external supervision, in our method, supervision of latter streams only depends on the outputs from their preceding streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall architecture of our method is shown in <ref type="figure">Fig. 3</ref>. Given an image, we first generate about 2, 000 object proposals by Selective Search <ref type="bibr" target="#b28">[29]</ref>. The image and these proposals are fed into some convolutional (conv) layers with Spatial Pyramid Pooling (SPP) layer <ref type="bibr" target="#b13">[14]</ref> to produce a fixed-size conv feature map per-proposal, and then they are fed into two fully connected (fc) layers to generate a collection of proposal feature vectors. These features are branched into different streams, i.e., different stages: the first one is the MIDN to train a basic instance classifier and others refine classifier. Specially, supervision for classifier refinement is decided by outputs from their preceding stages, e.g., supervision of the first refined classifier  <ref type="figure">Figure 3</ref>. The architecture of MIDN with OICR. Proposal/instance feature is generated by the spatial pyramid pooling layer on the convolutional feature map of image and two fully connected layers. These proposal feature vectors are branched into many streams for different stages: the first one for the basic multiple instance detection network and others for instance classifier refinement. Supervision for classifier refinement is decided by outputs from their preceding stages. All these stages share the same proposal representations.</p><p>depends on the output from the basic classifier, and supervision of k th refined classifier depends on outputs from {k − 1} th refined classifier. In this section, we will introduce the chosen basic MIDN, and explain our OICR algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multiple instance detection network</head><p>It is necessary to achieve instance-level supervision to train refined classifier, yet such supervision is unavailable. As we have stated before, the top-scoring proposal by instance classifiers and its adjacent proposals can be labelled to its image label as supervision. So we first introduce our MIDN to generate the basic instance classifier. There are many possible choices <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref> to achieve this. Here we choose the method by Bilen and Vedaldi <ref type="bibr" target="#b3">[4]</ref> which proposes a weighted pooling strategy to obtain the instance classifier, for its effectiveness and implementation convenience. Notice that our network is independent of special MIL methods, so any method that can be trained end-to-end could be embedded into our network.</p><p>As shown in the "Multiple instance detection network" block of <ref type="figure">Fig. 3</ref>, proposal features are branched into two streams to produce two matrices x c , x d ∈ R C×|R| of image by two fc layers, where C denotes the number of image classes and |R| denotes the number of proposals. Then the two matrices are passing through two softmax T ∈ R C×1 , where y c = 1 or 0 indicates the image with or without object c. We can train the basic instance classifier by standard multi-class cross entropy loss, as shown in Eq. (1), then the instance classifier can be obtained according to the proposal score x R . More details can be found in <ref type="bibr" target="#b3">[4]</ref>.</p><formula xml:id="formula_0">layer along different directions: [σ(x c )] ij =</formula><formula xml:id="formula_1">L b = − C c=1 {y c log φ c + (1 − y c ) log(1 − φ c )}.<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Online instance classifier refinement</head><p>In the last subsection, we have obtained the basic instance classifier. Here we will expound how to refine instance classifiers online. A natural way to refine classifier is an alternative strategy, that is, fixing the classifier and labelling proposals, fixing proposal labels and training the classifier. But it has some limitations: 1) It is very time-consuming as it requires training the classifier multiple times; 2) Training different classifiers in different refinement steps separately may harm the performance because it hinders the process to benefit from the shared representations. Hence, we integrate the basic MIDN and different classifier refinement stages into a single network and train it end-to-end.</p><p>The difficulty is how to obtain instance labels for refinement when there are no available labelled instances. To deal with this problem, we propose an online labelling and refinement strategy. Different from the basic instance classifier, the output score vector x Rk j of proposal j for refined classifier is a {C + 1}-dimensional vector, i.e., ∈ R (C+1)×1 , k ∈ {1, 2, ..., K}, where the k is for k th time refinement, K is the total refinement times, and the {C + 1} th dimension is for background (here we represent the proposal score vector from the basic classifier as x R0 j ∈ R C×1 ). The x Rk j , k &gt; 0 is obtained by passing the proposal feature vector through a single fc layer and a softmax over classes layer, as shown in the "Instance classifier refinement" block of <ref type="figure">Fig. 3</ref>.</p><p>Suppose the label vector for proposal j is Y</p><formula xml:id="formula_2">k j = [y k 1j , y k 2j , ..., y k (C+1)j ] T ∈ R (C+1)×1</formula><p>. In each training iteration, after the forward process of SGD, we can get a set of proposal scores x R(k−1) . Then we can obtain the supervision of refinement time k according to x R(k−1) . There are many possible methods to obtain instance labels using x R(k−1) , e.g., labeling an instance as positive if its score exceeds a threshold, otherwise as negative, as the mi-SVM <ref type="bibr" target="#b0">[1]</ref>. But in our case, the score for each instance is changed during each training iteration, and for different classes, using the same threshold may not be suitable, thus it is hard to settle a threshold. Here we choose a different strategy, inspired by the fact that highly spatially overlapped instances should have the same label. Suppose an image has class label c, we first select proposal j greater than a threshold I t which is determined by experiments. Meanwhile, if there is no object c in the image, we set all y k cj = 0. Using this supervision, we can train the refined classifier based on the loss function in Eq. (3). Through multiple times of refinement, our detector can detect larger parts of objects gradually.</p><formula xml:id="formula_3">j k−1 c = arg max r x R(k−1) cr .<label>(2)</label></formula><formula xml:id="formula_4">L k r = − 1 |R| |R| r=1 C+1 c=1 y k cr log x Rk cr .<label>(3)</label></formula><p>Actually the acquired supervision for refining classifier is very noisy, especially in the beginning of training, which will result in unstable solutions. To solve this problem, we change the loss in Eq. (3) to a weighted version, as in Eq. (4). T . Where r ∈ {1, ..., |R|} and k ∈ {1, ..., K}. 1: Feed X and its proposals into the network to produce proposal score matrices x Rk , k ∈ {0, ..., K − 1}.</p><formula xml:id="formula_5">L k r = − 1 |R| |R| r=1 C+1 c=1 w k r y k cr log x Rk cr ,<label>(4)</label></formula><formula xml:id="formula_6">2: for k = 0 to K − 1 do 3:</formula><p>Set all elements in I = [I 1 , ..., I |R| ] T to − inf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Set all y k+1 cr = 0, c ∈ {1, ..., C} and y k+1 (C+1)r = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for c = 1 to C do Choose the top-scoring proposal j k c by Eq. (2).</p><p>8:</p><p>for r = 1 to |R| do where w k r is the loss weight and can be acquired by the 11 th line of Algorithm 1. The explanation of such choice is as follows. In the beginning of training, the w k r is small, hence, the loss is also small. As a consequence, the performance of the network will not decrease a lot though good positive instances cannot be found. Meanwhile, during the training procedure, the network can achieve positive instances with high scores easily for easy bags, and these positive instances are always with high scores, i.e., w k r is large. On the contrary, it is difficult to get positive instances for difficult bags, as a result, these positive instances are always very noisy. Nevertheless, the refined classifier will not deviate from the correct solution a lot, because the scores of these noisy positive instances are relatively low, i.e., w k r is small. To make the OICR algorithm more clear, we summarize the process to obtain supervision in Algorithm 1, where I r indicates the maximum IoU between proposal r and the topscoring proposal. After obtaining supervision and loss for training refined classifiers, we can get the loss of our overall network by combining Eq. <ref type="formula" target="#formula_1">(1)</ref> and Eq. <ref type="formula" target="#formula_5">(4)</ref>, as Eq. <ref type="formula" target="#formula_7">(5)</ref>. Through optimizing this loss function, we can integrate the basic network and different classifier refinement stages into a single network, and share representations among different stages.  </p><formula xml:id="formula_7">L = L b + K k=1 L k r .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>In this section we will perform thorough experiments to analyse our OICR and its components for weakly supervised object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and evaluation measures</head><p>We evaluate our method on the challenging PASCAL VOC 2007 and 2012 datasets <ref type="bibr" target="#b10">[11]</ref> which have 9, 962 and 22, 531 images respectively for 20 object classes. These two datasets are divided into train, val, and test sets. Here we choose the trainval set (5, 011 images for 2007 and 11, 540 for 2012) to train our network. As we focus on weakly supervised detection, only image-level labels are utilized during training. For testing, there are two metrics for evaluation: mAP and CorLoc. Average Precision (AP) and the mean of AP (mAP) is the evaluation metric to test our model on the testing set, which follows the standard PASCAL VOC protocol <ref type="bibr" target="#b10">[11]</ref>. Correct localization (CorLoc) is to test our model on the training set measuring the localization accuracy <ref type="bibr" target="#b8">[9]</ref>. All these two metrics are based on the PASCAL criteria, i.e., IoU&gt;0.5 between ground truths and predicted boxes.</p><p>Implementation details Our method is built on two pretrained ImageNet <ref type="bibr" target="#b7">[8]</ref> networks: VGG M <ref type="bibr" target="#b5">[6]</ref> and VGG16 <ref type="bibr" target="#b26">[27]</ref>, each of which has some conv layers with max-pooling layer and three fc layers. We replace the last max-pooling layer of the two models by SPP layer, and the last fc layer and softmax loss layer by the layers described in Section 3. To increase the feature map size from the last conv layer, we replace the penultimate max-pooling layer and its subsequent conv layers by the dilated conv layers <ref type="bibr" target="#b31">[32]</ref>. The new added layers are initialized using Gaussian distributions with 0-mean and standard deviations 0.01. Biases are initialized to 0. During training, the mini-batch size for SGD is set to 2, and the learning rate is set to 0.001 for the first 40K iterations and then decrease to 0.0001 in the following 30K iterations. The momentum and weight decay are set to 0.9 and 0.0005 respectively. As we have stated in Section 3, Selective Search (SS) <ref type="bibr" target="#b28">[29]</ref> is adopted to generate about 2, 000 proposals perimage. For data augmentation, we use five image scales {480, 576, 688, 864, 1200} (resize the shortest side to one of these scales) and cap the longest image side to less than 2000 with horizontal flips for both training and testing. We refine instance classifier three times, i.e., K = 3 in Section 3.2, so there are four stages in total. The IoU threshold I t in the 12 th line of Algorithm 1 is set to 0.5. During testing, the mean output of these three refined classifiers is chosen. We also follow the <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> to train a supervised object detector by choosing top-scoring proposals given by our method as pseudo ground truths to further improve our results. Here we train a Fast RCNN (FRCNN) <ref type="bibr" target="#b11">[12]</ref> detector using the VGG16 model and the same five image scales (horizontal flips only in training). SS is also chosen for proposal generation to train the FRCNN. Non-maxima suppression (with 30% IoU threshold) is applied to compute AP and CorLoc.</p><p>Our experiments are implemented based on the Caffe <ref type="bibr" target="#b14">[15]</ref> deep learning framework. All of our experiments are running on a NVIDIA GTX TitanX GPU. Codes for reproducing the results are available at https://github. com/ppengtang/oicr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation experiments</head><p>We first conduct some ablation experiments to illustrate the effectiveness of our training strategy, including the influence of classifier refinement, OICR, weighted loss, and the IoU threshold I t . Without loss generality, we only perform experiments on VOC 2007 and use the VGG M model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">The influence of instance classifier refinement</head><p>As in the blue line of <ref type="figure" target="#fig_7">Fig. 4</ref>, we can observe that compared with the basic network, even just refining instance classifier one time can boost the performance a lot (mAP from 29.5 to 35.6 and CorLoc from 49.9 to 56.0), which confirms the necessity of refinement. If we refine the classifier multiple times, the results can be improved further. But when re-  finement is implemented too many times, the performance tends to be saturated (the improvement from 2 times to 3 times is small). Maybe this is because the network tends to converge so that the supervision of 3 rd time is similar to 2 nd time. In the rest of this paper we only refine the classifier 3 times. Notice that in <ref type="figure" target="#fig_7">Fig. 4</ref>, the "0 time" is similar to the WSDDN <ref type="bibr" target="#b3">[4]</ref> using SS as proposals. Our result is a little worse than theirs (30.9 mAP in their paper), due to the different implementing platform and details. <ref type="figure" target="#fig_7">Fig. 4</ref> compares the results of different refinement times and different training strategies for classifier refinement. As we can see, whether for our OICR algorithm or the alternative strategy, results can be improved by refinement. More importantly, compared with the alternatively refinement strategy, our OICR can boost the performance consistently and significantly, which confirms the necessity of sharing representations. Meanwhile, our method can also reduce the training time a lot, as it only requires to train a single model instead of training K + 1 models for K times refinement in the alternative strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">The influence of OICR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">The influence of weighted loss</head><p>We also study the influence of our weighted loss in Eq. (4). So here we train a network based on the Eq. (3). From Table 1, we can see that using the unweighted loss, the improvement from refinement is very scant, and the perfor-  <ref type="table">Table 4</ref>. Results for different methods on VOC 2012. Detailed per-class results can be found in <ref type="table">Table 1 and Table 2</ref> of the Supplementary Material.</p><p>mance is even worse than the alternative strategy. Using the weighted loss can achieve much better performance, which confirms our theory in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">The influence of IoU threshold</head><p>In previous experiments, we set the IoU threshold I t in the 12 th line of Algorithm 1 to 0.5. Here we conduct experiments to analyse the influence of I t . As in <ref type="figure" target="#fig_8">Fig. 5</ref>, I t = 0.5 outperforms other choices, and the results are not very sensitive to the I t : when changing I t from 0.5 to 0.6, the performance only drops a little (mAP from 37.9 to 37.8, CorLoc maintains 57.3). Here we set I t to 0.5 in other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with other methods</head><p>We report our results for each class on VOC 2007 and 2012 in <ref type="table" target="#tab_4">Table 2, Table 3, and Table 4</ref>. Compared with other methods, our method achieves the state-of-the-art performance using single model, and even outperforms the results by combining multiple different models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. Spe- cially, our methods achieves much better performance than the method by Bilen and Vedaldi <ref type="bibr" target="#b3">[4]</ref> using the same CNN model. Notice that <ref type="bibr" target="#b3">[4]</ref> not only uses the weighted pooling as we stated in Section 3.1, but also combines the objectness measure of EdgeBoxes <ref type="bibr" target="#b34">[35]</ref> and the spatial regulariser, which is much complicated than our basic MIDN. We believe that our performance can be improved by choosing better basic detection network, like the complete network in <ref type="bibr" target="#b3">[4]</ref> and using the context information <ref type="bibr" target="#b15">[16]</ref>. As reimplementing their method completely is trivial, here we only choose the simplest architecture in <ref type="bibr" target="#b3">[4]</ref>. Even in this simplified case, our method can achieve very promising results. We also show some visualization comparisons among the WSDDN <ref type="bibr" target="#b3">[4]</ref>, the WSDDN+context <ref type="bibr" target="#b15">[16]</ref>, and our method in <ref type="figure" target="#fig_0">Fig. 1</ref> of the Supplementary Material.</p><p>Our results can also be improved by combing multiple models. As shown in the tables, if we simply sum up the scores produced by the VGG M model and VGG16 model (OICR-Ens. in tables), there is little improvement. Also, as mentioned in Section 4.1, we train a FRCNN detector using top-scoring proposals produced by OICR-Ens. as ground truths (OICR-Ens.+FRCNN in tables). As we can see, the performance can be improved further.</p><p>Though our method significantly outperforms other methods for some class, like "bicyle", "bus", "motorbike", etc, the performance is poor for classes like "cat", "dog", and "person". For analysis, we visualize some success and failure detection results on VOC 2007 trainval by OICREns., as in <ref type="figure">Fig. 6</ref>. We can observe that, our method is robust to the size and aspect of objects, especially for rigid objects. The main failures for these rigid objects are always due to overlarge boxes that not only contain objects, but also include their adjacent similar objects. For non-rigid objects like "cat", "dog", and "person", they are always with great deformation, while there is less deformation of their most representative parts (like head), so our detector is still inclined to find these parts. An ideal solution is yet wanted because there is still room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel algorithm framework for weakly supervised object detection. Different from traditional approaches in this field, our method integrates a basic multiple instance detection network and multi-stage instance classifiers into a single network. Moreover, we propose an online instance classifier refinement algorithm to train the proposed network end-to-end. Experiments show substantial and consistent improvements by our method. Our learning algorithm is potential to be applied in many other weakly supervised visual learning tasks. In the future, we will explore other cues such as instance visual similarity for performing instance classifier refinement better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Detection results without/with classifier refinement (left/right). Detection scores are plotted in the bottom of the sampled proposals A, B, C, and D. In the left, the top ranking proposal A does not correctly localize the object. After instance classifier refinement, in the right, the correct proposal D is detected and more discriminative performance of instance classifier is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Detection results from different stages of classifier refinement. Each row represents one stage. Green/red rectangles indicate detected boxes having high/low overlap with ground truths, and digits in the top right corner of rectangles indicate the IoU. Through multi-stage refinement, the detector can cover the whole object instead of parts gradually.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>The proposal scores are generated by element-wise product x R = σ(x c ) ⊙ σ(x d ). At last, im- age score of c th class φ c can be obtained by the sum over all proposals: φ c = |R| r=1 x R cr . The interpretation of the two streams framework is as follows. The [σ(x c )] ij is the probability of proposal j be- longing to class i. The [σ(x d )] ij is the normalized weight that indicates the contribution of proposal j to image being classified to class i. So φ c is achieved by weighted sum pooling and falls in the range of (0, 1). Given image label Y = [y 1 , y 2 , ..., y C ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>score for {k − 1} th time as in Eq. (2), and label it to class c, i.e.,′ = c. As different proposals always have overlaps, and proposals with high overlap should belong to the same class, we can label proposal j k−1 c and its adjacent proposals to class c for k th refinement, i.e., if proposal j have a high overlap with proposal j1), otherwise we label proposal j as back- ground (y k (C+1)j = 1). Here we label proposal j to class c if the IoU between proposal j and j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results on VOC 2007 for different refinement times and different training strategies, where "OICR" indicates our OICR training strategy, "alternative" indicates the alternative strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results on VOC 2007 for different IoU threshold It.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>0 Figure 6 .</head><label>06</label><figDesc>Figure 6. Some detection results for class bicycle, bus, cat, chair, dog, motorbike, person, and train. Green rectangle indicates success cases (IoU&gt;0.5), and red rectangle indicates failure cases (IoU&lt;0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Fc layer . . . . . . . . Fc layer . . . . . . . . . . . .Two fc layers . . . . . . . . . . . . . . . . .</figDesc><table>Sum over 
proposals 

Element-wise 
product 
Softmax 
over proposals 

Softmax 
over classes 

Proposal 
scores 

. . . . 

Multiple instance detection network 

Conv 
layers 

Conv 
feature map 

SPP 
layer 

Proposal 
feature vector 

Image 
scores 

. . . . 

Fc layer 

Proposal 
scores 
Supervision 

Instance classifier refinement, 1-st time 

. . . . 

Softmax 
over classes 

. . . . 

Fc layer 

Proposal 
scores 
Supervision 

Instance classifier refinement, K-th time 

. . . . 

Softmax 
over classes 

… 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Algorithm 1 Online instance classifier refinement Input: Image X and its proposals; image label vector Y = [y 1 , ..., y C ]; refinement times K. Output: Loss weights w</figDesc><table>k 

r ; proposal label vectors Y 

k 

r = 
[y 

k 

1r , ..., y 

k 

(C+1)r ] 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>CorLoc (in %) for different methods on VOC 2007 trainval set. The upper part shows results using a single model. The lower part shows results of combing multiple models.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Regionbased convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Track and transfer: Watching videos to simulate strong human supervision for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3548" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Harvesting mid-level visual concepts from large-scale internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="851" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relaxed multipleinstance svm with application to object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">EM-DD: An improved multiple-instance learning technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CMU- CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
