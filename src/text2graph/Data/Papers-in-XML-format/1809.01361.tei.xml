<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
							<email>ycliu@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
							<email>yuyeh@eng.ucsd.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><forename type="middle">Frank</forename><surname>Wang</surname></persName>
							<email>ycwang@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning interpretable feature representation has been an active research topic in the fields of computer vision and machine learning. In particular, learning deep representation with the ability to exploit relationship between data across different data domains has attracted the attention from the researchers. Recent developments of deep learning technologies have shown progress in the tasks of cross-domain visual classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and cross-domain image translation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref>. While such tasks typically learn feature mapping from one domain to another or derive a joint representation across domains, the developed models have limited capacities in manipulating specific feature attributes for recovering cross-domain data.</p><p>With the goal of understanding and describing underlying explanatory factors across distinct data domains, cross-domain representation disentanglement aims to derive a joint latent feature space, where selected feature dimensions would represent particular semantic information <ref type="bibr" target="#b0">[1]</ref>. Once such a disentangled representation across domains is learned, one can describe and manipulate the attribute of interest for data in either domain accordingly. While recent work <ref type="bibr" target="#b17">[18]</ref> have demonstrated promising ability in the above task, designs of exisitng models typically require high computational costs when more than two data domains or multiple feature attributes are of interest.</p><p>To perform joint feature disentanglement and translation across multiple data domains, we propose a compact yet effective model of Unified Feature Disentanglement Network (UFDN), which is composed of a pair of unified encoder and generator as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. From this figure, it can be seen that our encoder takes data instances from multiple domains as inputs, and a domain-invariant latent feature space is derived via adversarial training, followed by a generator/decoder which recovers or translates data across domains. Our model is able to disentangle the underlying factors which represent domain-specific information (e.g., domain code, attribute of interest, etc.). This is achieved  by joint learning of our generator. Once the disentangled domain factors are observed, one can simply synthesize and manipulate the images of interest as outputs.</p><p>Later in the experiments, we show that the use of our derived latent representation achieves significant improvements over state-of-the-art methods in the task of unsupervised domain adaptation. In addition to very promising results in multi-domain image-to-image translation, we further confirm that our UFDN is able to perform continuous image translation using the interpolated domain code in the resulting latent space. Implementation of our proposed method and the datasets are now available <ref type="bibr" target="#b0">1</ref> .</p><p>The contributions of this paper are highlighted as follows:</p><p>• We propose a Unified Feature Disentanglement Network (UFDN), which learns deep disentangled feature representation for multi-domain image translation and manipulation.</p><p>• Our UFDN views both data domains and image attributes of interest as latent factors to be disentangled, wich realizes multi-domain image translation in a single unified framework.</p><p>• Continuous multi-domain image translation and manipulation can be performed using our UFDN, while the disentangled feature representation shows promising ability in crossdomain classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Representation Disentanglement Based on the development of generative models like generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> and variational autoencoders (VAEs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>, recent works on representation disentangling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref> aim at learning an interpretable representation using deep neural networks with different degrees of supervision. In a fully supervised setting, Kulkarni et al. <ref type="bibr" target="#b13">[14]</ref> learned invertible graphic codes for 3D image rendering. Odena et al. <ref type="bibr" target="#b19">[20]</ref> achieved representation disentanglement with the proposed auxiliary classifier GAN (AC-GAN). Kingma et al. <ref type="bibr" target="#b11">[12]</ref> also extended VAE into semi-supervised setting for representation disentanglement. Without utilizing any supervised data, Chen et al. <ref type="bibr" target="#b2">[3]</ref> decomposed representation by maximizing the mutual information between the latent factors and the synthesized images. Despite promising performances, the above works focused on learning disentangled representation of images in a single domain, and they cannot be easily extened to describe cross-domain data. While a recent work by Liu et al. <ref type="bibr" target="#b17">[18]</ref> addressed cross-domain disentangled representation with only supervision from single-domain data, empirical studies were performed to determine their network architecture (i.e., number of sharing layers across domains), which would limit its practical uses. Thus, a unified disentangled representation model (like ours) for describing and manipulating multi-domains data would be desirable.</p><p>Image-to-Image Translation Image-to-image translation is another line of research to deal with cross-domain visual data. With the goal of translating images across different domains, Isola et al. <ref type="bibr" target="#b9">[10]</ref> applied conditional GAN which is trained on pairwise data across source and target domains. Taigman et al. <ref type="bibr" target="#b23">[24]</ref> removed the restriction of pairwise training images and presented a Domain Transfer Network (DTN) which observes cross-domain feature consistency. Likewise, Zhu et al. <ref type="bibr" target="#b29">[30]</ref> employed a cycle consistency loss in the pixel space to achieve unpaired image translation. Similar ideas were applied by Kim et al. <ref type="bibr" target="#b10">[11]</ref> and Yi et al. <ref type="bibr" target="#b27">[28]</ref>. Liu et al. <ref type="bibr" target="#b16">[17]</ref> presented coupled GANs (CoGAN) with sharing weight on high-level layers to learn the joint distribution across domains. To achieve image-to-image translation, they further integrated CoGAN with two parallel encoders <ref type="bibr" target="#b15">[16]</ref>. Nevertheless, the above dual-domains models cannot be easily extended to multi-domain image translation without increasing the computation costs. Although Choi et al. <ref type="bibr" target="#b3">[4]</ref> recently proposed an unified model to achieve multi-domain image-to-image translation, their model does not exhibit ability in learning and disentangling desirable latent representations (as ours does).</p><p>Unsupervised Domain Adaptation (UDA) Unsupervised domain adaptation (UDA) aims at classifying samples in the target domain, using labeled and unlabeled training data in source and target domains, respectively. Inspired by the idea of adversarial learning <ref type="bibr" target="#b7">[8]</ref>, Ganin et al. <ref type="bibr" target="#b5">[6]</ref> proposed a method applying adversarial training between domain discriminator and normal convolution neural network based classifier, making the model invariant to the domain shift. Tzeng et al. <ref type="bibr" target="#b25">[26]</ref> also attempted to build domain-invariant classifier via introducing domain confusion loss. By advancing adversarial learning strategies, Bousmalis et al. <ref type="bibr" target="#b1">[2]</ref> chose to learn orthogonal representations, derived by shared and domain-specific encoders, respectively. Tzeng et al. <ref type="bibr" target="#b26">[27]</ref> addressed UDA by adapting CNN feature extractors/classifier across source and target domains via adversarial training. However, the above methods generally address domain adaptation by eliminating domain biases. There is no guarantee that the derived representation would preserve semantic information (e.g., domain or attribute of interest). Moreover, since the goal of UDA is visual classification, image translation (dual or multi-domains) cannot be easily achieved. As we highlighted in Sect. 1, our UFDN learns the multi-domain disentangled representation, which enables multi-domain image-to-image translation and manipulation and unsupervised domain adaption. Thus, our proposed model is very unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unified Feature Disentanglement Network</head><p>We present a unique and unified network architecture, Unified Feature Disentanglement Network (UFDN), which disentangles the domain information from latent space and derives domain-invariant representation from data across multiple domains (not just from a pair of domains). This not only enables the task of multi-domain image translation/manipulation, the derived feature representation can also be applied for unsupervised domain adaptation.</p><p>Given image sets {X c } N c=1 across N domains, our UFDN learns a domain-invariant representation z for the input image x c ∈ X c (in domain c). This is realized by disentangling the domain information in the latent space as domain vector v ∈ R N via self-supervised feature disentanglement (Sect. 3.1), followed by preserving the data recovery ability via adversarial learning in the pixel space (Sect. 3.2). We now detail our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-supervised feature disentanglement</head><p>To learn disentangled representation across data domains, one can simply apply a VAE architecture (e.g., components E and G in <ref type="figure" target="#fig_2">Figure 2</ref>). To be more specific, we have encoder E take the image x c as input and derive its representation z, which is combined with its domain vector v c to reconstruct the imagex c via Generator G. Thus, the objective function of VAE is defined as:</p><formula xml:id="formula_0">L vae = x c − x c 2 F + KL(q(z|x c )||p(z)),<label>(1)</label></formula><p>where the first term aims at recovering the synthesized output in the same domain c, and the second term calculates Kullback-Leibler divergence which penalizes deviation of latent feature from the prior distribution p(z c ) (as z ∼ N (0, I)). However, the above technical is not guaranteed to disentangle domain information from the latent space, since generator recovers the images simply based on the representation z without considering the domain information.</p><p>To address the above problem, we extend the aforementioned model to eliminate the domain-specific information from the representation z. This is achieved by exploiting adversarial domain classification in the resulting latent feature space. More precisely, the introduced domain discriminator D v in <ref type="figure" target="#fig_2">Figure 2</ref> only takes the latent representation z as input and produce domain code prediction l v . The objective function of this domain discriminator L adv Dv is derived as follows:</p><formula xml:id="formula_1">L adv Dv = E[log P (l v = v c |E(x c ))],<label>(2)</label></formula><p>where P is the probability distribution over domains l v , which is produced by the domain discriminator D v . The domain vector v c can be implemented by an one-hot vector, concatenation of multiple one-hot vectors, or simply a real-value vector describing the domain of interest. In contrast, the encoder E aims to confuse D v from correctly predicting the domain code. As a result, the objective of the encoder L adv E is to maximize the entropy of the domain discriminator:</p><formula xml:id="formula_2">L adv E = −L adv Dv = −E[log P (l v = v c |E(x c ))].<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial learning in pixel space</head><p>Once the above domain-invariant representation z is learned, we further utilize the reconstruction module in our UFDN to preserve the recovery ability of the disentangled representation. That is, the reconstructed imagex c can be supervised by its original image x c .</p><p>However, when manipulating the domain vector as vc in the above process, there is no guarantee that the synthesized imagexc could be practically satisfactory based on vc. This is due to the fact that there is no pairwise training data (i.e., x c and xc)) to supervise the synthesized imagexc in the training stage. Moreover, as noted in <ref type="bibr" target="#b28">[29]</ref>, the VAE architecture tends to generate blurry samples, which would not be desirable for practical uses.</p><p>To overcome the above limitation, we additionally introduce an image discriminator D x in the pixel space for our UFDN. This discriminator not only improves the image quality of the synthesized imagexc, it also enhances the ability of disentangling domain information from the latent space. We note that the objectives of this image discriminator D x are twofold: to distinguish whether the input image is real or fake, and to predict the observed images (i.e.,xc and x c ) into proper domain code/categories. </p><formula xml:id="formula_3">L adv Dx = E[log(D x (xc))] + E[log(1 − D x (x c )], L adv G = −E[log(D x (xc))].<label>(4)</label></formula><p>On the other hand, the objective function for domain classification is derived as:</p><formula xml:id="formula_4">L cls = E[log P (l x = vc|xc)] + E[log P (l x = v c |x c )],<label>(5)</label></formula><p>where l x denotes the domain prediction of image discriminator D x . This term implicitly maximizes the mutual information between the domain vector and the synthesized image <ref type="bibr" target="#b2">[3]</ref>.</p><p>To train our UFDN, we alternately update encoder E, generator G, domain discriminator D v , and image discriminator D x with the following gradients:</p><formula xml:id="formula_5">θ E + ← − −∆ θ E (L vae + L adv E ), θ G + ← − −∆ θ G (L vae + L adv G + L cls ), θ Dv + ← − −∆ θ Dv (L adv Dv ), θ Dx + ← − −∆ θ Dx (L adv Dx + L cls ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with state-of-the-art cross-domain visual tasks</head><p>To demonstrate the uniqueness of our proposed UFDN, we compare our model with several state-ofthe-art image-to-image translation works in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Without the need of pairwise training data, CycleGAN <ref type="bibr" target="#b9">[10]</ref> learns bidirectional mapping between two pixel spaces, while they needed to learn the multiple individual networks for the task of multi-domain image translation. StarGAN <ref type="bibr" target="#b3">[4]</ref> alleviates the above problem by learning a unified structure. However, it does not exhibit the ability to disentangle particular semantics across different domains. Another line of works on image translation is to learn a joint representation across image domains <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. While DTN <ref type="bibr" target="#b23">[24]</ref> learns a joint representation to translate the image from one domain to another, their model only allows the task of unidirectional image translation. UNIT <ref type="bibr" target="#b15">[16]</ref> addresses the above problem by jointly synthesizing the images in both domains. However, it is not able to learn disentangled representation as ours does. A recent work of E-CDRD <ref type="bibr" target="#b17">[18]</ref> derives cross-domain representation disentanglement. Their model requires high computational costs when more than two data domains are of interest, while ours is a unified architecture for multiple data domains (i.e., domain code as a vector).</p><p>It is worth repeating that our UFDN does not require pairwise training data for learning multi-domain disentangled feature representation. As verified later in the experiments, our model not only enables multi-domain image-to-image translation and manipulation, the derived domain-invariant feature further allows unsupervised domain adaptation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Digits MNIST, USPS, Street View House Number (SVHN) datasets are considered to be three different domains and used as benchmark datasets in unsupervised domain adaption (UDA) tasks. MNIST contains 60000/10000 training/testing images, and USPS contains 7291/2007 training/testing images. While the above two datasets are handwritten digits, SVHN consists of digit images with the complex background and various illuminations. We used the 60000 images from SVHN extra training set to train our model and few samples from the testing set to perform image translation. All images are converted to RGB images with the size 32x32 in our experiments.</p><p>Human faces We use the Large-scale CelebFaces Attributes (CelebA) Dataset <ref type="bibr" target="#b18">[19]</ref> in our experiment on human face images. CelebA includes more than 200k celebrity photos annotated with 40 facial attributes. Considering photo, sketch and paint as three different domains, we follow the setting of previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> to transfer half of the photos to sketch. We further transferred half of the remaining photos to paint through off-the-shelf style transfer software 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-domain image translation with disentangled representation</head><p>Most of the previous works focus on image translation between two domains as mentioned in Section 2. In our experiment, we use human face images from different domains to perform imageto-image translation. Although Choi et al. <ref type="bibr" target="#b3">[4]</ref> claim to have achieved multi-domain image-to-image translation on human face dataset, they define attribute, e.g., gender or hair color, as domain. In our work, we denote domain by the dataset properties rather than attributes. Images from different domains may share same attributes, but an image cannot belong to two domain at the same time.</p><p>With unified framework and no restriction on the dimension of domain vector, UFDN can perform image-to-image translation over multiple domains. As shown in <ref type="figure" target="#fig_3">Figure 3(a)</ref>, we demonstrate the results of image-to-image translations between domains sketch/photo/paint. Previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15]</ref> had discovered that even the disentangled feature to the generator/decoder is binary during training, it can be considered as continuous variable during testing. Our model also inherits this property of continuous cross-domain image translation by manipulating the value of domain vector.</p><p>Our model is also capable of generating unseen images by randomly sampled representation in the latent space. Since the representation is sampled from domain-invariant latent space, UFDN can further present them with any domain vector supplied. <ref type="figure" target="#fig_3">Figure 3(b)</ref> shows the result of translation for  six identities randomly sampled. It is worth noting that this cannot be done by those translation models without representation learning or using skipped connection between encoder and decoder/generator. <ref type="table" target="#tab_1">Table 2</ref> provides quantitative evaluation on the recovered images using our proposed UFDN with E-CDRD <ref type="bibr" target="#b17">[18]</ref> and StarGAN 3 <ref type="bibr" target="#b3">[4]</ref>. In our experiments, we convert photo images into sketches/paintings for the purpose of collecting training cross-domain image data (but did not utilize such pairwise information during training). This is the reason why we are able to observe the ground truth photo images and calculate SSIM/MSE/PSNR values for the translated outputs. While both learning disentangled representation, our UFDN outperformed E-CDRD in terms translation quality. It is also worth noting that our UFDN matched the performance of StarGAN, which was designed for image translation without learning any representation.</p><p>To further demonstrate the ability to disentangle representation, our model performs feature disentanglement of common attributes across domains simultaneously. This can be easily done by expanding domain vector with the annotated attribute from the dataset. In our experiment, Gender and Smiling are picked as the attribute of interest. The results are shown in the <ref type="figure" target="#fig_4">Figure 4</ref>. We used a fixed domain-invariant representation to show that features are highly disentangled by our UFDN. All information of interest (domain/gender/smiling) can be independently manipulated through our model. As a reminder, each and every result provided above was presented by the same single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised domain adaption with domain-invariant representation</head><p>Unsupervised domain adaption (UDA) aims to classify samples in target domain while labels are only available in the source domain. Previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref> dedicated to building a domain-invariant classifier for UDA task. Recent works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> addressed the problem by using classifier with highlevel layers tied across domain and synthesized training data provided by image-to-image translation. We followed the previous works to challenge UDA task on digit classification over three datasets MNIST/USPS/SVHN. The notation "→" denotes the relation between source and target domain. For example, SVHN→MNIST indicates that SVHN is the source domain with categorical labels.</p><p>To verify the robustness of our domain-invariant representation, we adapt our model to UDA task by adding a single fully-connected layer as the digit classifier. This classifier simply takes as input the  domain-invariant representation and predicts the digit label. The auxiliary classifier is jointly trained with our UFDN. <ref type="table" target="#tab_2">Table 3</ref> lists and compares the performance of our model to others. For the setting MNIST→USPS, our model surpasses UNIT <ref type="bibr" target="#b15">[16]</ref> which was the state-of-the-art. For SVHN→MNIST, our model also surpasses the state-of-the-art with significant improvement. While SVHN→MNIST is considered to be much more difficult than the other two settings, our model is able to decrease the classification error rate from 7.6% to 5%. It is also worth mentioning that our model used 60K images from SVHN, which is considerably less than 531K used by UNIT.</p><p>We visualize domain-invariant representations with t-SNE and show the results in <ref type="figure" target="#fig_5">Figure 5</ref>. From <ref type="figure" target="#fig_5">Figure 5</ref>(a) and 5(b) we can see that the representation is properly clustered with respect to class of digits instead of domain. We also provide the result of synthesizing images with the domain-invariant representation. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, by manipulating domain vector, the representation of SVHN image can be transformed to MNIST. It further strengthens our point of view that disentangled representation is worth learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>As mentioned in Section 3, we applied self-supervised feature disentanglement and adversarial learning in pixel space to build our framework. To verify the effect of these methods, we did ablation study on the proposed framework and show the results in <ref type="figure" target="#fig_7">Figure 7</ref>. We claimed that without selfsupervised feature disentanglement, i.e., without D v , the generator will be able to reconstruct images with the entangled representation and ignore the domain vector. This can be verified by <ref type="figure" target="#fig_7">Figure 7(a)</ref> where the self-supervised feature disentanglement is disabled, meaning that the representation is not trained to be domain-invariant. In such case, the decoder simply decodes the input representation back  to its source domain ignoring the domain vector. Next, we disabled pixel space adversarial learning in our framework to verify that the representation is indeed forced to be domain-invariant. As shown in <ref type="figure" target="#fig_7">Figure 7</ref>(b), the generator is now forced to synthesize image conditioning on the manipulated domain vector. However, without pixel space adversarial learning, the difference between domain photo and paint is not apparent comparing to the complete version of our UFDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel network architecture of unified feature disentanglement network (UFDN), which learns disentangled feature representation for data across multiple domains by a unique encodergenerator architecture with adversarial learning. With superior properties over recent image translation works, our model not only produced promising qualitative results but also allows unsupervised domain adaptation, which confirmed the effectiveness of the derived deep features in the above tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of multi-domain image translation and manipulation. With data from different domains (e.g., D 1 : sketch, D 2 : photo, D 3 : painting), the goal is to learn domain-invariant feature representation. With domain information disentangled from such representation, one can synthesize and manipulate image outputs in different domains of interests (including the intermediate ones across domains).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our Unified Feature Disentanglement Network (UFDN), consisting of an encoder E, a generator G, a discriminator in pixel space D x and a discriminator in feature space D v . Note that x c andx c denote input and reconstruct images with domain vector v c , respectively.xc indicates the synthesized image with domain vector vc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Examples results of image-to-image translation across data domains of sketch/photo/paint and (b) example image translation results with randomly generated identity (i.e., random sample z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example results of multi-domain image translation. Note that all images are produced by the same z with varying domain information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: t-SNE visualization of SVHN→MNIST. Note that different colors indicate data of (a) different domains and (b) digit classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example image translation results of SVHN→MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison between example image translation results: (a) our UFDN without self-supervised feature disentanglement, (b) our UFDN without adversarial training in the pixel space, and (c) the full version of UFDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with recent works on image-to-image translation.</figDesc><table>Unpaired Bidirectional 
Unified 
Multiple 
Joint 
Feature 
data 
translation 
structure domains representation disentanglement 

Pix2Pix [10] 

-
-
-
-
-
-

CycleGAN [30] 

-
-
-
-

StarGAN [4] 

-
-

DTN [24] 

-
-
-
-

UNIT [16] 

-
-
-

E-CDRD [18] 

-

UFDN (Ours) 

With the above discussions, we define the objective functions L 

adv 

Dx and L 

adv 

G for adversarial learning 
between image discriminator D x and generator G as: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation in terms of image-to-image translation on human face dataset.</figDesc><table>Sketch→Photo 
Paint→Photo 
SSIM 
MSE 
PSNR SSIM 
MSE 
PSNR 
E-CDRD [18] 0.6229 0.0207 16.86 0.5892 0.0174 17.61 
StarGAN [4] 0.8026 0.0142 19.04 0.8496 0.0060 22.53 
UFDN (Ours) 0.8222 0.0106 20.24 0.8798 0.0033 25.06 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons of unsupervised domain adaptation (i.e., classification accuracy for target-domain data). For example, MNIST→USPS denotes MNIST and USPS as source and target-domain data, respectively.</figDesc><table>MNIST→USPS USPS→MNIST SVHN→MNIST 
SA [5] 
67.78 
48.80 
59.32 
DANN [6] 
-
-
73.85 
DTN [24] 
-
-
84.88 
DRCN [7] 
91.8 
73.7 
82.00 
CoGAN [17] 
95.65 
93.15 
-
ADDA [27] 
89.40 
90.10 
76.00 
UNIT [16] 
95.97 
93.58 
90.53 
ADGAN [23] 
92.80 
90.80 
92.40 
CDRD [18] 
95.05 
94.35 
-
UFDN (Ours) 
97.13 
93.77 
95.01 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Alexander-H-Liu/UFDN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://fotosketcher.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We used the source code provided by the author at https://github.com/yunjey/StarGAN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detach and adapt: Learning cross-domain disentangled deep representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards deeper understanding of variational autoencoding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
