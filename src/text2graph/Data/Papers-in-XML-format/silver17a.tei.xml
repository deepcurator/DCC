<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Predictron: End-To-End Learning and Planning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Dulac-Arnold</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reichert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Barreto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Degris</surname></persName>
						</author>
						<title level="a" type="main">The Predictron: End-To-End Learning and Planning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-toend so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The central idea of model-based reinforcement learning is to decompose the RL problem into two subproblems: learning a model of the environment, and then planning with this model. The model is typically represented by a Markov reward process (MRP) or decision process (MDP). The planning component uses this model to evaluate and select among possible strategies. This is typically achieved by rolling forward the model to construct a value function that estimates cumulative reward. In prior work, the model is trained essentially independently of its use within the planner. As a result, the model is not well-matched with the overall objective of the agent. Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, <ref type="bibr">Australia, PMLR 70, 2017</ref><ref type="bibr">. Copyright 2017</ref> by the author(s). <ref type="bibr" target="#b10">(Oh et al., 2015;</ref><ref type="bibr" target="#b1">Chiappa et al., 2016)</ref>; but are yet to surpass state-of-the-art model-free methods in challenging RL domains with raw inputs (e.g., <ref type="bibr" target="#b8">Mnih et al., 2015;</ref><ref type="bibr" target="#b2">2016;</ref><ref type="bibr" target="#b5">Lillicrap et al., 2016)</ref>.</p><p>In this paper we introduce a new architecture, which we call the predictron, that integrates learning and planning into one end-to-end training procedure. At every step, a model is applied to an internal state, to produce a next state, reward, discount, and value estimate. This model is completely abstract and its only goal is to facilitate accurate value prediction. For example, to plan effectively in a game, an agent must be able to predict the score. If our model makes accurate predictions, then an optimal plan with respect to our model will also be optimal for the underlying game -even if the model uses a different state space (e.g., abstract representations of enemy positions, ignoring their shapes and colours), action space (e.g., highlevel actions to move away from an enemy), rewards (e.g., a single abstract step could have a higher value than any real reward), or even time-step (e.g., a single abstract step could "jump" the agent to the end of a corridor). All we require is that trajectories through the abstract model produce scores that are consistent with trajectories through the real environment. This is achieved by training the predictron end-to-end, so as to make its value estimates as accurate as possible.</p><p>An ideal model could generalise to many different prediction tasks, rather than overfitting to a single task; and could learn from a rich variety of feedback signals, not just a single extrinsic reward. We therefore train the predictron to predict a host of different value functions for a variety of pseudo-reward functions and discount factors. These pseudo-rewards can encode any event or aspect of the environment that the agent may care about, e.g., staying alive or reaching the next room.</p><p>We focus upon the prediction task: estimating value functions in MRP environments with uncontrolled dynamics. In this case, the predictron can be implemented as a deep neural network with an MRP as a recurrent core. The predictron unrolls this core multiple steps and accumulates rewards into an overall estimate of value.</p><p>We applied the predictron to procedurally generated ran-dom mazes, and a simulated pool domain, directly from pixel inputs. In both cases, the predictron significantly outperformed model-free algorithms with conventional deep network architectures; and was much more robust to architectural choices such as depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We consider environments defined by an MRP with states s ∈ S. The MRP is defined by a function, s , r, γ = p(s, α), where s is the next state, r is the reward, and γ is the discount factor, which can for instance represent the non-termination probability for this transition. The process may be stochastic, given IID noise α.</p><p>The return of an MRP is the cumulative discounted reward over a single trajectory, g t = r t+1 + γ t+1 r t+2 + γ t+1 γ t+2 r t+3 + ... , where γ t can vary per time-step. We consider a generalisation of the MRP setting that includes vector-valued rewards r, diagonal-matrix discounts γ γ γ, and vector-valued returns g; definitions are otherwise identical to the above. We use this bold font notation to closely match the more familiar scalar MRP case; the majority of the paper can be comfortably understood by reading all rewards as scalars, and all discount factors as scalar and constant, i.e., γ t = γ.</p><p>The value function of an MRP p is the expected return from state s, v p (s) = E p [g t | s t = s]. In the vector case, these are known as general value functions <ref type="bibr" target="#b18">(Sutton et al., 2011)</ref>. We will say that a (general) value function v(·) is consistent with environment p if and only if v = v p which satisfies the following Bellman equation <ref type="bibr" target="#b0">(Bellman, 1957)</ref>,</p><formula xml:id="formula_0">v p (s) = E p [r + γ γ γv p (s ) | s] .</formula><p>(1)</p><p>In model-based reinforcement learning <ref type="bibr" target="#b16">(Sutton &amp; Barto, 1998)</ref>, an approximation m ≈ p to the environment is learned. In the uncontrolled setting this model is normally an MRP s , r, γ γ γ = m(s, β) that maps from state s to subsequent state s and additionally outputs rewards r and discounts γ γ γ; the model may be stochastic given an IID source of noise β. A (general) value function v m (·) is consistent with model m (or valid, <ref type="bibr" target="#b17">(Sutton, 1995)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Predictron architecture</head><p>The predictron is composed of four main components. First, a state representation s = f (s) that encodes raw input s (this could be a history of observations, in partially observed settings, for example when f is a recurrent network) into an internal (abstract, hidden) state s. Second, a model s , r, γ γ γ = m(s, β) that maps from internal state s to subsequent internal state s , internal rewards r, and internal discounts γ γ γ. Third, a value function v that outputs internal values v = v(s) representing the remaining internal return from internal state s onwards. The predictron is applied by unrolling its model m multiple "planning" steps to produce internal rewards, discounts and values. We use superscripts • k to indicate internal steps of the model (which have no necessary connection to time steps • t of the environment). Finally, these internal rewards, discounts and values are combined together by an accumulator into an overall estimate of value g. The whole predictron, from input state s to output, may be viewed as a value function approximator for external targets (i.e., the returns in the real environment). We consider both k-step and λ-weighted accumulators.</p><p>The k-step predictron rolls its internal model forward k steps ( <ref type="figure">Figure 1a</ref>). The 0-step predictron return (henceforth abbreviated as preturn) is simply the first value g 0 = v 0 , the 1-step preturn is g 1 = r 1 +γ γ γ 1 v 1 . More generally, the kstep predictron return g k is the internal return obtained by accumulating k model steps, plus a discounted final value v k from the kth step:</p><formula xml:id="formula_1">g k = r 1 + γ γ γ 1 (r 2 + γ γ γ 2 (. . . + γ γ γ k−1 (r k + γ γ γ k v k ) . . .))</formula><p>The λ-predictron combines together many k-step preturns. Specifically, it computes a diagonal weight matrix λ λ λ k from each internal state s k . The accumulator uses weights λ λ λ 0 , ..., λ λ λ K to aggregate over k-step preturns g 0 , ..., g K and output a combined value that we call the λ-preturn g λ ,</p><formula xml:id="formula_2">g λ = K k=0 w k g k (2) w k =      (1 − λ λ λ k ) k−1 j=0 λ λ λ j if k &lt; K K−1 j=0 λ λ λ j otherwise.<label>(3)</label></formula><p>where 1 is the identity matrix. This λ-preturn is analogous to the λ-return in the forward-view TD(λ) algorithm <ref type="bibr" target="#b14">(Sutton, 1988;</ref><ref type="bibr" target="#b16">Sutton &amp; Barto, 1998)</ref>. It may also be computed by a backward accumulation through intermediate steps</p><formula xml:id="formula_3">g k,λ , g k,λ = (1 − λ λ λ k )v k + λ λ λ k r k+1 + γ γ γ k+1 g k+1,λ ,<label>(4)</label></formula><p>where g K,λ = v K , and then using g λ = g 0,λ . Computation in the λ-predictron operates in a sweep, iterating first through the model from k = 0 . . . K and then back through the accumulator from k = K . . . 0 in a single "forward" pass of the network (see <ref type="figure">Figure 1b)</ref>. <ref type="figure">Figure 1</ref>. a) The k-step predictron architecture. The first three columns illustrate 0, 1 and 2-step pathways through the predictron. The 0-step preturn reduces to standard model-free value function approximation; other preturns "imagine" additional steps with an internal model. Each pathway outputs a k-step preturn g k that accumulates discounted rewards along with a final value estimate. In practice all k-step preturns are computed in a single forward pass. b) The λ-predictron architecture. The λ-parameters gate between the different preturns. The output is a λ-preturn g λ that is a mixture over the k-step preturns. For example, if λ λ λ 0 = 1, λ λ λ 1 = 1, λ λ λ 2 = 0 then we recover the 2-step preturn, g λ = g 2 . Discount factors γ γ γ k and λ-parameters λ λ λ k are dependent on state s k ; this dependence is not shown in the <ref type="figure">figure.</ref> acts as a gate on the computation of the λ-preturn: a value of λ λ λ k = 0 will truncate the λ-preturn at layer k, while a value of λ λ λ k = 1 will utilise deeper layers based on additional steps of the model m; the final weight is always λ λ λ K = 0. The individual λ λ λ k weights may depend on the corresponding abstract state s k and can differ per prediction. This enables the predictron to compute to an adaptive depth <ref type="bibr" target="#b2">(Graves, 2016)</ref> depending on the internal state and learning dynamics of the network.</p><formula xml:id="formula_4">Each λ λ λ k weight a) k-step predictron b) λ-predictron ... ... ... γ 2 λ 2 r 2 λ 2 &amp; &amp; ... s 2 / / O O 9 9 v 2 / / + γ 1 s 2 O O / / 9 9 v 2 1−λ 2 / / + γ 1 λ 1 r 1 % % r 1 λ 1 &amp; &amp; ... s 1 / / O O 9 9 v 1 / / + γ 0 s 1 O O / / 9 9 + γ 0 s 1 O O / / 9 9 v 1 1−λ 1 / / + γ 0 λ 0 r 0 % % r 0 % % r 0 λ 0 &amp; &amp; s 0 / / O O 9 9 v 0 / / + s 0 O O / / 9 9 + s 0 O O / / 9 9 + s 0 O O / / 9 9 v 0 1−λ 0 / / + s O O g 0 s O O g 1 s O O g 2 s O O g λ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Predictron learning updates</head><p>We first consider updates that optimise the joint parameters θ of the state representation, model, and value function. We begin with the k-step predictron. We update the k-step preturn g k towards a target outcome g, e.g. the MonteCarlo return from the real environment, by minimising a mean-squared error loss,</p><formula xml:id="formula_5">L k = 1 2 E p [g | s] − E m g k | s 2 . ∂l k ∂θ = g − g k ∂g k ∂θ .<label>(5)</label></formula><p>where l k = 1 2 g − g k 2 is the sample loss. We can use the gradient of the sample loss to update parameters, e.g., by stochastic gradient descent. For stochastic models, independent samples of g k and ∂g k ∂θ are required for unbiased samples of the gradient of L k .</p><p>The λ-predictron combines many k-step preturns. To update the joint parameters θ, we can uniformly average the losses on the individual preturns g k ,</p><formula xml:id="formula_6">L 0:K = 1 2K K k=0 E p [g | s] − E m g k | s 2 , ∂l 0:K ∂θ = 1 K K k=0 g − g k ∂g k ∂θ .<label>(6)</label></formula><p>Alternatively, we could weight each loss by the usage w k of the corresponding preturn, such that the gradient is</p><formula xml:id="formula_7">K k=0 w k g − g k ∂g k</formula><p>∂θ . In the λ-predictron, the λ λ λ k weights (that determine the relative weighting w k of the k-step preturns) depend on additional parameters η, which are updated so as to minimise a mean-squared error loss L λ ,</p><formula xml:id="formula_8">L λ = 1 2 E p [g | s] − E m g λ | s 2 . ∂l λ ∂η = g − g λ ∂g λ ∂η .<label>(7)</label></formula><p>In summary, the joint parameters θ of the state representation f , the model m, and the value function v are updated to make each of the k-step preturns g k more similar to the target g, and the parameters η of the λ-accumulator are updated to learn the weights w k so that the aggregate λ-preturn g λ becomes more similar to the target g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Consistency updates</head><p>In model-based reinforcement learning architectures such as Dyna <ref type="bibr" target="#b15">(Sutton, 1990)</ref>, value functions may be updated using both real and imagined trajectories. The refinement of value estimates based on these imagined trajectories is often referred to as planning. A similar opportunity arises in the context of the predictron. Each rollout of the predictron generates a trajectory in abstract space, alongside with rewards, discounts and values. Furthermore, the predictron aggregates these components in multiple value estimates</p><formula xml:id="formula_9">(g 0 , ..., g k , g λ ).</formula><p>We may therefore update each individual value estimate towards the best aggregated estimate. This corresponds to adjusting each preturn g k towards the λ-preturn g λ , by minimizing:</p><formula xml:id="formula_10">L = 1 2 K k=0 E m g λ | s − E m g k | s 2 . ∂l ∂θ = K k=0 g λ − g k ∂g k ∂θ .<label>(8)</label></formula><p>Here g λ is considered fixed; the parameters θ are only updated to make g k more similar to g λ , not vice versa.</p><p>These consistency updates do not require any labels g or samples from the environment. As a result, it can be applied to (potentially hypothetical) states that have no associated 'real' (e.g. Monte-Carlo) outcome: we update the value estimates to be self-consistent with each other. This is especially relevant in the semi-supervised setting, where these consistency updates allow us to exploit the unlabelled inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conducted experiments in two domains. The first domain consists of randomly generated mazes. Each location either is empty or contains a wall. In these mazes, we considered two tasks. In the first task, the input was a 13 × 13 maze and a random initial position and the goal is to predict a trajectory generated by a simple fixed deterministic policy. The target g was a vector with an element for each cell of the maze which is either one, if that cell was reached by the policy, or zero. In the second random-maze task the goal was to predict for each of the cells on the diagonal of a 20 × 20 maze (top-left to bottom-right) whether it is connected to the bottom-right corner. Two locations in a maze are considered connected if they are both empty and we can reach one from the other by moving horizontally or vertically through adjacent empty cells. In both cases some predictions would seem to be easier if we could learn a simple algorithm, such as some form of search or flood fill; our hypothesis is that an internal model can learn to Our second domain is a simulation of the game of pool, using four balls and four pockets. The simulator is implemented in the physics engine Mujoco <ref type="bibr" target="#b20">(Todorov et al., 2012)</ref>. We generate sequences of RGB frames starting from a random arrangement of balls on the table. The goal is to simultaneously learn to predict future events for each of the four balls, given 5 RGB frames as input. These events include: collision with any other ball, collision with any boundary of the table, entering a quadrant (×4, for each quadrant), being located in a quadrant (×4, for each quadrant), and entering a pocket (×4, for each pocket). Each of these 14 × 4 events provides a binary pseudo-reward that we combine with 5 different discount factors {0, 0.5, 0.9, 0.98, 1} and predict their cumulative discounted sum over various time spans. This yields a total of 280 general value functions. An example trajectory is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In both domains, inputs are presented as minibatches of i.i.d. samples with their regression targets. Additional domain details are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Learning sequential plans</head><p>In the first experiment we trained a predictron to predict trajectories generated by a simple deterministic policy in 13×13 random mazes with random starting positions. <ref type="figure">Figure 3</ref> shows the weighted preturns w k g k and the resulting prediction g λ λ λ = k w k g k for six example inputs and targets. The predictions are almost perfect-the training error was very close to zero. The full prediction is composed from weighted preturns which decompose the trajectory piece by piece, starting at the start position in the first step k = 1, and where often multiple policy steps are added per planning step. The predictron was not informed about the sequential build up of the targets-it never sees a policy <ref type="figure">Figure 3</ref>. Indication of planning. Sampled mazes (grey) and start positions (black) are shown superimposed on each other at the bottom. The corresponding target vector g, arranged as a matrix for visual clarity, is shown at the top. The ensembled prediction k w k g k = g λ λ λ is shown just below the target-the prediction is near perfect. The weighted preturns w k g k that make up the prediction are shown below g λ λ λ . We can see that full predicted trajectory is built up in steps, starting at the start position and then planning through the trajectory in sequence.</p><p>walking through the maze, only the resulting trajectoriesand yet sequential plans emerged spontaneously. Notice also that the easier trajectory on the right was predicted in only two steps, while more thinking steps are used for more complex trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Exploring the predictron architecture</head><p>In the next set of experiments, we tackle the problem of predicting connectivity of multiple pairs of locations in a random maze, and the problem of learning many different value functions from our simulator of the game of pool. We use these more challenging domains to examine three binary dimensions that differentiate the predictron from standard deep networks. We compare eight predictron variants corresponding to the corners of the cube on the left in <ref type="figure" target="#fig_1">Figure 4</ref>.</p><p>The first dimension, labelled r, γ, corresponds to whether or not we use the structure of an MRP model. In the MRP case internal rewards and discounts are both learned. In the non-(r, γ) case, which corresponds to a vanilla hidden-tohidden neural network module, internal rewards and discounts are ignored by fixing their values to r k = 0 and γ γ γ k = 1.</p><p>The second dimension is whether a K-step accumulator or λ-accumulator is used to aggregate preturns. When a λ-accumulator is used, a λ-preturn is computed as described in Section 3. Otherwise, intermediate preturns are ignored by fixing λ λ λ k = 1 for k &lt; K. In this case, the overall output of the predictron is the maximum-depth preturn g K .</p><p>The third dimension, labelled usage weighting, defines the loss that is used to update the parameters θ. We consider two options: the preturn losses can either be weighted uniformly (see Equation 6), or the update for each preturn g k can be weighted according to the weight w k that determines how much it is used in the λ-predictron's overall output. We call the latter loss 'usage weighted'. Note that for architectures without a λ-accumulator, w k = 0 for k &lt; K, and w K = 1, thus usage weighting then implies backpropagating only the loss on the final preturn g K .</p><p>All variants utilise a convolutional core with 2 intermediate hidden layers; parameters were updated by supervised learning (see appendix for more details). Root mean squared prediction errors for each architecture, aggregated over all predictions, are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. The top row corresponds to the random mazes and the bottom row to the pool domain. The main conclusion is that learning an MRP model improved performance greatly. The inclusion of λ λ λ weights helped as well, especially on pool. Usage weighting further improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparing to other architecture</head><p>Our third set of experiments compares the predictron to feedforward and recurrent deep learning architectures, with and without skip connections. We compare the corners of a new cube, as depicted on the left in <ref type="figure" target="#fig_2">Figure 5</ref>, based on three different binary dimensions.</p><p>The first dimension of this second cube is whether we use a predictron, or a (non-λ, non-(r, γ)) deep network that does not have an internal model and does not output or learn from intermediate predictions. We use the most effective predictron from the previous section, i.e., the (r, γ, λ)-predictron with usage weighting.</p><p>The second dimension is whether all cores share weights (as in a recurrent network), or each core uses separate weights (as in a feedforward network). The non-λ, non-(r, γ) variants of the predictron then correspond to standard (convolutional) feedforward and (unrolled) recurrent neural networks respectively. The third dimension is whether we include skip connections. This is equivalent to defining the model step to output a change to the current state, ∆s, and then defining</p><formula xml:id="formula_11">s k+1 = h(s k + ∆s k ),</formula><p>where h is the non-linear functionin our case a ReLU, h(x) = max(0, x). The deep network with skip connections is a variant of ResNet <ref type="bibr" target="#b3">(He et al., 2015)</ref>.</p><p>Root mean squared prediction errors for each architecture are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. All (r, γ, λ)-predictrons (red lines) outperformed the corresponding feedforward or recurrent baselines (black lines) both in the random mazes and in pool. We also investigated the effect of changing the depth of the networks (see appendix); the predictron outperformed the corresponding feedforward or recurrent baselines for all depths, with and without skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Semi-supervised learning by consistency</head><p>We now consider how to use the predictron for semisupervised learning, training the model on a combination of labelled and unlabelled random mazes. Semi-supervised learning is important because a common bottleneck in applying machine learning in the real world is the difficulty of collecting labelled data, whereas often large quantities of unlabelled data exist.</p><p>We trained a full (r, γ, λ)-predictron by alternating standard supervised updates with consistency updates, obtained by stochastically minimizing the consistency loss (8), on additional unlabelled samples drawn from the same distribution. For each supervised update we apply either 0, 1, or 9 consistency updates. <ref type="figure">Figure 6</ref> shows that the performance improved monotonically with the number of consistency updates, measured as a function of the number of labelled samples consumed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis of adaptive depth</head><p>In principle, the predictron can adapt its depth to 'think more' about some predictions than others, perhaps depending on the complexity of the underlying target. We saw indications of this in <ref type="figure">Figure 3</ref>. We investigate this further by looking at qualitatively different prediction types in pool: ball collisions, rail collisions, pocketing balls, and entering or staying in quadrants. For each prediction type we consider several different time-spans (determined by the real-world discount factors associated with each pseudoreward). <ref type="figure" target="#fig_3">Figure 7</ref> shows distributions of depth for each type of prediction. The 'depth' of a predictron is here defined as the effective number of model steps. If the predictron relies fully on the very first value (i.e., λ λ λ 0 = 0), this counts as 0 steps. If, instead, it learns to place equal weight on all rewards and on the final value, this counts as 16 steps. Concretely, the depth d can be defined recursively as</p><formula xml:id="formula_12">d = d 0 where d k = λ λ λ k (1 + γ γ γ k d k+1 ) and d K = 0.</formula><p>Note that even for the same input state, each prediction has a separate depth.</p><p>The depth distributions exhibit three properties. First, different types of predictions used different depths. Second, depth was correlated with the real-world discount for the first four prediction types. Third, the distributions are not strongly peaked, which implies that the depth can differ per input even for a single real-world discount and prediction type. In a control experiment (not shown) we used a Number of labels Unshared cores <ref type="figure">Figure 6</ref>. Semi-supervised learning. Prediction errors of the (r, γ, λ)-predictrons (shared core, no skips) using 0, 1, or 9 consistency updates for every update with labelled data, plotted as function of the number of labels consumed. Learning performance improves with more consistency updates.</p><p>scalar λ shared among all predictions, which reduced performance in all scenarios, indicating that the heterogeneous depth is a valuable form of flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Using predictions to make decisions</head><p>We test the quality of the predictions in the pool domain to evaluate whether they are well-suited to making decisions. For each sampled pool position, we consider a set I of different initial conditions (different angles and velocity of the white ball), and ask which is more likely to lead to pocketing coloured balls. For each initial condition s ∈ I, we apply the (r, γ, λ)-predictron (shared cores, 16 model steps, no skip connections) to obtain predictions g λ . We ensemble the predictions associated to pocketing any ball (except the white one) with discounts γ = 0.98 and γ = 1. We select the condition s * that maximises this sum.</p><p>We then roll forward the pool simulator from s * and log the number of pocketing events. <ref type="figure" target="#fig_0">Figure 2</ref> shows a sampled rollout, using the predictron to pick s * . When providing the choice of 128 angles and two velocities for initial conditions (|I| = 256), this procedure resulted in pocketing 27 coloured balls in 50 episodes. Using the same procedure with an equally deep convolutional network only resulted in 10 pocketing events. These results suggest that the lower loss of the learned (r, γ, λ)-predictron translated into meaningful improvements when informing decisions. A video of the rollouts selected by the predictron is available at the following url: https://youtu.be/ BeaLdaN2C3Q. <ref type="bibr" target="#b4">Lee et al. (2015)</ref> introduced a neural network architecture where classifications branch off intermediate hidden layers. An important difference with respect to the λ-predictron is that the weights are hand-tuned as hyper-parameters, whereas in the predictron the λ weights are learnt and, more importantly, conditional on the input. Another difference is that the loss on the auxiliary classifications is used to speed up learning, but the classifications themselves are not combined into an aggregate prediction; the output of the model itself is the deepest prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related work</head><p>Graves <ref type="formula" target="#formula_6">(2016)</ref> introduced an architecture with adaptive computation time (ACT), with a discrete (but differentiable) decision on when to halt, and aggregating the outputs at each pondering step. This is related to our λ λ λ weights, but obtains depth in a different way; one notable difference is that the λ-predictron can use different pondering depths for each of its predictions.</p><p>Value iteration networks (VINs) <ref type="bibr" target="#b19">(Tamar et al., 2016</ref>) also learn value functions end-to-end using an internal model, similar to the (non-λ) predictron. However, VINs plan via convolutional operations over the full input state space; whereas the predictron plans via imagined trajectories through an abstract state space. This may allow the predictron architecture to scale much more effectively in domains that do not have a natural two-dimensional encoding of the state space.</p><p>The notion of learning about many predictions of the future relates to work on predictive state representations (PSRs; <ref type="bibr" target="#b6">Littman et al., 2001)</ref>, general value functions (GVFs; <ref type="bibr" target="#b18">Sutton et al., 2011)</ref>, and nexting <ref type="bibr" target="#b9">(Modayil et al., 2012)</ref>. Such predictions have been shown to be useful as representations <ref type="bibr" target="#b11">(Schaul &amp; Ring, 2013)</ref> and for transfer <ref type="bibr" target="#b12">(Schaul et al., 2015)</ref>. So far, however, none of these have been considered for learning abstract models. <ref type="bibr" target="#b13">Schmidhuber (2015)</ref> discusses learning abstract models, but maintains separate losses for the model and a controller, and suggests training the model unsupervised to compactly encode the entire history of observations, through predictive coding. The predictron's abstract model is instead trained end-to-end to obtain accurate values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>The predictron is a single differentiable architecture that rolls forward an internal model to estimate external values. This internal model may be given both the structure and the semantics of traditional reinforcement learning models. But, unlike most approaches to model-based reinforcement learning, the model is fully abstract: it need not correspond to the real environment in any human understandable fashion, so long as its rolled-forward "plans" accurately predict outcomes in the true environment.</p><p>The predictron may be viewed as a novel network architecture that incorporates several separable ideas. First, the predictron outputs a value by accumulating rewards over a series of internal planning steps. Second, each forward pass of the predictron outputs values at multiple planning depths. Third, these values may be combined together, also within a single forward pass, to output an overall ensemble value. Finally, the different values output by the predictron may be encouraged to be self-consistent with each other, to provide an additional signal during learning. Our experiments demonstrate that these differences result in more accurate predictions of value, in reinforcement learning environments, than more conventional network architectures.</p><p>We have focused on value prediction tasks in uncontrolled environments. However, these ideas may transfer to the control setting, for example by using the predictron as a Qnetwork <ref type="bibr" target="#b8">(Mnih et al., 2015)</ref>. Even more intriguing is the possibility of learning an internal MDP with abstract internal actions, rather than the MRP considered in this paper. We aim to explore these ideas in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Top: Two sample mazes from the random-maze domain. Light blue cells are empty, darker blue cells contain a wall. One maze is connected from top-left to bottom-right, the other is not. Bottom: An example trajectory in the pool domain (before downsampling), selected by maximising the prediction by a predictron of pocketing balls.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Exploring predictron variants. Aggregated prediction errors over all predictions (20 for mazes, 280 for pool) for the eight predictron variants corresponding to the cube on the left (as described in the main text), for both random mazes (top) and pool (bottom). Each line is the median of RMSE over five seeds; shaded regions encompass all seeds. The full (r, γ, λ)-prediction (red) consistently performed best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparing predictron to baselines. Aggregated prediction errors on random mazes (top) and pool (bottom) over all predictions for the eight architectures corresponding to the cube on the left. Each line is the median of RMSE over five seeds; shaded regions encompass all seeds. The full (r, γ, λ)-predictron (red), consistently outperformed conventional deep network architectures (black), with and without skips and with and without weight sharing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Thinking depth. Distributions of thinking depth on pool for different types of predictions and for different real-world discounts.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Recurrent environment simulators</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1603.08983</idno>
		<ptr target="http://arxiv.org/abs/1603.08983" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saining</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predictive representations of state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1555" to="1561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puigdomènech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dharshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multitimescale nexting in a reinforcement learning robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Simulation of Adaptive Behavior</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="299" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaoxiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honglak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Better Generalization with Forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal Value Function Approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.09249</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integrated architectures for learning, planning and reacting based on dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the Seventh International Workshop</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling the world at a mixture of time scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Td</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">M</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 10th International Conference on Autonomous Agents and Multiagent Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="761" to="768" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
