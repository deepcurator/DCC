<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Query-guided Regression Network with Context Policy for Phrase Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">Ram Nevatia University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Kovvuri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">Ram Nevatia University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:lang="en">Query-guided Regression Network with Context Policy for Phrase Grounding</title>
						<title level="a" xml:lang="en">novel Query-guided Regression network with Context pol- icy (QRC Net) which jointly learns a Proposal Genera- tion Network (PGN), a Query-guided Regression Network (QRN) and a Context Policy Network (CPN). Experiments show QRC Net provides a significant improvement in ac- curacy on two popular datasets: Flickr30K Entities and Referit Game, with 14.25% and 17.14% increase over the state-of-the-arts respectively.</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given an image and a related textual description, phrase grounding attempts to localize objects which are mentioned by corresponding phrases in the description. It is an important building block in computer vision with natural language interaction, which can be utilized in high-level tasks, such as image retrieval <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">26]</ref>, image captioning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> and visual question answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Phrase Grounding is a challenging problem that involves parsing language queries and relating the knowledge to localize objects in visual domain. To address this problem, typically a proposal generation system is first applied to produce a set of proposals as grounding candidates. The main difficulties lie in how to learn the correlation between language (query) and visual (proposals) modalities, and how to localize objects based on multimodal correlation. Stateof-the-art methods address the first difficulty by learning a subspace to measure the similarities between proposals and queries. With the learned subspace, they treat the second difficulty as a retrieval problem, where proposals are ranked based on their relevance to the input query. Among these, Phrase-Region CCA <ref type="bibr" target="#b25">[25]</ref> and SCRC <ref type="bibr" target="#b13">[14]</ref> models learn a multimodal subspace via Canonical Correlation Analysis (CCA) and a Recurrent Neural Network (RNN) respectively. Varun et al. <ref type="bibr" target="#b22">[22]</ref> learn multimodal correlation aided by context objects in visual content. GroundeR <ref type="bibr" target="#b29">[29]</ref> introduces an attention mechanism that learns to attend on related proposals given different queries through phrase reconstruction.</p><p>These approaches have two important limitations. First, proposals generated by independent systems may not always cover all mentioned objects given various queries; since retrieval based methods localize objects by choosing one of these proposals, they are bounded by the performance limits from proposal generation systems. Second, even though query phrases are often selected from image descriptions, context from these descriptions is not utilized to reduce semantic ambiguity. Consider example in <ref type="figure" target="#fig_0">Fig 1.</ref> Given the query "a man", phrases "a guitar" and "a little girl" can be considered to provide context that proposals overlapping with "a guitar" or "a little girl" are less likely to be the ones containing "a man".</p><p>To address the aforementioned issues, we propose to predict mentioned object's location rather than selecting candidates from limited proposals. We adopt a regression based method guided by input query's semantics. To reduce semantic ambiguity, we assume that different phrases in one sentence refer to different visual objects. Given one query phrase, we evaluate predicted proposals and down-weight those which cover objects mentioned by other phrases (i.e., context). For example, we assign lower rewards for proposals containing "a guitar" and "a little girl" in <ref type="figure" target="#fig_0">Fig 1</ref> to guide system to select more discriminative proposals containing "a man". Since this procedure depends on prediction results and is non-differentiable, we utilize reinforcement learning <ref type="bibr" target="#b31">[31]</ref> to adaptively estimate these rewards conditioned on context information and jointly optimize the framework.</p><p>In implementation, we propose a novel Query-guided Regression network with Context policy (QRC Net) which consists of a Proposal Generation Network (PGN), a Queryguided Regression Network (QRN) and a Context Policy Network (CPN). PGN is a proposal generator which provides candidate proposals given an input image (red boxes in <ref type="figure" target="#fig_0">Fig. 1</ref>). To overcome performance limit from PGN, QRN not only estimates each proposal's relevance to the input query, but also predicts its regression parameters to the mentioned object conditioned on the query's intent (yellow and green boxes in <ref type="figure" target="#fig_0">Fig. 1</ref>). CPN samples QRN's prediction results and evaluates them by leveraging context information as a reward function. The estimated reward is then back propagated as policy gradients <ref type="bibr">(</ref>Step 3 in <ref type="figure" target="#fig_0">Fig. 1</ref>) to assist QRC Net's optimization. In training stage, we jointly optimize PGN, QRN and CPN using an alternating method in <ref type="bibr" target="#b28">[28]</ref>. In test stage, we fix CPN and apply trained PGN and QRN to ground objects for different queries.</p><p>We evaluate QRC Net on two grounding datasets: Flickr30K Entities <ref type="bibr" target="#b25">[25]</ref> and Referit Game <ref type="bibr" target="#b14">[15]</ref>. Flickr30K Entities contains more than 30K images and 170K query phrases, while Referit Game has 19K images referred by 130K query phrases. Experiments show QRC Net outperforms state-of-the-art methods by a large margin on both two datasets, with more than 14% increase on Flickr30K Entities and 17% increase on Referit Game in accuracy.</p><p>Our contributions are twofold: First, we propose a queryguided regression network to overcome performance limits of independent proposal generation systems. Second, we introduce reinforcement learning to leverage context information to reduce semantic ambiguity. In the following paper, we first discuss related work in Sec. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Phrase grounding requires learning correlation between visual and language modalities. Karpathy et al. <ref type="bibr" target="#b15">[16]</ref> propose to align sentence fragments and image regions in a subspace, and later replace the dependency tree with a bidirectional RNN in <ref type="bibr" target="#b0">[1]</ref>. Hu et al. <ref type="bibr" target="#b13">[14]</ref> propose a SCRC model which adopts a 2-layer LSTM to rank proposals using encoded query and visual features. Rohrbach et al. <ref type="bibr" target="#b29">[29]</ref> employ a latent attention network conditioned on query which ranks proposals in unsupervised scenario. Other approaches learn the correlation between visual and language modalities based on Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b10">[11]</ref> methods. Plummer et al. <ref type="bibr" target="#b24">[24]</ref> first propose a CCA model to learn the multimodal correlation. Wang et al. <ref type="bibr" target="#b34">[34]</ref> employ structured matching and use phrase pairs to boost performance. Recently, Plummer et al. <ref type="bibr" target="#b25">[25]</ref> augment the CCA model to leverage extensive linguistic cues in the phrases. All of the above approaches are reliant on external object proposal systems and hence, are bounded by their performance limits.</p><p>Proposal generation and spatial regression. Proposal generation systems are widely used in object detection and phrase grounding tasks. Two popular methods: Selective Search <ref type="bibr" target="#b33">[33]</ref> and EdgeBoxes <ref type="bibr" target="#b38">[38]</ref> employ efficient low-level features to produce proposals on possible object locations. Based on proposals, spatial regression method is successfully applied in object detection. Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> first employs a regression network to regress proposals generated by Selective Search <ref type="bibr" target="#b33">[33]</ref>. Based on this, Ren et al. <ref type="bibr" target="#b28">[28]</ref> incorporate the proposal generation system by introducing a Region Proposal Network (RPN) which improves both accuracy and speed in object detection. Redmon et al. <ref type="bibr" target="#b27">[27]</ref> employ regression method in grid level and use nonmaximal suppression to improve the detection speed. Liu et al. <ref type="bibr" target="#b19">[20]</ref> integrate proposal generation into a single network and use outputs discretized over different ratios and scales of feature maps to further increase the performance. Inspired by the success of RPN in object detection, we build a PGN and regress proposals conditioned on the input query.</p><p>Reinforcement learning is first introduced to deep neural network in Deep Q-learning (DQN) <ref type="bibr" target="#b21">[21]</ref>, which teaches an agent to play ATARI games. Lillicrap et al. <ref type="bibr" target="#b18">[19]</ref> modify DQN by introducing deep deterministic policy gradients, which enables reinforcement learning framework to be optimized in continuous space. Recently, Yu et al. <ref type="bibr" target="#b37">[37]</ref> adopt a reinforcer to guide speaker-listener network to sample more discriminative expressions in referring tasks. Liang et al. <ref type="bibr" target="#b17">[18]</ref> introduce reinforcement learning to traverse a directed semantic action graph to learn visual relationship and attributes of objects in images. Inspired by the successful applications of reinforcement learning, we propose a CPN to assign rewards as policy gradients to leverage context information in training stage.  <ref type="bibr" target="#b28">[28]</ref>. QRN encodes input query's semantics by an LSTM <ref type="bibr" target="#b12">[13]</ref> model and regresses proposals conditioned on the query. CPN samples the top ranked proposals, and assigns rewards considering whether they are foreground (FG), background (BG) or context. These rewards are back propagated as policy gradients to guide QRC Net to select more discriminative proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">QRC Network</head><p>QRC Net is composed of three parts: a Proposal Generation Network (PGN) to generate candidate proposals, a Query-guided Regression Network (QRN) to regress and rank these candidates and a Context Policy Network (CPN) to further leverage context information to refine ranking results. In many instances, an image is described by a sentence that contains multiple noun phrases which are used as grounding queries, one at a time. We consider the phrases that are not in the query to provide context; specifically to infer that they refer to objects not referred to by the query. This helps rank proposals; we use CPN to optimize using a reinforcement learning policy gradient algorithm.</p><p>We first present the framework of QRC Net, followed by the details of PGN, QRN and CPN respectively. Finally, we illustrate how to jointly optimize QRC Net and employ QRC Net in phrase grounding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>The goal of QRC Net is to localize the mentioned object's location y given an image x and a query phrase q. To achieve this, PGN generates a set of N proposals {r i } as candidates. Given the query q, QRN predicts their regression parameters {t i } and probability {p i } of being relevant to the input query. To reduce semantic ambiguity, CPN evaluates prediction results of QRN based on the locations of objects mentioned by context phrases, and adopts a reward function F to adaptively penalize high ranked proposals containing context-mentioned objects. Reward calculation depends on predicted proposals, and this procedure is non-differentiable. To overcome this, we deploy a reinforcement learning procedure in CPN where this reward is back propagated as policy gradients <ref type="bibr" target="#b32">[32]</ref> to optimize QRN's parameters, which guides QRN to predict more discriminative proposals. The objective for QRC Net is:</p><formula xml:id="formula_0">arg min θ q [L gen ({r i }) + L cls ({r i }, {p i }, y) +λL reg ({r i }, {t i }, y) + J(θ)]<label>(1)</label></formula><p>where θ denotes the QRC Net's parameters to be optimized and λ is a hyperparameter. L gen is the loss for generation proposals produced by PGN. L cls is a multi-class classification loss generated by QRN in predicting the probability p i of each proposal r i . L reg is a regression loss from QRN to regress each proposal r i to the mentioned object's location y. J(θ) is the reward expectation calculated by CPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposal Generation Network (PGN)</head><p>We build PGN with a similar structure as that of RPN in <ref type="bibr" target="#b28">[28]</ref>. PGN adopts a fully convolutional neural network (FCN) to encode the input image x as an image feature map x. For each location (i.e., anchor) in image feature map, PGN uses different scales and aspect ratios to generate proposals {r i }. Each anchor is fed into a multiple-layer perceptron (MLP) which predicts a probability p oi estimating the objectness of the anchor, and 4D regression parameters t i = [(x − x a )/w a , (y − y a )/h a , log(w/w a ), log(h/h a )] as defined in <ref type="bibr" target="#b28">[28]</ref>. The regression parameters t i estimate the offset from anchor to mentioned objects' bounding boxes. Given all mentioned objects' locations {y l }, we consider a proposal to be positive when it covers some object y l with Intersection over Union (IoU) &gt; 0.7, and negative when IoU &lt; 0.3. The generation loss is:</p><formula xml:id="formula_1">L gen ({r i }) = − 1 N cls N cls i=1 δ(i ∈ S y ∪ Sȳ) log(p oi ) + λ g N reg Nreg i=1 δ(i ∈ S y ) 3 j=0 f (|t * i [j] − t i [j]|)<label>(2)</label></formula><p>where δ(.) is an indicator function, S y is the set of positive proposals' indexes and Sȳ is the set of negative proposals' indexes. N reg is the number of all anchors and N cls is the number of sampled positive and negative anchors as defined in <ref type="bibr" target="#b28">[28]</ref>. t * i represents regression parameters of anchor i to corresponding object's location y l . f (.) is the smooth L1 loss function: f (x) = 0.5x 2 (|x| &lt; 1), and f (x) = |x| − 0.5(|x| ≥ 1).</p><p>We sample the top N anchors based on {p oi } and regress them as proposals {r i } with predicted regression parameters t i . Through a RoI pooling operation <ref type="bibr" target="#b28">[28]</ref>, we extract visual feature v i ∈ R dv for each proposal r i . {r i } and {v i } as fed into QRN as visual inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Query guided Regression Network (QRN)</head><p>For input query q, QRN encodes its semantics as an embedding vector q ∈ R dq via a Long Short-Term Memory (LSTM) model. Given visual inputs {v i }, QRN concatenates the embedding vector q with each of the proposal's visual feature v i . It then applies a fully-connected (fc) layer to generate multimodal features {v q i } ∈ R m for each of the q, r i pair in an m-dimensional subspace. The multimodal feature v q i is calculated as:</p><formula xml:id="formula_2">v q i = ϕ(W m (q||v i ) + b m )<label>(3)</label></formula><p>where W m ∈ R (dq+dv)×m , b m ∈ R m are projection parameters. ϕ(.) is a non-linear activation function. "||" denotes a concatenation operator.</p><p>Based on the multimodal feature v q i , QRN predicts a 5D vector s p i ∈ R 5 via a fc layer for each proposal r i (superscript "p" denotes prediction).</p><formula xml:id="formula_3">s p i = W s v q i + b s<label>(4)</label></formula><p>where W s ∈ R m×5 and b s ∈ R 5 are projection weight and bias to be optimized. The first element in s p i estimates the confidence of r i being related to input query q's semantics. The next four elements are regression parameters which are in the same form as t i defined in Sec. 3.2, where x, y, w, h are replaced by regressed values and x a , y a , w a , h a are proposal's parameters.</p><p>We denote {p i } as the probability distribution of {r i } after we feed {s p i [0]} to a softmax function. Same as <ref type="bibr" target="#b29">[29]</ref>, we consider one proposal as positive which overlaps most with ground truth and with IoU &gt; 0.5. Thus, the classification loss is calculated as:</p><formula xml:id="formula_4">L cls ({r i }, {p i }, y) = − log(p i * )<label>(5)</label></formula><p>where i * is positive proposal's index in the proposal set.</p><note type="other">Given the object's location y mentioned by query q, each proposal's ground truth regression data s q i ∈ R 4 is calculated in the same way as the last four elements of s p i , by replacing [x, y, w, h] with the ground truth bounding box's location information. The regression loss for QRN is:</note><formula xml:id="formula_5">L reg ({t i }, {r i }, y) = 1 4N N i=1 3 j=0 f (|s p i [j + 1] − s q i [j]|)<label>(6)</label></formula><p>where f (.) is the smooth L1 function defined in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Context Policy Network (CPN)</head><p>Besides using QRN to predict and regress proposals, we further apply a CPN to guide QRN to avoid selecting proposals which cover the objects referred by query q's context in the same description. CPN evaluates and assigns rewards for top ranked proposals produced from QRN, and performs a non-differentiable policy gradient <ref type="bibr" target="#b32">[32]</ref> to update QRN's parameters.</p><p>Specifically, proposals {r i } from QRN are first ranked based on their probability distribution {p i }. Given the ranked proposals, CPN selects the top K proposals {r ′ i } and evaluates them by assigning rewards. This procedure is non-differentiable, since we do not know the proposals' qualities until they are ranked based on QRN's probabilities. Therefore, we use policy gradients reinforcement learning to update the QRN's parameters. The goal is to maximize the expectation of predicted reward F ({r ′ i }) under the distribution of {r ′ i } parameterized by the QRN, i.e., J = E {pi} <ref type="bibr">[F ]</ref>. According to the algorithm in <ref type="bibr" target="#b35">[35]</ref>, the policy gradient is</p><formula xml:id="formula_6">∇ θr J = E {pi} [F ({r ′ i })∇ θr log p ′ i (θ r )]<label>(7)</label></formula><p>where θ r are QRN's parameters and ∇ θr log p ′ i (θ r ) is the gradient produced by QRN for top ranked proposal r i .</p><p>To predict reward value F ({r </p><formula xml:id="formula_7">F ({r ′ i }) = σ(W c (v c ||q) + b c )<label>(8)</label></formula><p>where "||" denotes concatenation operation and σ(.) is a sigmoid function. W c and b c are projection parameters which produce a scalar value as reward.</p><p>To train CPN, we design a reward function to guide CPN's prediction. The reward function performs as feedback from environment and guide CPN to produce meaningful policy gradients. Intuitively, to help QRN select more discriminative proposals related to query q rather than context, we assign lower reward for some top ranked proposal that overlaps the object mentioned by context and higher reward if it overlaps with the mentioned object by query. Therefore, we design the reward function as:</p><formula xml:id="formula_8">R({r ′ i }) = 1 K K i=1 [δ(r ′ i ∈ S q ) + βδ(r ′ i / ∈ (S q ∪ S bg ))] (9)</formula><p>where S q is the set of proposals with IoU &gt; 0.5 with mentioned objects by query q, and S bg is the set of background proposals with IoU &lt; 0.5 with objects mentioned by all queries in the description. δ(.) is an indicator function and β ∈ (0, 1) is the reward for proposals overlapping with objects mentioned by context. The reward prediction loss is:</p><formula xml:id="formula_9">L rwd ({r ′ i }) = ||F ({r ′ i }) − R({r ′ i })|| 2 (10)</formula><p>During training, L rwd is backpropagated only to CPN for optimization, while CPN backpropagates policy gradients (Eq. 7) to optimize QRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Inference</head><p>We train PGN based on an RPN pre-trained on PASCAL VOC 2007 <ref type="bibr" target="#b5">[6]</ref> dataset, and adopt the alternating training method in <ref type="bibr" target="#b28">[28]</ref> to optimize PGN. We first train PGN and use proposals to train QRN and CPN, then initialize PGN tuned by QRN and CPN's training, which iterates one time. Same as <ref type="bibr" target="#b29">[29]</ref>, we select 100 proposals produced by PGN (N = 100) and select top 10 proposals (K = 10) predicted by QRN to assign reward in Eq. 9. After calculating policy gradient in Eq. 7, we jointly optimize QRC Net's objective (Eq. 1) using Adam algorithm <ref type="bibr" target="#b16">[17]</ref>. We choose the rectified linear unit (ReLU) as the non-linear activation function.</p><p>During testing stage, CPN is fixed and we stop its reward calculation. Given an image, PGN is first applied to generate proposals and their visual features. QRN regresses these proposals and predicts the relevance of each proposal to the query. The regressed proposal with highest relevance is selected as the prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate QRC Net on Flickr30K Entities <ref type="bibr" target="#b25">[25]</ref> and Referit Game datasets <ref type="bibr" target="#b14">[15]</ref> for phrase grounding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Flickr30K Entities <ref type="bibr" target="#b25">[25]</ref>: The numbers of training, validation and testing images are 29783, 1000, 1000 respectively. Each image is associated with 5 captions, with 3.52 query phrases in each caption on average. There are 276K manually annotated bounding boxes referred by 360K query phrases in images. The vocabulary size for all these queries is 17150.</p><p>Referit Game <ref type="bibr" target="#b14">[15]</ref> consists of 19,894 images of natural scenes. There are 96,654 distinct objects in these images. Each object is referred to by 1-3 query phrases (130,525 in total). There are 8800 unique words among all the phrases, with a maximum length of 19 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Setup</head><p>Proposal generation. We adopt a PGN (Sec. 3.2) to generate proposals. During training, we optimize PGN based on an RPN pre-trained on PASCAL VOC 2007 dataset <ref type="bibr" target="#b5">[6]</ref>, which does not overlap with Flickr30K Entities <ref type="bibr" target="#b25">[25]</ref> or Referit Game <ref type="bibr" target="#b14">[15]</ref>. We also evaluate QRC Net based on Selective Search <ref type="bibr" target="#b33">[33]</ref> (denoted as "SS") and EdgeBoxes <ref type="bibr" target="#b38">[38]</ref> (denoted as "EB"), and an RPN <ref type="bibr" target="#b28">[28]</ref> pre-trained on PASCAL VOC 2007 <ref type="bibr" target="#b23">[23]</ref> (denoted as "RPN"), which are all independent of QRN and CPN.</p><p>Visual feature representation. For QRN, the visual features are directly generated from PGN via a RoI pooling operation. Since PGN contains a VGG Network <ref type="bibr" target="#b30">[30]</ref> to process images, we denote these features as "VGG pgn ". To predict regression parameters, we need to include spatial information for each proposal. For Flickr30K Entities, we augment each proposal's visual feature with its spatial information [x tl /W, y tl /H, x br /H, y br /W, wh/W H] as defined in <ref type="bibr" target="#b36">[36]</ref>. These augmented features are 4101D vectors (d v = 4101). For Referit Game, we augment VGG pgn with each proposal's spatial information [x min , y min , x max , y max , x center , y center , w box , h box ] which is same as <ref type="bibr" target="#b29">[29]</ref> for fair comparison. We denote these features as "VGG pgn -SPAT", which are 4104D vectors (d v = 4104).</p><p>To compare with other approaches, we replace PGN with a Selective Search and an EdgeBoxes proposal generator. Same as <ref type="bibr" target="#b29">[29]</ref>, we choose a VGG network finetuned using Fast-RCNN <ref type="bibr" target="#b8">[9]</ref> on PASCAL VOC 2007 <ref type="bibr" target="#b5">[6]</ref> to extract visual features for Flickr30K Entities. We denote these features as "VGG det ". Besides, we follow <ref type="bibr" target="#b29">[29]</ref> and apply a VGG network pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> to extract proposals' features for Flickr30K Entities and Referit Game, which are denoted as "VGG cls ". We augment VGG det and VGG cls with spatial information for Flickr30K Entities and Referit Game datasets following the method mentioned above.</p><p>Model initialization. Following same settings as in <ref type="bibr" target="#b29">[29]</ref>, we encode queries via an LSTM model, and choose the last hidden state from LSTM as q (dimension d q = 1000). All convolutional layers are initialized by MSRA method <ref type="bibr" target="#b11">[12]</ref> and all fc layers are initialized by Xavier method <ref type="bibr" target="#b9">[10]</ref>. We introduce batch normalization layers after projecting visual and language features (Eq. 3).</p><p>During training, the batch size is 40. We set weight λ for regression loss L reg as 1.0 (Eq. 1), and reward value β = 0.2 (Eq. 9). The dimension of multimodal feature vector v Compared approaches SCRC <ref type="bibr" target="#b13">[14]</ref> 27.80 Structured Matching <ref type="bibr" target="#b34">[34]</ref> 42.08 SS+GroundeR (VGG cls ) <ref type="bibr" target="#b29">[29]</ref> 41.56 RPN+GroundeR (VGG det ) <ref type="bibr" target="#b29">[29]</ref> 39.13 SS+GroundeR (VGG det ) <ref type="bibr" target="#b29">[29]</ref> 47.81 MCB <ref type="bibr" target="#b7">[8]</ref> 48.69 CCA embedding <ref type="bibr" target="#b25">[25]</ref> 50.89</p><p>Our Metric. Same as <ref type="bibr" target="#b29">[29]</ref>, we adopt accuracy as the evaluation metric, defined to be the ratio of phrases for which the regressed box overlaps with the mentioned object by more than 50% IoU.</p><p>Compared approaches. We choose GroundeR <ref type="bibr" target="#b29">[29]</ref>, CCA embedding <ref type="bibr" target="#b25">[25]</ref>, MCB <ref type="bibr" target="#b7">[8]</ref>, Structured Matching <ref type="bibr" target="#b34">[34]</ref> and SCRC <ref type="bibr" target="#b13">[14]</ref> for comparison, which all achieve leading performances in phrase grounding. For GroundeR <ref type="bibr" target="#b29">[29]</ref>, we compare with its supervised learning scenario, which achieves the best performance among different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance on Flickr30K Entities</head><p>Comparison in accuracy. We first evaluate QRN performance based on different independent proposal generation systems. As shown in <ref type="table">Table 1</ref>, by adopting QRN, RPN+QRN achieves 14.35% increase compared to RPN+GorundeR. We further improve QRN's performance by adopting Selective Search (SS) proposal generator. Compared to SS+GroundeR, we achieve 8.18% increase in accuracy. We then incorporate our own PGN into the framework, which is jointly optimized to generate proposals as well as features (VGG pgn ). By adopting PGN, PGN+QRN achieves 4.22% increase compared to independent proposal generation system (SS+QRN) in accuracy. Finally, we include CPN to guide QRN in selecting more discriminative proposals during training. The full model (QRC Net) achieves 4.93% increase compared to PGN+QRN, and 14.25% increase over the state-of-the-art CCA embedding <ref type="bibr" target="#b25">[25]</ref> in accuracy.</p><p>Detailed comparison. <ref type="table" target="#tab_4">Table 6</ref> provides the detailed phrase localization results based on the phrase type information for each query in Flickr30K Entities. We can observe that QRC Net provides consistently superior results. CCA embedding <ref type="bibr" target="#b25">[25]</ref> model is good at localizing "instruments" while GroundeR <ref type="bibr" target="#b29">[29]</ref> is strong in localizing "scene". By using QRN, we observe that the regression network achieves consistent increase in accuracy compared to GroundeR model (VGG det ) in all phrase types except for class "instruments". Typically, there is a large increase in performance of localizing "animals" (with increase of 11.39%). By using PGN, we observe that PGN+QRN has surpassed state-of-the-art method in all classes, with largest increase in class "instruments". Finally, by applying CPN, QRC Net achieves more than 8.03%, 9.37%, 8.94% increase in accuracy in all categories compared to CCA embedding <ref type="bibr" target="#b25">[25]</ref>, Structured Matching <ref type="bibr" target="#b34">[34]</ref> and GroundeR <ref type="bibr" target="#b29">[29]</ref> respectively. QRC Net achieves the maximum increase in performance of 15.73% for CCA embedding <ref type="bibr" target="#b25">[25]</ref> ("scene"), 32.90% for Structured Matching <ref type="bibr" target="#b34">[34]</ref> ("scene") and 21.46% for GroundeR <ref type="bibr" target="#b29">[29]</ref> ("clothing").</p><p>Proposal generation comparison. We observe proposals' quality plays an important role in final grounding performance. The influence has two aspects. First is the Upper Bound Performance (UBP) which is defined as the ratio of covered objects by generated proposals in all ground truth objects. Without regression mechanism, UBP directly determines the performance limit of grounding systems. Another aspect is the average number of surrounding Bounding boxes Per Ground truth object (BPG). Generally, when BPG increases, more candidates are considered as positive, which reduces the difficulty for following grounding system. To evaluate UBP and BPG, we consider that a proposal covers the ground truth object when its IoU &gt; 0.5. The statistics for RPN, SS and PGN in these two aspects  are provided in <ref type="table">Table 2</ref>. We observe that PGN achieves increase in both UBP and PBG, which indicates PGN provides high quality proposals for QRN and CPN. Moreover, since QRN adopts a regression-based method, it can surpass UBP of PGN, which further relieves the influence from UBP of proposal generation systems.</p><p>Hyperparameters. We evaluate QRC Net for different sets of hyperparameters. To evaluate one hyperparameter, we fix other hyperparameters to default values in Sec. 4.2. We first evaluate QRC Net's performance for different regression loss weights λ. The results are shown in <ref type="table">Table 3</ref>. We observe the performance of QRC Net fluctuates when λ is small and decreases when λ becomes large.</p><p>We then evaluate QRC Net's performance for different  <ref type="table">Table 7</ref>. Different models' performance on Referit Game dataset.</p><p>dimensions m for multimodal features in Eq. 3. The performances are presented in <ref type="table">Table 4</ref>. We observe QRC Net's performance fluctuates when m &lt; 1000. When m becomes large, the performance of QRC Net decreases. Basically, these changes are in a small scale, which shows the insensitivity of QRC Net to these hyperparameters. Finally, we evaluate different reward values β for proposals covering objects mentioned by context. We observe QRC Net's performance fluctuates when β &lt; 0.5. When β is close to 1.0, the CPN assigns almost same rewards for proposals covering ground truth objects or context mentioned objects, which confuses the QRN. As a result, the performance of QRC Net decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance on Referit Game</head><p>Comparison in accuracy. To evaluate QRN's effectiveness, we first adopt an independent EdgeBoxes <ref type="bibr" target="#b38">[38]</ref> (EB) as proposal generator, which is same as <ref type="bibr" target="#b29">[29]</ref>. As shown in <ref type="table">Table 7</ref>, by applying QRN, we achieve 5.28% improvement compared to EB+GroundeR model. We further incorporate PGN into the framework. PGN+QRN model brings 11.36% increase in accuracy, which shows the high quality of proposals produced by PGN. Finally, we evaluate the full QRC Net model. Since Referit Game dataset only contains independent query phrases, there is no context information available. In this case, only the first term in Eq. 9 guides the learning. Thus, CPN does not contribute much to performance (0.50% increase in accuracy).</p><p>Hyperparameters. We evaluate QRC Net's performances for different hyperparameters on Referit Game dataset. First, we evaluate QRC Net's performance for different weights λ of regression loss L reg . As shown in Table 8, performance of QRC Net fluctuates when λ is small. When λ becomes large, regression loss overweights classification loss, where a wrong seed proposal may be selected which produces wrong grounding results. Thus, the performance decreases.</p><p>We then evaluate QRC Net's performance for different multimodal dimensions m of v q i in Eq. 3. In <ref type="table" target="#tab_6">Table 9</ref>, we observe performance changes in a small scale when m &lt; 1000, and decreases when m &gt; 1000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>We visualize some phrase grounding results of Flickr30K Entities and Referit Game for qualitative evaluation <ref type="figure">(Fig. 3)</ref>. For Flickr30K Entities, we show an image with its associated caption, and highlight the query phrases in it. For each query, we visualize the ground truth box, the selected proposal box by QRN and the regressed bounding box based on the regression parameters predicted by QRN. Since there is no context information in Referit Game, we visualize query and ground truth box, with selected proposal and regressed box predicted by QRN.</p><p>As shown in <ref type="figure">Fig 3,</ref> QRC Net is strong in recognizing different people ("A young tennis player" in the first row) and clothes ("purple beanie" in the second row), which is also validated in <ref type="table" target="#tab_4">Table 6</ref>. However, when the query is ambiguous without further context description, QRC Net may be confused and produce reasonably incorrect grounding result (e.g., "hat on the right" in the third row of <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel deep learning network (QRC Net) to address the phrase grounding task. QRC Net adopts regression mechanism and leverages context information, which achieves 14.25% and 17.14% increase in accuracy on Flickr30K Entities <ref type="bibr" target="#b25">[25]</ref> and Referit Game <ref type="bibr" target="#b14">[15]</ref> datasets respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. QRC Net first regresses each proposal based on query's semantics and visual features, and then utilizes context information as rewards to refine grounding results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Query-guided Regression network with Context policy (QRC Net) consists of a Proposal Generation Network (PGN), a Queryguided Regression Network (QRN) and a Context Policy Network (CPN). PGN generates proposals and extracts their CNN features via a RoI pooling operation [28]. QRN encodes input query's semantics by an LSTM [13] model and regresses proposals conditioned on the query. CPN samples the top ranked proposals, and assigns rewards considering whether they are foreground (FG), background (BG) or context. These rewards are back propagated as policy gradients to guide QRC Net to select more discriminative proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>′i</head><label></label><figDesc>}), CPN averages top ranked proposals' visual features {v ′ i } as v c . The predicted reward is computed as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to m = 512 (Eq. 3). Analysis of hyperparameters is provided in Sec. 4.3 and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. More details of QRC Net are provided in Sec. 3. Finally we analyze and compare QRC Net with other approaches in Sec. 4.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 3. QRC Net's performances on Flickr30K Entities for differ- ent weights λ of Lreg.Table 5. QRC Net's performances on Flickr30K Entities for differ- ent reward values β of CPN.</figDesc><table>Weight λ 
0.5 
1.0 
2.0 
4.0 
10.0 
Accuracy (%) 64.15 65.14 64.40 64.29 63.27 

Dimension m 
128 
256 
512 
1024 
Accuracy (%) 64.08 64.59 65.14 62.52 

Table 4. QRC Net's performances on Flickr30K Entities for differ-
ent dimensions m of v 

q 

i . 

Reward β 
0.1 
0.2 
0.4 
0.8 
Accuracy (%) 64.10 65.14 63.88 62.77 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>selected proposal box and regressed bounding box in blue, green and red resepctively. When query is not clear without further context information, QRC Net may ground wrong objects (e.g., image in row three, column four).</figDesc><table>Query 1: woman in funny 
hat on right 

(Referit Game does not 
provide image descriptions) 

A young tennis player 
wearing a yellow shirt and 
shorts hits the tennis ball 

Query 1: A young tennis player 
Query 2 : yellow shirt 
Query 3 : tennis ball 

A little girl with a purple 
beanie waits near a 
domino's pizza sign 

Query 1: A little girl 
Query 2 : purple beanie 
Query 3 : domino's pizza sign 

Query 2 : leftmost person 
Query 3 : hat on right 

Figure 3. Some phrase grounding results in Flickr30K Entities [25] (first two rows) and Referit Game [15] (third row). We visualize ground 
truth bounding box, Phrase Type 
people clothing body parts animals vehicles instruments scene other 

GroundeR (VGG cls ) [29] 
53.80 
34.04 
7.27 
49.23 
58.75 
22.84 
52.07 24.13 
GroundeR (VGG det ) [29] 
61.00 
38.12 
10.33 
62.55 
68.75 
36.42 
58.18 29.08 
Structured Matching [34] 57.89 
34.61 
15.87 
55.98 
52.25 
23.46 
34.22 26.23 
CCA embedding [25] 
64.73 
46.88 
17.21 
65.83 
68.75 
37.65 
51.39 31.77 

SS+QRN 
68.24 
47.98 
20.11 
73.94 
73.66 
29.34 
66.00 38.32 
PGN+QRN 
75.08 
55.90 
20.27 
73.36 
68.95 
45.68 
65.27 38.80 
QRC Net 
76.32 
59.58 
25.24 
80.50 
78.25 
50.62 
67.12 43.60 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Phrase grounding performances for different phrase types defined in Flickr30K Entities. Accuracy is in percentage.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 9 .</head><label>9</label><figDesc>Accuracy (%) 43.71 44.07 43.61 43.60 42.75 Table 8. QRC Net's performances on Referit Game for different weights λ of Lreg.Accuracy (%) 42.95 43.80 44.07 43.51QRC Net's performances on Regerit Game for different dimensions m of v</figDesc><table>Weight λ 
0.5 
1.0 
2.0 
4.0 
10.0 
Dimension m 
128 
256 
512 
1024 
q 

i . 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This paper is based, in part, on research sponsored by the Air Force Research Laboratory and the Defense Advanced Research Projects Agency under agreement number FA8750-16-2-0204. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory and the Defense Advanced Research Projects Agency or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andrej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AMC: Attention guided multi-modal correlation learning for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ABC-CNN: An attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Continuous control with deep reinforcement learning. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CNN image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Smeulders. Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structured matching for phrase localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A joint speakerlistener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
