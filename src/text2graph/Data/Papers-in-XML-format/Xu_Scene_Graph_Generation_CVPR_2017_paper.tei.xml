<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
							<email>danfei@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
							<email>yukez@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Today's state-of-the-art perceptual models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> have mostly tackled detecting and recognizing individual objects in isolation. However, understanding a visual scene often goes beyond recognizing individual objects. Take a look at the two images in <ref type="figure" target="#fig_1">Fig. 1</ref>. Even a perfect object detector would struggle to perceive the subtle difference between a man feeding a horse and a man standing by a horse. The rich semantic relationships between these objects have been largely untapped by these models. As indicated by a series of previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>, one crucial step towards a deeper understanding of visual scenes is building a structured representation that captures objects and their semantic relationships. Such representation not only offers contextual cues for fundamental recognition tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> but also provide values in a larger variety of high-level visual tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>The recent success of deep learning-based recognition models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref> has surged interest in examining the detailed structures of a visual scene, especially in the form of  Object detectors perceive a scene by attending to individual objects. As a result, even a perfect detector would produce similar outputs on two semantically distinct images (first row). We propose a scene graph generation model that takes an image as input, and generates a visually-grounded scene graph (second row, right) that captures the objects in the image (blue nodes) and their pairwise relationships (red nodes).</p><p>object relationships <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>. Scene graph, proposed by Johnson et al. <ref type="bibr" target="#b17">[18]</ref>, offers a platform to explicitly model objects and their relationships. In short, a scene graph is a visually-grounded graph over the object instances in an image, where the edges depict their pairwise relationships (see example in <ref type="figure" target="#fig_1">Fig. 1</ref>). The value of scene graph representation has been proven in a wide range of visual tasks, such as semantic image retrieval <ref type="bibr" target="#b17">[18]</ref>, 3D scene synthesis <ref type="bibr" target="#b3">[4]</ref>, and visual question answering <ref type="bibr" target="#b36">[37]</ref>. Anderson et al. recently proposed SPICE <ref type="bibr" target="#b0">[1]</ref> as an enhanced automated caption evaluation metric defined over scene graphs. However, these models that use scene graphs either rely on groundtruth annotations <ref type="bibr" target="#b17">[18]</ref>, synthetic images <ref type="bibr" target="#b36">[37]</ref>, or extract a scene graph from text domain <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. To truly take advantage of such rich structure, it is crucial to devise a model that automatically generates scene graphs from images. In this work, we address the problem of scene graph generation, where the goal is to generate a visually-grounded scene graph from an image. In a generated scene graph, an object instance is characterized by a bounding box with an object category label, and a relationship is characterized by a directed edge between two bounding boxes (i.e., ob-ject and subject) with a relationship predicate (red nodes in <ref type="figure" target="#fig_1">Fig. 1</ref>). The major challenge of generating scene graphs is reasoning about relationships. Much effort has been expended on localizing and recognizing semantic relationships in images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>. Most methods have focused on making local predictions of object relationships <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>, which essentially simplify the scene graph generation problem into independently predicting relationships between pairs of objects. However, by doing local predictions these models ignore surrounding context, whereas joint reasoning with contextual information can often resolve ambiguity due to local predictions in isolation.</p><p>To capture this intuition, we propose a novel end-toend model that learns to generate image-grounded scene graphs <ref type="figure" target="#fig_2">(Fig. 2)</ref>. The model takes an image as input and outputs a scene graph that consists of object categories, their bounding boxes, and semantic relationships between pairs of objects. Our major contribution is that instead of inferring each component of a scene graph in isolation, the model passes messages containing contextual information between a pair of bipartite sub-graphs of the scene graph, and iteratively refines its predictions using RNNs. We evaluate our model on a new scene graph dataset based on Visual Genome <ref type="bibr" target="#b19">[20]</ref>, which contains human-annotated scene graphs on 108,077 images. On average, each image is annotated with 25 objects and 22 pairwise object relationships. We show that relationship prediction in scene graphs can be significantly improved by our model. Furthermore, we also apply our model to the NYU Depth v2 dataset <ref type="bibr" target="#b27">[28]</ref>, establishing new state-of-the-art results in reasoning about spatial relations, such as horizontal and vertical supports.</p><p>In summary, we propose an end-to-end model that generates visually-grounded scene graphs from images. The model uses a novel inference formulation that iteratively refines its prediction by passing contextual messages along the topological structure of a scene graph. We demonstrate its use for generating semantic scene graphs from a new scene graph dataset as well as predicting support relations using the NYU Depth v2 dataset <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene understanding and relationship prediction. Visual scene understanding often harnesses the statistical patterns of object co-occurrence <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> as well as spatial layout <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. A series of contextual models based on surrounding pixels and regions have also been developed for perceptual tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. Recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref> exploits more complex structures for relationship prediction. However, these works focus on image-level predictions without detailed visual grounding. Physical relationships, such as support and stability, have been studied in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref>. Lu et al. <ref type="bibr" target="#b25">[26]</ref> directly tackled the semantic relationship detection by combining visual inputs with lan- An overview of our model architecture. Given an image as input, the model first produces a set of object proposals using a Region Proposal Network (RPN) <ref type="bibr" target="#b31">[32]</ref>, and then passes the extracted features of the object regions to our novel graph inference module. The output of the model is a scene graph <ref type="bibr" target="#b17">[18]</ref>, which contains a set of localized objects, categories of each object, and relationship types between each pair of objects.</p><p>guage priors to cope with the long-tail distribution of realworld relationships. However, their method predicts each relationship independently. We show that our model outperforms theirs with joint inference.</p><p>Visual scene representation. One of the most popular ways of representing a visual scene is through text descriptions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>. Although text-based representation has been shown to be helpful for scene classification and retrieval, its power is often limited by ambiguity and lack of expressiveness. In comparison, scene graphs <ref type="bibr" target="#b17">[18]</ref> offer explicit grounding of visual concepts, avoiding referential uncertainty in text-based representation. Scene graphs have been used in many downstream tasks such as image retrieval <ref type="bibr" target="#b17">[18]</ref>, 3D scene synthesis <ref type="bibr" target="#b3">[4]</ref> and understanding <ref type="bibr" target="#b9">[10]</ref>, visual question answering <ref type="bibr" target="#b36">[37]</ref>, and automatic caption evaluation <ref type="bibr" target="#b0">[1]</ref>. However, previous work on scene graphs shied away from the graph generation problem by either using ground-truth annotations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>, or extracting the graphs from other modalities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>. Our work addresses the problem of generating scene graphs directly from images.</p><p>Graph inference. Conditional Random Fields (CRF) have been used extensively in graph inference. Johnson et al. used CRF to infer scene graph grounding distributions for image retrieval <ref type="bibr" target="#b17">[18]</ref>. Yatskar et al. <ref type="bibr" target="#b39">[40]</ref> proposed situationdriven object and action prediction using a deep CRF model. Our work is closely related to CRFasRNN <ref type="bibr" target="#b42">[43]</ref> and Graph-LSTM <ref type="bibr" target="#b22">[23]</ref> in that we also formulate the graph inference problem using an RNN-based model. A key difference is that they focus on node inference while treating edges as pairwise constraints, whereas we enable edge predictions using a novel primal-dual graph inference scheme. We also share the same spirit as Structural RNN <ref type="bibr" target="#b15">[16]</ref>. A crucial distinction is that our model iteratively refines its predictions through message passing, whereas the Structural RNN model only makes one-time predictions along the temporal dimension, and thus cannot refine its past predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scene Graph Generation</head><p>A scene graph, as defined by Johnson et al. <ref type="bibr" target="#b17">[18]</ref>, is a structured representation of an image, where nodes in a scene graph correspond to object bounding boxes with their object categories, and edges correspond to their pairwise relationships between objects. The task of scene graph generation is to generate a visually-grounded scene graph that most accurately correlates with an image. Intuitively, individual predictions of objects and relationships can benefit from their surrounding context. For instance, knowing "a horse is on grass field" is likely to increase the chance of detecting a person and predicting the relationship of "man riding horse". To capture this intuition, we propose a joint inference framework to enable contextual information to propagate through the scene graph topology via a message passing scheme.</p><p>Inference on a densely connected graph can be very expensive. As shown in previous work <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b42">[43]</ref>, dense graph inference can be approximated by mean field in Conditional Random Fields (CRF). Our approach is inspired by Zheng et al. <ref type="bibr" target="#b42">[43]</ref>, which designs fully differentiable layers to enable end-to-end learning with recurrent neural networks (RNN). Yet their model relies on purpose-built RNN layers. To achieve greater flexibility in a more principled training framework, we use a generic RNN unit instead, in particular a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref>. At each iteration, each GRU takes its previous hidden state and an incoming message as input, and produces a new hidden state as output. Each node and edge in the scene graph maintains its internal state in its corresponding GRU unit, where all nodes share the same GRU weights (node GRUs), and all edges share the other set of GRU weights (edge GRUs). This setup allows the model to pass messages (i.e., aggregation of GRU hidden states) among the GRU units along the scene graph topology. We also propose a message pooling function that learns to dynamically aggregate the hidden states of the GRUs into messages.</p><p>We further observe that the unique structure of scene graphs forms a bipartite structure of message passing channels. Since messages only pass along the topological structure of a scene graph, the set of edge GRUs and the set of node GRUs form a bipartite graph, where no message is passed inside each set. Inspired by this observation, we formulate two disjoint sub-graphs that are essentially the dual graph to each other. The primal graph defines channels for messages to pass from edge GRUs to node GRUs. The dual graph defines channels for messages to pass from node GRUs to edge GRUs. With such primal-dual formulation, we can therefore improve inference efficiency by iteratively passing messages between these sub-graphs instead of through a densely connected graph. <ref type="figure" target="#fig_3">Fig. 3</ref> gives an overview of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We first lay out the mathematical formulation of our scene graph generation problem. To generate a visually grounded scene graph, we need to obtain an initial set of object bounding boxes. These bounding boxes can be either from ground-truth human annotation or algorithmically generated. In practice, we use the Region Proposal Network (RPN) <ref type="bibr" target="#b31">[32]</ref> to automatically generate a set of object bounding box proposals B I from an image I as the base input to the inference procedure ( <ref type="figure" target="#fig_3">Fig. 3(a)</ref>).</p><p>For each object box proposal, we need to infer two types of object-centric variables: 1) an object class label, and 2) four bounding box offsets relative to the proposal box coordinates, which are used for refining the proposal boxes. In addition, we need to infer a relationship-centric variable between every pair of proposal boxes, which denotes the predicate type of the relationship between the corresponding object pair. Given a set of object classes C (including background) and a set of relationship types R (including none relationship), we denote the set of all variables to be</p><formula xml:id="formula_0">x = {x cls i , x bbox i , x i→j |i = 1 . . . n, j = 1 . . . n, i = j},</formula><p>where n is the number of proposal boxes, x cls i ∈ C is the class label of the i-th proposal box, x bbox i ∈ R 4 is the bounding box offsets relative to the i-th proposal box coordinates, and x i→j ∈ R is the relationship predicate between the i-th and the j-th proposal boxes.</p><p>At the high level, the inference task is to classify objects, predict their bounding box offsets, and classify relationship predicates between each pair of objects. Formally, we formulate the scene graph generation problem as finding the optimal x * = arg max x Pr(x|I, B I ) that maximizes the following probability function given the image I and box proposals B I :</p><formula xml:id="formula_1">Pr(x|I, B I ) = i∈V j =i Pr(x cls i , x bbox i , x i→j |I, B I ). (1)</formula><p>In the next subsection, we introduce a way to approximate the inference procedure using an iterative message passing scheme modeled with Gated Recurrent Units <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference using Recurrent Neural Network</head><p>We use mean field to perform approximate inference. We denote the probability of each variable x as Q(x|·), and assume that the probability only depends on the current state of each node and edge at each iteration. In contrast to Zheng et al. <ref type="bibr">[</ref> ... 3). The model first extracts visual features of nodes and edges from a set of object proposals, and edge GRUs and node GRUs then take the visual features as initial input and produce a set of hidden states (a). Then a node message pooling function computes messages that are passed to the node GRU in the next iteration from the hidden states. Similarly, an edge message pooling function computes messages and feed to the edge GRU (b). The ⊕ symbol denotes a learnt weighted sum. The model iteratively updates the hidden states of the GRUs (c). At the last iteration step, the hidden states of the GRUs are used to predict object categories, bounding box offsets, and relationship types (d).</p><formula xml:id="formula_2">T = 0 T = 1 T = 2 T = N</formula><p>the hidden states. In particular, we choose Gated Recurrent Units <ref type="bibr" target="#b6">[7]</ref> due to its simplicity and effectiveness. We use the hidden state of the corresponding GRU, a high-dimensional vector, to represent the current state of each node and each edge. As all the nodes (edges) share the same update rule, we share the same set of parameters among all the node GRUs, and the other set of parameters among all the edge GRUs <ref type="figure" target="#fig_3">(Fig. 3)</ref>. We denote the current hidden state of node i as h i and the current hidden state of edge i → j as h i→j . Then the mean field distribution can be formulated as</p><formula xml:id="formula_3">Q(x|I, B I ) = n i=1 Q(x cls i , x bbox i |h i )Q(h i |f v i ) j =i Q(x i→j |h i→j )Q(h i→j |f e i→j )<label>(2)</label></formula><p>where f v i is the visual feature of the i-th node, and f e i→j is the visual feature of the edge from the i-th node to the j-th node. In the first iteration, the GRU units take the visual features f v and f e as input ( <ref type="figure" target="#fig_3">Fig. 3(a)</ref>). We use the visual feature of the proposal box as the visual feature f v i for the i-th node. We use the visual feature of the union box over the proposal boxes b i , b j as the visual feature f e i→j for edge i ∈ j. These visual features are extracted by a ROI-pooling layer <ref type="bibr" target="#b11">[12]</ref> from the image. In later iterations, the inputs are the aggregated messages from other GRU units of the previous step. We talk about how the messages are aggregated and passed in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Primal Dual Update and Message Pooling</head><p>Sec. 3.2 offers a generic formulation for solving graph inference problem using RNNs. However, we observe that we can further improve the inference efficiency by leveraging the unique bipartite structure of a scene graph. In the scene graph topology, the neighbors of the edge GRUs are node GRUs, and vice versa. Passing messages along this structure forms two disjoint sub-graphs that are the dual graph to each other. Specifically, we have a node-centric primal graph, in which each node GRU gets messages from its inbound and outbound edge GRUs. In the edge-centric dual graph, each edge GRU gets messages from its subject node GRU and object node GRU <ref type="figure" target="#fig_3">(Fig. 3(b)</ref>). We can therefore improve inference efficiency by iteratively passing messages between these two sub-graphs instead of through a densely connected graph <ref type="figure" target="#fig_3">(Fig. 3(c)</ref>).</p><p>As each GRU receives multiple incoming messages, we need an aggregation function that can fuse information from all messages into a meaningful representation. A naïve approach would be standard pooling methods such as averageor max-pooling. However, we found that it is more effective to learn adaptive weights that can modulate the influences of incoming messages and only keep the relevant information. We introduce a message pooling function that computes the weight factors for each incoming message and fuse the messages using a weighted sum. We provide an empirical analysis of different message pooling functions in Sec. 4.</p><p>Formally, given the current GRU hidden states of nodes and edges h i and h i→j , we denote the messages to update the i-th node as m i , which is computed by a function of its own hidden state h i , and the hidden states of its outbound edge GRUs h i→j and inbound edge GRUs h j→i . Similarly, we denote the message to update the edge from the i-th node to the j-th node as m i→j , which is computed by a function of its own hidden state h i→j , the hidden states of its subject node GRU h i and its object node GRU h j . To be more specific, m i and m i→j are computed by the following two adaptively weighted message pooling functions:</p><formula xml:id="formula_4">m i = j:i→j σ(v T 1 [h i , h i→j ])h i→j + j:j→i σ(v T 2 [h i , h j→i ])h j→i (3) m i→j = σ(w T 1 [h i , h i→j ])h i + σ(w T 2 [h j , h i→j ])h j<label>(4)</label></formula><p>where [·] denotes a concatenation of vectors, and σ denotes a sigmoid function. w 1 , w 2 and v 1 , v 2 are learnable parameters. These two equations describe the primal-dual update rules, as shown in (b) of <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Our final output layers follow closely with the faster R-CNN setup <ref type="bibr" target="#b31">[32]</ref>. We use a softmax layer to produce the final scores for the object class as well as relationship predicate. We use a fully-connected layer to regress to the bounding box offsets for each object class separately. We use the cross entropy loss for the object class and the relationship predicate. We use ℓ1 loss for the bounding box offsets.</p><p>We use an MS COCO-pretrained VGG-16 network to extract visual features from images. We freeze the weights of all convolution layers, and only finetune the fully connected layers, including the GRUs. The node GRUs and the edge GRUs have both 512-dimensional input and output. During training, we first use NMS to select at most 2,000 boxes from all proposed boxes B I , and then randomly select 128 boxes as the object proposals. Due to the quadratic number of edges and sparsity of the annotations, we first sample all edges that have labels. If an image has less than 128 labeled edges, we fill the rest with unlabeled edges. At test time, we use NMS to select at most 50 boxes from the object proposals with an IoU threshold of 0.3. We make predictions on all edges except the self-connections at the test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model on generating scene graphs from images. We compare our model against a recently proposed model on visual relationship prediction <ref type="bibr" target="#b25">[26]</ref>. Our goal is to analyze our model in datasets with both sparse and dense relationship annotations. We use a new scene graph dataset based on the VisualGenome dataset <ref type="bibr" target="#b19">[20]</ref> in our main experiment. We also evaluate our model on the support relation inference task in the NYU Depth v2 dataset. The key difference between these two datasets is that scene graph annotation is very sparse: among all possible pairing of objects, only 1.6% of them are labeled with a relationship predicate. The NYU Depth v2 dataset, on the other hand, exhaustively annotates the support of every labeled object.</p><p>Our experiments show that our model outperforms the baseline model <ref type="bibr" target="#b25">[26]</ref>, and can generalize to other types of relationships, in particular support relations <ref type="bibr" target="#b27">[28]</ref>, without any architecture change.</p><p>Visual Genome We introduce a new scene graph dataset based on the Visual Genome dataset <ref type="bibr" target="#b19">[20]</ref>. The original VG scene graph dataset contains 108,077 images with an average of 38 objects and 22 relationships per image. However, a substantial fraction of the object annotations have poorquality and overlapping bounding boxes and/or ambiguous object names. We manually cleaned up per-box annotations. On average, this annotation refinement process corrected 22 bounding boxes and/or names, deleted 7.4 boxes, and merged 5.4 duplicate bounding boxes per image. The new dataset contains an average of 25 distinct objects and 22 relationships per image. In this experiment, we use the most frequent 150 object categories and 50 predicates for evaluation. As a result, each image has a scene graph of around 11.5 objects and 6.2 relationships. We use 70% of the images for training and the remaining 30% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU Depth V2</head><p>We also evaluate our model on the support relation graphs from the NYU Depth v2 dataset <ref type="bibr" target="#b27">[28]</ref>. The dataset contains 1,449 RGB-D images captured in 27 indoor scenes. Each image is annotated with instance segmentation, region class labels, and support relations between regions. We use the standard split, with 795 images used for training and 654 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Scene Graph Generation</head><p>Setup Given an image, the scene graph generation task is to localize a set of objects, classify their category labels, and predict relationships between each pair of the objects. We evaluate our model on the new scene graph dataset. We analyze our model in three setups below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The predicate classification (PREDCLS) task is to</head><p>predict the predicates of all pairwise relationships of a set of localized objects. This task examines the model's performance on predicate classification in isolation from other factors.</p><p>2. The scene graph classification (SGCLS) task is to predict the predicate as well as the object categories of the subject and the object in every pairwise relationship given a set of localized objects.</p><p>3. The scene graph generation (SGGEN) task is to simultaneously detect a set of objects and predict the predicate between each pair of the detected objects. An object is considered to be correctly detected if it has at least 0.5 IoU overlap with the ground-truth box.</p><p>We adopted the image-wise recall evaluation metrics, R@50 and R@100, that are used in Lu et al. <ref type="bibr" target="#b25">[26]</ref> for all the three setups. The R@k metric measures the fraction of ground-truth relationship triplets (subjectpredicate-object) that appear among the top k most confident triplet predictions in an image. The choice of this metric is, as explained in <ref type="bibr" target="#b25">[26]</ref>, due to the sparsity of the relationship annotations in Visual Genome -metrics like mAP would falsely penalize positive predictions on unlabeled relationships. We also report per-type recall@5 of classifying individual predicate. This metric measures the fraction of the time the correct predicate is among the top 5 most confident predictions of each labeled relationship triplet. As shown in <ref type="table">Table 2</ref>, many predicates have very similar semantic meanings, for example, on vs. over and hanging from vs. attached to. The less frequent predicates would be overshadowed by the more frequent ones during training. We use the recall metric to alleviate such an effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Network Models</head><p>We evaluate our final model and a number of baseline models. One of the key components in our primal-dual formulation is the message pooling functions that use learnt weighted sum to aggregate hidden states of nodes and edges into messages (see Eq. 3 and Eq. 4). In order to demonstrate its effectiveness, we evaluate variants of our model with standard pooling methods. The first is to use averagepooling (avg. pool) instead of the learnt weighted sum to aggregate the hidden states. The second is similar to the first one, but uses max-pooling (max pool). We also evaluate our models against a relationship detection model proposed by Lu et al. <ref type="bibr" target="#b25">[26]</ref>. Their model consists of two components -a vision module that makes predictions from images, and a language module that captures language priors. We compare with their vision module, which uses the same inputs as ours; their language module is orthogonal to our model, and can be added independently. Note that this model is equivalent to our final model without any message passing.  <ref type="table" target="#tab_1">Table 1</ref> shows the performances of our model and the baselines. The baseline model <ref type="bibr" target="#b25">[26]</ref> makes individual predictions on objects and relationships in isolation. The only information that the predicate classifier takes is a bounding box covering the union of the two objects, making it likely to confuse the subject and the object. We showcase some of the errors later in a qualitative analysis. Our final model with learnt weighted sum over the connecting hidden states greatly outperforms the baseline model (18% gain on predicate classification with R@100 metric) and the model variants. This shows that learning to modulate the information from other hidden states enables the network to extract more relevant information and yields superior performances. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the predicate classification performances of our models trained with different numbers of iterations. The performance of our final model peaks at training with two iterations, and gradually degrades afterwards. We hypothesize that this is because as the number of iterations increases, noisy messages start to permeate through the graph and hamper the final prediction. The max-pooling and average-pooling models, on the other hand, barely improve after the first iteration, showing ineffective message passing due to these naïve aggregation methods.</p><p>Finally, <ref type="table">Table 2</ref> shows results of per-type predicate re- The models take images and object bounding boxes as input, and produce object class labels (blue boxes) and relationship predicates between each pair of objects (orange boxes). In order to keep the visualization interpretable, we only show the relationship (edge) predictions for the pairs of objects (nodes) that have ground-truth relationship annotations.</p><p>call. Both the baseline model and our final model perform well in predicting frequent predicates. However, the gap between the models expands for less frequent predicates. This is because our model uses contextual information to cope with the uneven distribution in the relationship annotations, whereas the baseline model suffers more from the skewed distribution by making predictions in isolation.  nal model trained with one iteration is able to resolve some of the ambiguity in the object-subject direction. For example, it predicts (umbrella-on-woman) and (head-of-man) in (b), but it still predicts cyclic relationships like (vase-in-flower-in-vase). Finally, the final model trained with two iterations is able to make semantically correct predictions, e.g., (umbrella-behind-man), and resolves the cyclic relationships, e.g., (vase-with-flower-in-vase).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Qualitative results</head><p>Our model also often predicts predicates that are semantically more accurate than the ground-truth annotations, e.g., our model predicts (man-wearing-hat) in (a) and table-under-vase in (c), whereas the ground-truth labels are (man-has-hat) and (table-has-vase), respectively. The bottom part of <ref type="figure" target="#fig_5">Fig. 5</ref> showcases more qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Support Relation Prediction</head><p>We then evaluate on the NYU Depth v2 dataset <ref type="bibr" target="#b27">[28]</ref> with densely labeled support relations. We show that our model can generalize to other type of relationships and is effective on both sparsely and densely labeled relationships.</p><p>Setup The NYU Depth v2 dataset contains three types of support relationships: an object can be supported by an object from behind, by an object from below, or supported by a hidden object. Each object is also labeled with one of the four structure classes: {floor, structure, furniture, prop}. We define the support graph generation task as to predicting both the support relation type between objects and the structure class of each object. We take the smallest bounding box that encloses an object segmentation mask as its object region. We assume groundtruth object locations in this task.</p><p>We compare our final model with two previous models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref> on the support graph generation task. Following the metric used in previous work, we report two types of support relation accuracies <ref type="bibr" target="#b27">[28]</ref>: type-aware and typeagnostic. We also report the performance with R@50 and R@100 measurements of the predicate classification task introduced in Sec. 4.1. Note that both <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b23">[24]</ref> use RGB-D images, whereas our model uses only RGB images. <ref type="figure">Figure 6</ref>. Sample support relation predictions from our model on the NYU Depth v2 dataset <ref type="bibr" target="#b27">[28]</ref>. →: support from below, ⊸: support from behind. Red arrows are incorrect predictions. We also color code structure classes: ground is in blue, structure is in green, furniture is in yellow, prop is in red. Purple indicates missing structure class. Note that the segmentation masks are only shown for visualization purpose.</p><p>Results Our model outperforms previous work, achieving new state-of-the-art performance using only RGB images. Our results show that having contextual information further improves support relation prediction, even compared to purpose-built models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref> that used RGB-D images. <ref type="figure">Fig. 6</ref> shows some sample predictions using our final model. Incorrect predictions typically occur in ambiguous supports, e.g., books in shelves can be mistaken as being supported from behind (row 1, column 2). Geometric structures that have weak visual features also cause failures. In row 2, column 1, the ceiling at the top left corner of the image is predicted as supported from behind instead of supported below by the wall, but the boundary between the ceiling and the wall is nearly invisible. Such visual uncertainty may be resolved by having additional depth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We addressed the problem of automatically generating a visually grounded scene graph from an image by a novel end-to-end model. Our model performs iterative message passing between the primal and dual sub-graph along the topological structure of a scene graph. This way, it improves the quality of node and edge predictions by incorporating informative contextual cues. Our model can be considered a more generic framework for graph generation problem. In this work, we have demonstrated its effectiveness in predicting Visual Genome scene graphs as well as support relations in indoor scenes. A possible future direction would be to explore its capability in other structured prediction problems in vision and other problem domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Object detectors perceive a scene by attending to individual objects. As a result, even a perfect detector would produce similar outputs on two semantically distinct images (first row). We propose a scene graph generation model that takes an image as input, and generates a visually-grounded scene graph (second row, right) that captures the objects in the image (blue nodes) and their pairwise relationships (red nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An overview of our model architecture. Given an image as input, the model first produces a set of object proposals using a Region Proposal Network (RPN) [32], and then passes the extracted features of the object regions to our novel graph inference module. The output of the model is a scene graph [18], which contains a set of localized objects, categories of each object, and relationship types between each pair of objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An illustration of our model architecture (Sec. 3). The model first extracts visual features of nodes and edges from a set of object proposals, and edge GRUs and node GRUs then take the visual features as initial input and produce a set of hidden states (a). Then a node message pooling function computes messages that are passed to the node GRU in the next iteration from the hidden states. Similarly, an edge message pooling function computes messages and feed to the edge GRU (b). The ⊕ symbol denotes a learnt weighted sum. The model iteratively updates the hidden states of the GRUs (c). At the last iteration step, the hidden states of the GRUs are used to predict object categories, bounding box offsets, and relationship types (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Predicate classification performance (R@100) using our models with different numbers of training iterations. Note that the baseline model is equivalent to our model with zero iteration, as it feeds the node and edge visual features directly to the classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Sample predictions from the baseline model and our final model trained with different numbers of message passing iterations. The models take images and object bounding boxes as input, and produce object class labels (blue boxes) and relationship predicates between each pair of objects (orange boxes). In order to keep the visualization interpretable, we only show the relationship (edge) predictions for the pairs of objects (nodes) that have ground-truth relationship annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 shows qualitative results that compare our final model trained with different numbers of iterations and the baseline model. The results show that the baseline model tends to confuse about the subject and the object in a relationship. For example, it predicts (umbrella-holding-man) in (b) and (counter-on-vase) in (c). Our fi-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>43], we use a generic RNN module to compute</figDesc><table>edge 
GRU 

node 
GRU 

primal 
graph 

edge 
feature 

node 
feature 

node 
state 

outbound 
edge states 

inbound 
edge states 

dual 
graph 

edge 
state 

subject 
state 

object 
state 

edge 
GRU 

node 
GRU 

node 
message 

edge 
message 

node message pooling 

message 
passing 

edge 
GRU 

node 
GRU 

node message 
pooling 

edge message 
pooling 

message 
passing 

edge message pooling 

edge 
GRU 

node 
GRU 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Evaluation results of the scene graph generation task on 
the Visual Genome dataset [20]. We compare a few variations of 
our model against a visual relationship detection module proposed 
by Lu et al. [26] (Sec. 4.1.1). 

[26] avg. pool max pool 
final 

PREDCLS 
R@50 
27.88 
32.39 
34.33 
44.75 
R@100 
35.04 
39.63 
41.99 
53.08 

SGCLS 
R@50 
11.79 
15.65 
16.31 
21.72 
R@100 
14.11 
18.27 
18.70 
24.38 

SGGEN 
R@50 
0.32 
2.70 
3.03 
3.44 
R@100 
0.47 
3.42 
3.71 
4.24 

Table 2. Predicate classification recall. We compare our final 
model (trained with two iterations) with Lu et al. [26]. Top 20 
most frequent types (sorted by frequency) are shown. The evalua-
tion metric is recall@5. 

predicate 
[26] 
ours 
predicate 
[26] 
ours 
on 
99.71 99.25 
under 
28.64 52.73 
has 
98.03 97.25 
sitting on 
31.74 50.17 
in 
80.38 88.30 
standing on 
44.44 61.90 
of 
82.47 96.75 
in front of 
26.09 59.63 
wearing 
98.47 98.23 
attached to 
8.45 
29.58 
near 
85.16 96.81 
at 
54.08 70.41 
with 
31.85 88.10 hanging from 
0.00 
0.00 
above 
49.19 79.73 
over 
9.26 
0.00 
holding 
61.50 80.67 
for 
12.20 31.71 
behind 
79.35 92.32 
riding 
72.43 89.72 

4.1.2 Results 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results of support graph generation task. t-ag stands for type-agnostic and t-aw stands for type-aware.</figDesc><table>Support Accuracy 
PREDCLS 
t-ag 
t-aw 
R@50 R@100 
Silberman et al. [28] 75.9 
72.6 
-
-
Liao et al. [24] 
88.4 
82.1 
-
-
Baseline [26] 
87.7 
85.3 
34.1 
50.3 
Final model (ours) 
91.2 
89.0 
41.8 
55.5 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We would like to thank Ranjay Krishna, Judy Hoffman, JunYoung Gwak, and anonymous reviewers for useful comments. This research is partially supported by a Yahoo Labs Macro award, and an ONR MURI award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistics of 3d object locations in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04143</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning spatial knowledge for text to 3d scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative models for static human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterizing structural relationships in scenes using graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2011 papers</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regionbased convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05298</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d-based reasoning with blocks, support, and stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph cut based inference with co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On support relations and semantic scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05834</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning semantic relationships for better action retrieval in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rossenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Describing common human visual actions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to share visual appearance for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05600</idno>
		<title level="m">Graph-structured representations for visual question answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Contextual priming for object detection. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="169" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3119" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scene understanding by reasoning stability and safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
