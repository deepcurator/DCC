<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image 3D Without a Single 3D Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wajahat</forename><surname>Hussain</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Aragón Institute of Engineering Research (I3A)</orgName>
								<orgName type="institution" key="instit2">Universidad de Zaragoza</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single Image 3D Without a Single 3D Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the image in <ref type="figure">Fig. 1</ref>. When we see this image, we can easily recognize and compensate for the underlying 3D structure: for example, we have no trouble recognizing the orientation of the bookshelves and the floor. But how can computers do this? Traditionally, the answer is to use a supervised approach: simply collect large amounts of labeled data to learn a mapping from RGB to 3D. In theory, this is mathematically impossible, but the argument is that there is sufficient regularity to learn the mapping from data. In this paper, we take this argument one step further: we claim that there is enough regularity in indoor scenes to learn a model for 3D scene understanding without ever seeing an explicit 3D label.</p><p>At the heart of our approach is the observation that images are a product of two separate phenomena. From a graphics point of view, the image we see is a combination of (1) the coarse scene geometry or meshes in our coordinate frame and (2) the texture in some canonical representation that is put on top of these meshes. For instance, the scene in <ref type="figure">Fig. 1</ref> is the combination of planes at particular orientations for the bookshelf and the floor, as well as the fronto-parallel rectified texture maps representing the books and the alphabet tiles. We call the coarse geometry the 3D structure and the texture maps the style <ref type="bibr" target="#b0">1</ref> . In the 3D world these are dis- <ref type="bibr" target="#b0">1</ref> Of course, the books in <ref type="figure">Fig. 1</ref> themselves could be further represented by 3D models. However, in this paper, we ignore this fine change in far structure, and represent the books in terms of their contribution to texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Structure</head><p>Style <ref type="figure">Figure 1</ref>. How can we learn to understand images in a 3D way?</p><p>In this paper, we show a way to do this without using a single 3D label. Our approach treats images as a combination of a 3D model (3D structure) with canonical textures (style) applied on top. In this paper, we learn style elements that recognize texture (e.g., bookshelves, tile floors) rectified to a canonical view. Rather than use explicit supervision, we use the regularity of indoor scenes and a hypothesize-and-verify approach to learn these elements. We thus learn models for single image 3D without seeing a single explicit 3D label. 3D model from <ref type="bibr" target="#b17">[18]</ref>.</p><p>tinct, but when viewed as a single image, the signals for both get mixed together with no way to separate them. Based on this observation, we propose style elements as a basic unit of 3D inference. Style elements detect the presence of style, or texture that is correctly rectified to a canonical fronto-parallel view. They include things like cabinets, window-blinds, and tile floors. We use these style elements to recognize when a texture has been rectified to fronto-parallel correctly. This lets us recognize the orientation of the scene in a hypothesize-and-verify framework: for instance, if we warp the bookshelf in <ref type="figure">Fig. 2</ref> to look as if it is facing right, our rectified bookshelf detector will respond strongly; if we warp it to look as if it is facing left, our rectified bookshelf detector will respond poorly.</p><p>In this paper, we show that we can learn these style elements in an unsupervised manner by leveraging the regular-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image Style Elements</head><p>Final Interpretation Rectified to Scene Directions <ref type="figure">Figure 2</ref>. We infer a 3D interpretation of a new scene with style elements by detecting them in the input image rectified to the main directions of the scene. For instance, our bookshelf style-element (orange) will respond well to the bookshelf when it is rectified with the correct direction (facing leftwards) and poorly when it is not. We show how we can automatically learn these style elements, and thus a model for 3D scene understanding without any 3D supervision. Instead, the regularity of the world acts as the supervisory signal.</p><p>ity of the world's 3D structure. The key assumption of our approach is that we expect the structure of indoor scenes to resemble an inside-out-box on average: on the left of the image, surfaces should face right and in the middle, they should face us. We show how this prior belief can validate style elements in a hypothesize-and-verify approach: we propose a style element and check how well its detections match this belief about 3D structure over a large set of unlabeled images; if an element's detections substantially mismatch, our hypothesis was probably wrong. To the best of our knowledge, this is the first paper to propose an unsupervised learning-based approach for 3D scene understanding from a single image. Why unsupervised? We wish to show that unsupervised 3D learning can be effective for predicting 3D. We do so on two datasets: NYUv2, a standard 3D dataset, and Places-205, which contains scenes not covered by Kinect datasets, such as supermarkets and airports. Our method is unsupervised and does not use any training data or any pre-trained geometry models; nevertheless: (1) Our method nearly matches comparable supervised approaches on NYUv2: it is within &lt; 3</p><p>• of 3DP <ref type="bibr" target="#b15">[16]</ref> and better in many metrics on vertical regions. (2) When fused with 3DP, our method achieves state-of-the-art results in 4/6 metrics on NYUv2. (3) As an unsupervised approach, our method can learn from unlabeled Internet images like Places. This is fundamentally impossible for supervised methods, which must resort to pre-trained models and suffer performance loss from the domain shift. Our approach can use this data and outperforms 3DP by 3.7%. Why Style Elements? Operating in this style space lets us learn about the world in a viewpoint-independent fashion. In this paper, we show how this enables us to learn unsupervised models for 3D, but we see broader advantages to this: first, we can detect particular combinations of style and structure that were not present at training time, which is impossible in many existing models; second, since our style elements are viewpoint-independent, we can share information across different viewpoints. We illustrate these advantages in <ref type="figure" target="#fig_0">Fig. 3</ref>: our method learns one element for all the orientations of the cabinets, but a standard viewercentric approach learns one element per orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of predicting the 3D structure or layout from a single image is arguably as old as computer vision. Early work used extracted contours <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7]</ref> or geometric primitives <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> and rules to infer structure. However, these primitives were too difficult to detect reliably in natural images, and the community moved towards learning-based approaches. Over the past decade, one dominant paradigm has emerged: at training time, one takes a large collection of images and 3D labels and learns a mapping between the two. The argument for this paradigm is that scenes are sufficiently regular so that such a mapping can be learned from data. The mapping is often learned over segments <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref>, discriminative patches <ref type="bibr" target="#b15">[16]</ref>, or pixels <ref type="bibr" target="#b25">[26]</ref>. At test time, this local mapping is used on a single image to infer the 3D labels; in other works, it is again presumed that there is such regularity that one can impose even more top-down constraints, such as the Manhattan-world assumption <ref type="bibr" target="#b7">[8]</ref>, an indoor box model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref>, or others <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b43">44]</ref>. In this work, we tackle the same class of problem, but show that there is enough regularity to even do unsupervised learning of models. In particular, we do not use an explicit 3D supervisory signal at any point. Additionally, our method learns across viewpoints, unlike most work on single-image 3D which learn view-dependent representations. The most related work among these methods is <ref type="bibr" target="#b24">[25]</ref>, which recognizes regions at canonical depths; in contrast, our method is unsupervised and predicts surface normals.</p><p>Our approach uses visual elements discovered from a large dataset and draws from a rich literature on discriminative patch-discovery <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref>. Like Juneja et</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Primitives Element 1 Element 2 Element 3 Element 4 Element 5 Style Elements</head><p>One Canonical Element al. <ref type="bibr" target="#b23">[24]</ref>, we take a hypothesize-and-verify approach which filters a large set of candidate patch hypotheses by patch detections on a dataset. Among these works, our work is most closely related to discriminative patches for 3D <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref> or visual-style-sensitive patches <ref type="bibr" target="#b28">[29]</ref>. These frameworks, however, capture the Cartesian product of appearance and the label (style or 3D), meaning that for these frameworks to capture an oven-front at a particular angle, they need to see an oven-front at that particular angle. On the other hand, our approach analytically compensates for 3D structure by rectifying the image data. Thus our elements can predict labels not seen at training time (e.g., an oven at a previously unseen angle). We illustrate this in <ref type="figure" target="#fig_0">Fig. 3</ref>.</p><p>Warping images to a canonical view has been used to boost performance of local patch descriptors for tasks like location recognition <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38]</ref>, in which 3D structure is known or estimated via pre-trained models, or in detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14]</ref>, in which it is given at training time. Our work, on the other hand, is unsupervised and jointly reasons about 3D structure and style.</p><p>The idea of figuring out the 3D structure by optimizing properties of the unwarped image has been used in shapefrom-texture (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>) and modern texture analysis <ref type="bibr" target="#b42">[43]</ref> and compression <ref type="bibr" target="#b38">[39]</ref> approaches. These works are complementary to our own: many obtain a detailed interpretation on presegmented regions or in specific domains by optimizing some criterion such as regularity within one image or a single domain. Our style elements on the other hand, are discovered automatically via the regularity in large amounts of data, and are more general than instancelevel texture patterns. They can further interpret novel, generic non-presegmented scenes, although our interpretations on these cluttered scenes are more coarse in comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Given a dictionary of discovered style elements, we can use this dictionary of detectors in rectified images to determine the orientation of surfaces: the elements only respond when the scene is rectified correctly. But how do we obtain this dictionary of correctly rectified style elements if we do not have 3D labels?</p><p>In Section 4.2, we show how to solve this chicken-andegg problem with a hypothesize-and-verify approach: we hypothesize a style element, run it on the dataset, and check whether its pattern of detections is plausible. We evaluate the style element's detections by comparing it with a prior that assumes that the world is an inside-out-box. Training thus takes a collection of RGB images as input, and produces a dictionary of detectors as output. In Section 4.3, we describe how to use these style elements to interpret a new image: we run our style elements in a new image, and the detector responses vote for the underlying structure.</p><p>As this work is unsupervised, we make some assumptions. We use the Manhattan-world assumption <ref type="bibr" target="#b7">[8]</ref> to reduce our label space to three orthogonal directions; we find these directions and rectification homographies for them using vanishing points estimated by <ref type="bibr" target="#b19">[20]</ref>. We note, however, that there can be other directions present; we simply do not learn or detect style elements on them. We further assume that the images are upright so we can process the horizontal and vertical directions separately. Finally, our method models each style element as having a single label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Our method begins with a discovery set of images and finds style elements that will help us interpret a new image. This task entails determining the orientation of surfaces throughout the discovery set so that we can obtain fronto-parallel rectified representations of the texture.</p><p>Since we have no explicit 3D labels, this task seems hopeless: in theory, each part of each image could face any direction! We take a hypothesize-and-verify approach that lets us inject knowledge via a prior on the 3D structure of scenes. We guess a large number of style elements by rectifying the images and sampling patches. Most guesses are wrong, but some are right. We identify the correct ones by computing agreement between our prior and what each hypothesis would imply about the 3D structure of the world. <ref type="figure">Figure 4</ref>. Selected style elements automatically discovered by our method. In each, we show the element on the left and its top detections on the discovery set; these and other detections are used to identify good and bad style elements. Notice that the top detections of most vertical style elements have a variety of orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vertical Horizontal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Prior</head><p>Consider the TV on the top-left of <ref type="figure">Fig. 4</ref>. How can we know that it is a good style element (i.e., rectified to be fronto-parallel) without knowing the underlying 3D of the image? While we do not know the 3D at that location, if we looked at the whole discovery set, we would observe a distinct pattern in terms of where TVs appear and in what direction they face: due to the regularity of human scenes, TVs on the left-hand-side of the image tend to face rightwards; on the right-hand-side, they tend to face leftwards. Thus, if we were to run our TV detector over the discovery set, we would expect to see this same distribution. On the other hand, it would be suspicious if we had a detector that only found leftwards facing TVs irrespective of where they appear in the image. We now explain how to formalize this intuition by constructing a prior that gives a probability of each orientation as a function of image location; this lets us score hypothetical style elements by their detections.</p><p>Our goal is to build a prior that evaluates the likelihood of a surface orientation as a function of pixel coordinate. Our overarching assumption is that our images are taken with an upright camera inside a box. Then, as in <ref type="bibr" target="#b21">[22]</ref>, we factor the question of orientation into two independent questions -"is the region vertical or horizontal?" and "if it is vertical, which vertical direction does it face?". We then assume the probability of vertical/horizontal depends on the y-coordinate in the image. For the vertical direction, we note that if we assume the world is a box, we can determine how likely each vertical direction is at each pixel as a function of its x coordinate.</p><p>We formalize this prior as follows, proceeding analytically since we do not have access to data. Since we expect horizontal surfaces like floors to be more common at  <ref type="figure">Figure 5</ref>. Selecting hypotheses by their detections. We compare hypothesized style elements' detections with our prior. We show a good (left) and a bad (right) style hypothesis in red squares. For each, we show a scatter plot of their detections on the discovery set, plotting the x-location in the image on the x-axis and how left-vs-right the detection is on the y-axis. We illustrate a few of these detections: for instance, the bedroom scene in the middle is a leftwards facing detection on the right-side of the image. On the far left, we show what our prior expects -a steady change from right-facing to left-facing. We rank elements by how well their detections match this prior: for instance, we reject the incorrect style element on the right since it predicts that everything faces left, which is unlikely under the prior. the bottom of the image, we model the vertical/horizontal distinction with a negative exponential on the y-location, ∝ exp(−y 2 /σ 2 ). Since the camera is upright, the horizon determines the sign of the horizontal directions. For vertical directions, we assume the camera is inside a box with aspect ratio ∼ Uniform <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and all rotations equally likely. The likelihood of each direction (left-to-right) as a function of x location can then be obtained in a Monte-Carlo fashion: we histogram normals at each location over renderings of 100K rooms sampled according to the assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hypothesizing-and-Verifying Style Elements</head><p>Now that we have a way to verify a style element, we can use it in a hypothesize-and-verify approach. We first explain how we generate our hypotheses and then how we use the prior introduced in the the previous section to verify hypothesized style patches.</p><p>We first need a way to generate hypotheses. Unfortunately, there are an infinite number of possible directions to try at each pixel. However, if we assume the world is a box, our search space is dramatically smaller: there are only 6 possible directions and these can be obtained by estimating Manhattan-world vanishing points in the image. Once we have rectified the image to these main scene directions, we sample a large collection (≈ 25K total) of patches on these rectified images. Each patch is converted to a detector via an ELDA detector <ref type="bibr" target="#b18">[19]</ref> over HOG <ref type="bibr" target="#b8">[9]</ref>. Most patches will be wrong because the true scene geometry disagrees with them. One wrong hypothesis appears on the right of <ref type="figure">Fig. 5</ref> in which a shelf has been rectified to the wrong direction.</p><p>We sift through these hypotheses by comparing what their detection pattern over the discovery set implies about 3D structure with our prior. For instance, if a style patch corresponds to a correctly rectified TV monitor, then our detections should, on average, match our box assumption. If it corresponds to an incorrectly rectified monitor then it will not match. We perform this by taking the ELDA detector for each patch and looking at the location and implied orientations of the top 1K detections over the training set. Since our prior assumes vertical and horizontal are separate questions, we have different criteria for each. For vertical surfaces, we compute average orientation as a function of x location and compare it to the average orientation under the prior, using the mean absolute difference as our criterion. For horizontal surfaces, our prior assumes that x location is independent from horizontal sign (i.e., floors do not just appear on the left); we additionally do not expect floor to share many style elements with ceilings. We thus compute the correlation between x and horizontal sign and the purity of up-vs-down labelings in the top firings. We illustrate this for two hypothesized vertical style elements in <ref type="figure">Fig. 5</ref>.</p><p>We use these criteria to rank a collection of hypothetical vertical and horizontal style elements. Our final model is the top 500 from each. We show some of the style elements we discover on NYU v2 in <ref type="figure">Fig. 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference</head><p>Given a new image and our style elements, we combine our prior and detections of the style elements to interpret the scene. We extract three directions from vanishing points to get our label space and run the style elements on the rectified images. The detections and the prior then vote for the final label. We maintain a multinomial distribution at each pixel over both whether the pixel is horizontal-vsvertical and the vertical direction. Starting with the prior, we add a likelihood from detections: we count overlapping detections agreeing with the direction, weighted by score. We then take the maximum response, deciding whether the pixel is horizontal or vertical, and if the latter, the vertical orientation.</p><p>Our method produces good interpretations in many places, but does not handle ambiguous parts like untextured carpets well. These ambiguities are normally handled by transferring context <ref type="bibr" target="#b15">[16]</ref> or doing some form of learned rea-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Prediction</head><p>Input Prediction Input Prediction <ref type="figure">Figure 6</ref>. Sample results on NYUv2. First two rows: selected; last two row: random. In row 1, notice that the sides of objects are being labeled (discernible via conflicting colors), even though this severely violates the prior. In row 2, notice that our predictions can extend across the image or form convex corners: even though our prior is a box, we ignore in light of evidence from style elements.</p><p>Input GT 3DP Prior Proposed Fusion Fus. Choice <ref type="figure">Figure 7</ref>. Comparison with the supervised 3DP method. The methods have complementary errors and we can fuse their results: 3DP often struggles with near-perpendicular surfaces; however these are easy to recognize once rectified by the proposed method. Our method has more trouble distinguishing floors from walls. We show the fusion result and which prediction is being used (Red: proposed; blue: 3DP).</p><p>soning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref>. Without explicit 3D signal, we rely on unsupervised label propagation over segments: we extract multiple segmentations by varying the parameters of <ref type="bibr" target="#b12">[13]</ref> 2 . Each segment assumes the mode of its pixels, and the final label of a pixel is the mode over the segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details:</head><p>We finally report a number of implementation details of our method; more details appear in the supplement. Patch representation: Throughout the approach, we use HOG features <ref type="bibr" target="#b8">[9]</ref> with a 8 × 8 pixel cells at a canonical size of 80 pixels. Rectification: we obtain Manhattan-world vanishing points from <ref type="bibr" target="#b19">[20]</ref> and rectify following <ref type="bibr" target="#b41">[42]</ref>: after autocalibration, the remaining parameters up to a similarity transform are determined via vanishing point orthogonality; the similarity transform is handled by aligning the Manhattan 2 (σ = 0.5, 1, 1.5, 2; k = 100, 200; min = 50, 100) directions with the image axes and by operating at multiple scales. Sample rectified images appear in <ref type="figure">Fig. 2</ref>: our detectors are discovered and tested on these images. At test time, we max-pool detector responses over multiple rectifications per vertical direction. Initial Patch Pool: Our hypotheses are obtained by rectifying each image in the discovery set to the scene directions and following the sampling strategy of <ref type="bibr" target="#b36">[37]</ref> while rejecting patches whose corresponding quadrilateral has area &lt; 100 2 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Validation</head><p>We now describe experiments done to validate the approach. We are guided by the following questions: (1) How well does the method work? (2) Can the approach be combined with supervised methods? and (3) Are there scenarios that only an unsupervised approach can handle?</p><p>To answer the first two questions, we use the NYUv2 <ref type="bibr" target="#b35">[36]</ref> dataset, which has gained wide acceptance; we find that our method does nearly as well as a comparable supervised method and that a simple learned fusion of the methods matches or surpasses the state-of-the-art in 4/6 metrics among methods not using the larger video dataset. To answer the final question, we use the Places-205 dataset <ref type="bibr" target="#b44">[45]</ref>, which has many locations not covered by Kinect datasets. Supervised approaches must resort to a model trained on existing datasets, but our method can adapt to the dataset. We find that this enables us to outperform a comparable supervised method by a large margin (3.7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on NYU v2</head><p>We first document our experimental setup. We follow standard protocols and compare against the state-of-the-art. Data: We use the standard splits of <ref type="bibr" target="#b35">[36]</ref>. We use the ground-truth normals from <ref type="bibr" target="#b25">[26]</ref> but found similar conclusions on those from <ref type="bibr" target="#b15">[16]</ref>. Evaluation Criteria: As introduced by <ref type="bibr" target="#b15">[16]</ref>, we evaluate results on a per-pixel basis over all over valid pixels. We report the mean, median, RMSE and the fraction of pixels with error below a threshold t, or PGP-t (percent good pixels) for t = 11.25</p><p>• , 22.5</p><p>• , 30</p><p>• . Like <ref type="bibr" target="#b21">[22]</ref>, our model breaks the task into a vertical/horizontal problem and a vertical subcategory problem. We evaluate both, and evaluate the vertical task on surfaces within 30</p><p>• of the y-axis. Baselines: We stress that our goal as an unsupervised method is not to outperform supervised methods but instead to show that our approach is effective. We report results of all methods that could be considered state-of-the-art at the time of submission, including even those from methods using the much larger video dataset <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12]</ref>. The most informative comparison is with the Manhattan-world version of 3DP <ref type="bibr" target="#b15">[16]</ref> because it keeps two sources of variance fixed, the base feature and the Manhattan-world assumption. Combining with supervised methods: We learn a model, termed 3DP+Prop that fuses our method with 3DP. Following <ref type="bibr" target="#b29">[30]</ref>, we learn random forests (100 trees, crossvalidated min-children) on training data to predict whether each method's outputs are within 22.5</p><p>• of the ground-truth. We train separate forests for our method before segmentation propagation, its vertical predictions only, and 3DP. We use for features the confidences and normals from all methods and the image location. At test time, we take the prediction that is most likely to be correct. Results: We first report qualitative results in <ref type="figure">Fig. 6</ref>. Our method is unsupervised but obtains an accurate interpretation on most scenes. The method frequently picks up on small details, for instance, the right-wards facing chair back (1st row, left) and the side of the shelving in (1st, middle). These small details, as well as the correct inwards-pointing box of the refrigerator (2, middle), are unlikely under the box prior and demonstrate that the method is not simply regurgitating the prior. Our unsupervised propagation is We report quantitative results in <ref type="table" target="#tab_0">Table 1</ref>. Our method is always within 2.5</p><p>• of the most immediately comparable supervised method, 3DP, and is always within 3.8</p><p>• of <ref type="bibr" target="#b16">[17]</ref>, which fuses multiple models in a complex MIQP formulation. We also substantially outperform <ref type="bibr" target="#b27">[28]</ref>, the other unsupervised method for this task. Our simple fusion of 3DP and our method obtains state-of-the-art results in 4/6 metrics among methods using only the original 795 training images. Since the Manhattan-world assumption is rewarded by some metrics and not by others, fair comparison with non-Manhattan-world methods such as <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b11">12]</ref> is difficult: e.g., on PGP-11.25, due to our use of vanishing points, our method is within 2.4% of <ref type="bibr" target="#b11">[12]</ref>, which uses orders of magnitude more data.</p><p>We report ablative analysis in <ref type="table">Table 2</ref>: we greatly outperform relying on only the prior (i.e., not using detection ev-  idence), especially on the median, and segmentation helps but does not drive performance. Training 3DP using label maps from the prior yielded worse performance. This is because 3DP relies heavily on junctions and edges in the normal map, and the the prior does not provide this detailed information. We found our method to be insensitive to parameters: changing the box prior's aspect ratio or the prior weight by a factor of two yields changes &lt; 0.7</p><p>• and &lt; 0.6% across metrics; many settings produced better results than the settings used (more details in the supplement).</p><p>Our result is better in relative terms on the vertical task, as can be seen in in <ref type="table">Table 3</ref>: it matches 3DP in 2 metrics and bests it in 3. This is because many indoor horizontal surfaces are defined by location and lack of texture, which our single-plane HOG patch approach cannot leverage; 3DP, on the other hand, learns structured patches and its vocabulary captures horizontal surfaces via edges and corners. Again, fusing our method with 3DP improves results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Internet Images</head><p>We now investigate tasks that only an unsupervised method can do. Suppose one wants to interpret pictures of supermarkets, airport terminals, or another place not covered by existing RGBD datasets. The only option for a supervised approach (besides collecting new data at great expense) is to use a pre-trained model. However, with our unsupervised 3D approach, we can learn a model from images alone. Data: We collected a subset of 10 categories 3 from the Places-205 dataset <ref type="bibr" target="#b44">[45]</ref> that are not present in 3D datasets and annotated them with Manhattan-world labelings. We 62.9 took at most 700 images from each category, and set aside 200 for evaluation. We sparsely annotated 10 superpixels in each image, each randomly selected to avoid bias; each could be labeled as one of the 6 possible Manhattan normals or passed if multiple directions were present. Since our label space is Manhattan world, we removed images where vanishing points could not be estimated, identified as ones where the independent estimates of <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20]</ref> disagree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We learned an unsupervised model on each category and compared its performance with models pretrained on NYU, including 3DP and a fully convolutional variant of <ref type="bibr" target="#b39">[40]</ref> which obtains within 1.1% PGP-30 on NYU. Note that standard supervised methods cannot compensate for this shift. We show qualitative results illustrating the dataset and our approach in <ref type="figure" target="#fig_2">Fig. 8</ref>: even NYU contains no washing machines, but our approach can learn elements for these from Internet data, as seen in <ref type="figure" target="#fig_3">Fig. 9</ref>. On the other hand, pretrained methods struggle to adapt to this new data. We report results in <ref type="table" target="#tab_1">Table 4</ref>. Our approach outperforms 3DP, pretrained on NYUv2, in 9/10 categories and overall by 3.7% and outperforms <ref type="bibr" target="#b39">[40]</ref> in 4/10 categories, with gains as large as 3.9%. Our labels are sparse so we verify the gain over 3DP with a 95% bootstrapped confidence interval; our approach consistently outperforms 3DP ([2.7, 4.8]).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An illustration of sharing enabled by style elements. Top: elements from a 3DP model; Bottom: (left) a style element and (right) selections from its top 15 discovery-set detections. 3DP detects each and every cabinet orientation with a separate element because it is viewer-centric; our model compactly recognizes all cabinets orientations with one element.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Results on Places-205. Note the stark contrast with NYUv2. Our method can learn from this new data despite a lack of labels. Museum Subway Supermarket Laundromat Locker Rm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Example vertical elements learned on Internet images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Evaluation on all pixels on NYU v2. The most infor- mative comparison is with 3DP and UNFOLD. Our unsupervised approach nearly matches 3DP and in combination with 3DP, ob- tains state-of-the-art results in 4/6 metrics. Starred methods do not use the Manhattan-world assumption.Table 3. Vertical evaluation. Our method outperforms a 3DP on 3/6 metrics, but fusing the two gives a substantial boostProposed 33.9 19.7 46.5 35.1 53.2 59.7 3DP [16] 33.9 19.9 46.5 36.4 52.6 58.8 3DP+Prop. 32.1 18.0 44.6 37.5 55.2 61.5 sometimes too aggressive, as seen in (2nd, right, under the blinds; 3rd, left, on the floor), but helps with boundaries.</figDesc><table>Summary Stats. ( 
• ) % Good Pixels 
(Lower Better) 
(Higher Better) 
Mean Median RMSE 11.25 
• 22.5 
• 30 

• 

3DP+Prop. 
34.6 17.5 48.7 40.6 54.7 59.7 
Ladicky 
 *  [26] 33.5 23.1 44.6 27.7 49.0 58.7 
UNFOLD [17] 35.2 17.9 49.6 40.5 54.1 58.9 
3DP [16] 
36.3 19.2 50.4 39.2 52.9 57.8 
Proposed 
38.6 21.7 52.6 36.7 50.6 55.4 
Lee et al. [28] 43.8 35.8 55.8 26.8 41.2 46.6 
(With External Data) 
Wang [40] 
26.9 14.8 -NR-42.0 61.2 68.2 
Eigen 
 *  [12] 
23.7 15.5 -NR-39.2 62.0 71.1 

Table 2. Ablative analysis 

Mean Median RMSE 11.25 
• 22.5 
• 30 

• 

Full 
38.6 21.7 52.6 36.8 50.6 55.4 
No Segm. 
39.6 23.4 53.5 35.7 49.3 54.0 
Prior 
43.2 30.2 56.7 33.1 45.2 49.8 

3DP on Prior 41.7 27.0 55.6 34.6 47.1 51.6 

Summary Stats. ( 
• ) % Good Pixels 
(Lower Better) 
(Higher Better) 
Mean Median RMSE 11.25 
• 22.5 
• 30 

• 

Prior 
39.3 26.0 52.0 46.7 46.7 53.2 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Accuracy on Places-205, subdivided by category. Airport Art Gallery Conference. Locker Rm. Laundromat</figDesc><table>3DP 
50.1 
64.2 
63.0 
65.9 
65.1 
FC-[40] 
51.6 
70.3 
71.2 
69.4 
71.9 
Prop. 
54.0 
71.1 
64.3 
67.8 
71.4 

Museum 
Restaurant Shoe Shop Subway Supermarket 
Avg. 

58.9 
57.6 
59.9 
58.2 
49.3 

59.2 

61.2 
63.2 
64.0 
56.2 
57.7 

63.7 

64.0 
60.5 
62.1 
52.3 
61.9 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Airport, art gallery, conference room, locker room, laundromat, museum, restaurant, shoe shop, subway, supermarket</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the 3D layout of a cluttered room from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual perception by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Binford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Systems and Controls</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ACRONYM model-based vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Creiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Binford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Layout estimation of highly cluttered indoor scenes using geometric and semantic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIAP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding indoor scenes using 3D geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On seeing things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clowes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Manhattan world assumption: Regularities in scene statistics which enable bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mid-level visual element discovery as discriminative mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What makes Paris look like Paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>SIGGRAPH</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4734</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">3D object detection and viewpoint estimation with a deformable 3D cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape from texture without boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-driven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support surface prediction in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thinking inside the box: Using appearance models and context based on room geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Impossible objects as nonsense sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="475" to="492" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blocks that shout: Distinctive parts for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Style-aware mid-level representation for discovering visual connections in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Brostow. Learning a confidence measure for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Computing local surface orientation and shape from texture for curved surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shape anchors for data-driven multi-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Machine perception of 3D solids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient Exact Inference for 3D Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">MatchBox: Indoor Image Matching via Box-like Scene Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Srajer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Factoring repeated content within and among images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">3D model matching with viewpoint-invariant patches (VIP)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shape from angular regularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tilt: Transforminvariant low-rank textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
