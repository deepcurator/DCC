<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perturbative Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
							<email>felixu@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><forename type="middle">Naresh</forename><surname>Boddeti</surname></persName>
							<email>vishnu@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Perturbative Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> (MNIST,  PASCAL VOC, and ImageNet)  <p>with fewer parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs) have been overwhelmingly successful across a variety of visual perception tasks. The past several years have witnessed the evolution of many successful CNN architectures such as AlexNet <ref type="bibr" target="#b13">[14]</ref>, VGG <ref type="bibr" target="#b26">[27]</ref>, GoogLeNet <ref type="bibr" target="#b29">[30]</ref>, ResNet <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, MobileNet <ref type="bibr" target="#b9">[10]</ref>, and DenseNet <ref type="bibr" target="#b10">[11]</ref>, etc. Much of this effort has been focused on the topology and connectivity between convolutional and other modules while the convolutional layer itself has continued to remain the backbone of these networks. Convolutional layers are characterized by two main properties <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref>, namely, local connectivity and weight sharing, both of which afford these layers with significant computational and statistical efficiency over densely connected layers. Ever since the introduction of AlexNet <ref type="bibr" target="#b13">[14]</ref>, there has been steady refinements to a standard convolutional layer. While AlexNet utilized convolutional filters with large receptive fields <ref type="bibr">(11 × 11, 5 × 5, etc.)</ref>, the VGG network <ref type="bibr" target="#b26">[27]</ref> demonstrated the utility of using convolutional weights with very small receptive fields (3 × 3) that are both statistically and computationally efficient for learning deep convolutional neural networks. As convolutional layers are often the main computational bottleneck of CNNs, there has been steady developments in improving the computational efficiency of convolutional layers. MobileNets <ref type="bibr" target="#b9">[10]</ref> introduced efficient reparameterization of standard 3 × 3 convolutional weights, in terms of depth-wise convolutions and 1 × 1 convolutions. Convolutional networks with binary weights <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23]</ref> have been proposed to significantly improve the computational efficiency of CNNs. Recent work has also demonstrated that sparse convolutional weights <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18]</ref> perform comparably to dense convolutional weights while also being computationally efficient. However, across this entire body of work the basic premise of a convolutional layer itself has largely remained unchanged.</p><p>This paper seeks to rethink the basic premise of the necessity of convolutional layers for the task of image classification. The success of a wide range of approaches that utilize convolutional layers that have, a) very small receptive fields (3 × 3), b) sparse convolutional weights, and c) convolutional weights with binary weights, motivates our hypothesis that one can perhaps completely do away with convolutional layers for learning high performance image classification models. We propose a novel module, dubbed the perturbation layer <ref type="bibr" target="#b0">1</ref> , that conforms to our hypothesis and is devoid of standard convolutional operations. Given an input, the perturbation layer first perturbs the input additively through random, but fixed, noise followed by a weighted combination of non-linear activations of the input perturbations. The weighted linear combinations of activated perturbations are conceptually similar to 1×1 convolutions, but are not strictly convolutional since their receptive field is just one pixel, as opposed to the receptive fields of standard convolutional weights. This layer is thus an extreme version of sparse convolutional weights with sparsity of one non-zero element and fixed non-zero support at the center of the filter. Avoiding convolutions with receptive fields larger than one offers immediate statistical savings in the form of fewer learnable network parameters, computational savings from more efficient operations (weighted sum vs. convolution) and more importantly allows us to rethink the premise and utility of convolutional layers in the context of image classification models. Our theoretical analysis shows that the perturbation layer can approximate the response of a standard convolutional layer. In addition, we empirically demonstrate that deep neural networks with the perturbation layers as replacements for standard convolutional layers perform as well as an equivalent network with convolutional layers across a variety of datasets of varying difficulty and scale, MNIST, CIFAR-10, PASCAL VOC, and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a huge body of work on the design and applications of CNNs for image classification, the full treatment of which is beyond the scope of this paper. We will however note a few major advances that were motivated by improving the performance of these networks, such as, AlexNet <ref type="bibr" target="#b13">[14]</ref>, VGG <ref type="bibr" target="#b26">[27]</ref>, GoogLeNet <ref type="bibr" target="#b29">[30]</ref>, Residual Networks <ref type="bibr" target="#b7">[8]</ref>, etc.</p><p>The idea of using convolutional weights with small receptive fields is not new. While the VGG <ref type="bibr" target="#b26">[27]</ref> network was the first model to demonstrate the efficacy of small convolutional weights in deep CNNs, other researchers have explored the use of small convolutional weights, including 1 × 1 convolutional weights. For instance the GoogLeNet <ref type="bibr" target="#b29">[30]</ref> architecture comprises of weights with different receptive fields including 1 × 1 weights. The Network in Network architecture <ref type="bibr" target="#b18">[19]</ref> also utilizes 1 × 1 convolutions. However, all of these approaches have used 1 × 1 convolutions in conjunction with convolutional filters with larger receptive fields. In contrast the perturbative layer that we introduce in this paper is devoid of any convolutional layers with receptive fields larger than one pixel and combines information from multiple noise perturbed versions of the input.</p><p>Efficient characterization of convolutional layers have also been proposed from the perspective of computational efficiency. Networks with binary weights <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref>, networks with sparse convolutional weights <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18]</ref>, networks with efficient factorization of the convolutional weights <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> and networks with a hybrid of learnable and fixed weights <ref type="bibr" target="#b11">[12]</ref>. While the proposed perturbation layer does offer computational benefits in terms of fewer parameters and more efficient inference our aim in this paper is to motivate the need to rethink the premise and utility of a standard convolutional layer for image classification tasks. The proposed perturbation layer serves as evidence that perhaps convolutional layers are not very critical to learning image classification models that can perform as well as, if not better than, standard convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first detail the motivation and formulation of the proposed perturbative neural networks (PNN), and then discuss its relation to standard CNNs from both a macro as well as a micro viewpoint. Finally, we discuss some properties associated with PNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting LBCNN and an Observation</head><p>Recently, local binary convolutional neural networks (LBCNN) (as in <ref type="bibr" target="#b11">[12]</ref>) have been motivated from the local binary patterns (LBP) descriptor. The basic LBCNN module is shown in the middle row of <ref type="figure" target="#fig_1">Figure 1</ref>, where the input image (or tensor at subsequent layers) is first convolved with a set of fixed, randomly generated sparse binary filters, and the resulting response map is propagated through a nonlinear activation, such as ReLU, and then, the ReLU activated response map is linearly combined to generate an output feature map that feeds into the next layer. These linear combination weights are the only learnable parameters.</p><p>Specifically, let us assume that the input image (or tensor) Then the transfer function between input and output of layer l can be expressed as:</p><formula xml:id="formula_0">x t l+1 = m i=1 σ relu s b s l,i * x s l · W t l,i (1)</formula><p>where t is the output channel, s is the input channel, and * is the channel-wise convolution operation. Again, linear weights W are the only learnable parameters of an LBCNN layer. In this way, LBCNN can yield much lower model complexity with significant savings in the number of learnable parameters. LBCNN's idea of formulating a deep learning model with a hybrid of fixed convolutional weights and learnable linear combination weights in each layer is intriguing. In <ref type="figure">Figure 2</ref>, we can see that a 3 × 3 patch from the kitten image is extracted, and the pixels are labeled as x 1 , . . . , x 9 . This patch, in LBCNN, will first be convolved with a binary filter, which is shown on the far right as an example. Since the filter itself is binary with +1 and −1, the resulting scalar on the response map will simply be the additions and subtractions among the 9 neighboring pixels. In this case, it reduces to y = x 1 + x 3 + x 5 + x 7 + x 9 − x 2 − x 4 − x 6 − x 8 . The same process repeats for the next 3 × 3 patch until the entire response map is generated.</p><p>Mathematically, this convolution operation is transforming the patch center pixel x c = x 5 to one particular point y </p><formula xml:id="formula_1">W l V l CNN PNN LBCNN (a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+1 -1 +1</head><p>Figure 2: Convolving with a binary filter is equivalent to addition and subtraction among neighbors within the patch. Similarly, convolving with a real-valued filter is equivalent to the linear combination of the neighbors using filter weights.</p><p>on the response map: y = f (x c ), and in this case, the function f is defined as above by including 8 neighboring pixels of x c as well as the binary filter itself. which determines the pixels that are added or subtracted. The same notion can be easily extended towards standard convolution operations with real-valued convolutional filters, either learnable or nonlearnable, and then the additions and subtractions among the neighboring pixels become a linear weighted combinations. This seemingly simple inner product operation f , whether it has been efficiently implemented in the frequency domain or spatial domain, is a major computational bottleneck of deep CNN models. The key takeaway concept from LBCNN is that the convolutional weights could be made non-learnable and the learning can be carried out solely through the linear combination weights, suggesting that there may be more potential simplifications to the spatial convolution layer, since the primary network optimization happens through the linear combination weights. This begs the question, can we build upon LBCNN and arrive at a much simpler functionf ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Perturbative Neural Networks Module</head><p>The aforementioned observation motivates the formulation of the PNN. The pictorial illustration of the proposed PNN module is shown in the bottom row of <ref type="figure" target="#fig_1">Figure 1</ref>. When the input image comes in, it will be perturbed with a set of pre-defined random additive noise masks, each with the same size as the input image, resulting in a set of noise-perturbed maps. These maps will go through a ReLU non-linearity and are then linearly combined to form one final feature map. Again, the random additive noise masks are pre-defined and non-learnable, and the only learnable parameters are the linear combination weights. Mathematically, PNN transforms the input and output of layer l in the following way:</p><formula xml:id="formula_2">x t l+1 = m i=1 σ relu N i l + x i l · V t l,i (2)</formula><p>where t is the output channel index, i is the input channel index, and N i l is the i-th random additive perturbation mask in layer l. Similar to LBCNN, the linear weights V are the only learnable parameters of a perturbation layer. Not surprisingly, PNN is able to save lots of learnable parameters as will be discussed in the following sections.</p><p>From Eq. 2 we observe that the computationally expensive convolution operation is replaced by an element-wise noise addition which is significantly more efficient. Recall in the previous section we ask the question whether it is possible to arrive at a much simpler functionf that transforms the patch center x c to one point y on the feature map. Now we can have y = f (x c ) = x c + n c , where n c is the added noise corresponding to x c location. An attractive attribute of the PNN formulation is that repetitive operation such as the convolution (moving from one patch to the other) is no longer needed. A single pass for adding the noise perturbation mask to the entire input channel completes the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Relating PNN and CNN: A Macro View</head><p>Let x ∈ R d be a vectorized input image of dimension d and let y ∈ R d be a vectorized feature map after convolving x with a 2D convolutional filter w ∈ R k×k . We use the notation vec(·) to represent the vectorization of a matrix and mat(·) to represent the opposite, which is reshaping the vector to its original matrix form. The following discussion will be done by using 2D matrices but the same technique applies for high-dimensional tensors as practiced in CNN layers as well. Therefore, the standard CNN convolution operation is as follows, assuming no bias in the convolution:</p><formula xml:id="formula_3">CNN : y = vec(mat(x) * w) = k 2 i x i,shift · w i (3)</formula><p>where x i,shift is the i-th spatially shifted version of the input in vectorized form, and w i is the i-th element in the convolution filter w.</p><p>For PNN, the same input x will be perturbed with N random noise masks n i , and then linearly combined using weight vector v whose elements are v i 's to form the output response vectorŷ. Therefore, for PNN, the operation follows:</p><formula xml:id="formula_4">PNN :ŷ = N i=1 (x + n i ) · v i<label>(4)</label></formula><p>If we arrange x + n i as the columns vectors of a matrix X ∈ R d×N , we can rewrite the PNN operation as:</p><formula xml:id="formula_5">y =Xv = (X + N)v = (x1 ⊤ + N)v<label>(5)</label></formula><p>where X ∈ R d×N has vector x repeated in its columns and N ∈ R d×N is a perturbation matrix with noise vector n i in its columns.</p><p>Given the CNN output vector y, we can always find the vector v for PNN such that the PNN outputŷ is equal to or approximately equal to y. If N = d,X is a full rank square matrix so an exact solution for v can be found as:</p><formula xml:id="formula_6">v * =X −1 y = (x1 ⊤ + N) −1 y (6) = N −1 − N −1 x1 ⊤ N −1 1 + 1 ⊤ N −1 x y<label>(7)</label></formula><p>where the last step is due to Sherman-Morrison formula <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22]</ref>. If N &lt; d,X is a tall matrix, so a least square solution can be found for v as:</p><formula xml:id="formula_7">v * = (X ⊤X ) −1X⊤ y<label>(8)</label></formula><p>Next, we will derive a relationship between the convolutional weights in CNN and the perturbation weights in PNN assumingŷ = y. Recall that convolution is a linear operation that transforms input x to output y and can be viewed as multiplication of a matrix. So we can rewrite the convolution operation simply as:</p><formula xml:id="formula_8">y = Ax<label>(9)</label></formula><p>where A is a doubly block circulant matrix which corresponds to convolutional weights w with proper manipulation. Using the derived optimal linear weights vector v * , the PNN reconstruction simplifies to:</p><formula xml:id="formula_9">y r = (x1 ⊤ + N)v * (10) = (x1 ⊤ + N) N −1 − N −1 x1 ⊤ N −1 1 + 1 ⊤ N −1 x y (11) = Ax = y<label>(12)</label></formula><p>Therefore, we can establish the following relationship:</p><formula xml:id="formula_10">⇒ (x1 ⊤ + N) N −1 y − N −1 x1 ⊤ N −1 y 1 + 1 ⊤ N −1 x = Ax (13) ⇒ x 1 ⊤ N −1 n ⊤ (Ax) 1 ⊤ N −1 n ⊤ x = x 1 ⊤ N −1 n ⊤ x 1 ⊤ N −1 n ⊤ (Ax) ⇒ xn ⊤ (Axn ⊤ )x = xn ⊤ (xn ⊤ A)x<label>(14)</label></formula><p>By observation, the following must hold:</p><formula xml:id="formula_11">Axn ⊤ = xn ⊤ A<label>(15)</label></formula><formula xml:id="formula_12">⇒ AXN −1 = XN −1 A<label>(16)</label></formula><formula xml:id="formula_13">⇒ (X + AX)N −1 = N −1 A<label>(17)</label></formula><p>where X + is the Moore-Penrose inverse of X. Rearranging the terms, we can arrive at the Sylvester equation <ref type="bibr" target="#b28">[29]</ref> commonly used in control theory:</p><formula xml:id="formula_14">(X + AX) Sa N −1 + N −1 (−A) S b = 0 Sc<label>(18)</label></formula><p>Reformulating in terms of Kronecker tensor product we have:</p><formula xml:id="formula_15">I ⊗ S a + S ⊤ b ⊗ I N −1 (:) = S c (:)<label>(19)</label></formula><p>in that N −1 will have a unique solution when the eigenvalues of S a and −S b are distinct, meaning the spectra of (X + AX) and A are disjoint. In this way, given the known input x and convolution transformation matrix A, we can always solve for the matching noise perturbation matrix N using linear algebra toolbox such as the Matlab Sylvester equation routine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relating PNN and CNN: A Micro View</head><p>Now let us consider a single neighborhood (patch) in the input tensor where the convolution is taking place, and obtain a relation between PNN and CNN with some mild assumptions. Let us assume that each pixel x i within this patch is a random variable and we call the central pixel x c for simplicity which has a total of card(N c ) neighbors where N c is a set containing the indices of the neighboring pixels of x c . Let us further make assumptions on the first and second order statistics of x i . In this case, we assume that E(x i ) = 0 and E(x 2 i ) = σ 2 . Let ǫ i = x i − x c , i ∈ N c be the difference between neighbor x i and the central pixel x c . Next we want to examine the following three quantities, namely E(ǫ i ), E(ǫ 2 i ), and E(ǫ i ǫ j ), which will be used for the subsequent derivation.</p><p>First, it is quite easy to see that: E(ǫ i ) = E(x i − x c ) = 0. Next, for the second order statistics E(ǫ 2 i ), we have:</p><formula xml:id="formula_16">E(ǫ 2 i ) = E[(x i − x c ) 2 ] = E(x 2 i + x 2 c − 2x i x c ) = E(x 2 i ) + E(x 2 c ) − 2E(x i x c ) = 2σ 2 − 2ρσ 2 = 2σ 2 δ<label>(20)</label></formula><p>where δ = 1 − ρ. In this case, we assume that ρ ≈ 1 because neighboring pixels usually have high correlations. Therefore, δ is usually very small meaning that E(ǫ 2 i ) is very small as well. Lastly, for E(ǫ i ǫ j ), i = j we have:</p><formula xml:id="formula_17">E(ǫ i ǫ j ) = E[(x i − x c )(x j − x c )] = E(x 2 c ) − E(x i x c ) − E(x j x c ) + E(x i x i ) = σ 2 − ρσ 2 − ρσ 2 +ρσ 2 (assumingρ ≈ ρ) = σ 2 − ρσ 2 = σ 2 δ = (1/2)E(ǫ 2 i )<label>(21)</label></formula><p>For CNN, the convolution operation maps the central pixel x c to one point y on the output feature map with convolutional weights w i 's as follows. Let N = card(N c ) + 1:</p><formula xml:id="formula_18">y = N i=1 x i w i = x c + i∈Nc x i w i (22) ⇒ x c w c + i∈Nc (x c + ǫ i )w i = y (23) ⇒ x c w c + i∈Nc w i + i∈Nc ǫ i w i = y (24) ⇒ x c + i∈Nc ǫ i w i N i w i = y N i w i (25) ⇒ x c + i∈Nc ǫ i w ′ i nc = y ′<label>(26)</label></formula><p>Establishing that n c = i∈Nc ǫ i w ′ i behaves like additive perturbation noise, will allows us to relate the CNN forumulation to the PNN formulation.</p><p>Next, we will examine E(n c ) and E(n 2 c ). First, it can be easily shown that E(n c ) = E i∈Nc ǫ i w ′ i = 0 since E(ǫ i ) = 0. Next, for the second order statistics, we have:</p><formula xml:id="formula_19">E(n 2 c ) = E i∈Nc ǫ i w ′ i 2 = E N i=1 ǫ i w ′ i 2 since ǫ c = 0 = E(ǫ 2 1 w ′2 1 + . . . ǫ 2 i w ′2 i + . . . + ǫ 1 ǫ 2 w ′ 1 w ′ 2 + . . . cross−terms ) = E(ǫ 2 i ) N i=1 w ′2 i + E(ǫ i ǫ j ) i j =i w ′ i w ′ j = (2σ 2 δ) w ′ 2 2 + (σ 2 δ) i j =i w ′ i w ′ j = 2σ 2 δ w ′ 2 2 + (1/2) i j =i w ′ i w ′ j = 2σ 2 δ ′ (small)<label>(27)</label></formula><p>where</p><formula xml:id="formula_20">δ ′ = δ[ w ′ 2 2 + (1/2) i j =i w ′ i w ′ j ]</formula><p>. Therefore, this analysis of the CNN operation establishes a relation between the CNN and the PNN formulation. However, one may notice that in Eq. 26, the RHS is y ′ instead of y. By allowing multiple perturbation maps to combine using the linear combination weights as shown before leads to y on the RHS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Properties of PNN</head><p>Recall that convolution leverages two important ideas that can help improve a machine learning system: sparse interactions and parameter sharing <ref type="bibr" target="#b4">[5]</ref>. Not surprisingly, the proposed PNN also share many of these nice properties. Sparse interactions: Firstly, PNN adds perturbation to the input with perturbation mask of the same size as the input. Therefore, it is easy to see that it only needs a single element in the input to contribute to one element in the output perturbation map. Hence, sparse interaction. Secondly, PNN utilizes a set of learnable linear weights, or equivalently 1×1 convolution, to combine various perturbation maps to create one feature map. When a 1 × 1 convolution is applied on the input map, only one element contributes to the one output elements, as opposed to a 3 × 3 convolution which involve 9 elements of the input as depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. Therefore, 1 × 1 convolution provides the sparsest interactions possible. <ref type="figure" target="#fig_2">Figure 3</ref> shows various commonly practiced convolutions such as (a) the regular convolution, (b) locally connected convolution, (c) tiled convolution, and finally (d) fully connected layer. It is important to note that while a perturbation layer by itself has a receptive field of one pixel, the receptive field of a PNN would typically cover the entire image with an appropriate size and number of pooling layers.</p><p>Parameter sharing: Although the fixed perturbation masks are shared among different inputs, they are not learnable, and therefore not considered as the parameters in this context. Here, the parameter sharing is carried out again by the 1 × 1 convolution that linearly combines the various non-linearly activated perturbation masks.</p><p>In addition, PNN has other nice properties such as multi-scale equivalent convolutions i.e., adding different amounts of perturbation noise is equivalent to applying convolutions at different scales. More specifically, adding small noise corresponds to applying a small-sized convolutional filter, and adding larger noise corresponds to convolving with a larger filter. Without explicitly setting the filter sizes throughout the network layers, PNN allows the network to adapt to different filter sizes automatically and optimally. Please refer to the supplementary materials for more analysis and discussions. Finally, PNN also has distance preserving property. Please also see supplementary for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Parameter Savings</head><p>The number of learnable parameters in the perturbation layer is significantly lower than those of a standard convolutional layer for the same number of input and output channels. Let the number of input and output channels be p and q respectively. With a convolutional kernel of size of h × w, a standard convolutional layer consists of p · h · w · q learnable parameters. The corresponding perturbation layer consists of p · m fixed perturbation masks and m · q learnable parameters (corresponding to the 1 × 1 convolution), where m is the number of fan-out channels of the perturbation layer, and the fan-out ratio (m/p) is essentially the number of perturbation masks applied on each input channel. The 1 × 1 convolutions act on the m perturbed maps of the fixed filters to generate the q-channel output. The ratio of the number of parameters in CNN and PNN is:</p><formula xml:id="formula_21"># param. in CNN # param. in PNN = p · h · w · q m · q = p · h · w m<label>(28)</label></formula><p>For simplicity, assuming fan-out ratio m/p = 1 reduces the parameter ratio to h · w. Therefore, numerically, PNN saves k 2 parameters during learning for k × k convolutional filters. Also, PNN allows flexible adjustment of the fan-out ratio to trade-off between efficiency and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning with Perturbation Layers</head><p>Training a network end-to-end with perturbation layers instead of standard convolutional layers is straightforward. The gradients can be back propagated through the 1 × 1 convolutional layer and the additive perturbation masks in much the same way as they can be back propagated through standard convolutional layers. Backpropagation through the noise perturbation layer is similar in spirit to propagating gradients through layers without learnable parameters (e.g., ReLU, max pooling, etc.). However during learning, only the learnable 1 × 1 filters are updated while the additive perturbation masks remain unaffected. For the forward propagation defined in Eq. 4, backpropagation can be computed as:</p><formula xml:id="formula_22">∂ŷ ∂x = N i=1 v i and ∂ŷ ∂v i = x + n i<label>(29)</label></formula><p>The perturbation masks are of the same spatial size as the input tensor, and for each input channel, we can generate m/p masks separately (m/p is the fan-out ratio). Specifically, the additive noise in the perturbation masks are independently uniformly distributed. The formulation of PNN does not require the perturbation noise to be a specific type, as long as it is zero-mean and has finite variance. Empirically, we have observed that adding zero-mean Gaussian noise with different variances performs comparably to adding zero-mean uniform noise with different range levels. Since uniform distribution provides better control over the energy level of the noise, our main experiments are carried out by using uniformly distributed noise in the perturbation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ImageNet-1k Classification and Analysis</head><p>We evaluate our method on the ImageNet ILSVRC-2012 classification dataset <ref type="bibr" target="#b23">[24]</ref> which consists of 1000 classes, with 1.28 million images in the training set and 50k images in the validation set, where we use for testing as commonly practiced. We report the top-1 classification accuracy. All the images are first resized so that the long edge is 256 pixels, and then a 224 × 224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted. During testing, we adopt the single center-crop testing protocol. The network architecture we use for this experiment is PNN-ResNet-18 <ref type="bibr" target="#b7">[8]</ref>, where each standard convolutional layer in a residual unit is replaced by the proposed perturbation layer, as shown in  We have experimented with various number of perturbation masks per layer (64, 128, and 256) on the same PNNResNet-18 model. The results are consolidated in <ref type="table" target="#tab_0">Table 1</ref> and in <ref type="figure" target="#fig_6">Figure 5</ref>. As can be seen, compared to the state-ofthe-art ResNet-18 performance (single center-crop protocol) on a standard CNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, the proposed PNN achieves comparable classification accuracy on ImageNet-1k. It is worth noting that the lightweight design in PNN allows significant parameter savings as well as statistical efficiency compared to the standard CNNs. We have shown the parameter ratio of CNN over PNN in the last column in <ref type="table" target="#tab_0">Table 1</ref>. We also  <ref type="table" target="#tab_2">Table 2</ref> show that PNN-ResNet-50 performs competitively with the corresponding network with CNN layers <ref type="bibr" target="#b3">[4]</ref> on ImageNet-1k.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet-100 Classification and Analysis</head><p>In this section, we examine the image classification accuracy versus the noise level in the additive perturbation masks. As discussed in the previous section, we are using zero-mean uniform random noise in the perturbation masks, and the noise level ℓ here refers to the range of the noise and the PDF is:</p><formula xml:id="formula_23">f (n i ) = 1/(ℓ − (−ℓ)) for −ℓ ≤ n i ≤ ℓ.</formula><p>For faster roll-out, we randomly select 100 classes with the largest number of images (1300 training images in each class, with a total of 130k training images and 5k testing images), and report top-1 accuracy on this ImageNet-100 subset. The network architecture we use for this experiment is PNN-ResNet-50 with 256 perturbation masks per layer. The experimental results are shown in <ref type="table" target="#tab_3">Table 3</ref> as well as in <ref type="figure" target="#fig_8">Figure 6</ref>. As we observe, adding different amount of perturbation noise does affect classification performance. Moreover, the proposed PNN's performance is similar when the noise level is low, say less than 1. As the noise levels increase, the performance begins to deteriorate. This is expected, as adding too much noise will suppress the useful information carried by the signal itself. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">CIFAR-10, MNIST Classification and Analysis</head><p>In this section, we carry out further classification experiments on CIFAR-10 <ref type="bibr" target="#b12">[13]</ref> and MNIST <ref type="bibr" target="#b15">[16]</ref> datasets. For both datasets, the initial learning rate is set to 10 −3 and is reduced by a factor of 10 at epoch 60, and then again at epoch 90. The best performing PNN models for each dataset are detailed as follows.   PNN-ResNet-50, 32 perturbation masks per layer, and batch size of 10. <ref type="table" target="#tab_5">Table 4</ref> consolidates the image classification accuracy from our experiments. The best performing PNNs are compared to the state-of-the-art methods as listed in the curated leader board on various image classification tasks <ref type="bibr" target="#b0">[1]</ref>, as well as several other leading methods such as ResNet <ref type="bibr" target="#b7">[8]</ref>, Maxout Network <ref type="bibr" target="#b5">[6]</ref>, Network in Network (NIN) <ref type="bibr" target="#b18">[19]</ref>, and LBCNN <ref type="bibr" target="#b11">[12]</ref>. Our results indicate that PNN is highly competitive with the state-of-the-art results on CIFAR-10. In addition, <ref type="table" target="#tab_6">Table 5</ref> shows the CIFAR-10 performance on PNN-ResNet-18 model with different number of perturbation masks per layer. As long as the number of perturbation masks is not too few, the network is able to converge and provide competitive classification performance. The other factor we want to examine is the learning rate. Since we know that learning rate is tightly connected to the batch size from recent findings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>, we vary the batch size as we fix the initial learning rate to be 10 −3 . <ref type="table" target="#tab_7">Table 6</ref> shows the CIFAR-10 classification performance with varying batch size as well as varying number of perturbation masks. For this dataset, smaller batch size seems to always work better. This could be due to a batch size of 10 corresponds to the optimal initial learning rate in this case. Also, PNN does not require lots of perturbation masks per layer. Usually, the optimal range is around 48-80 masks per layer for this dataset, and doubling the number of masks from 64 to 128 does not seem to help improve the performance. PNN achieves its best performance on CIFAR-10 with 64 perturbation masks with a ResNet-50 architecture, while the best results for ResNet architecture are obtained with 110 layers, resulting in a 3.1× reduction in parameters while achieving similar performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Object Detection and Analysis</head><p>For this task, we look at the Faster R-CNN model <ref type="bibr" target="#b30">[31]</ref> whose region proposal network and detector network both share a common pretrained (on ImageNet) CNN. We study both CNN architectures (VGG-16, ResNet-50) by replacing the convolution layers with the proposed PNN modules. <ref type="table" target="#tab_8">Table 7</ref> shows the mean average precision (mAP) on PAS-CAL VOC'07 testing set (trained on VOC'07 train+val set, scale=600, batchsize=1, and with ROI align). We observe that PNN-VGG-16 and PNN-ResNet-50 perform comparably to the corresponding network with conv layers <ref type="bibr" target="#b30">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Convolutional layers have become the mainstay of stateof-the-art image classification tasks. Many different deep neural network architectures have been proposed building upon convolutional layers, including convolutional layers with small receptive fields, sparse convolutional weights, binary convolutional weights, factorizations of convolutional weights, etc. However, the basic premise of a convolutional layer has remained the same through these developments. In this paper, we sought to validate the utility of convolutional layers through a module that is devoid of convolutional weights and only computes weighted linear combinations of non-linear activations of additive noise perturbations of the input. Our experimental evaluations yielded a surprising result, deep neural networks with the perturbation layers perform as well as networks with standard convolutional layers across different scales and difficulty of image classification and detection datasets. Our findings suggest that perhaps high performance deep neural networks for image classification and detection can be designed without convolutional layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>x l is filtered by m pre-defined fixed binary filters b i , i ∈ [m], to generate m difference maps that are then activated through ReLU, resulting in m response maps. The linear combination weights for the m response maps are W l,i , i ∈ [m] for obtaining one final feature map. The combined set of feature maps serve as the input x l+1 for the next layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Basic modules in CNN, LBCNN [12], and PNN. W l and V l are the learnable weights for local binary convolution layer and the proposed perturbation layer respectively. Inspired by the formulation of LBCNN, the proposed PNN method also uses a set of linear weights to combine various perturbation maps. For CNN: (a) input, (b) learnable convolutional filter, (c) response map, (d) ReLU, (g) feature map. For LBCNN: (a) input, (b) fixed non-learnable binary filters, (c) difference maps by convolving with binary filters, (d) ReLU, (e) activated difference maps, (f) learnable linear weights for combining the activated difference maps, (g) feature map. For PNN: (a) input, (b) fixed non-learnable perturbation masks, (c) response maps by addition with perturbation masks, (d) ReLU, (e) activated response maps, (f) learnable linear weights for combining the activated response maps, (g) feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Variations in connectivity patterns among commonly practiced types of convolutional operations such as (a) the regular convolution, (b) locally connected convolution, (c) tiled convolution, and finally (d) fully connected layer. For (a-c), top row is a 3 × 3 convolution and the bottom row is a 1 × 1 convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 Figure 4 :</head><label>44</label><figDesc>Figure 4: Perturbation residual module. Figure 4: Perturbation residual module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Perturbation residual module. Figure 4: Perturbation residual module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy and loss on ImageNet-1k classification using PNN (ResNet-18) with various number of perturbation masks per layer. Table 2: Classification accuracy (%) on ImageNet-1k (PNN vs. CNN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>For CIFAR-10: PNN-ResNet-50, 64 per- turbation masks per layer, and batch size of 10. For MNIST:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Accuracy on ImageNet-100 classification using PNN (ResNet-50, 256 perturbation masks) with various noise levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%) on ImageNet-1k (PNN vs. CNN)</figDesc><table>#Mask 
PNN (ResNet-18) ResNet [4] 
Param. Ratio 

256 
71.84 
73.27 (34) 
0.9 
128 
61.74 
69.57 (18) 
1.8 
64 
45.92 
69.57 (18) 
5.9 

present additional experimental results of the PNN-ResNet-
50 (with 256 perturbation masks) on ImageNet-1k. The 
results in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Classification accuracy (%) on ImageNet-1k (PNN vs. CNN) 

PNN-ResNet-18 ResNet-18 
PNN-ResNet-50 ResNet-50 

71.84 
69.57 
76.23 
75.99 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy (%) on 100-class ImageNet with varying perturbation noise levels. (PNN: ResNet-50, 256 perturbation masks)</figDesc><table>Noise 
0.01 
0.05 
0.1 
0.5 
1 
5 

PNN 
81.09 81.19 81.41 81.96 76.84 60.90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy (%) on CIFAR-10 and MNIST. PNN columns only show the best performing model.</figDesc><table>PNN 
SoTA [1] 
LBCNN 
ResNet 
Maxout 
NIN 

CIFAR-10 
94.05 
96.53 
92.99 
93.57 
90.65 
91.19 
MNIST 
99.39 
99.79 
99.51 
-
99.55 
99.53 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracy (%) and ratio of parameters on CIFAR- 10 with varying number of perturbation masks. (PNN: ResNet-18, CNN: standard ResNet-18)</figDesc><table>#Mask 
160 
128 
96 
64 
32 
16 
8 

PNN 
90.43 89.48 90.25 89.99 93.08 87.16 79.98 
Ratio 
1.3 
2.0 
3.5 
7.9 
31.0 
120.1 451.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 :</head><label>6</label><figDesc>Classification accuracy (%) on CIFAR-10 with varying batch size and number of perturbation masks. (PNN: ResNet-50)</figDesc><table>#Mask\ Batch Size 
10 
20 
40 
80 

32 
90.23 87.29 83.96 79.16 
64 
94.05 90.93 88.36 85.29 
128 
93.71 90.05 88.63 85.14 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Detection results (mAP) on PASCAL VOC 2007 testing set.</figDesc><table>PNN-VGG-16 
VGG-16 
PNN-ResNet-50 ResNet-101 

69.6 
70.2 
72.8 
75.2 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Are We There Yet? Classification Datasets Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<ptr target="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" />
		<imprint>
			<biblScope unit="page" from="2017" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BinaryConnect: Training Deep Neural Networks with Binary Weights During Propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3105" to="3113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ResNet training in Torch by Facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Facebook</surname></persName>
		</author>
		<idno>2017-11-15. 7</idno>
		<ptr target="https://github.com/facebook/fb.resnet.torch.Ac-cessed" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maxout Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely Connected Convolutional Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local Binary Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images. CIFAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6553</idno>
		<title level="m">Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradientbased Learning Applied to Document Recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal Brain Damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08597</idno>
		<title level="m">Enabling Sparse Winograd Convolution by Native Pruning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network in Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01409</idno>
		<title level="m">Holistic SparseCNN: Forging the Trident of Accuracy, Speed, and Size</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01409</idno>
		<title level="m">Faster cnns with direct sparse convolutions and guided pruning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Section 2.7.1 Sherman-Morrison Formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical Recipes: The Art of Scientific Computing</title>
		<imprint/>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">XNORNet: ImageNet Classification Using Binary Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adjustment of an Inverse Matrix Corresponding to Changes in the Elements of a Given Column or a Given Row of the Original Matrix (abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">621</biblScope>
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="127" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Don&apos;t Decay the Learning Rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
	</analytic>
	<monogr>
		<title level="m">Increase the Batch Size</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sur l&apos;equations en matrices px = xq</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sylvester</surname></persName>
		</author>
		<idno>1884. 5</idno>
	</analytic>
	<monogr>
		<title level="j">C. R. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A Faster Pytorch Implementation of Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://github.com/jwyang/faster-rcnn.pytorch.8" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
