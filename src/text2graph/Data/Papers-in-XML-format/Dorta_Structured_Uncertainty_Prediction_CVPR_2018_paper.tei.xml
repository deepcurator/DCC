<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Uncertainty Prediction Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garoe</forename><surname>Dorta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bath</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Anthropics Technology Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Vicente</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Anthropics Technology Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
							<email>3l.agapito@cs.ucl.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neill</forename><forename type="middle">D F</forename><surname>Campbell</surname></persName>
							<email>n.campbell@bath.ac.uk2sara</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bath</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Simpson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Anthropics Technology Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Uncertainty Prediction Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep probabilistic generative models have recently become the most popular tool to synthesize novel data and reconstruct unseen examples from target image distributions. At their heart lies a density estimation problem, which for reconstruction models is commonly solved using a factorized Gaussian likelihood. While attractive for its simplicity, this factorized Gaussian assumption comes at the cost of overly-smoothed predictions, as shown in <ref type="figure" target="#fig_0">Fig. 1d</ref>. In contrast, this paper introduces the first attempt to train a deep neural network to predict full structured covariance matrices to model the residual distributions of unseen image reconstructions. We postulate that the residuals are highly structured and reflect limitations in model capacity -we therefore propose to estimate the reconstruction uncertainty using a structured Gaussian model, to capture pixel-wise correlations which will in turn improve the sampled reconstructions, as shown in <ref type="figure" target="#fig_0">Fig. 1c and 1f</ref>.</p><p>Learning in generative models involves some form of density estimation, where the model parameters are fitted to generate samples from the target image distribution. This paper concentrates on methods, such as the Variational Autoencoder (VAE) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, where an explicit representation of the data density is given by the model, however the re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagonal</head><p>Our structured Gaussian Gaussian sults could in principle be extended to implicit approaches in future work. In VAE models, a mapping is learned from a latent representation to image space. These models commonly impose some conditions on the latent space, in the form of a prior, and the nature of the residual distribution.</p><p>It is common practice to use a Gaussian likelihood, as this provides a simple formulation for maximum likelihood estimation. However, it is well known that samples from factorized Gaussian distributions tend to be overly-smooth. This effect is pronounced in common simplifications of the likelihood, such as the mean squared error, which assumes the errors at all pixels are i.i.d (independent and identically distributed) or a diagonal covariance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>, which allows some local estimation of noise level but maintains the strong and flawed assumption that pixels in the residual are uncor-related, as shown in <ref type="figure" target="#fig_0">Fig. 1b and 1c</ref>. These common choices of likelihood mean that if one were to draw a sample from any of these models including the noise term, white noise would be added to the reconstruction, which is unlikely to ever appear realistic, as shown in <ref type="figure" target="#fig_0">Fig. 1e</ref>. This emphasizes the incoherence of these simplifications, and this is addressed in this paper.</p><p>This work proposes to overcome the problems of factorized likelihoods by training a deep neural network to predict a more complex residual distribution for samples drawn from a trained probabilistic generative model. Specifically, we model the residual distribution as a structured Gaussian distribution. This work demonstrates that a network to predict this distribution can be tractably learned through maximum likelihood estimation of residual images; these predictions generalize well to previously unseen examples. Samples from this model are plausible, coherent, and can capture high-frequency details that may have been missed when training the original model with a factorized likelihood. We demonstrate the efficacy of this approach by: estimating ground truth covariances from synthetic data, adding coherent residual samples to face reconstructions from a VAE and include a further motivating example application for denoising face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Statistical quantification of uncertainty has been an area of interest for a long time. Many traditional statistical estimation models provide some measure of uncertainty on the inferred parameters of a fitted model. An additional source of uncertainty arises from the distribution of the model residual, which is often modeled as a spherical or diagonal Gaussian distribution. A common issue with these models is the false assumption of independence between pixels in the reconstruction residual image. Previous work on modeling correlated Gaussian noise is limited, it has been used for small data scenarios <ref type="bibr" target="#b18">[19]</ref>, for temporally correlated noise models <ref type="bibr" target="#b23">[24]</ref> and in Gaussian processes <ref type="bibr" target="#b21">[22]</ref>.</p><p>The most recent related work on uncertainty prediction for deep generative models has been the prediction of heteroscedastic Gaussian noise for encoder/decoder models <ref type="bibr" target="#b12">[13]</ref>. This approach is similar to the variational autoencoder with a diagonal covariance likelihood <ref type="bibr" target="#b14">[15]</ref>, but can be applied when the input and generated output are different, for example in semantic segmentation tasks. Interestingly, the maps of predicted variance in <ref type="bibr" target="#b12">[13]</ref> correspond well to high frequency structured image features, similarly to the VAE with a diagonal noise model (see <ref type="figure" target="#fig_0">Fig. 10</ref>). These are structured regions that the model consistently struggles to accurately predict, which is a further encouragement for our work.</p><p>This paper proposes an approach to predict a structured uncertainty distribution for generated images from trained networks. This method is applicable to models that reconstruct images without embellishing the prediction with details that were not present in the original input.</p><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref> are an implicit density estimation method widely used for generating novel images with a great deal of success. Samples from GANs have been shown to contain fine details and can be created at very high resolutions <ref type="bibr" target="#b11">[12]</ref>. Although, they are not designed to provide reconstructions, some methods have been proposed that enable this <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref>. Reconstructions from GAN models contain realistic high frequency details, however the generated images usually do not resemble well the inputs. This might be caused by mode-dropping <ref type="bibr" target="#b6">[7]</ref>, where parts of the image distribution are not well modeled. Despite recent work <ref type="bibr" target="#b1">[2]</ref> addressing this issue, it remains an open problem. Furthermore, the residual distribution for reconstruction using GAN models is likely to be highly complex and therefore we do not use a GAN model as a starting point in this work.</p><p>Explicit density methods for learning generative models use either a tractable <ref type="bibr" target="#b19">[20]</ref> or approximate <ref type="bibr" target="#b22">[23]</ref> density to model the image distribution. These models allow maximization of the likelihood of the set of training observations directly through reconstruction. These approaches generate data that is more appropriate for the task this paper addresses and offer potential likelihood models to learn the uncertainty of prediction.</p><p>To train a network to predict the uncertainty of a reconstruction model, we need to choose a reconstruction likelihood that is efficient to calculate and allows structured prediction. PixelCNN <ref type="bibr" target="#b19">[20]</ref> and derived work <ref type="bibr" target="#b8">[9]</ref> provide an autoregressive sampling model, where the likelihood of a pixel is modeled by a multinomial distribution conditioned on the previously generated pixels. Although these models are capable of producing images with details, the generation process is computationally expensive. Approximate density models, such as the Variational Autoencoder (VAE) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, use a variational approximation of the marginal likelihood of the data under a diagonal Gaussian likelihood. These models are very efficient to reconstruct from, and the Gaussian noise model has an efficient likelihood that can be extended to include off-diagonal terms to allow correlated noise prediction. Therefore, the focus of this work is on explicit generative models with approximate likelihoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Many generative models, with a (diagonal) Gaussian likelihood, define the conditional probability of the target image, x, given some latent variable, z, as:</p><formula xml:id="formula_0">p θ (x | z) = N µ(z), σ(z) 2 I ,<label>(1)</label></formula><p>where x is the target image flattened to a column vector, θ are the model parameters, and the mean µ(z) and variance σ(z) 2 are (non-linear) functions of the latent variables. This is equivalent to the forward model:</p><formula xml:id="formula_1">x = µ(z) + ǫ(z),<label>(2)</label></formula><p>where ǫ(z) ∼ N 0, σ(z) 2 I is commonly considered as unstructured noise inherent in the data. However, reconstruction errors (or residuals) are often caused by data deficiencies, limitations in the model capacity and suboptimal model parameter estimation, while the residual itself is generally highly structured, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The noise term is being used mostly to explain failures in the model rather than just additive noise in the input. The novelty of this work is to encode coherent information about uncertainty in the reconstruction in ǫ(z).</p><p>For the majority of models, σ 2 is never used in practice when sampling reconstructions; it only adds white noise that would rarely improve the residual error, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Although in some cases σ 2 is inferred from the data, in many cases it is assumed that σ 2 i = σ 2 , a user defined constant, which simplifies the log likelihood from Eq. 1 to a sum of squared errors scaled by 1/σ 2 . In contrast, this paper extends the noise model to use a multivariate Gaussian likelihood with a full covariance matrix</p><formula xml:id="formula_2">p θ (x | z) = N µ(z), Σ ψ (z) ,<label>(3)</label></formula><p>where Σ ψ (z) is a (non-linear) function parametrized by ψ; this is equivalent to ǫ(z) ∼ N 0, Σ ψ (z) . The covariance matrix captures the correlations between pixels to allow sampling of structured residuals as demonstrated in our experiments.</p><p>A maximum likelihood approach is used to train the covariance prediction. We optimize the log-likelihood with respect to ψ keeping the generative model parameters θ fixed:</p><formula xml:id="formula_3">arg min ψ log Σ ψ (z) + x − µ(z) T Σ ψ (z) −1 x − µ(z) .<label>(4)</label></formula><p>To simplify notation, in subsequent sections Σ and µ are used to denote Σ ψ (z) and µ(z) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Covariance estimation</head><p>A deep neural network is used to estimate the covariance matrix Σ from the latent vector z, henceforth referred to as the covariance network. This learning task is challenging on two fronts. It is ill-posed as no ground truth residual covariances are available for real data. Moreover, for each training example, a full covariance matrix must be estimated from a single target image. By definition, the covariance matrix is symmetric and positive definite. Therefore, for an image x with n pixels, the matrix contains (n 2 − n)/2 + n unique parameters.</p><p>For any covariance estimation method there are three aspects to consider: (i) how difficult is it to sample from this covariance? (ii) how difficult is it to compute the terms in Eq. 4? and (iii) how difficult is it to impose symmetry and positive definiteness?</p><p>The need to draw samples arises from the fact that the covariance captures structured information about reconstruction uncertainty of the generative model. This means that drawing a sample from N 0, Σ and adding that to the reconstructed output may produce a result that is more representative of the target image, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Given a decomposition of the covariance matrix: Σ = MM T , samples can be drawn from N µ, Σ as x = Mu + µ, where u ∼ N 0, I is a vector of standard Gaussian samples.</p><p>If Σ is the direct output of the covariance network, it needs to be inverted to calculate the negative log-likelihood in Eq. 4. Hence, it is more practical to estimate the precision matrix Λ = Σ −1 as this term appears directly in the log likelihood, and the log determinant term can be equivalently computed as log(|Σ|) = − log(|Λ|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cholesky Decomposition We represent the precision matrix via its Cholesky decomposition, Λ = LL</head><p>T , where L is a lower triangular matrix, and the covariance network only explicitly estimates L.</p><p>Using the Cholesky decomposition, it is trivial to evaluate both terms in the negative log likelihood. The reconstruction error is y T y, where y = L T (x − µ). The log determinant is log(|Σ|) = −2 n i log(l ii ), where l ii is the i th element in the diagonal of L.</p><p>Sampling from Σ involves solving the triangular system of equations L T y = u with backwards substitution, which requires O(n 2 ) operations. By construction, the estimated precision matrix Λ is symmetric. To ensure that it is also positive-definite it is sufficient to constrain the diagonal entries of L to be strictly positive, e.g. by having the network estimate log(l ii ) element-wise.</p><p>However, estimating this matrix directly is only a feasible solution for datasets with small dimensionality, as the number of parameters to be estimated increases quadratically with the number of pixels in x.</p><p>Sparse Cholesky Decomposition To scale to larger resolution images we can reduce complexity by imposing a fixed sparsity pattern in the matrix L, and only estimate the non-zero values of the matrix via the covariance network.</p><p>The sparsity pattern depends on the type of data being modeled. For image data, we propose that l ij is only nonzero if i ≥ j and i and j are neighbours in the image plane, where pixels i and j are neighbours if a patch of size f centred at i contains j. These reduces the maximum number of non-zero elements in the matrix L to n((f 2 − 1)/2 + 1), where f &lt;&lt; n.</p><p>The resulting sparse matrix L is both lower-triangular and band-diagonal as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. This leads to a precision matrix Λ with a similar sparsity pattern with additional bands.</p><p>With a sparsity pattern of this form, our uncertainty model for each image can be interpreted as a Gaussian Random Field on its residual. A zero value in the precision matrix for pixels i and j implies that they are conditionally independent given all the other pixels. Similar Markov properties have been extensively used to model images <ref type="bibr" target="#b2">[3]</ref>.</p><p>With this representation the terms in the negative log likelihood can be evaluated efficiently, without the need to build the full dense matrix. Similarly, sampling can be performed by solving a sparse system of equations. Moreover, this approach is amenable to parallelization on the GPU, as each patch can be evaluated independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We evaluate our model on two custom synthetic datasets to demonstrate the capability of our model to accurately describe known residual distributions. We also demonstrate our model on gray-scale cropped face images from the CelebA <ref type="bibr" target="#b17">[18]</ref> dataset for sampling high frequency details to improve reconstructions. Finally, we show some examples of image denoising that takes advantage of the predicted covariance to better preserve structure. Results of our model evaluated on the CIFAR10 <ref type="bibr" target="#b15">[16]</ref> dataset can be found in the supplemental material. All our models are implemented in Tensorflow <ref type="bibr" target="#b0">[1]</ref> and they are trained on a single Titan X GPU using the Adam <ref type="bibr" target="#b13">[14]</ref> optimizer. Unless otherwise stated, the input data for all the experiments is normalized in [−1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic datasets</head><p>The goal of the two synthetic experiments is to evaluate the feasibility of training a covariance network to accurately estimate the residual distribution, where the true mean and covariance matrix are available for validation purposes.</p><p>Since the goal here is to evaluate the covariance prediction network, we simplify these experiments by bypassing the use of a generative model, and directly predicting Σ from µ. This means that the input to our covariance prediction network is µ.</p><p>Each dataset contains 35,000 training examples and 1,000 test examples. In all the experiments, the test examples are reconstructed by drawing a sample from the estimated covariance and adding it to the true mean µ.</p><p>Both datasets are constructed by generating a set of µ, and then adding a random sample of correlated noise to them: x ∼ N (µ, Σ), where Σ is a function of µ. Therefore, the added noise is dependent on the structure in the image, which imitates the situation on real data. This also reflects the main assumption of our model: that there is sufficient information in the latent variables z (or in µ for the synthetic experiments) to estimate the residual distribution Σ.</p><p>We emphasize that despite the true covariance matrices being known for these synthetic experiments, we do not use them at train time. We train the prediction network using the objective in Eq. 4, which makes no use of the true covariance and mimics the situation with real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Splines</head><p>The first synthetic dataset is composed of one dimensional signals, with 50 points per example. Each spline is comprised of a low frequency component and a correlated highfrequency one. The high-frequency component is produced by a unique covariance matrix per example, that is generated by a deterministic function that takes as input the lowfrequency signal. For more details please refer to the supplemental material.</p><p>The covariance prediction network is a multi-layer perceptron (MLP) with two layers of 100 units with relu activations and batch normalization <ref type="bibr" target="#b10">[11]</ref>, and a final layer with 1275 units. The final layer directly outputs the lower triangular part of the matrix L, which does not use our proposed sparsity approach. The model is trained with a learning rate of 1e-4 for 200 epochs.</p><p>Reconstructions for this dataset are obtained by adding a sample from Σ to µ. We show results for reconstructions in <ref type="figure" target="#fig_2">Fig. 3</ref>, where the uncertainty model is able to add a plausible  <ref type="figure">(N 0, Σ(µ) || N 0, Σ gt )</ref>, where Σ gt is the ground truth covariance matrix, and Σ(µ) is the estimated covariance. Our model obtains significant improvements under all metrics over a diagonal covariance model.</p><formula xml:id="formula_4">− log p(x | Σ(µ)) KL ||Σ(µ) − Σgt||2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Estimated</head><p>Figure 4: Estimated covariance matrices for the spline dataset. Each row corresponds to a different spline example on the test set. Our model is able to learn the variations in the structured residual distributions.</p><p>high-frequency component to the input µ.</p><p>Quantitative results are presented in <ref type="table">Table 1</ref>. We compare with a covariance network that only estimates a diagonal covariance matrix. As the ground-truth covariances contain off-diagonal structure, the diagonal model is bound to fail in representing it. Our model achieves a negative log likelihood similar to the one evaluated using the real covariance matrices.</p><p>As we have access to the ground-truth covariance matrices for each test example, we can directly compare the estimated covariance with the ground-truth ones. We show qualitative results in <ref type="figure">Fig. 4</ref>. Note how the model is able to recover most of the off-diagonal values in the covariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Ellipses</head><p>The second synthetic dataset was built to evaluate the covariance prediction network for images and to highlight the µ x µ + ǫ <ref type="figure">Figure 5</ref>: Reconstructions from our model, left column: input µ, middle column: original x, and right column: reconstruction using a sample from the learned residual distribution, where ǫ ∼ N 0, Σ .</p><formula xml:id="formula_5">− log p(x | Σ(µ)) KL ||Σ(µ) − Σgt||2 Ground truth -286 ± 2.8 - - Diagonal model -149 ± 3.3 707 ± 7.2 1.80 ± 0.21</formula><p>Ours -259 ± 2.7 113 ± 2.6 1.06 ± 0.34 <ref type="table">Table 2</ref>: Quantitative comparison on the ellipses dataset (see <ref type="table">Table 1</ref> for a description of the metrics). Our model is able to better model the real covariance matrices with its more complex uncertainty distribution.</p><p>limitations of estimating a dense matrix L.</p><p>We generate a dataset of synthetic gray-scale 16×16 images. For each example, the mean image contains an ellipse with random width, height, position and rotation angle. The prototype covariance matrix for this dataset generates lines and is rotated by the same random rotation angle that was used for the ellipse, thus generating random lines that are aligned with the ellipse.</p><p>For this dataset, estimating directly a dense Cholesky matrix L requires 32,896 values per image. Even at this limited image size we were unable to train a dense prediction model. Instead, we use the sparse prediction model with a neighborhood of size 5 × 5. The model was trained for 200 epochs with a learning rate of 1e-3, Reconstructions of the test set are shown in <ref type="figure">Fig. 5</ref>, where we show results of taking a sample from Σ which is added to µ. The covariance prediction network is successful in mapping the uncertainty distribution from the mean µ. The samples from the covariance exhibit high-frequency detail that matches the true residual. A quantative comparison with a diagonal Gaussian model is presented in <ref type="table">Table 2</ref>. Our model achieves a negative log likelihood similar to the one evaluated using the real covariance matrices.</p><p>Examples of ground-truth and estimated covariances are shown in <ref type="figure" target="#fig_3">Fig. 6</ref> and illustrate the accuracy of the covariance prediction. The covariance structure for this model is more complex than the one for the splines, and yet the model is still able to recover it effectively with our sparse estimation of the Cholesky matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">CelebA</head><p>We now show results of employing the covariance prediction network on a real images dataset: CelebA <ref type="bibr" target="#b17">[18]</ref>. The aligned and cropped version of the dataset is used, where a further cropping and resizing to 64×64 is performed and the images are converted from RGB to gray-scale. The dataset consists of 202,599 images of faces, which we split into 182,637 for training and 19,962 for testing as recommended by the authors.</p><p>We train both an Autoencoder (AE) <ref type="bibr" target="#b4">[5]</ref> and a VAE on this dataset using the architecture in <ref type="bibr" target="#b20">[21]</ref>. The Autoencoder is trained with a mean squared error, and it serves to show the performance of our model when using as input a z from an uncontrolled latent space. We then trained two covariance prediction networks, one for each model, to estimate the residual uncertainty from the latent variables, z. These networks have an initial block that is similar to the decoder of the VAE and a second block with four convolutional layers. The output of the networks is the sparse Cholesky decomposition of the precision matrix with a neighborhood of size 7 × 7 pixels. The sparsity imposed on the matrix means that instead of estimating 8, 390, 656 values as outInput AE AE-Ours VAE VAE-Ours <ref type="figure">Figure 7</ref>: Comparison of image reconstructions for the different models. The AE and VAE both generate oversmoothed images. For both the AE and VAE, our model adds plausible high-frequencies from a single sample drawn from the predicted uncertainty distribution.</p><formula xml:id="formula_6">Model NLL − log p(x | z) AE [5] - - VAE [15] −5378 ± 931 −6079 ± 936</formula><p>Ours-AE -−8242 ± 433</p><p>Ours-VAE −7753 ± 1323 −8386 ± 1339 <ref type="table">Table 3</ref>: Quantitative comparison of density estimation error. NLL denotes the upper bound of the marginal negative log likelihood, lower is better. The residuals are structured, thus the diagonal model produces poor estimations. Our estimation method is able to significantly improve over the AE and VAE simplified noise model. put, the network only needs to estimate 102, 400. The covariance networks are trained with a learning rate of 1e-3 for 50 epochs. Additional implementation details are given in the supplemental material. Example reconstructions for the VAE and the AE models are shown in <ref type="figure">Fig. 7</ref>. Reconstructions from both methods suffer from the over-smoothing effects of using a simplified Gaussian likelihood and in both cases high frequency details are lost. By adding a random sample of the predicted residual distribution our method is able to recover plausible high-frequency details, resulting in more realistic looking images. The added detail corresponds to important face features, which is lost by both autoencoder models, like teeth and hair.</p><p>A quantitative comparison of using either the predicted covariance or a diagonal covariance for calculating the negative log-likelihood of the reconstructions µ is presented in  T . The residuals are smoothly interpolated for the different images, which suggests that the estimated covariance matrices also vary smoothly. <ref type="table">Table 3</ref>. The reported values for VAE-based methods follow the protocol described in <ref type="bibr" target="#b3">[4]</ref>, where the upper bound of the marginal negative log likelihood is evaluated by numerically integrating over z with 500 samples per image. For VAE and our model using as base a VAE, the − log p(x | z) term is evaluated imposing zero variance in z.</p><p>Images generated by decoding samples from the prior distribution on the latent space of a β-VAE with added residuals from our model are shown in <ref type="figure" target="#fig_4">Fig. 8</ref>. We found that the VAE with a diagonal Gaussian likelihood overfitted the reconstruction error, thus neglecting the KL term for the prior on the latent space, which in turn produces low quality samples. Instead, we trained our structured uncertainty network on a β-VAE <ref type="bibr" target="#b9">[10]</ref> with β = 5. This corresponds to increasing the weight of the KL term on a VAE, which is known to improve sample quality. Our model is able to generate structured residuals of similar quality as those whose z was created by encoding an image. The covariance network is still able to learn meaningful structured uncertainty for the VAE model, as shown in the supplemental material.</p><p>To evaluate the generalization of the model to different regions in the latent space, we show in <ref type="figure" target="#fig_5">Fig. 9</ref>   interpolating between an image and its x-flipped mirror image. The generated images are plausible and the sampled residuals are consistent across the interpolated images.</p><p>To further highlight the differences between the diagonal and predicted covariance noise models, the variance maps for both are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. The diagonal model must explain all the errors with variance, while our model is able to explain some with correlations. The effect of this is evident when sampling from the estimated Σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Denoising Example</head><p>One possible application of the covariance prediction network is image denoising, as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. We hypothesize that our predicted covariance matrices will only span the space of valid face residuals, thus projecting a noisy residual in that space will remove the noise. Here, the noisy image is reconstructed using a VAE, that was not trained with noisy data. The difference between the noisy input and the reconstruction is computed, and projected ontoΣ, which is created by taking the 1000 eigenvectors of Σ with the largest eigenvalues. The projected difference is added to the VAE reconstruction to produce the final denoised image. A comparison is shown with an Autoencoder trained explicitly for denosing with the same architecture as the VAE. Note how the use of the structure covariance model is able to filter the noise to generate plausible structured high-frequency details, while the denoising Autoencoder fails to recover those details.</p><p>Quantitative results for this experiment are shown in Table 4, where MSE is reported for the first 2000 images in the test set. Our model achieves significantly lower error than an Autoencoder trained specifically for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have demonstrated what we believe is the first attempt to train a deep neural network to predict structured covariance matrices for the residual image distribution of unseen image reconstructions. Our results show that ground truth covariances can be learned for toy data and good samples generated for celebA data. A further motivating experiment is also shown for denoising face images.</p><p>An interesting question is whether we can train a generative model and covariance network in tandem to produce better reconstructions. We postulate that a better residual  <ref type="figure" target="#fig_0">Figure 11</ref>: Denoising experiment, left column: original image without noise, second column: image with added noise, third column: denoising autoencoder (DAE) result, fourth column: our result, fifth column: VAE reconstruction from the noisy input, sixth column: difference between the VAE reconstruction and the noisy input, right column: the difference projected onΣ, the matrix constructed with 1000 eigenvectors of Σ. Our result is the sum of the projected difference and the VAE reconstruction. Our model is able to recover fine details that are lost with the DAE approach.</p><formula xml:id="formula_7">x µ + f (s) µ s = x − µ f (s)</formula><p>model would improve the reconstructions, as it may consider plausible but different realizations of high frequency image features as likely. However, there may be some effort required to keep the two networks consistent and additional investigations are needed into priors on the predicted covariances.</p><p>Another questions is what is the best input data to use for the covariance network. Is it best to learn directly from z or µ or some other pre-learned function of either of these? Finally, in this work we have only examined the residual image distribution for reconstruction models trained with a Gaussian likelihood. An interesting avenue to explore would be a structured pixel uncertainty distribution for other reconstruction error metrics, such as perceptual loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Given an input image (a), a simple diagonal Gaussian likelihood model learns a smooth reconstruction (d), with an unstructured residual sample (b). When (b) is added to (d) it generates the unrealistic image (e), demonstrating a failure to capture residual structure. In contrast, we learn a structured residual model, with residual samples like (c) that, when added to (d), generate a plausible and realistic image (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left, an example of the sparsity patterns in the band-diagonal lower-triangular matrices L, that are estimated by our model. Right, the precision matrix Λ = LL T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Reconstructions with samples from Σ. Each image corresponds to a different spline on the test dataset. The ground-truth spline is depicted in blue, and the estimated one in orange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Covariance matrices estimated by our model, where each row corresponds to a different ellipse example on the test set. Much of the structure of the real covariance matrices is recovered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Samples from a β-VAE with residual samples from our model. The covariance network predicts plausible structured residuals for the synthesized images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Samples drawn with our model while interpolating on the latent space, from the top-left input to the bottomright one. Using a fixed noise vector u, samples are drawn from our model as x = µ + Mu, where the covariance Σ = MM T . The residuals are smoothly interpolated for the different images, which suggests that the estimated covariance matrices also vary smoothly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Variance maps for different inputs, where µ and σ 2 are predicted by a diagonal covariance VAE, and diag(Σ) is the diagonal of our estimated covariance matrix. Residual predictions are sampled as ǫ σ 2 ∼ N 0, σ 2 I for the VAE, and ǫ Σ ∼ N 0, Σ for our model. The diagonal noise estimation model mistakenly identifies teeth or skin wrinkles as variance, whereas the covariance model properly identifies them as regions with high covariance, yet low variance. Original Input DAE Ours VAE-Recons Difference Proj. difference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>the result of</figDesc><table>Model 
MSE 

DAE 
5.13e-3 ± 2.52e-3 

Ours 
2.99e-3 ± 7.98e-4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison for denoising in terms of mean squared error (MSE) with respect to the noise-free input. Our model is on average able to produce better results than an Autoencoder trained for denoising.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by the EPSRC CDE (EP/L016540/1) and CAMERA (EP/M023281/1) grants.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 4</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserstein Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Importance weighted autoencoders. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning internal representations from gray-scale images: An example of extensional programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Cognitive Science Society</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial feature learning. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PixelVAE: A Latent Variable Model for Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">betavae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time delay estimation in unknown gaussian spatially correlated noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Nikias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1706" to="1714" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<title level="m">Adversarial symmetric variational autoencoder. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal autocorrelation in univariate linear modeling of fmri data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Woolrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1370" to="1386" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
