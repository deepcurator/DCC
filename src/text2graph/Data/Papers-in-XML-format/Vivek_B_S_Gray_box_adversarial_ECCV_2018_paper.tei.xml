<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gray-box Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><forename type="middle">B S</forename></persName>
							<email>svivek@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gray-box Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>adversarial perturbations</term>
					<term>attacks on machine learning mod- els</term>
					<term>adversarial training</term>
					<term>robust machine learning models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks.</p><p>In this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed "gray-box adversarial attacks" based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named "Graybox Adversarial Training" that uses intermediate versions of the models to seed the adversaries. Experimental evaluation demonstrates that the models trained using our method exhibit better robustness compared to both undefended and adversarially trained models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning models are observed ( <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>) to be susceptible to adversarial examples: samples perturbed with mild but structured noise to manipulate model's output. Further, Szegedy et al. <ref type="bibr" target="#b19">[20]</ref> demonstrated that adversarial samples are transferable across multiple models, i.e., samples crafted to mislead one model often fool other models also. This will enable to launch simple black-box attacks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref> on the models deployed in real world. These methods to generate adversarial samples, generally known as adversaries, range from simple gradient ascent <ref type="bibr" target="#b3">[4]</ref> to complex optimization procedures (e.g., <ref type="bibr" target="#b13">[14]</ref>).</p><p>Augmenting the training data with adversarial samples, known as Adversarial Training (AT) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref> has been introduced as a simple defense mechanism against these attacks. In the adversarial training regime, models are trained with mini-batches comprising of both clean and adversarial samples, typically obtained from the same model. It is shown by Madry et al. <ref type="bibr" target="#b12">[13]</ref> that adversarial training helps to learn models robust to white-box attacks, provided the perturbations computed during the training closely maximize the model's loss. However, in order to scale adversarial training for large datasets, the perturbations can only be crafted with fast and simple methods such as single-step FGSM <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, an attack based on linearization of the model's loss. Tramèr et al. <ref type="bibr" target="#b20">[21]</ref> demonstrated that adversarial training with single-step attacks leads to a degenerate minimum where linear approximation of model's loss is not reliable. They revealed that the model's decision surface exhibits sharp curvature near the data points which leads to overfitting in adversarially trained models. Thus, (i) adversarially trained models using single-step attacks remain susceptible to simple attacks, and (ii) perturbations crafted on undefended models transfer and form black-box attacks.</p><p>Tramèr et al. <ref type="bibr" target="#b20">[21]</ref> proposed to decouple the adversary generation process from the model parameters and to increase the diversity of the perturbations shown to the model during training. Their training mechanism, called Ensemble Adversarial Training (EAT), incorporates perturbations from multiple (e.g., N different) pre-trained models. They showed that EAT enables to learn models with increased robustness against black-box attacks.</p><p>However, EAT has severe drawbacks in presenting diverse perturbations during the training. Since they augment the white-box perturbations (from the model being learned) with black-box perturbations from an ensemble of different pre-trained models, it is required to train those models before we start learning a robust model. Therefore, the computational cost increases linearly with the population of the ensemble. Because of this, the experiments presented in <ref type="bibr" target="#b20">[21]</ref> have a maximum of 4 members in the ensemble. Though it is argued that diverse set of perturbations is important to learn robust models, EAT fails to efficiently bring diversity to the table.</p><p>Unlike EAT , we demonstrate that it is feasible to efficiently generate diverse set of perturbations and augment the white-box perturbations. Further, utilizing these additional perturbations we learn models that are significantly robust compared to those learned with vanilla and ensemble adversarial training (EAT ). The major contributions of this work can be listed as follows:</p><p>-We bring out an important observation that the pseudo robustness of an adversarially trained model is due to the limitations in the existing evaluation procedure. -We introduce a novel evaluation procedure via robustness plots (3.2) and a derived metric "Worst-case Performance (A w )" that can assess the susceptibility of the learned models. For that, we present variants of the white-box and black-box attacks, termed "Gray-box adversarial attacks" that can be launched by temporally evolving intermediate models. Given the efficiency to generate and the ability to examine the robustness, we strongly recommend the community to consider robustness plots and "Worst-case Performance" as standard bench-marking for evaluating the models. Overview of existing and proposed evaluation methods for testing the robustness of adversarially trained network against adversarial attacks. For existing evaluation, best model's robustness against adversarial attack is tested by obtaining it's prediction on adversarial samples generated by itself. Whereas, for the proposed evaluation method adversarial samples are not only generated by best model but also by the intermediate models obtained while training.</p><p>-Harnessing the above observations, we propose a novel variant of adversarial training, termed "Gray-box Adversarial Training" that uses our gray-box perturbations in order to learn robust models.</p><p>The paper is organized as follows: section 2 introduces the notation followed in the subsequent sections of the paper, section 3 presents the drawbacks in the current robustness evaluation methods for deep networks and proposes improved procedure, section 4 hosts the experiments and results, section 5 discusses existing works that are relevant, and section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and terminology</head><p>In this section we define the notations followed throughout this paper:</p><p>-x : clean image from the dataset.</p><p>-x * : a potential adversarial image corresponding to the image x. -y true : ground truth label corresponding to the image x. -y pred : prediction of the neural network for the input image x.</p><p>-ǫ : defines the strength of perturbation added to the clean image.</p><p>-θ : parameters of the neural network.</p><p>-J : loss function used to train the neural network.</p><p>-∇ x J : gradient of the loss J with respect to image x. 3 Gray-box Adversarial Attacks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Limitations of existing evaluation method</head><p>Existing ways of evaluating an adversarially trained network consists of evaluating the best model's accuracy on adversarial samples generated by itself. This way of evaluating the networks gives false inference about their robustness against adversarial attacks. This assumes (though explicitly not mentioned) that robustness to the adversarial samples generated by the best model extends to the adversarial samples generated by the intermediate models evolved during the training, crafted via linear approximation of the loss (e.g., FGSM <ref type="bibr" target="#b3">[4]</ref>). Clearly, this is not true, as shown by our robustness plots (see <ref type="figure" target="#fig_3">Figure 2</ref>). We show this by obtaining robustness plot which captures accuracy of best-model not only on adversarial samples generated by itself but also on adversarial samples generated by the intermediate models, which are obtained during training. Based on the above facts we propose two new ways of evaluating adversarially trained network, shown in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Robustness plot and Worst-case performance</head><p>We propose a new way of evaluating the robustness of a network, which is a plot of recognition accuracy of the best model M Best on adversarial samples of different perturbation strengths ǫ, generated by multiple intermediate models that are obtained during training. That is, performance of the model under investigation is evaluated against the adversaries of different perturbation strengths ǫ, generated by checkpoints or models saved during training. Based on the source of these saved models which seed adversarial samples, we differentiate robustness plot into two categories.</p><p>-If the saved models and the best model are obtained from the same network and also they both have the same training procedure, then we name such robustness plot as Extended White-box robustness plot. Further, we name the attacks as "Extended White-box adversarial attacks". -Else, if the network trained or the training procedure used are different, then we call such robustness plot as Extended Black-box robustness plot and such attacks as "Extended Black-box adversarial attacks".</p><p>In general, we call these attacks as Gray-box adversarial attacks. We believe it is intuitive to call the proposed attacks as "Extensions" to existing white and black box attacks. White-box attack means the attacker has full access to the Whereas, "extended white-box" attack means, some aspects of the setup are known while some are not. Specifically, the architecture and the training procedure of the source and target model are same while the model parameters differ. Similarly, black-box attack means the scenario where the attacker has no information about the target such as architecture, parameters, training procedure, etc. Generally, the source model would be a fully trained model that has different architecture (and hence parameters) compared to the target model. However, the "extended black-box" attack mean, the source model can be a partially trained model having different network architecture. Note that it is not very different compared to the existing black-box attack except the source model can now be a partially trained model. We jointly call these two extended attacks as "Gray-box Adversarial Attacks". <ref type="table" target="#tab_1">Table 1</ref> lists the definitions of both the existing and our extended attacks with the notation we introduced earlier in the paper.</p><p>Worst-case performance (A w ): We introduce a metric derived from the proposed robustness plot to infer the susceptibility of the trained model quantitatively in terms of its weakest performance. We name it "Worst-case Performance (A w )" of the model, which is the least recognition accuracy achieved for a given attack strength (ǫ). Ideally, for a robust model the value of A w should be high (close to its performance on clean samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Which attack to use for adversarial training, FGSM, FGSM-LL or FGSM-Rand?</head><p>Kurakin et al. <ref type="bibr" target="#b8">[9]</ref> suggested to use FGSM-LL or FGSM-Rand variants for adversarial training, in order to reduce the effect of label leaking <ref type="bibr" target="#b8">[9]</ref>. Label leaking effect is observed in adversarially trained networks, where the accuracy of the model on the adversarial samples of higher perturbation is greater than that on the adversarial samples of lower perturbation. It is necessary for adversarial training to include stronger attacks in the training process in order to make the model robust. We empirically show (in sec. 4.2 and <ref type="figure" target="#fig_4">Fig. 3</ref>) that FGSM is stronger attack compared to FGSM-LL and FGSM-Rand through robustness plots with the three different attacks for a normally trained network.</p><p>In addition, for models (of same architecture) adversarially trained using FGSM, FGSM-LL and FGSM-Rand attacks respectively, FGSM attack causes more damage compared to other two attacks. We show (in sec. 4.2 and <ref type="figure" target="#fig_5">Fig. 4</ref>) this by obtaining robustness plots with FGSM, FGSM-LL, and FGSM-Rand attacks respectively, for all three variants of adversarial training methods. Based on these observations we use FGSM for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Gray-box Adversarial Training</head><p>Based on the observations presented in sec. 3.1, we propose Gray-box Adversarial Training in Algorithm 1 to alleviate the drawbacks of existing adversarial training. During training, for every iteration, we replace a portion of clean samples in the mini-batch with its corresponding adversarial samples which are generated not only by the current state of the network but also by one of the saved intermediate models. We use "drop in training loss" as a criterion for saving these intermediate models during training i.e., for every D drop in the training loss we save the model and this process of saving the models continues until training loss reaches minimum prefixed value EL (End Loss).</p><p>The intuition behind using "drop in training loss" as a criterion for saving the intermediate models is that, models substantially apart in the training (evolution) process can source different set of adversaries. Having variety of adversarial samples to participate in the adversarial training procedure makes the model robust <ref type="bibr" target="#b20">[21]</ref>. As training progresses, network representations evolve and loss decreases from the initial value. Thus, we use the "drop in training loss" as a useful index to pick source models that can potentially generate different adversaries. We represent this quantity as D in the proposed algorithm.</p><p>Ideally we would like to have an ensemble of as many different source models as possible. However, bigger ensembles would pose additional challenges such as computational, memory overheads and slow down the training process. Therefore, D has to be picked depending on the trade-off between performance and overhead. Note that too small value of D will include highly correlated models in the ensemble. Also, towards later stages of the training, model evolves very slowly and representations would not change significantly. So, after some time into the training, we stop augmenting the ensemble of source models. For this we define a parameter denoted as EL (End Loss) which is a threshold on the loss that defines when to stop saving the intermediate models. During the training process, once the loss falls below EL, we stop saving of intermediate models and prevent augmenting the ensemble with redundant models.</p><p>Further, in the best case we would like to pick different saved model for every iteration during our Gray-box Adversarial Training. However, this creates bottleneck because loading a saved model at each iteration is time consuming. In order to reduce this additional overhead, we pick a saved model and use this for T consecutive iterations after which we pick another saved model in a roundrobin fashion. In total, we have three hyper-parameters namely, D, EL, and T , and we show the effect of these hyper-parameters in sec. 4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In our experiments we show results on CIFAR-100, CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>, and MNIST <ref type="bibr" target="#b9">[10]</ref> dataset. We work with WideResNet-28-10 <ref type="bibr" target="#b21">[22]</ref> for CIFAR-100, ResNet-18 <ref type="bibr" target="#b5">[6]</ref> for CIFAR-10 and LeNet <ref type="bibr" target="#b10">[11]</ref> for MNIST dataset, all these networks achieve near state of the art performance on the respective dataset. These networks are trained for 100 epochs (25 epochs for LeNet) using SGD with momentum, and models are saved at the end of each epoch. For learning rate scheduling, steppolicy is used. We pre-process images to be in [0, 1] range, and random crop and horizontal flip are performed for data-augmentation (except for MNIST). Experiments and results on MNIST dataset are shown in supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Limitations of existing evaluation method</head><p>In this subsection, we present the relevant experiments to understand the issues present in the existing evaluation method as discussed in Section. 3.1. We adversarially train, WideResNet-28-10 and ResNet-18 on CIFAR-100 and CIFAR-10 datasets respectively and while training, FGSM is used for adversarial sample generation process. After training, we obtain their corresponding Extended White-box robustness plot using FGSM attack. <ref type="figure" target="#fig_3">Figure 2</ref> shows the obtained Ex-</p><formula xml:id="formula_0">M 1</formula><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 20</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 40</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 60</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 80</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 100</head><p>Adv.</p><p>Model from which adversarial samples are generated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 20</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 40</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 60</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 80</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 100</head><p>Adv.</p><p>Model from which adversarial samples are generated ) is poor for the attacks generated by early models (towards origin) as opposed to that by the later models.</p><p>tended White-box robustness plot. It can be observed that adversarially trained networks are not robust to the adversaries generated by the intermediate models, which the existing way of evaluation fails to capture. It also infers that the implicit assumption of best model's robustness to adversaries generated by the intermediate models is false. We also reiterate the fact that existing adversarial training formulation does not make the network robust but makes them to generate weaker adversaries. We perform normal training of WideResNet-28-10 on CIFAR-100 dataset and obtain its corresponding Extended White-box robustness plots using FGSM, FGSM-LL, and FGSM-Rand attacks respectively. <ref type="figure" target="#fig_4">Figure 3</ref> shows the obtained plots. It is clear that FGSM attack (column-1) produces stronger attacks compared to FGSM-LL (column-2) and FGSM-Rand (column-3) attacks. That is, drop in the model's accuracy is more for FGSM attack. Additionally, we adversarially train the above network using FGSM, FGSM-LL and FGSM-Rand respectively for adversarial sample generation process. After training, we obtain robustness plots with FGSM, FGSM-LL and FGSM-Rand attacks respectively. <ref type="figure" target="#fig_5">Figure 4</ref> shows the obtained Extended White-box robustness plots for all the three versions of adversarial training. It is observed that the model is more susceptible to FGSM attack (column-1) compared to other two attacks. Similar trends are observed for networks trained on CIFAR-10 and MNIST datasets and corresponding results are shown in supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Gray-box adversarial training</head><p>We train WideResNet-28-10 and Resent-18 on CIFAR-100 and CIFAR-10 datasets respectively, using the proposed Gray-box Adversarial Training (GAT) algorithm. In order to create strong adversaries, we chose FGSM to generate adversarial samples. We use the same set of hyper-parameters D= 0.2, EL= 0.5, and T = (1/4 th of an epoch), for all the networks trained. Specifically, we save intermediate models for every 0.2 drop (D) in the training loss till the loss falls below 0.5 (EL). Also, each of the saved models are sampled from the ensemble, generates adversarial samples for 1/4 th of an epoch. After training, we obtain Extended White-box robustness plots and Extended Black-box robustness plots for networks trained with the Gray-box adversarial training (Algorithm 1) and for adversarially trained networks. <ref type="figure" target="#fig_10">Figure 5</ref> shows the obtained robustness plots, it is observed from the plots that in the proposed gray-box adversarial training (row-2) there are no deep valleys in the robustness plots, whereas for the networks trained using existing adversarially training method (row-1) exhibits deep valley in the robustness plots. <ref type="table" target="#tab_2">Table 2</ref> presents the worst-case accuracy A w of the models trained using different training methods. Note that A w is significantly better for the proposed GAT compared to the model trained with AT and EAT for a wide range of attack strength (ǫ). Effect of hyper-parameters: In order to study the effect of hyper parameters, we train ResNet-18 on CIFAR-10 dataset using Gray-box adversarial training for different hyper-parameter settings. Extended White-box robustness plots are obtained for each setting with two of them fixed and the other being varied. The hyper-parameter D defines the value of "drop in training loss" for which intermediate models are saved to generate adversarial samples. <ref type="figure">Figure 6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 20</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 40</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 60</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 80</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 100</head><p>Adv. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 20</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 40</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 60</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 80</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 100</head><p>Adv. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 20</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 40</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 60</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 80</head><p>Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M 100</head><p>Adv. Adversarial training using FGSM, Column-3:Gray-box adversarial training observe that as EL increases the width of the valley increases since higher values of EL prevents saving of potential models. Finally, the hyper-parameter T decides the duration for which a member of ensemble is used after getting sampled from the ensemble to generate adversarial samples. <ref type="figure">Figure 6(c)</ref> shows the effect of varying T from 1/4 th epoch to 3/4 th epoch. Note that T has minimal effect on the robustness plot within that range.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensemble adversarial training</head><p>In this subsection, we compare our Gray-box Adversarial Training against Ensemble Adversarial Training <ref type="bibr" target="#b20">[21]</ref>. We work with the networks on CIFAR-100 and CIFAR-10 datasets. Ensemble adversarial training uses fixed set of pre-trained models for generating adversarial samples along with the current state of the network. For each iteration during training, the source model for generating adversaries is picked at random among the current and ensemble models. <ref type="table" target="#tab_3">Table 3</ref> shows the setup used for ensemble adversarial training, which contains network to be trained, pre-trained source models used for generating adversarial samples and the held-out model used for black-box attack. <ref type="figure" target="#fig_12">Figure 7</ref> shows the Extended White-box robustness plots for the networks trained using ensemble adversarial training algorithm. Note the presence of deep and wide valleys in the plot. Whereas, the Extended White-box robustness plots for the models trained using the Gray-box adversarial training shown in <ref type="figure" target="#fig_10">figure 5</ref> (row:2,column:3), do not have deep and wide valley. Also, because of the space restrictions, Extended Black-box robustness plots for the above trained networks using ensemble adversarial training algorithm are shown in supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Following the findings of Szegedy et al. <ref type="bibr" target="#b19">[20]</ref>, various attack methods (e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>) and various defense techniques (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19]</ref>) have been proposed. On the defense side, adversarial training shows promising results. In order to scale adversarial training <ref type="bibr" target="#b8">[9]</ref> to large datasets, single-step attack methods (use 1st order approximation of model's loss to generate attack ) are used while training. Goodfellow et al. <ref type="bibr" target="#b3">[4]</ref> observed that adversarially trained models, incurs higher loss on transferred samples than on the white-box single-step attacks. Further, Kurakin <ref type="bibr" target="#b8">[9]</ref> observed that adversarially trained models were susceptible to adversarial samples generated using multi-step methods under white-box setting. This paradoxical behaviour is explained by Tramèr et al. <ref type="bibr" target="#b20">[21]</ref> through the inaccuracy of linear approximation of the model's loss function in the vicinity of the data samples. Madry et al. <ref type="bibr" target="#b12">[13]</ref> showed that adversarially trained model can become robust against white-box attacks, if adversaries added during training closely maximizes the model's loss. However, such methods are hard to scale to difficult tasks such as ILSVRC <ref type="bibr" target="#b8">[9]</ref>. Tramèr et al. <ref type="bibr" target="#b20">[21]</ref> showed that having diversity in the adversarial samples presented during the training can alleviate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Presence of adversarial samples indicates the vulnerability of the machine learning models. Learning robust models and measuring their susceptibility against adversarial attacks are need of the day. In this paper, we demonstrated important gaps in the existing evaluation method for testing the robustness of a model against adversarial attacks. Also, we proposed a novel evaluation method called "Robustness plots" and a derived metric "Worst-case performance (A w )". From the proposed evaluation method, it is observed that the existing adversarial training methods which use first order approximation of loss function for generating samples, do not make the model robust. Instead, they make the model to generate weaker adversaries. Finally, the proposed "Gray-box Adversarial Training (GAT )" which harnesses the presence of stronger adversaries during training, achieves better robustness against adversarial attacks compared to the existing adversarial training methods (that follow single step adversarial generation process).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Overview of existing and proposed evaluation methods for testing the robustness of adversarially trained network against adversarial attacks. For existing evaluation, best model's robustness against adversarial attack is tested by obtaining it's prediction on adversarial samples generated by itself. Whereas, for the proposed evaluation method adversarial samples are not only generated by best model but also by the intermediate models obtained while training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Gray-box adversarial training of network N Input: m = Size of the training minibatch k = No. of adversarial images in minibatch generated using current state of the network N p = No. of adversarial images in minibatch generated using i th state of the network N M axItertion = Maximum training iterations Number of clean samples in minibatch = m − k − p Hyper-parameters EL, D and T 1 Initialization Randomly initialize network N /*Set containing the iterations at which seed models are saved*/ AdvBag ={} AdvP tr=0 /*Pointer to elements in set 'AdvBag'*/ i=0 /*Refers to initial state of the network*/ Initialize LossSetP oint with initial training loss iteration = 0 2 while iteration = M axItertion do 3 Read minibatch B = {x 1 , ..., x m } from training set 4 Generate 'k' adversarial examples {x 1 adv , ..., x k adv } from corresponding clean samples {x 1 , ..., x k } using current state of the network N 5 Generate 'p' adversarial examples {x k+1 adv , ..., x k+p adv } from corresponding clean samples {x k+1 , ..., x k+p } using i th state of the network Npass, compute loss, backward pass, and update parameters*/ 8 Do one training step of Network N using minibatch B * 9 /*moving average loss computed over 10 iterations*/ LossCurrentV alue = M ovingAverage(loss) 10 /*Logic for saving seed model */ 11 if (LossSetP oint − LossCurrentV alue) ≥D and LossSetP oint ≥EL then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Extended White-box robustness plots with FGSM attack, obtained for (a) WideResNet-28-10 adversarially trained on CIFAR-100 dataset, (b) ResNet-18 adversarially trained on CIFAR-10 dataset. Note that the classification accuracy of the best model (M Best Adv ) is poor for the attacks generated by early models (towards origin) as opposed to that by the later models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Extended White-box robustness plots of WideResNet-28-10 normally trained on CIFAR-100 dataset, obtained using FGSM (column-1), FGSM-LL (column-2) and FGSM-Rand (column-3) attacks. Note that for a wide range of perturbations, FGSM attack causes more dip in the accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Extended White-box robustness plots of WideResNet-28-10 trained on CIFAR-100 dataset using different adversarial training methods, obtained using FGSM (column-1), FGSM-LL (column-2) and FGSM-Rand (column-3) attacks. Rows represents training method used, Row-1 : Adversarial training using FGSM, Row-2 : Adversarial training using FGSM-LL and Row-3 : Adversarial training using FGSM-Rand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) shows the effect of varying D from 0.2 to 0.4. It is observed that for higher values of D, the depth and the width of the valley increases. This is because choosing higher values of D might miss saving of models that are potential sources for generating stronger adversarial samples, and also choosing very low values of D will results in saving large number of models that are redundant and may not be useful. The hyper-parameter EL decides when to stop saving of intermediate seed models. Figure 6(b) shows the effect of EL for fixed values of D and T . We</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Gray.Adv. M 20 Gray.Adv. M 40 Gray.Adv. M 60 Gray.Adv. M 80 Gray.Adv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Gray.Adv. M 20 Gray.Adv. M 40 Gray.Adv. M 60 Gray.Adv. M 80 Gray.Adv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Robustness plots obtained using FGSM attack for (a)WideResNet-28-10 trained on CIFAR-100 and (b) ResNet-18 trained on CIFAR-10. Rows represents the training method, Row-1:Model trained adversarially using FGSM and Row-2:Model trained using Gray-box adversarial training. Adversarial samples are generated by intermediate models of Column-1:Normal training, Column-2: Adversarial training using FGSM, Column-3:Gray-box adversarial training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>5 Fig. 6 :</head><label>56</label><figDesc>Fig. 6: Extended White-box robustness plots of ResNet-18 trained on CIFAR-10 dataset using Gray-box adversarial training (algorithm 1), for different hyperparameters settings. Effects of hyper-parameters are shown in (a) Effect of D, (b) Effect of EL and (c) Effect of T (measured in fraction of an epoch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Extended White-box robustness plots of models trained using ensemble adversarial training algorithm. (a)models trained on CIFAR-100 dataset and (b)models trained on CIFAR-10 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>-Best Model (M Best ): Model corresponding to least validation loss, typically obtained at the end of the training. -M i t : represents model at i th epoch of training, obtained when network M is trained using method 't'.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>List of the adversarial attacks. Note that the subscript denotes the training procedure how the model is trained, superscript denotes the training epoch at which the model is considered. MAdv. i = 1, . . . , MaxEpoch MAdv. Extended Black-box attack target model: architecture, parameters, training data and procedure. Typically, source model that creates adversaries is same as the target model under attack.</figDesc><table>i denotes intermediate model and 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>GAT (ours) 89.46 85.89 79.28 60.81</figDesc><table>Worst case accuracy of models trained on (a) CIFAR-100 and (b) 
CIFAR-10, using different training methods. For ensemble adversarial training 
(EAT ) refer to section 4.4. A, B and C refers to the ensemble used in EAT . 

(a) CIFAR-100 

Training 
Aw for various ǫ 
Method 
2/255 4/255 8/255 16/255 
Normal 
20.81 11.58 7.09 
4.8 
AT 
70.04 62.34 44.04 18.58 

EAT 

A 65.39 55.55 36.8 
15.2 
B 65.45 54.63 35.65 14.26 
C 65.71 56.05 37.43 15.98 
GAT (ours) 70.84 66.5 65.43 67.41 

(b) CIFAR-10 

Training 
Aw for various ǫ 
Method 
2/255 4/255 8/255 16/255 
Normal 
38.15 20.84 12.2 
9.34 
Adversarial 87.08 79.25 56.09 22.87 

EAT 

A 
83.27 73.4 53.96 31.01 
B 
82.56 72.35 56.11 34.84 
C 
82.64 73.84 55.19 32.33 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Setup for ensemble adversarial training</figDesc><table>Network to be trained 
Pre-trained Models 
Held-out Model 
WideResNet-28-10(Ensemble-A) 
Resnet-50, ResNet-34 
WideResNet-28-10 
CIFAR-100 WideResNet-28-10(Ensemble-B) WideResNet-28-10, ResNet-50 
ResNet-34 
WideResNet-28-10(Ensemble-C) WideResNet-28-10, ResNet-34 
ResNet-50 
ResNet-34(Ensemble-A) 
ResNet-34, ResNet-18 
VGG-16 
CIFAR-10 
ResNet-34(Ensemble-B) 
ResNet-34, VGG-16 
ResNet-18 
ResNet-34(Ensemble-C) 
ResNet-18, VGG-16 
ResNet-34 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pattern recognition systems under attack: Design issues and research challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-22" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence. AISec &apos;11</title>
		<meeting>the 4th ACM Workshop on Security and Artificial Intelligence. AISec &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/7" />
		<title level="m">The mnist database of handwritten digits</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/lenet/8" />
		<title level="m">Lenet-5</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dimitris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast feature fool: A data independent approach to universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC</title>
		<meeting>the British Machine Vision Conference (BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NAG: Network for adversary generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE European Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Security and Privacy</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia Conference on Computer and Communications Security</title>
		<imprint>
			<publisher>ASIACCS</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Defense-GAN: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>Richard C. Wilson, E.R.H., Smith, W.A.P.</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
