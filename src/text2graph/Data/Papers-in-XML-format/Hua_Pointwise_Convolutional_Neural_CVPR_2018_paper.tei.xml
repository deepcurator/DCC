<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pointwise Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pointwise Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning with 3D data has received great research interests recently, which leads to noticeable advances in typical applications including scene understanding, shape completion, and shape matching. Among these, scene understanding is considered as one of the most important tasks for robots and drones as it can assist exploratory scene navigations. Tasks such as semantic scene segmentation and object recognition are often performed to predict contextual information about objects for both indoor and outdoor scenes.</p><p>Unfortunately, deep learning in 3D was deemed difficult due to the fact that there are several ways to represent 3D data such as volumes, point clouds, or multi-view images. Volume representation is a true 3D representation and straightforward to implement but often requires a large amount of memory for data storage. By contrast, multi-view representation is not a true 3D representation but shows promising prediction accuracy as existing pre-trained weights from 2D networks can be utilized. Among such representations, point clouds have been the most flexible as they are compact and <ref type="figure">Figure 1</ref>: Pointwise convolution. We define a new convolution operator for point cloud input. For each point, nearest neighbors are queried on the fly and binned into kernel cells before convolving with kernel weights. By stacking pointwise convolution operators together, we can build fully convolutional neural networks for scene segmentation and object recognition for point clouds.</p><p>could be exported from a wide range of CAD modelling and 3D reconstruction software. However, the capability of using point clouds with neural network has been so far not fully explored.</p><p>In this paper, we present a convolutional neural network for semantic segmentation and object recognition with 3D point clouds. At the core of our network is a new convolution operator, called pointwise convolution, which can be applied at each point in a point cloud to learn pointwise features. This leads to surprisingly simple and fully convolutional network designs for scene segmentation and object recognition. Our experiments show that pointwise convolution can yield competitive accuracy to previous techniques while being much simpler to implement. In summary, our contributions are:</p><p>• A pointwise convolution operator that can output features at each point in a point cloud;</p><p>• Two pointwise convolutional neural networks for semantic scene segmentation and object recognition. (n × 9) (n × c) (n × 9) (n × 9) (n × 9) (n × 36) Semantic segmentation Category (n × 40)</p><formula xml:id="formula_0">(512)<label>(40)</label></formula><p>Pointwise convolution Concatenation (concat)</p><p>Fully connected (fc)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point cloud</head><p>Figure 2: Pointwise convolutional neural network. The input point cloud is fed into each convolution operator, and all outputs are concatenated before being fed to a a final convolution layer for dense semantic segmentation, or to fully connected layers for object recognition. In this figure, we assume point cloud with n points and c attributes (colors, normals, coordinates, etc.). We use 9 output channels for each convolution operator before concatenation. Source code is available at our homepage <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Recently, there has been a great number of works about deep learning with 3D data. Let us focus on those for scene understanding tasks such as semantic segmentation and object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Shape descriptors</head><p>Hand-crafted shape descriptors were widely used in computer vision and graphics applications before the era of deep learning. For example, 3D shapes can be projected into 2D images and represented by a set of 2D descriptors on such images. Shapes can then be represented as histograms or bag-of-feature models which can be constructed from surface normals and curvatures <ref type="bibr" target="#b11">[12]</ref>. 3D shapes can also be represented by their inherent statistical properties, such as distance distribution <ref type="bibr" target="#b24">[25]</ref> and harmonic descriptors <ref type="bibr" target="#b14">[15]</ref>. Heat kernel signatures extract shape descriptions by simulating an heat diffusion process on 3D shapes <ref type="bibr" target="#b37">[38]</ref>. The Light Field Descriptor (LFD) is another popular descriptor useful in the shape classification tasks. It extracts geometric and Fourier descriptors from object silhouettes rendered from several different viewpoints <ref type="bibr" target="#b3">[4]</ref>. Despite their long history and being widely used, hand-crafted 3D shape descriptors do not generalize well across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object recognition</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b16">[17]</ref> has been successfully applied in various areas of computer vision and artificial intelligence. Recently, significant achievements have been reached in understanding images through learning features by CNNs. Large RGB image datasets like ImageNet <ref type="bibr" target="#b6">[7]</ref> can be used in training a CNN, which is in turn able to learn general purpose image descriptors from such datasets. Image descriptors generated by CNNs are proved to greatly outperform other hand-crafted features for various tasks, including object detection <ref type="bibr" target="#b8">[9]</ref>, scene recognition <ref type="bibr" target="#b7">[8]</ref>, texture recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b5">6]</ref> and classification <ref type="bibr" target="#b9">[10]</ref>.</p><p>Recently, several approaches to using 3D convolutional networks to extract shape descriptor have been proposed, ranging from voxel-based representation <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b19">20]</ref> panorama <ref type="bibr" target="#b31">[32]</ref>, feature pooling from 2D projections from multiple viewpoints <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27]</ref>, to point set <ref type="bibr" target="#b26">[27]</ref>. Among these, Qi et al. <ref type="bibr" target="#b25">[26]</ref>'s PointNet is one of the first network architectures that can handle point cloud data. PointNet is robust as it can learn an order-invariance function to canonicalize input point clouds. Subsequently, PointCNN <ref type="bibr" target="#b17">[18]</ref> explored the idea of equivariance instead of invariance and demonstrated competitive performance to PointNet. To achieve scalability, it is also possible to learn representations on unstructured point clouds by building computational graphs based on hierarchical data structures such as octree <ref type="bibr" target="#b29">[30]</ref> and kd-tree <ref type="bibr" target="#b15">[16]</ref>.</p><p>Despite their competitive performance, network structures based on PointNet <ref type="bibr" target="#b25">[26]</ref> are rather complex. In this work, we show that it is possible to perform scene understanding tasks such as semantic segmentation and object recognition on ordered point clouds. We design pointwise convolution, a simple convolution operator for 3D point cloud and use it to make (fully) convolutional neural networks for object recognition and semantic segmentation. With the availability of our pointwise convolution, we aim to pave the way towards adapting many existing network architecture designed for scene understanding with color and RGB-D images <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref> to the 3D domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Semantic segmentation</head><p>There are considerably great numbers of related works in semantic segmentation. Since the introduction of the NYUv2 dataset from Silberman et al. <ref type="bibr" target="#b32">[33]</ref>, there has been a spark in the direction of RGBD semantic segmentation. The work from Long et al. <ref type="bibr" target="#b18">[19]</ref> showed how to adopt a conventional classifcation network for the semantic segmentation problem. Since then, different techniques have been proposed to further improve the segmentation results. Some notable examples are SegNet <ref type="bibr" target="#b1">[2]</ref> which employs an encoder-decoder architecture, or the dilation filter <ref type="bibr" target="#b43">[43]</ref>.</p><p>In the 3D domain, interactive semantic segmentation <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b39">39]</ref> relied on user strokes to propagate segmentation. McCormac et al. <ref type="bibr" target="#b20">[21]</ref> explored transfering semantic segmentation from 2D predictions to the 3D domain. An advantage of such methods is that they can produce high-resolution segmentation. However, none of the predictions can be performed directly in the 3D domain.</p><p>SSCNet <ref type="bibr" target="#b35">[36]</ref> applied convolutional neural network to a 3D volume representation to classify each voxel in the scene. This could be flexible as real-time scene reconstruction techniques such as KinectFusion <ref type="bibr" target="#b22">[23]</ref> and voxel hashing <ref type="bibr" target="#b23">[24]</ref> are often based on volumes. PointNet <ref type="bibr" target="#b25">[26]</ref> can also be used for semantic segmentation with minor modifications from their object recognition network.</p><p>Recently, Qi et al. <ref type="bibr" target="#b28">[29]</ref> proposed to build a graph neural network for semantic segmentation on a point cloud, where each graph node is a group of points and graph edges are constructed by nearest neighbor search on the point cloud. Their results are shown with RGB-D images, where color features from a pre-trained VGG-16 network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref> are used to initialize the prediction. Here, we demonstrate a fully convolutional neural network for 3D point cloud segmentation. Compared to the method by Qi et al. <ref type="bibr" target="#b28">[29]</ref>, we train our network from scratch. The input point cloud is also more general such as CAD models or 3D meshes reconstructed from RGB-D sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pointwise Convolution</head><p>Before presenting pointwise convolution, we briefly revise a few possibilities to represent 3D data for neural network. The most straightforward approach is perhaps to employ volumetric representation. For example, VoxNet <ref type="bibr" target="#b19">[20]</ref> represents each object by a volume up to 64 × 64 × 64 resolution. This is natural because almost existing network architecture for image applications can be adopted. However, a significant drawback is that volumetric representation requires a large amount of memory while the number of non-zero values in a volume only accounts for a very small percentage. This could be addressed by a sparse representation <ref type="bibr" target="#b29">[30]</ref>.</p><p>A second possibility is to use point clouds. This is a direct representation as point cloud is often the output of many applications such as RGB-D reconstruction and CAD modeling. However, mapping point cloud to neural network is not natural because traditional convolution operators are only designed for grid and volumes. PointNet <ref type="bibr" target="#b25">[26]</ref> implements point feature learning by fully connected layers.</p><p>The previous limitations motivate us to design fully convolutional networks for point clouds. The basic building block of our architecture is a convolution operator applied at each point in a point cloud, which we term the pointwise convolution. This operator works as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution.</head><p>A convolution kernel is centered at each point of a point cloud. Neighbor points within the kernel support can contribute to the center point. Each kernel has a size or radius value, which can be adjusted to account for different number of neighbor points in each convolution layer. <ref type="figure">Figure 1</ref> shows a diagram that demonstrates this idea.</p><p>Formally, pointwise convolution can be written as</p><formula xml:id="formula_1">x ℓ i = k w k 1 | Ω i (k) | pj ∈Ωi(k) x ℓ−1 j ,<label>(1)</label></formula><p>where k iterates over all sub-domains in the kernel support; Ω i (k) is the k-th sub-domain of the kernel centered at point i; p i is the coordinate of point i; | · | counts all points within the sub-domain; w k is the kernel weight at the k-th subdomain, x i and x j the value at point i and j, and ℓ − 1 and ℓ the index of the input and output layer.</p><p>Gradient backpropagation. To make pointwise convolution trainable, it is necessary to compute the gradients with respects to the input data and the kernel weights. Let L is the loss function. The gradient with respect to input could be defined as</p><formula xml:id="formula_2">∂L ∂x ℓ−1 j = i∈Ωj ∂L ∂x ℓ i ∂x ℓ i ∂x ℓ−1 j (2)</formula><p>where we iterate over all neighbor points i of a given point j. In the chain rule, ∂L/∂x ℓ i is the gradient up to layer ℓ, which is known during back propagation. The derivative ∂x ℓ i /∂x ℓ−1 j could be written as</p><formula xml:id="formula_3">∂x ℓ i ∂x ℓ−1 j = k w k 1 | Ω i (k) | pj ∈Ωi(k) 1<label>(3)</label></formula><p>Similarly, the gradient with respect to kernel weights could be defined by iterating over all points i:</p><formula xml:id="formula_4">∂L ∂w k = i ∂L ∂x ℓ i ∂x ℓ i ∂w k<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">∂x ℓ i ∂w k = 1 | Ω i (k) | pj ∈Ωi(k) x ℓ−1 j (5)</formula><p>Note that the above formula does not assume a specific shape for convolution kernel. Here we simply use a uniform grid kernel. In conjunction with an acceleration structure for neighbor query, e.g., grid, the convolution operator can be efficiently implemented on both CPU and GPU. In this paper, we use convolution kernels of size 3 × 3 × 3. All points within each kernel cell have the same weights.</p><p>Unlike convolution in volumes, in our design, we do not use pooling. There are some advantages of doing so. First, it is no longer required to deal with point cloud downsampling and upsampling, which is not straightforward when the point attributes become high dimensional when the point cloud is processed in the network. Second, by keeping the point cloud unchanged in the entire network, acceleration structures for neighbor query only need to be built once. This significantly speeds up computation and simplifies network design.</p><p>Point order. A notable difference between our design and PointNet <ref type="bibr" target="#b25">[26]</ref> is how points are ordered before being fed to the network. In PointNet, point cloud is orderless, and the training process of PointNet learns a symmetric function to turn an ordered point cloud into order invariant. However, we argue that this might not be necessary. In our method, we input points sorted in a specific order, e.g., XYZ or Morton curve <ref type="bibr" target="#b21">[22]</ref>, to the network and can still achieve competitive performance in the object recognition task. In this task, the order of the points only affects the final global feature vector used to predict the object category. In semantic segmentation, in principle we can leverage local features at each point, and hence point order is not necessary.</p><p>A-trous convolution. The original pointwise convolution can be easily extended toà-trous convolution by including a stride parameter that determines the gaps between kernel cells. The benefit of pointwiseà-trous convolution is that it is possible to extend the kernel size, and hence the perceptive field, without actually processing too many points in the convolution. This yields significant speed up without sacrificing accuracy as to be demonstrated in our experiments.</p><p>Point attributes. For easy housekeeping in the implementation of our convolution operator, we separately store point coordinates and other point attributes such as colors, normals, or other high-dimensional features output from preceding convolutional layers. Point coordinates can be passed to any layer despite the layer depth so that they can be used for neighbor queries to determine which points can participate in the convolution at a particular point. Point attributes can then be retrieved accordingly.</p><p>Relevance to geometric deep learning. Our pointwise convolution is relevant to geodesic convolution in geometric deep learning <ref type="bibr" target="#b2">[3]</ref>, which is more robust for tasks such as non-rigid shape correspondences and retrieval. To compute a geodesic convolution at a particular point, only neighbor points on its local surface manifold are considered. This is achieved by definition because the filter support in geodesic convolution is directly defined on the surface manifold. By contrast, our pointwise convolution operates adaptively in the 3D Euclidean space, and does not require any surface definition to operate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluations</head><p>Semantic segmentation. We evaluate our pointwise convolutional neural network with semantic scene segmentation and object recognition. For scene segmentation, we first experiment with the S3DIS dataset <ref type="bibr" target="#b0">[1]</ref>, which has 13 categories of indoor scene objects. Each point has 9 attributes: XYZ coordinates, RGB color, and normalized coordinates w.r.t. the room space it belongs to. To perform segmentation of a scene, each squared-meter block of the scene (measured on the floor), sampled to 4096 points, are fed into the network. The predictions of all blocks are then assembled to obtain the prediction of the entire scene.</p><p>We report per-point accuracy of the semantic segmentation. As shown in <ref type="table">Table 1</ref>, our network is able to produce comparable accuracy to PointNet <ref type="bibr" target="#b25">[26]</ref>, with the accuracy of 81.5%. <ref type="table">Table 2</ref> reports per-class accuracy. <ref type="figure">Figure 3</ref> shows visualization of predictions and ground truths of the scenes in the evaluation dataset. To further test semantic segmentation with more categories and more complex indoor scenes, we annotate 76 scenes from the SceneNN dataset <ref type="bibr" target="#b12">[13]</ref> with 40 categories defined by the NYU v2 dataset <ref type="bibr" target="#b32">[33]</ref>. Scenes in this dataset appear to be more cluttered, which poses great challenges to semantic segmentation. We use 56 scenes for training, and 20 scenes for evaluation. In each scene, a 2 × 2 sqm. window with stride 0.2 meter and height 2 meters is used to scan the floor area, resulting in approximately 30K scene blocks for training and 15K blocks for testing. Each block is sampled to 4096 points.</p><p>For SceneNN dataset, we additionally compare with VoxNet <ref type="bibr" target="#b19">[20]</ref>, a voxel-based representation technique, and SemanticFusion <ref type="bibr" target="#b20">[21]</ref>, a multi-view 2D-3D semantic segmentation with RGB-D images. For VoxNet <ref type="bibr" target="#b19">[20]</ref>  <ref type="table">Table 2</ref>: Per-class accuracy of semantic segmentation on S3DIS dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>network to predict labels of scene blocks as described above and gather all outputs into a final scene prediction. For SemanticFusion <ref type="bibr" target="#b20">[21]</ref>, we perform 2D semantic segmentation on the RGB-D images independently and then integrate all 2D predictions to a 3D point cloud to generate the final segmentation.</p><p>(a) Our predictions (b) Ground truth <ref type="figure">Figure 3</ref>: Semantic segmentation on the S3DIS dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>The visualization of the predictions and ground truth are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. It can be seen that structures like wall and floor have very good accuracy, and small objects are moderately well segmented. A notable issue is noise due to prediction inconsistency in the overlap regions of the blocks. This could be addressed by a conditional random field and would be an interesting future work. <ref type="table">Table 3</ref> reports the accuracy of a few common categories. While structures and chairs are quite accurate, table and desk are often ambiguous, resulting in lower accuracy for both classes. In general, the performance of VoxNet <ref type="bibr" target="#b19">[20]</ref> is inferior to ours and SemanticFusion <ref type="bibr" target="#b20">[21]</ref> due to limited resolution (we used 64 3 volume). Our method works competitively to SemanticFusion, but note that our method does not apply any label smoothing while SemanticFusion has a conditional random field to remove noise after propagating predictions from 2D to 3D.   <ref type="table">Table 3</ref>: Per-class accuracy of semantic segmentation on SceneNN dataset <ref type="bibr" target="#b12">[13]</ref>.</p><p>Object recognition. We evaluate object recognition with two datasets, ModelNet40 <ref type="bibr" target="#b42">[42]</ref> and ObjectNN <ref type="bibr" target="#b13">[14]</ref>. ModelNet40 is a CAD model dataset of 40 categories which has served as a standard benchmark for object recognition in the recent years. On the other hand, ObjectNN is an object dataset from RGB-D scene reconstruction mixed with CAD models for studying 3D object retrieval. Objects in ObjectNN is particularly difficult to classify because they are reconstructed from noisy RGB-D data and often has missing parts. For object recognition, our point attributes are simple XYZ coordinates. In fact, we also trained the network with point attributes set to one, making the convolution equivalent to density estimation, and found no significant change in accuracy. Our results on ModelNet40 are shown in <ref type="table">Table 4</ref>  <ref type="table">Table 4</ref>: Comparison of performance of network architectures using 3D object representations on the ModelNet40 dataset <ref type="bibr" target="#b42">[42]</ref>.</p><p>of-the-art methods. Note that compared to VoxNet <ref type="bibr" target="#b19">[20]</ref>, our input point cloud is more compact. Our network is also significantly simpler in design compared to PointNet <ref type="bibr" target="#b25">[26]</ref> and PointNet++ <ref type="bibr" target="#b27">[28]</ref> while being close to their accuracy. The results on ObjectNN are shown in <ref type="table" target="#tab_5">Table 5</ref>. In this dataset, again our method performs comparably to PointNet,   but overall both methods are less effective due to the ambiguity in learning features from both CAD models and RGB-D objects. <ref type="table" target="#tab_10">Table 9</ref> and <ref type="table" target="#tab_11">Table 10</ref> further provide per-class accuracy on the ModelNet40 and the ObjectNN dataset, respectively.</p><p>Convergence. <ref type="figure" target="#fig_2">Figure 5</ref> shows a plot of the training and test accuracy of our networks over time. The graph shows that our pointwise convolutional neural network can be trained effectively.</p><p>Ablation experiments. Here we analyze the effectiveness of pointwise convolution. We first start with with a basic 4-layer model as in <ref type="figure">Figure 2</ref>. The accuracy improvement when more features are added are presented in <ref type="table">Table 6</ref>. As can be seen, feature concatenation,à-trous convolution, SELU activation, and dropout each contributes a small improvement to the final result.  <ref type="table">Table 6</ref>: Ablation experiment. Accuracy improvement is achieved when pointwise convolution is combined with feature concatenation (Concat.),à-trous convolution, selfnormalization activation function (SELU), and dropout.</p><p>Point order. In object recognition, the order of the input points determine the orders of the features in the fully connected layers. As long as this layer has an order, it is sufficient to discriminate their features and predict the categories. We experiment with different orders of the input point set and report the results in <ref type="table" target="#tab_6">Table 7</ref>(a). We found that point orders sorted by space filling curve techniques such as Morton curve <ref type="bibr" target="#b21">[22]</ref> yields comparable accuracy, which means that it is sufficient to just follow an order, but not a particular one. However, a benefit is that space filling curves organize points such that nearby points in space are stored close to each other in memory, allowing more memory coherence.</p><p>Neighborhood radius. So far we have been setting the radius for neighbor query as constant in each convolution layer. In our experience, this works well for both tasks. We also explore the capability of adaptive radius using k-nearest neighbors. The modification for the convolution operator is as follows.</p><p>At each point, a k-nearest neighbor is performed, and the query radius is set to the distance to the furthest neighbor. This radius is used each time neighbor points have to be queried for convolution. To compute gradients for backpropagation for this operator, it is worth noting that in this case, neighbor lookup is no longer symmetric. Therefore, at a point j, it is required to look up all points i such that point i can contribute to point j in the forward convolution.</p><p>We compare the performance of the k-nearest neighbor and the fixed radius convolution for object recognition task. The result is shown in <ref type="table" target="#tab_6">Table 7</ref>(b). In general, we found no significant difference in terms of accuracy.  Deeper networks. Finally, we study the capability of learning with deeper networks using pointwise convolution. From the basic model, we increase the number of layers from 4 to 8 and 16, and then retrain from scratch. The performance are reported in <ref type="table" target="#tab_8">Table 8</ref>    <ref type="bibr" target="#b10">[11]</ref> would be an interesting future work.</p><p>Running time. A key challenge when implementing pointwise convolution is how to perform fast nearest neighbor query without impacting too much the network training and prediction time. To make the training feasible, we choose to use a grid for neighbor query because this is a lightweight and GPU-friendly data structure to build and query on the fly. In fact, we experimented with kd-tree, but found that on modern CPUs and GPUs, a kd-tree query does not outperform a grid unless the number of points are more than 16K points, not to mention extra time needed for tree construction that has O(n log n) complexity.</p><p>Our pointwise convolution is currently implemented with Tensorflow. We report the running time, including grid build and query each time convolution is invoked, as follows. For a batch size of 128 point clouds, each with 2048 points, a forward convolution of our network takes 1.272 seconds on an Intel Core i7 6900K with 16 threads, and a backward propagation takes 2.423 seconds to compute the gradients. Our GPU implementation on an NVIDIA TITAN X can further improve the running time for about 10%. Compared to PointNet <ref type="bibr" target="#b25">[26]</ref> and VoxNet <ref type="bibr" target="#b19">[20]</ref> which leverage Tensorflow's optimized convolution operators, our pointwise convolution is not yet engineering optimized. Our training time is about 2× slower which we currently compensate by using multiple CPUs and GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed pointwise convolution and leveraged it to build convolutional neural networks for scene understanding with point cloud data. We demonstrated two scene understanding applications including scene segmentation and object recognition. We showed that it is practical to simply sort input point clouds in a specific order before feature learning for object classification. Our pointwise convolution can offer competitive accuracy while being simple to implement, allowing us to create effective and simple neural networks for learning local features of point clouds.</p><p>There are several research avenues to be further explored. For example, finding a robust solution to handle large-scale point clouds for scene understanding would be an interesting future work. Currently, we just circumvent the large-scale issue in semantic segmentation by simply dividing the scene into blocks and resample each block to fixed number of points for prediction. In addition, it would be of great interest to extend pointwise convolutional neural networks to geometry point cloud processing <ref type="bibr" target="#b44">[44]</ref>, or explore the connection of pointwise convolution to tensor voting <ref type="bibr" target="#b41">[41]</ref>, which was used in the literature to detect structures in a local point neighborhood.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Layer Visualization</head><p>Intuitively, pointwise convolution works by summarizing local spatial point distributions to build feature vectors for each point in a point cloud. As shown in per-class accuracy tables, local features work the most effectively in classifying structures such as ceiling, floor, or walls and common furniture such as tables and chairs. In our observation, it is quite challenging to differentiate between tables (for dining) and desks (for study and work).</p><p>We visualize the filters of the first four layers in the object recognition network in <ref type="figure" target="#fig_5">Figure 6</ref>. Here we display each 3 × 3 × 3 filter on a row in the visualization. The number of rows is equal to the product of the total number of input and output channels of each filter (27 for the first layer, and 81 for the subsequent layers). In the visualization, blue and red represent positive and negative values, respectively. White represents zero. This shows that the filters in the network are relatively sparse and smooth. We also observed that positive and negative values dominate the filters interchangeably in each layer. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Semantic segmentation on SceneNN dataset [13].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Train and test accuracy over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the filters in pointwise convolution network for object recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>, we apply their</figDesc><table>Network 

ceiling floor wall 
column 

PointNet [26] 98.3 
98.8 
83.3 
63.4 
Ours 
97.4 
99.1 
89.1 
56.2 

door 
table chair sofa 
clutter 

PointNet [26] 84.6 
70.3 
66.0 
56.7 
69.0 
Ours 
62.9 
73.7 
68.4 
54.6 
65.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>. As can be seen, our network performs comparably to state-</figDesc><table>Network 
Accuracy 
(per class) 
Accuracy 

VoxNet [20] 
83 
-
MO-SubvolumeSup [27] 86 
89.2 
PointNet [26] 
86.2 
89.2 
PointNet++ [28] 
-
90.7 
Ours 
81.4 
86.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of object recognition accuracy on the 
ObjectNN dataset [14]. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>(a) Object recognition with different ways of or-
dering the input point cloud. (b) Object recognition with 
convolution using neighbor queries with adaptive radius. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Deep pointwise convolutional neural network. We compare object recognition performance with 4-, 8-, and 16-layer architecture. longer to train networks with 8 and 16 layers, resulting in slightly slower accuracy. Experimenting the training with residual learning</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 9 :</head><label>9</label><figDesc>Per-class accuracy of object recognition on the ModelNet40 dataset. Average: PointNet: 86.3. Ours 81.4.</figDesc><table>Network 
chair display 
desk 
book 
storage box 
table bin 
bag 
keyboard 

PointNet [26] 84.2 
85.4 
56.7 
30.1 
62.5 
23.8 80.0 
75.0 47.4 
82.4 
Ours 
83.1 
85.4 
70.0 
57.7 
45.8 
23.8 60.0 
65.0 36.8 
88.2 

sofa 
bookshelf pillow machine pc case light oven cup 
printer bed 

PointNet [26] 76.5 
23.1 
84.6 
18.2 
36.4 
77.8 60.0 
37.5 50.0 
28.6 
Ours 
88.2 
38.5 
76.9 
18.2 
54.5 
88.9 30.0 
75.0 12.5 
42.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 10 :</head><label>10</label><figDesc>Per-class accuracy of object recognition on the ObjectNN dataset. Average: PointNet: 56.0. Ours: 57.1.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">This work was done when Binh-Son Hua was a postdoctoral researcher in Singapore University of Technology and Design in 2017.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We thank Quang-Hieu Pham for helping with the 2D-to-3D semantic segmentation experiment and proofreading the paper, Quang-Trung Truong and Benjamin Kang Yue Sheng for their kind support for the neural network training experiments. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond euclidean data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On visual similarity based 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extended gaussian images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1671" to="1686" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scenenn: A scene meshes dataset with annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<ptr target="http://www.scenenn.net.2,4" />
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-T</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Rgb-d to cad retrieval with objectnn dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3 d shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on geometry processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semanticfusion: Dense 3d semantic mapping with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05130</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A computer oriented geodetic data base and a new technique in file sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Morton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Business Machines Company</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realtime 3d reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="807" to="832" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeppano: Deep panoramic representation for 3-d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A concise and provably informative multi-scale signature based on heat diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiley Online Library</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A robust 3d-2d interactive tool for scene segmentation and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semanticpaint: Interactive 3d labeling and learning at your fingertips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A closed-form solution to tensor voting: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pu-net: Point cloud upsampling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
