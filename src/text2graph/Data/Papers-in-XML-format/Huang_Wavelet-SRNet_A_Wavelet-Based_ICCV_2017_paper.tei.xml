<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
							<email>huaibo.huang@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering Science</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering Science</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering Science</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering Science</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Most modern face super-resolution methods resort to convolutional neural networks (CNN)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face super-resolution (SR), also known as face hallucination, refers to reconstructing high resolution (HR) face images from their corresponding low resolution (LR) inputs. It is significant for most face-related applications, e.g. face recognition, where captured faces are of low resolution and lack in essential facial details. It is a special case of single image super resolution and many methods have been proposed to address it. It is a widely known undetermined inverse problem, i.e., there are various corresponding highresolution answers to explain a given low-resolution input.</p><p>Most current single image super-resolution methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> depend on a pixel-wise mean squared er- ror (MSE) loss in image space to push the outputs pixelwise closer to the ground-truth HR images in training phase. However, such approaches tend to produce blurry and oversmoothed outputs, lacking some textural details. Besides, they seem to only work well on limited up-scaling factors (2× or 4×) and degrades greatly when ultra-resolving a very small input (like 16 × 16 or smaller). Several recent efforts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> have been developed to deal with this issue based on convolutional neural networks. Dahl et al. <ref type="bibr" target="#b4">[5]</ref> use PixelCNN <ref type="bibr" target="#b26">[27]</ref> to synthesize realistic details. Yu et al. <ref type="bibr" target="#b32">[33]</ref> investigate GAN <ref type="bibr" target="#b7">[8]</ref> to create perceptually realistic results. Zhu et al. <ref type="bibr" target="#b34">[35]</ref> combine dense correspondence field estimation with face super-resolution. However, the application of these methods in super-resolution in image space faces many problems, such as computational complexity <ref type="bibr" target="#b4">[5]</ref>, instability in training <ref type="bibr" target="#b32">[33]</ref> and poor robustness for pose and occlusion variations <ref type="bibr" target="#b34">[35]</ref>. Therefore, due to various problems yet to be solved, image SR remains an open and challenging task.</p><p>Wavelet transform (WT) has been shown to be an efficient and highly intuitive tool to represent and store multiresolution images <ref type="bibr" target="#b17">[18]</ref>. It can depict the contextual and textural information of an image at different levels, which motivates us to introduce WT to a CNN-based super-resolution system. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the approximation coefficients(i.e. the top-left patches in (b-d)) of different-level wavelet packet decomposition <ref type="bibr" target="#b3">[4]</ref> compress the face's global topology information at different levels; the detail coefficients(i.e. the rest patches in (b-d)) reveal the face's structure and texture information. We assume that a high-quality HR image with abundant textural details and global topology information can be reconstructed via a LR image as long as the corresponding wavelet coefficients are accurately predicted. Hence, the task of inferring a high-resolution face is transformed to predicting a series of wavelet coefficients. Emphasis on the prediction of high-frequency wavelet coefficients helps recovering texture details, while constraints on the reconstruction of low-frequency wavelet coefficients enforces consistence on global topology information. The combination of the two aspects makes the final HR results more photo-realistic.</p><p>To take full advantage of wavelet transform, we present a wavelet-based convolutional neural network for face superresolution, which consists of three subnetworks: embedding, wavelet prediction and reconstruction networks. The embedding net takes the low-resolution face as an input and represents it as a set of feature maps. The wavelet prediction net is a series of parallel individual subnetworks, each of which aims to learn a certain wavelet coefficient using the embedded features. The number of these subnetworks is flexible and easy to adjust on demand, which makes the magnification factor flexible as well. The reconstruction network is used to recover the inferred wavelet coefficients to the expected HR image, acting as a learned matrix. These three subnetworks are coordinated with three types of loss: wavelet prediction loss, texture loss and fullimage loss. The wavelet prediction loss and texture loss correspond with the wavelet prediction subnetwork, imposing constraint in wavelet domain. The full-image loss is used after the reconstruction subnetwork to add a traditional MSE constraint in image space. Besides, as each wavelet coefficient shares the same size with the low-resolution input, we use a network configuration to make every feature map keep the same size with the input, which reduces the difficulty of training. As our network is fully convolutional and trained with simply-aligned faces, it can apply to different input resolutions with various magnifications, regardless of pose and occlusion variations. Experimental results collaborate with our assumption and demonstrate that our method can well capture both global topology information and local textural details of human faces.</p><p>Main contributions of our work can be summarized as follows:</p><p>1) A novel wavelet-based approach is proposed for CNN-based face SR problem. To the best of our knowledge, this is the first attempt to transform single image S-R to wavelet coefficients prediction task in deep learning framework -albeit many wavelet-based researches exist for SR.</p><p>2) A flexible and extensible fully convolutional neural network is presented to make the best use of wavelet transform. It can apply to different input-resolution faces with multiple upscaling factors.</p><p>3) We qualitatively and quantitatively explore multiscale face super-resolution, especially on very low input resolutions. Experimental results show that our proposed approach outperforms state-of-the-art face SR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In general, image super-resolution methods can be divided into three types: interpolation-based, statisticsbased <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> and learning-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>. In the early years, the former two types have attracted most of attention for their computationally efficiency. However, they are always limited to small upscaling factors. Learning based methods employ large quantities of LR/HR image pair data to infer missing high-frequency information and promises to break the limitations of big magnification. Recently deep learning based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref> have been introduced into SR problem due to their powerful ability to learn knowledge from large database. Most of these convolutional methods use MSE loss to learn the map function of LR/HR image pairs, which leads to over-smooth outputs when the input resolution is very low and the magnification is large.</p><p>Specific to face super-resolution, there have been about three ways to alleviate this problem. The first one <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> is to exploits the specific static information of face images with the help of face analysis technique. Yang et al. <ref type="bibr" target="#b28">[29]</ref> estimate landmarks and facial pose before reconstructing HR images while the accurate estimation is difficult for rather small faces. Zhu et al. <ref type="bibr" target="#b34">[35]</ref> present a unified framework of face super-resolution and dense correspondence field estimation to recover textural details. They achieve state-of-the-art results for very low resolution inputs but fail on faces with various poses and occlusions, due to the difficulty of accurate spatial prediction.</p><p>The second way <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b4">5]</ref> is to bring in image prior knowledge with the help of generative models. Yu et al. <ref type="bibr" target="#b32">[33]</ref> propose a generative adversarial network (GAN <ref type="bibr" target="#b7">[8]</ref>) to re-solve 16×16 pixel-size faces to its 8× larger versions. Dahl et al. <ref type="bibr" target="#b4">[5]</ref> present a recursive framework based on PixelCN-N <ref type="bibr" target="#b26">[27]</ref> to synthesize details of 4× magnified images with 8 × 8 LR inputs. The 32 × 32 outputs are not sufficiently perceptual appealing, and their method suffers from high computational complexity.</p><p>The last way is to introduce perceptual losses to improve the outputs' perceptual quality directly. Johnson et al. <ref type="bibr" target="#b11">[12]</ref> use feature reconstruction loss as perceptual loss to recover more semantic information. However, reconstruction features are not as intuitive as wavelet coefficients to depict perceptual quality.</p><p>Many wavelet-based methods have already been proposed for super resolution problem. A large percentage of them focus on multiple images SR <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10]</ref>, which means inferring a high-resolution image from a sequence of low-resolution images. As for single image super resolution, wavelet transform is mostly used to help interpolationbased <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> and static-based <ref type="bibr" target="#b33">[34]</ref> methods. Naik et al. <ref type="bibr" target="#b20">[21]</ref> propose a modified version of classical wavelet-based interpolation method <ref type="bibr" target="#b0">[1]</ref>. Gao et al. <ref type="bibr" target="#b6">[7]</ref> propose a hybrid wavelet convolution network. They use wavelet to provide a set of sparse coding candidates and another convolution net for sparse coding, which is totally different with ours. Besides, Mallat <ref type="bibr" target="#b18">[19]</ref> uses wavelet transform to separate the variations of data at different scales, while we predict the wavelets from LR inputs, designed especially for super resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we present a novel framework for face super resolution, which predicts a series of corresponding wavelet coefficients instead of HR images directly. Special losses in wavelet domain are designed to capture both global topology information and local textural details. Then, an extensible fully convolutional neural network (WaveletSRNet) is proposed for multi-scale face super resolution. At last, implement details of Wavelet-SRNet are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Wavelet packet transform</head><p>Our method is based on wavelet transform, more specifically wavelet packet transform (WPT), which decomposes an image into a sequence of wavelet coefficients of the same size. We choose the simplest wavelet, Haar wavelet, for it is enough to depict different-frequency facial information. We use 2-D fast wavelet transform (FWT) <ref type="bibr" target="#b19">[20]</ref> to compute Haar wavelets. The wavelet coefficients at different levels are computed by repeating the decomposition in <ref type="figure" target="#fig_1">Figure 2</ref> to each output coefficient iteratively. Example results of WPT is showed in <ref type="figure" target="#fig_0">Figure 1 (b-d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Wavelet-based super resolution</head><p>Generic single image super resolution aims to learn a map function f θ (x) defined by the parameter θ to estimate </p><formula xml:id="formula_0">= (1/ √ 2, 1/ √ 2) and h high = (1/ √ 2, −1/ √ 2).</formula><p>a high resolution imageŷ with a given low resolution input x. Suppose that y denotes a ground-truth HR image and</p><formula xml:id="formula_1">D ≡ {(x i , y i )} N i</formula><p>represents a large dataset of LR/HR image pairs, then most current learning-based SR methods optimize the parameter θ through the following form arg max</p><formula xml:id="formula_2">θ (x,y)∈D log p(y|x)<label>(1)</label></formula><p>The most common loss function is pixel-wise MSE in HR image space</p><formula xml:id="formula_3">l mse (ŷ, y) = ŷ − y 2 F (2)</formula><p>As argued in many papers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b4">5]</ref>, merely minimizing MSE loss can hardly capture high-frequency texture details to produce satisfactory perceptual results. As texture details can be depicted by high-frequency wavelet coefficients, we transform super resolution problem from original image space to wavelet domain and introduce waveletbased losses to help texture reconstruction.</p><p>Consider n-level full wavelet packet decomposition, where n determines the scaling factor r of super resolution and the number of wavelet coefficients N w , i.e., r = 2 n , N w = 4 n . Let C = (c 1 , c 2 , · · · , c Nw ) and C = (ĉ 1 ,ĉ 2 , · · · ,ĉ Nw ) denote the ground-truth and inferred wavelet coefficients, the model parameter θ of the map function g θ (x) = (g θ,1 (x), g θ,1 (x), · · · , g θ,Nw (x)) can be optimized by the form arg max</p><formula xml:id="formula_4">θ (x,C)∈D log p(C|x)<label>(3)</label></formula><p>We propose two kinds of wavelet-based loss: wavelet prediction loss and texture loss. The former one is a weighted version of MSE in wavelet domain, defined as</p><formula xml:id="formula_5">l wavelet (Ĉ, C) = W 1/2 ⊙ (Ĉ − C) 2 F = Nw i=1 λ i ĉ i − c i 2 F = λ 1 ĉ 1 − c 1 2 F + Nw i=2 λ i ĉ i − c i 2 F<label>(4)</label></formula><p>where W = (λ 1 , λ 2 , · · · , λ Nw ) is the weight matrix to balance the importance of different-band wavelet coefficients. More attention can be paid on local textures with bigger weights appointed to high-frequency coefficients. Meanwhile, the term ĉ 1 − c 1 2 F captures global topology information and serves as the loss function of an auto-encoder when the approximation coefficient c 1 is taken as input, which is helpful for maintaining training stability.</p><p>The texture loss is designed to prevent high-frequency wavelet coefficients from converging to zero, defined as</p><formula xml:id="formula_6">l texture = Nw i=k γ i max(α c i 2 F + ε − ĉ i 2 F , 0) (5)</formula><p>where k indicates the start index of the wavelet coefficients to be penalized for taking small values, γ i is balance weights, α and ε are slack values. It keeps high-frequency wavelet coefficients non-zero and hence prevents the degradation of texture details.</p><p>A traditional MSE loss in image space, which is called full-image loss, is also used to get a balance between smoothness and textures. The unified loss function is defined as follows</p><formula xml:id="formula_7">l total = l wavelet + µl texture + νl f ull−image = Nw i=1 λ i ĉ i − c i 2 F + µ Nw i=k γ i max(α c i 2 F + ε − ĉ i 2 F , 0) + ν RĈ − y 2 F (6)</formula><p>where µ and ν are the balance parameters, and R is the reconstruction matrix to generateŷ fromĈ, i.e.,ŷ = RĈ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>As outlined in <ref type="figure" target="#fig_2">Figure 3</ref>, our wavelet-based convolutional neural network consists of three subnetworks: embedding, wavelet prediction, reconstruction networks. The embedding net represents the low-resolution input as a set of feature maps. Then the wavelet prediction net estimates the corresponding wavelet coefficient images. Finally the reconstruction net reconstructs the high-resolution image from these coefficient images.</p><p>The embedding net takes a low-resolution image of the size 3 × h × w as input and represents it as a set of feature maps. All the convolution fillers share the same size of 3×3 with a stride of 1 and a pad of 1, which makes every feature map in the embedding net the same size with the input image. The number of feature maps (or the channel-size) increases in the forward direction to explore enough information for wavelet prediction. Through the embedding net, the input LR image is mapped to feature maps of the size N e × h × w without up-sampling or down-sampling, where N e is the last layer's channel-size.</p><p>The wavelet prediction net can be split into N w parallel independent subnets, where N w = 4 n on the condition that the level of wavelet-packet decomposition is n and the magnification r = 2 n . Each of these subnets takes the output feature maps of the embedding net as input and generates the corresponding wavelet coefficient. We set all the convolution fillers the size of 3 × 3 with a stride of 1 and a pad of 1 similarly with the embedding net, so that every inferred wavelet coefficient is the same size with the LR input, i.e, 3 × h × w. Besides, motivated by the high independence between the coefficients of Haar wavelet transform, no information is allowed to interflow between every two subnets, which makes our network extensible. It is easy to realize different magnifications with different numbers of the subnets in the prediction net. For example, N w = 16 and N w = 64 stand for 4× and 8× magnifications, respectively.</p><p>The reconstruction net is used to transform the wavelet images of the total size N w × 3 × h × w into the original image space of the size 3 × (r × h) × (r × w). It comprises a deconvolution layer with a filler size of r × r and a stride of r. Although the size of the deconvolution layer is dependent on the magnification r, it can be initialized by a constant wavelet reconstruction matrix and fixed in training. Hence it has no effect on the extensibility of the whole networks.</p><p>As mentioned above, all the convolution fillers of the embedding and wavelet prediction nets share the same size of 3 × 3 with a stride of 1 and a pad of 1, keeping every feature map the same spatial size with the input image. This reduces both the size of model parameters and the computation complexity. Besides, to prevent gradients exploding/vanishing and accelerate convergence, we use skipconnections between every two layers except the first layer. Batch-norm is also used after every layer.</p><p>The definition of our networks can be formulated as followŝ</p><formula xml:id="formula_8">y = φ(Ĉ) = φ{(ĉ 1 ,ĉ 2 , · · · ,ĉ Nw )} = φ{(ϕ 1 (ẑ), ϕ 2 (ẑ), · · · , ϕ Nw (ẑ))} = φ{(ϕ 1 (ψ(x)), ϕ 2 (ψ(x)), · · · , ϕ Nw (ψ(x)))}<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">ψ : R 3×h×w → R Ne×h×w ϕ i : R Ne×h×w → R 3×h×w , i = 1, 2, · · · , N w φ : R Nw×3×h×w → R 3×(r×h)×(r×w)<label>(8)</label></formula><p>are mappings of the embedding,wavelet prediction, reconstruction nets, respectively.  training and robust toward unknown down-sampling methods. Two types of low-resolution images are taken as input, one of which is down-sampled by bicubic interpolation and the other is the approximation coefficient of wavelet packet decomposition. Take the case of 16 × 16 input-resolution resolved to 128 × 128 for example. We resize all centercropped faces to 134 × 134 with bicubic interpolation and randomly crop them to 128 × 128 images. Wavelet packet decomposition at 3 level is used to get the ground-truth wavelet coefficients c i in (6). The approximation coefficient c 1 is treated as one version of low-resolution input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>With the mapping functionĉ 1 = ϕ 1 (ψ(c 1 )), and the distance constraint ĉ 1 − c 1 2 F , the embedding and prediction nets serve as an auto-encoder, which assures no loss of the original input information and facilitates training stability. Another version of low-resolution input, directly downsampled by bicubic interpolation, is used cooperatively with the wavelet version, which helps maintaining the robustness of our model. In the testing phase, we evaluate on faces down-sampled by bicubic interpolation.</p><p>Since our network is a fully convolutional architecture without fully-connected layers, it can be applied to the input of arbitrary size. We firstly train a model for 16 × 16 input resolution with 8× magnification, and then fine-tune it for 8 × 8 input resolution with 8× magnification. For 8 × 8 input resolution with 16× magnification, we initialize the parameters by the overlapping ones of the model for 8 × 8 with 8× magnification before fine-tuning it. For other cases, we just choose the closest model for evaluation.</p><p>Our model is implemented with the Caffe framework <ref type="bibr" target="#b10">[11]</ref>. The loss in (6) is minimized using SGD with a batch size of 256. For the hyper-parameters, we set em-</p><formula xml:id="formula_10">pirically λ 1 = 0.01, λ 2 = λ 3 = · · · = λ Nw = 1, µ = 1, k = 2, γ k = γ k+1 = · · · = γ Nw = 1, ν = 0.1.</formula><p>The learning rate is set to 0.01 initially and reduced by a factor of 10 each 3000 iterations. It takes about 14, 000 ∼ 16, 000 iterations for our network to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our experiments are implemented on two datasets: CelebA <ref type="bibr" target="#b35">[36]</ref> and Helen <ref type="bibr" target="#b15">[16]</ref>. There are 202,599 faces in CelebA and 2,230 faces in Helen. In the training phase, we use the large train set of CelebA(162,700 images) for training and the validation set(19,867 images) of CelebA for validation. In the testing phase, we evaluate with the 19,962-image test set of CelebA and the 330-image test set of Helen, assuring no over-lapped images appearing in both the training and testing phase. The images are cropped around the face with eyes aligned horizontally.</p><p>We evaluate the performance of Wavelet-SRNet on multiple input resolutions, comparing with bicubic interpolation, wavelet-based interpolation (WaveletIP, for short) <ref type="bibr" target="#b20">[21]</ref> and state-of-the-art methods: SRCNN <ref type="bibr" target="#b5">[6]</ref>, URDGN <ref type="bibr" target="#b32">[33]</ref>, CBN <ref type="bibr" target="#b34">[35]</ref>. WaveletIP <ref type="bibr" target="#b20">[21]</ref> upsample images in both spatial and wavelet domain. SRCNN <ref type="bibr" target="#b5">[6]</ref> is a generic cnn-based super resolution method so we retrain it on CelebA training set to suit better for face images. URDGN <ref type="bibr" target="#b32">[33]</ref> and CB-N <ref type="bibr" target="#b34">[35]</ref> are trained on CelebA. URDGN chooses 15,000 and 500 images randomly from CelebA for training and evaluation respectively. CBN uses the whole CelebA dataset for training. Hence their results on Helen may be more persuasive than on the CelebA test set. For a fair comparison, we use the same set of eyes-aligned faces for all the methods with no extra preprocessing before down-sampling. We adopt PSNR(dB) and SSIM for quantitative metric, and calculate PSNR on the luminance channel, following by <ref type="bibr" target="#b34">[35]</ref>, and SSIM on the three channels of RGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on multiple resolutions</head><p>As mentioned above, our method can apply to different input resolutions with multiple magnifications. In <ref type="figure" target="#fig_4">Figure 4</ref>, we show the qualitative results of our method on different input resolutions comparing with the bicubic interpolation baseline. Our method can reconstruct faces from very small inputs of 8 × 8 pixel-size and the inferred outputs are perceptually identity-persistent to some degree, which implies that a small number of 64 pixels constains most of a face's identity information. Besides, while the outputs of 8 × 8 input resolution are still a little blurry, the outputs of the larger input resolutions are very close to the original high resolution faces in human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison on very low resolutions</head><p>We compare our method qualitatively with state-of-theart methods on two very low resolution cases, 16 × 16 and 8 × 8, both with a magnification of 8.</p><p>As for 16 × 16 input resolution in <ref type="figure" target="#fig_5">Figure 5</ref> (a-h), our method achieves the best perceptual performance. Bicubic interpolation, WaveletIP <ref type="bibr" target="#b20">[21]</ref> and SRCNN <ref type="bibr" target="#b5">[6]</ref> fail to infer high-frequency details and generate over-smoothed outputs. URDGN <ref type="bibr" target="#b32">[33]</ref> promises to predict high-frequency information with an adversarial loss. However, we evaluate URDGN with their offered model and find texture details over-synthesized, as <ref type="figure" target="#fig_5">Figure 5</ref> (f) illustrates. It is perhaps because that their train and test sets are much smaller than ours and their adversarial networks lack in robustness. CB-N <ref type="bibr" target="#b34">[35]</ref> achieves the second-place performance except deformation in some cases. They hallucinate faces with the help of dense correspondence field estimation and consequently encounter abnormal results when unable to estimate facial locations accurately. Comparing with other methods, our network infers the high-frequency details directly in wavelet domain and the results prove its effectiveness.</p><p>As for 8 × 8 input resolution in <ref type="figure" target="#fig_5">Figure 5</ref> (i-p), only our method, URDGN <ref type="bibr" target="#b32">[33]</ref> and CBN <ref type="bibr" target="#b34">[35]</ref> can reconstruct faces. While URDGN <ref type="bibr" target="#b32">[33]</ref> contains much weird textures and CB-N <ref type="bibr" target="#b34">[35]</ref> tends to generate faces closer to the mean face, our results are more identity-similar to the ground-truth and plausible for human vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion of the robustness</head><p>We evaluate the robustness of our method toward unknown Gaussian blur, poses and occlusions. In this section, we still adopt the same model used above, with no extra efforts to deal with these special cases.</p><p>In <ref type="figure" target="#fig_6">Figure 6</ref>, the low-resolution faces are generated by a Gaussian blur kernel with a stride of 8 corresponding to 8× down-sampling. σ for Gaussian blur kernel increases from 0 to 6, where σ = 0 means nearest-neighbor interpolation down-sampling. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, our method demonstrates certain robustness when σ &lt; 4 and generates  smoother faces when σ &gt;= 4. As a comparison, the results of CBN become more similar with mean face.</p><p>For pose variations, as shown in <ref type="figure">Figure 7</ref>, CBN fails to reconstruct plausible faces of large poses, perhaps due to inaccurate spatial prediction. Meanwhile, our method can still infer high-quality results.</p><p>For occlusion variations, we take some faces with natural occlusions for example. As shown in <ref type="figure">Figure 8</ref>, CBN tends to over-synthesize occluded facial parts, e.g., the eyes and lips, while ours resolves the occluded parts and the rest dependently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative results</head><p>We evaluate Wavelet-SRNet quantitatively using average PSNR(dB) and SSIM on the two test sets of CelebA and Helen. We conduct evaluation on four cases: ( fication of n). As is shown in loss without considering the characteristics of human face.</p><formula xml:id="formula_11">a) σ = 0 (b) σ = 0.5 (c) σ = 1 (d) σ = 2 (e) σ = 3 (f) σ = 4 (g) σ = 5 (h) σ = 6</formula><p>The results in <ref type="table" target="#tab_1">Table 1</ref> demonstrate the fact that our method preserves the pixel-wise consistence between LR inputs and HR ground-truth while generates perceptually plausible faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel wavelet-based approach for multiscale face super resolution, which transforms single image super resolution to wavelet coefficients prediction task in deep learning framework. A flexible wavelet-based convolutional neural network (Wavelet-SRNet) is presented, which consists of three subnetworks: embedding, wavelet prediction and reconstruction networks. Three types of loss, wavelet prediction loss, texture loss and full-image loss, are designed to capture both the global topology information and local texture information of human faces. Due to its extensible fully convolutional architecture trained with simply-aligned faces, our network is applicable to different input resolutions with various magnifications. Experimental results show that our method demonstrates promising robustness toward unknown Gaussian blur, poses and occlusions, and achieves better performance both qualitatively and quantitatively than the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of wavelet decomposition and our waveletbased SR. Top row: (a) The original 128 × 128 high-resolution face image and its (b) 1 level, (c) 2 level, (d) 3 level, full wavelet packet decomposition image. Middle row: (h) The 16 × 16 lowresolution face image and its (g) 2×, (f) 4×, (e) 8×, upscaling versions inferred by our network. Bottom row: similar with the middle row except the low-resolution input (l) is 8 × 8 pixel-size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of fast wavelet transform (FWT). FWT uses low-pass and high-pass decomposition fillers iteratively to compute wavelet coefficients, where Haar-based h low = (1/ √ 2, 1/ √ 2) and h high = (1/ √ 2, −1/ √ 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The architecture of our wavelet-based super-resolution net (Wavelet-SRNet). All the convolution layers have the same filter map size of 3x3 and each number below them defines their individual channel size. Skip connections exist between every two convolution layers (except the first layer) in the embedding and wavelet predicting nets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8</head><label></label><figDesc>× 8 input-size, 16× upscaling 16 × 16 input-size, 8× upscaling 32 × 32 input-size, 4× upscaling 128 × 128 ground-truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results of various input resolutions: 8 × 8, 16 × 16 and 32 × 32. For each input resolution, the first row is generated by bicubic interpolation and the second is ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison with state-of-the-art methods on very low input resolutions. The input resolutions are 16 × 16 and 8 × 8 for the top three and bottom three rows, respectively. The magnifications are both 8×. Images are selected randomly from Helen test set. We do not try to crop the green area caused by the shape transform of CBN in (g) and (o) to avoid facial deformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Robustness toward unknown gaussian blur on Helen test set. The input resolution is 16 × 16 and the magnification is 8×. The top, middle and bottom rows are the results of bicubic interpolation, CBN and ours, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>A novel training technique for face super-resolution, called as co-training, is used to make our model stable in</figDesc><table>Wavelet Prediction Net 
Embedding Net 

HR 
SR 

Full-Image 
Loss 

Wavelet-
based 
Loss 

. 
. 
. 

. 
. 
. 

. 
. 
. 

W a v e le t P a ck e t 
D e co m p o se 

D 
o 
w 
n 
s 
a 
m 
p 
le 

R 

LR Input 

128 
128 
128 

256 
256 

512 
512 

1024 
1024 

32 
32 

32 
32 
64 
64 

64 
64 

64 
64 

64 
64 

32 
32 

32 
32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 ,</head><label>1</label><figDesc>our method achieves the best quantitative performance. As expected, bicubic interpolation is better than other state-of-the-art method- s because it is designed to minimize the pixel-wise MSETable 1. Quantitative results on CelebA and Helen test sets.</figDesc><table>Dataset Method 

32 × 32, 4× 
16 × 16, 8× 
8 × 8, 8× 
8 × 8, 16× 
PSNR SSIM 
PSNR 
SSIM PSNR SSIM PSNR SSIM 

Celeba 

Bicubic 
29.20 0.9258 24.83 0.8525 21.84 0.7687 21.36 0.7838 
WPSR 
25.01 0.8467 21.50 0.7234 19.44 0.6476 19.03 0.6332 
SRCNN 
20.61 0.8004 20.15 0.7954 17.84 0.6927 18.39 0.6880 
URDGN 
-
-
24.63 0.8527 21.41 0.7614 
-
-
CBN 
25.93 0.8749 24.68 0.8369 19.93 0.7201 19.78 0.7327 
Ours 
30.56 0.9432 26.61 0.8949 23.35 0.8370 22.65 0.8201 

Helen 

Bicubic 
27.44 0.8762 23.96 0.7916 21.12 0.7068 20.96 0.7084 
WPSR 
24.17 0.7845 21.10 0.6494 19.45 0.5881 18.85 0.5580 
SRCNN 
21.93 0.8227 19.814 0.7321 17.46 0.6353 18.51 0.7367 
URDGN 
-
-
23.12 0.7708 19.32 0.6416 
-
-
CBN 
23.39 0.7773 22.44 0.7486 19.58 0.6301 19.78 0.7201 
Ours 
27.94 0.8827 24.63 0.8276 21.83 0.7662 21.80 0.7491 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image super resolution based on interpolation of wavelet domain high frequency subbands and the spatial domain input image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anbarjafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Demirel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Etri Journal</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="390" to="394" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Super-resolution with deep convolutional sufficient statistics. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entropy-based algorithms for best basis selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Wickerhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="713" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00783</idno>
		<title level="m">Pixel recursive super resolution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hybrid wavelet convolution network with sparse-coding for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1439" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Robust wavelet-based superresolution reconstruction: theory and algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="649" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno>arX- iv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Position-patch based face hallucination using convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="367" to="370" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wavelets for a vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="604" to="614" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">20150203</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: the wavelet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single image super resolution in spatial and wavelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Multimedia &amp; Its Applications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A wavelet-based interpolationrestoration method for superresolution (wavelet superresolution)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circuits, Systems, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="338" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Super-resolving noisy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelC-NN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigentransformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structured face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast direct super-resolution by simple functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wavelet-domain HMT-based image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="953" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep cascaded binetwork for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
