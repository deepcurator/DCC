<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple People Tracking by Lifted Multicut and Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple People Tracking by Lifted Multicut and Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multiple people tracking has improved considerably in the last two years, driven also by the MOT challenges <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. One trend in this area of research is to develop CNN-based feature representations for people appearance to effectively model relations between detections <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>. This trend has two advantages: Firstly, representations of people appearance can be learned for varying camera positions and motion, a goal less easy to achieve with simple motion models, especially for monocular video due to the complexity of motion under perspective projection. Secondly, appearance facilitates the re-identification of people across long distances, unlike motion models that become asymptotically uncorrelated. Yet, incorporating long-range re-identification into algorithms for tracking remains challenging. One reason is the simple fact that similar looking people are not necessarily identical. To address these challenges, in this paper, we generalize the mathematical model of <ref type="bibr" target="#b27">[28]</ref> so as to express the fact that similar looking people are considered as the same person only if they are connected by at least one feasible track (possibly skipping occlusion). More specifically, every detection is represented by a node in a graph; edges connect detections within and across time frames, and costs assigned to the edges can be positive, to encourage the incident nodes to be in the same track, or negative, to encourage the incident nodes to be in distinct tracks. Such mathematical abstraction has several advantages. Firstly, the number of persons is not fixed or biased by the definition of the problem, but is estimated in an unbiased fashion from the video sequence and is determined by the solution of the problem. Secondly, multiple detections of the same person in the same frame are effectively clustered, which eliminates the need for heuristic non-maxima suppression. In order to avoid that distinct but similar looking people are assigned to the same track, a distinction must be made between the edges that define possible connections (i.e., a feasible set) and the edges that define the costs or rewards for assigning the incident nodes to distinct tracks (i.e., an objective function). We achieve this, while maintaining the advantages of <ref type="bibr" target="#b27">[28]</ref>, by casting the multi-person tracking problem as a minimum cost lifted multicut problem <ref type="bibr" target="#b0">[1]</ref>.</p><p>Specifically, we make three contributions: Firstly, we design and train deep networks for reidentifying persons by fusing human pose information. This provides a mechanism for associating person hypotheses that are temporally distant and allows to obtain correspondence before and after occlusion.</p><p>Secondly, we propose to cast multi-person tracking as the minimum cost lifted multicut problem. We introduce two types of edges (regular and lifted edges) for the tracking graph. The regular edges define the set of feasible solutions in the graph, namely, which pair of nodes can be joint/cut. The lifted edges add additional long range information to the objective on which node should be joint/cut without modifying the set of feasible solutions. Our formulation encodes long-range information, yet penalizes long-term false joints (e.g., similar looking people) by forcing valid paths in the feasible solution in a unified and rigorous manner.</p><p>Thirdly, we show that the tracks defined by local optima of this optimization problem define a new state-of-the-art for the MOT16 benchmark <ref type="bibr" target="#b19">[20]</ref>. Related Work. Recent works on multi-person tracking focus on the tracking-by-detection approach <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. Tracking is performed either directly on people detections <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>, or on a set of confident tracklets, which are obtained by first grouping detections <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>. Introducing tracklets can reduce the state space; however, such approaches need a separate tracklet generation step, and any mistakes introduced by the tracklet generation are likely to be propagated to the final solution. In this work, our model takes detection as input. As the detections are clustered jointly in space and time, our model is able to handle multiple detection hypotheses of the same target on each frame.</p><p>One common formulation for multi-person tracking are network flow-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr" target="#b2">[3]</ref> proposes to model all potential locations over time and find trajectories that produce the minimum cost. <ref type="bibr" target="#b30">[31]</ref> extends the work <ref type="bibr" target="#b2">[3]</ref> to track interacting objects simultaneously by using intertwined flow and imposing linear flow constraints. <ref type="bibr" target="#b22">[23]</ref> shows that their network flow formulation can be solved in polynomial time by a successive shortest path algorithm. A maximum weight independent set formulation followed by hierarchical merging and linking is proposed for the tracking task in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Recently, minimum cost multicut formulation has been proposed to address multi person tracking <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15]</ref>. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> propose to jointly cluster detections over space and time. The optimal number of people as well as the cluster of each person are obtained by partitioning the graph with attractive and repulsive terms. <ref type="bibr" target="#b14">[15]</ref> proposes to partition the detection graph by considering point tracks, speed, appearance and trajectory straightness. The optimization is performed by a combination of message passing and movemaking algorithms. <ref type="bibr" target="#b24">[25]</ref> proposes to solve the minimum cost multicut problem by a multi-stage cascade with a temporal sliding window. Our work is different from the previous multicut based works; our lifted multicut formulation introduces additional edges in the graph to incorporate long-range information into the tracking formulation.</p><p>Many works have been proposed to exploit appearance information. <ref type="bibr" target="#b13">[14]</ref> proposes a target-specific appearance model which integrates long-term information and utilizes features from a generic deep convolutional neural network. <ref type="bibr" target="#b33">[34]</ref> proposes to formulate tracking as a Markov decision process with a policy estimated on the labeled training data and presents novel appearance representations that rely on the temporal evolution in appearance of the tracked target. Recently, <ref type="bibr" target="#b16">[17]</ref> proposes to model the similarity between pairs of detections by CNNs. Several architectures have been explored and they present similar findings to our work, that forming a stacked input to CNNs performs the best. Our work additionally incorporates human pose information, which improves the similarity measures by a notable margin.</p><p>There are several multi person tracking works that aim to recover people tracks by incorporating longer-range connections between detection hypotheses <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>. <ref type="bibr" target="#b20">[21]</ref> employs a simple color appearance model and proposes a continuous formulation, where mutual occlusions, dynamics and long-range trajectory continuity are effectively modeled. <ref type="bibr" target="#b34">[35]</ref> proposes a generalized minimum clique formulation which is solved by a greedy iterative optimization scheme that finds one track at a time. In <ref type="bibr" target="#b6">[7]</ref>, their target appearance model is learned online, and it relies on a heuristic procedure to determine which track segment is valid and the creation/termination of tracks. <ref type="bibr" target="#b32">[33]</ref> relies on first grouping detections into tracklets, and then in the subsequent stage into long-range tracks with a greedy heuristic approach. In our approach, frame-to-frame and long-range similarity is incorporated into the objective function in a unified manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>We now turn to our mathematical abstraction of multiple people tracking as a minimum cost lifted multicut problem (LMP). The LMP is an optimization problem whose feasible solutions can be identified with decomposition of a graph. The minimum cost multicut problem (MP) <ref type="bibr" target="#b27">[28]</ref> is defined w.r.t. a graph whose edges define possibilities of joining nodes directly into the same track. The LMP is defined, in addition, w.r.t. additional lifted edges that do not define possibilities of directly joining nodes.</p><p>Our motivation for modeling the lifted edges comes from the simple fact that persons of similar appearance are not necessarily identical. Given two detections that are far apart in time and similar in appearance, it is more likely that they represent the same person. At the same time, this decision has to be certified a posteriori by a track connecting the two. We achieve precisely this by introducing two classes of edges: regular edges and lifted edges. In order to assign two detections that are far apart in time and similar in appearance to the same cluster (person), there must exist a path (track) along the regular edges, that certifies this decision.</p><p>Two intuitive examples are given in <ref type="figure" target="#fig_1">Fig. 2</ref>. In (a) and (b) there are three persons in the scene, v 1 is the detection on the first person, v 2 and v 3 are the detections on the second, v 4 is on the third. The costs on the edges v 1 v 2 and v 3 v 4 are −3, suggesting strong rewards towards cutting the edges, and this is correct. However, the cost on the edge v 1 v 4 suggests that the first and the third person look similar and introduces a strong reward towards connecting them. As a result, the MP incorrectly connects v 1 and v 4 as the same person; the LMP does not connect v 1 and v 4 , as such long-range join is not supported by the local edges. (c) and (d) is another example where all the detections are on the same person, namely, a track that connects all the nodes in the graph is desirable. Due to partial occlusion or inaccurate bounding box localization, the costs on the local edges v 1 v 2 and v 3 v 4 could be ambiguous, sometimes even reverse. The longrange edge v 1 v 4 correctly re-identifies the person. The MP, however, produces two clusters for a single person because the long-range edge does not introduce additional constraints on the local connections. In contrast, the LMP allows us to influence the entire chain of connections between person hypotheses with a single confident long-range observation.</p><p>In the following, we discuss in detail first the parameters, then the feasible set, and finally the objective function.</p><p>Parameters. Given an image sequence, we consider an instance of the LMP with respect to the parameters defined below. The estimation of these parameters from the image sequence is discussed in the next section.</p><p>• A finite set V in which every element v ∈ V represents a detection of one person in one image, i.e., a bounding box. For every detection v ∈ V , we also define its height h v ∈ R + , the image coordinates x v , y v ∈ R + of its center and its frame number t v ∈ N.</p><p>• For every pair v, w ∈ V : a conditional probability p vw ∈ (0, 1) of v and w to represent distinct persons, given their height, coordinates and appearance.</p><p>• A graph G = (V, E) whose edges are regular edges that connect detections v, w in the same image t v = t w and also connect detections v, w in distinct images t v = t w that are close in time, i.e., for some fixed upper bounds</p><formula xml:id="formula_0">δ t ∈ N : |t v − t w | ≤ δ t . • A graph G ′ = (V, E ′ ) with E ⊆ E ′ whose additional edges {v, w} ∈ E</formula><p>′ \ E are lifted edges which connect detections v, w that are far apart in time and similar in appearance, i.e., for some fixed p 0 ∈ (0,</p><formula xml:id="formula_1">1 2 ): |t v − t w | &gt; δ t and p vw ≤ p 0 .</formula><p>The graph G defines the decomposition space, and the graph G ′ adds lifted edges E ′ \ E on top of G and defines the structure of the cost function. The lifted edges are introduced for the detections that are far apart in time and similar in appearance, because such pair of detections potentially indicates the same person that reappears after long-term occlusion.</p><p>Feasible Set. The feasible solutions of the LMP can be identified with the decomposition (clusterings) of the graph G. Here, in the context of tracking, every component (cluster) of detections defines a track of one person. It is therefore reasonable to think of our approach as tracking by clustering.</p><p>Formally, any feasible solution of the LMP is a 01-vector x ∈ {0, 1} E ′ in which x vw = 1 indicates that the nodes v and w are in distinct components. In order to ensure that x well-defines a decomposition of G, it is further constrained to the set X GG ′ ⊆ {0, 1} E ′ of those x ∈ {0, 1} E ′ that satisfy the system of linear inequalities written below.</p><p>∀C ∈ cycles(G) ∀e ∈ C :</p><formula xml:id="formula_2">x e ≤ e ′ ∈C\{e} x e ′<label>(1)</label></formula><p>∀vw ∈ E ′ \ E ∀P ∈ vw-paths(G) :</p><formula xml:id="formula_3">x vw ≤ e∈P x e<label>(2)</label></formula><p>∀vw ∈ E ′ \ E ∀C ∈ vw-cuts(G) :</p><formula xml:id="formula_4">1 − x vw ≤ e∈C (1 − x e )<label>(3)</label></formula><p>The constraints (1) are generalized transitivity constraints which mean: For any neighboring nodes v and w, if there exists a path from v to w in G along which all edges are labeled as 0, then the edge vw can only be labeled as 0. The constraints (2) and (3) guarantee, for every feasible solution and every lifted edge vw ∈ E ′ \ E, that the label x vw of this edge is 0 (indicating that v and w belong to the same track) if <ref type="bibr" target="#b1">(2)</ref> and only if (3) v and w are connected in the smaller graph G by a path of edges labeled 0. By assigning a cost or reward c vw ∈ R to a lifted edge vw ∈ E ′ \ E, we can thus assign this cost or reward precisely to those feasible solutions for which v and w belong to distinct tracks, without introducing the additional possibility of joining v and w directly.</p><p>Objective function. We consider instances of the LMP of the form</p><formula xml:id="formula_5">min x∈X GG ′ e∈E ′ c e x e<label>(4)</label></formula><p>with the costs c e defined as</p><formula xml:id="formula_6">c e = log 1 − p e p e .<label>(5)</label></formula><p>The objective function is chosen such that solutions are decompositions of G into tracks that maximize the probability of detections representing the same or distinct persons. More specifically, we define p e as a logistic form:</p><formula xml:id="formula_7">p e := 1 1 + exp(− θ γ , f (e) ) .<label>(6)</label></formula><p>Then the cost c e has the form:</p><formula xml:id="formula_8">c e := log 1 − p e p e = − θ γ , f (e) .<label>(7)</label></formula><p>The model parameter θ γ is estimated on the training set by means of logistic regression. γ is the length of temporal interval between pair of detections. We estimate a separate set of edge-cost parameters θ γ for each temporal interval between the detections. The feature f (e) describes the similarity between detections. In this work, f (e) is defined as a combination of person re-identification confidence (Sec. 3), deep correspondence matching, and spatio-temporal relations, which is discussed in Sec. 4</p><p>Optimization. The minimum cost lifted multicut problem defined by (4) is APX-hard <ref type="bibr" target="#b7">[8]</ref>. Given the size of instances of our tracking problems, solving to optimality or within tight bounds using branch and cut is beyond feasibility. In this work, we exploit a primal heuristic proposed by <ref type="bibr" target="#b11">[12]</ref>, where the bi-partitions of a subgraph are updated by a set of sequences of transformations. The update has the worst-case complexity of O(|V ||E|) which is almost never reached in practice. Detailed run time analysis can be found in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Person Re-identification for Tracking</head><p>Traditionally, person re-identification is the task to associate observed pedestrians in non-overlapping camera views. In the context of multi-person tracking, linking the detected pedestrians across the whole video can be viewed as reidentification with special challenges: occlusions, cluttered background, large difference in image resolution and inaccurate bounding box localization. In this section, we investigate several CNN architectures for re-identification for the multi-person tracking task. Our basic CNN architecture is VGG-16 Net <ref type="bibr" target="#b25">[26]</ref>. Particularly, we propose a novel person re-identification model that combines the body pose layout obtained with state-of-the-art pose estimation methods. Data Collection. One of the key ingredients of deep CNNs is the availability of large amounts of training data. To apply re-identification to tracking, we collect images from the MOT15 benchmark <ref type="bibr" target="#b17">[18]</ref> training set and 5 sequences of the MOT16 benchmark <ref type="bibr" target="#b19">[20]</ref> training set. We also collect person identity examples from the CUHK03 <ref type="bibr" target="#b18">[19]</ref>, Market-1501 <ref type="bibr" target="#b36">[37]</ref> datasets that are captured by 6 surveillance cameras. We use the MOT16-02 and MOT16-11 sequences from the MOT16 training set as test sets. Overall a total of 2511 identities is used for training and 123 identities for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architectures</head><p>In this work, we explore three architectures, namely IDNet, SiameseNet, and StackNet. ID-Net. We first learn a VGG net Φ to recognize N = 2511 unique identities from our data collection as a N -way classification problem. We re-size the training images to 112 × 224 × 3. Each image x i , i = 1, ..., M associates to a ground truth identity label y i ∈ {1, ..., N }. The VGG estimates the probability of each image being each label as p i = Φ(x i ) by a forward pass. The network is trained by the softmax loss.</p><p>During testing, given an image from unseen identities, the final softmax layer is removed and the output of the fully-connected layer Φ f 7 is used as the identity feature. Given a pair of images, the Euclidean distance between the two identity features can be used to decide whether the pair contains the same identity. In the experiments we observe that this identity feature already provides good accuracy. However, the performance is boosted by turning to a Siamese architecture and a StackNet, explained next. SiameseNet. A Siamese architecture means the network contains two symmetry CNNs which share the parameters. We start with a commonly used Siamese architecture as shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. To model the similarity we use fully connected layers on top of the twin CNNs. More specifically, the features F C 6 (x i ) and F C 6 (x j ) from a pair of images are extracted from the first fully-connected layer of the VGG-based Siamese network that shares the weights. Then the features are concatenated and transformed by two fully-connected layers (F C 7 , F C 8 ), where F C 7 are followed by a ReLU non-linearity. F C 8 uses a softmax function to produce a probability estimation over a binary decision, namely the same identity or different identities. StackNet. The most effective architecture we explored is the StackNet, where we stack a pair of images together along the RGB channel. The input to the network becomes 112 × 224 × 6. Then the filter size of the first convolutional layer is changed from 3 × 3 × 3 to 3 × 3 × 6, and for the rest of the network we follow the VGG architecture. The last fully-connected layer models a 2-way classification problem, namely the same identity or different identities. During testing, given a pair of images, both SiameseNet and StackNet produce the probability of the pair being the same/different identities by a forward pass. The StackNet allows a pair of images to communicate at the early stage of the network, but it is still limited by the lack of ability to incorporate body part correspondence between the images. Next, we propose a body part fusing method to explicitly allow modeling the semantic body part information within the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fusing Body Part Information</head><p>A desirable property of the network is to localize the corresponding regions of the body parts, and to reason about the similarity of a pair of pedestrian images based on the localized regions and the full images. We implement such model by fusing body part detections into the CNN. More specifically, we utilize the body part detector <ref type="bibr" target="#b23">[24]</ref> to produce individual score maps for 14 body parts, namely, head, shoulders, elbows, wrists, hips, knees, and ankles, each with left/right symmetry body parts except the head which is indicated by head top and head bottom. We combine the score maps from every two symmetry body parts which results in 7 scores maps; each has the same size as the input image. We stack the pair of images as well as the 14 score maps together to form a 112×224×20 input volume. Now the filter size of the first convolutional layer is set as 3 × 3 × 20, and the rest of the network follows the VGG16 architecture with a 2-way classification layer in the end. In <ref type="figure" target="#fig_2">Fig. 3(d)</ref> we show several examples of estimated body poses on our dataset. Note that augmenting the network with body layout information can be interpreted as an attention mechanism that allows us to focus on the relevant part on the input image. It can also be seen as a mechanism to highlight the foreground and to enable the network to establish corresponding regions between input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Analysis</head><p>Training. Our implementation is based on the Caffe deep learning framework <ref type="bibr" target="#b10">[11]</ref>. To learn the ID-Net, our VGG model is pre-trained on the ImageNet Classification task. Following a common practice in face recognition/verfication literature <ref type="bibr" target="#b21">[22]</ref>, we use our ID-Net as initialization for learning the SiameseNet, StackNet and StackNetPose, which makes the training faster and produces better results. Setup. We have 123 person identities as test examples which are collected from MOT16-02 and MOT16-11. More specifically, on these two sequences, detections that are considered as true positives for a certain identity are those whose intersection-over-union with the ground truth of the identity are larger than 0.5. Given the true positive detections for all the identities, we randomly select 1,000 positive pairs from the detections assigned to the same identity and 4000 negative pairs from the detections assigned to different identities as our test set. A larger ratio of negative pairs in the test set is to simulate the positive/negative distribution during the tracking. For every test pair, we estimate the probability of the pair of images containing the same person. For the positive (negative) pairs, if the estimated probabilities are larger (smaller) than 0.5, they are considered as correctly classified examples. The metric is the verification accuracy, the ratio of correctly classified pairs. For the ID-Net, the verification result of pairs of images is obtained by testing whether the distance between the extracted features is smaller than a threshold. The threshold is obtained on a separate validation data to maximize the verification accuracy. Results. It can be seen from <ref type="figure" target="#fig_2">Fig. 3</ref>(e) that the l 2 distance of the Φ f 7 features from the ID-Net already produces reasonable accuracy. The performance is improved by applying the SiameseNet, from 80.4% to 84.7%. The accuracy is further improved when using the StackNet, achieving 86.9% accuracy. Fusing the body part information (StackNetPose) outperforms all other models by a large margin, achieving 90.0% accuracy. For our tracking task, we use the StackNetPose model to generate person re-identification confidence. We show three pairs of detections that are correctly estimated by StackNetPose in <ref type="figure" target="#fig_2">Fig. 3(d)</ref>. It can be seen that the body part maps enable the network to localize the person despite the inaccurate bounding boxes (the first/second pairs) and cluttered background (the third pair).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pairwise Potentials</head><p>As discussed in Sec. 2, the cost c e in the objective function (4) is defined as c e = − θ γ , f <ref type="bibr">(e)</ref> . In this section, we introduce the feature f (e) , which is based on three information sources: spatio-temporal relations (ST), dense correspondence matching (DM) and person re-identification confidence (Re-ID) that is described in the previous section. ST. The spatio-temporal relation based feature is commonly used in many multi-person tracking works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref>, as it is a good affinity measure for pairs of detections that are in close proximity. Given two detections v and w, each has spatio-temporal locations (x, y, t) and height h. The ST feature is defined as</p><formula xml:id="formula_9">f st = √ (xv−xw) 2 +(yv−yw) 2 h , wherē h = (hv+hw) 2</formula><p>. Intuitively, the ST features are able to provide useful information within a short temporal window. They model the geometric relations between bounding boxes but do not take image content into account. DM. DeepMatching <ref type="bibr" target="#b31">[32]</ref> is introduced as a powerful pairwise affinity for multi-person tracking by <ref type="bibr" target="#b27">[28]</ref>. We apply it in this work as well. Given two detections v and w, each has a set of matched keypoints M . We define M U = |M v ∪ M w |, and M I = |M v ∩ M w | between the set M v and M w . Then the pairwise feature between the two detections is defined as f dm = M I/M U . Re-ID. The DM feature is based on local image patch matching, which makes it robust to irregular camera motion and to partial occlusion in short temporal distance. As shown in <ref type="bibr" target="#b27">[28]</ref> and in the experiment section of our work, the performance of the DM feature drops dramatically when increasing temporal distance. ReID is explicitly trained for the task of person re-identification. It is robust with respect to large temporal and spatial distance and allows long-range association. In this work, we utilize our deep reidentification model (StackNetPose) for modeling the longrange connections. Our final pairwise feature</p><formula xml:id="formula_10">f (e) is defined as (f st , f dm , f reID , ξ min , f 2 st , f st · f dm , . . . , ξ 2 min )</formula><p>, where ξ min is the lower detection confidence within the pair, and f reID is the probability estimated by our StackNetPose. The quadratic terms introduce a non-linear mapping from the feature space to the cost space. In total the pairwise feature has 14 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Analysis</head><p>In this section, we present an analysis of our pairwise features. We also choose MOT16-02 and MOT16-11 from the MOT16 training set for the analysis, as the imaging conditions and camera motion are largely different between these two sequences. The test example collection and the evaluation metric are the same as for evaluating the person re-identification networks, namely for every test pair, we estimate the probability of the pair of images containing the same person. For the positive (negative) pairs, if the estimated probabilities are larger (smaller) than 0.5, they are considered as correctly classified examples. Any bias toward cut or joint decreases the tracking performance. A higher accuracy leads to a better tracking performance. We conduct a comparison between features as a function of temporal distance. we demonstrate long temporal distance (200 frames), as our model is able to incorporate such information. Results. It can be seen from <ref type="figure" target="#fig_3">Fig. 4</ref> that the DM feature achieves good accuracy up to 10 frames, but its performance deteriorates for connections at longer time span. The performance of the ST feature drops quickly after 5 frames. This  is especially pronounced on the MOT16-11 sequence that has rapid camera motion. In contrast, the Re-ID feature is effective and maintains high accuracy over time. For example on the MOT16-11 sequence the Re-ID (red line) improves over DM (black line) by a notable margin for the temporal distances that are larger than 50 frames. When we combine the three features (Comb, green line in <ref type="figure" target="#fig_3">Fig. 4)</ref>, we obtain the best accuracy at all the temporal distances. The reason is that, at different temporal distance, our combined feature is able to take advantage from different information sources. E.g., when the temporal distance is smaller than 30 frames (1 sec. for these two sequences), the DM and ReID features combine both low-level (local image patch matching) and high-level (person-specific appearance similarity) to produce high accuracy pairwise affinity measures. When the temporal distance increases gradually, the ReID feature becomes more and more informative. However, still adding the ST and DM feature improves the overall accuracy, because they act as a regularizer, that forbids physically impossible associations. Based on these results, we use the combined feature in our tracking experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Tracking Experiments and Results</head><p>We perform our tracking experiments and compare to prior works on the MOT16 Benchmark <ref type="bibr" target="#b19">[20]</ref>. The test set contains 7 sequences, where camera motion, camera angle, and imaging condition are largely different. For each test sequence, the benchmark also provides a training sequence that is captured in the similar setting. Therefore, we learn the model parameter θ γ (defined in Eq. <ref type="formula" target="#formula_8">(7)</ref>) for the test sequences on the corresponding training sequences.</p><p>For analyzing our tracking models, we use MOT16-02 and MOT16-11 from the training set as the validation sequences, the same as previous sections. The model parameter θ γ trained on MOT16-02 is used for MOT16-11 and vice versa. To obtain the final tracks from the clusters generated by MP or LMP, we estimate a smoothed trajectory from the detections that belongs to the same cluster, by using the code from <ref type="bibr" target="#b20">[21]</ref>. When there are gaps in time due to occlusion or detection failures, we fill in the missing detections along the estimated trajectory. We do not consider any clusters whose size are less than 5 in all the experiments. Evaluation Metric. We follow the standard CLEAR MOT metrics <ref type="bibr" target="#b3">[4]</ref> for evaluating multi-person tracking performance. The metrics includes multiple object tracking accuracy (MOTA), which combines identity switches (IDs), false positives (FP), and false negatives (FN). Beside we also report multiple object tracking precision (MOTP), mostly tracked (MT), mostly lost (ML) and fragmentation (FM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Lifted Edges versus Regular Edges</head><p>The graph for the lifted multicut (LMP) includes two types of edges: regular edges and lifted edges. The regular edges define the decomposition of the graph. The lifted edges introduce long-range information on which nodes should be joint/cut without modifying the set of feasible solutions. They penalize long-term false joint (e.g. similar looking people) by forcing valid paths in the feasible solution. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, even beyond 50 frames, the accuracy of our pairwise affinity measure is still above 90%, Such good pairwise affinity should be leveraged into the tracking model. However, if we encode them by regular edges, we have 10% chances of making a false joint, such errors directly produce long false-positive tracks. If they are lifted edges, connecting those detections must be certified by the local regular edges. Two intuitive examples are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this section we perform experimental analysis on the two graph variants: Multicut (MP) and Lifted Multicut (LMP), to validate the effectiveness of the proposed methods. Note that we use the same pairwise feature (Comb. in <ref type="figure" target="#fig_3">Fig. 4)</ref> for the MP and LMP problems.</p><p>Given a tracking instance, intuitively, we would connect detections with regular edges up to a certain temporal distance to overcome potential missing detections due to occlusion. For the further distant detections, we would connect them with lifted edges to incorporate person re-identification information into the model to gain better tracking performance. Following the intuition, our <ref type="bibr">MP</ref>  way that besides having the regular edges between neighboring frames, we also introduce regular edges between all pairs of detections whose temporal distance are up to δ max . The LMP has a combination of regular edges and lifted edges, we denote the temporal distance where we start to change the regular edges to the lifted edges as δ t . Varying δ max . In our first analysis, we gradually change the value of δ max from 1 to 150 frames. As shown in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>, on the MOT16-11 sequence, the MP achieves competitive MOTA (54.2%) when δ max equals 30 frames, but the performance decreases significantly when δ max is increased to 150 frames (5 sec on the MOT16-11). The reason is that the long-range regular edges change the feasible set of the MP. Although the accuracy of the pairwise affinity at 150 frames is near 90%, the model can still make catastrophic false joint, which introduces long-term false positive tracks. Similar results are obtained on the MOT16-02 sequence, MOTA drops to 17.2% when δ max = 150.</p><p>For the LMP, we also change δ max from 1 to 150 frames and we set δ t = δ max /2. Comparing to the MP, the LMP obtains the best MOTA on the MOT16-11 sequence (55.3%) as well as on the MOT16-02 sequence (22.4%). Moreover, it presents a superior performance in all the settings. Particularly for the long-range connections, the margin between the MP and the LMP is more than 10% on the MOT16-11 sequence. Note that, these experiment results reveal a very desirable property of the LMP: stability with respect to the range of connections. Given a new tracking instance, due to unknown camera motion and imaging condition, it is not trivial to build a proper graph for the MP. As to the LMP, due to its robustness and stability, we are free to choose any sensible range of connections. In the next experiment, we further reveal the stability of the LMP by varying δ t . Varying δ t . As shown in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, we evaluate the influence of δ t on LMP under 3 different δ max settings, namely δ max = 60, 90, 120. As a baseline, the tracking performance of MP with δ max = 15, 30, 45, 60, 75, 90 is also shown in the <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, depicted as the green line. It can be seen that at all the temporal distances, adding lifted edges improves the tracking performance over MP, suggesting that long-range person re-identification information is useful for the tracking task. Furthermore, for the longer temporal distance (e.g. δ max = 90), MOTA of the MP drops significantly (49.4%); however, for the LMP with δ max = 90, MOTA maintains at higher levels for δ t = 15, 30, 45, 60 (black line), indicating that LMP is also robust to a large range of δ t . Overall, the results show that our LMP is able to encode long-range information in a more rigorous manner, such that it produces much more stable and robust tracking results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on the MOT16 Benchmark</head><p>Here we present our results on the MOT16 test set. We compare our method with the best published results on the benchmark, including NOMT <ref type="bibr" target="#b5">[6]</ref>, MHT-DAM <ref type="bibr" target="#b13">[14]</ref>, OVBT <ref type="bibr" target="#b1">[2]</ref>,LTTSC-CRF <ref type="bibr" target="#b15">[16]</ref>, CEM <ref type="bibr" target="#b20">[21]</ref>, TBD <ref type="bibr" target="#b9">[10]</ref> and Multicut <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr" target="#b27">[28]</ref> is the most relevant approach comparing to our model, where the deep matching feature is employed and tracking is cast as the minimum cost multicut problem. It can be seen from Tab. 1 that our method establishes a new state-of-the-art performance in terms of MOTA, MOTP and false negative (FN). Comparing to the previous best result, we improve MOTA by 2.4% and MOTP by 3.1%. For FAF, MT, ML and FM, our method achieves the second best performance. The improvement over Multicut <ref type="bibr" target="#b27">[28]</ref> demonstrates the advantage of incorporating the long-range person re-identification information with the lifted multicut formulation. The complete metrics and visualization are presented on the MOT16 benchmark website 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Incorporating long-range information for multi-person tracking is challenging. In this work, we propose to model such long-range information by pose aided deep neural networks. Given the fact that similar looking people are not necessarily identical, we propose a minimum cost lifted multicut formulation where the long-range person re-identification information is encoded in the way that it forces valid paths along the local edges. In the end, we show that the proposed tracking method outperforms previous works on the challenging MOT16 benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Qualitative results on the MOT16 benchmark. The solid line under each bounding box indicates the life time of the track. The lifted multicut tracking model is able to link people through occlusions and produces persistent long-lived tracks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison between Multicut Problem (MP) and Lifted Multicut Problem (LMP). Ground truth track of each person is depicted in gray. Regular edges are depicted in black, lifted edges are in green. Solid lines indicate joints, dotted lines indicate cuts. Costs of cutting edges are indicated by the numbers on the corresponding edges. (Best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) SiameseNet. (b) StackNet. (c) StackNetPose. Red rectangles indicate the convolutional, relu and pooling layers of VGG16. Blue rectangles indicate the fully-connected layers. Grey rectangles on the top of each network are the loss layers. Green boxes are the stacked body part score maps. (d) Example results from StackNetPose. (e) Comparison of the person re-identification models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Accuracy of pairwise affinity measures on the MOT16-02 (a) and the MOT16-11 (b) sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of Multicut model (MP) and Lifted Multicut model (LMP) with different δmax values (a) and different δt values (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>is constructed in theTable 1. Tracking Performance on the MOT16 test set. Best in bold, second best in blue.</figDesc><table>Method 

MOTA MOTP FAF 
MT 
ML 
FP 
FN 
ID Sw 
Frag 
Hz 
Detector 

CEM [21] 
33.2 
75.8 
1.2 
7.8% 
54.4% 
6837 
114322 
642 
731 
0.3 
Public 
TBD [10] 
33.7 
76.5 
1.0 
7.2% 
54.2% 
5804 
112587 
2418 
2252 
1.3 
Public 
LTTSC-CRF [16] 
37.6 
75.9 
2.0 
9.6% 
55.2% 
11,969 
101,343 
481 
1,012 0.6 
Public 
OVBT [2] 
38.4 
75.4 
1.9 
7.5% 
47.3% 
11,517 
99,463 
1,321 
2,140 0.3 
Public 
LINF1 [9] 
41.0 
74.8 
1.3 
11.6% 
51.3% 
7896 
99224 
430 
963 
4.2 
Public 
MHT [14] 
42.9 
76.6 
1.0 
13.6% 
46.9% 
5668 
97919 
499 
659 
0.8 
Public 
NOMT[6] 
46.4 
76.6 
1.6 
18.3% 
41.4% 
9753 
87565 
359 
504 
2.6 
Public 

Multicut [28] 
46.3 
75.7 
1.1 
15.5% 
39.7% 
6373 
90914 
657 
1114 
0.8 
Public 
Lifted Multicut (LMP) 
48.8 
79.0 
1.1 
18.2% 
40.1% 
6654 
86245 
481 
595 
0.5 
Public 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://motchallenge.net/results/MOT16/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Lifting of multicuts. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<idno>abs/1503.03791</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking Multiple Persons Based on a Variational Bayesian Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Benchmarking Mutliple Object Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Türetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Evaluating multiple object tracking performance: The CLEAR MOT metrics. Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiobject tracking as maximum weight independent set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">Target identityaware network flow for online multiple target tracking. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Correlation clustering in general weighted graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Demaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving multi-frame data association with sparse representations for robust near-online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fagot-Bouquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lerasle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d traffic scene understanding from movable platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient decomposition of image and mesh graphs by lifted multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A multi-cut formulation for joint segmentation and tracking of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06317</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiple object tracking by efficient graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long-term time-sensitive costs for crf-based tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Benchmarking Mutliple Object Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07866</idno>
		<title level="m">Learning by tracking: Siamese CNN for robust target association</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">MOTChallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Globallyoptimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tracking multiple people online and in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Subgraph decomposition for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiperson tracking by multicuts and deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Benchmarking Mutliple Object Tracking</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning people detectors for tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection and tracking of occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tracking interacting objects using intertwined flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiple target tracking based on undirected hierarchical relation hypergraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GMCP-Tracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
