<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantized Convolutional Neural Networks for Mobile Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
							<email>jiaxiang.wu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Leng</surname></persName>
							<email>cong.leng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
							<email>yuhang.wang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
							<email>qinghao.hu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<email>jcheng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quantized Convolutional Neural Networks for Mobile Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, we have witnessed the great success of convolutional neural networks (CNN) <ref type="bibr" target="#b18">[19]</ref> in a wide range of visual applications, including image classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>, age estimation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>, etc. This success mainly comes from deeper network architectures as well as the tremendous training data. However, as the network grows deeper, the model complexity is also increasing exponentially in both the training and testing stages, which leads to the very high demand in the computation ability. For instance, the 8-layer AlexNet <ref type="bibr" target="#b15">[16]</ref> involves 60M parameters and requires over 729M FLOPs <ref type="bibr" target="#b0">1</ref> to classify a single image. Although the training stage can be offline carried out on high performance clusters with GPU acceleration, the testing computation cost may be unaffordable for common personal computers and mobile devices. Due to the limited computation ability and memory space, mobile * The corresponding author. devices are almost intractable to run deep convolutional networks. Therefore, it is crucial to accelerate the computation and compress the memory consumption for CNN models. For most CNNs, convolutional layers are the most timeconsuming part, while fully-connected layers involve massive network parameters. Due to the intrinsical difference between them, existing works usually focus on improving the efficiency for either convolutional layers or fully-connected layers. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>, lowrank approximation or tensor decomposition is adopted to speed-up convolutional layers. On the other hand, parameter compression in fully-connected layers is explored in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. Overall, the above-mentioned algorithms are able to achieve faster speed or less storage. However, few of them can achieve significant acceleration and compression simultaneously for the whole network.</p><p>In this paper, we propose a unified framework for con-volutional networks, namely Quantized CNN (Q-CNN), to simultaneously accelerate and compress CNN models with only minor performance degradation. With network parameters quantized, the response of both convolutional and fully-connected layers can be efficiently estimated via the approximate inner product computation. We minimize the estimation error of each layer's response during parameter quantization, which can better preserve the model performance. In order to suppress the accumulative error while quantizing multiple layers, an effective training scheme is introduced to take previous estimation error into consideration. Our Q-CNN model enables fast test-phase computation, and the storage and memory consumption are also significantly reduced. We evaluate our Q-CNN framework for image classification on two benchmarks, MNIST <ref type="bibr" target="#b19">[20]</ref> and ILSVRC-12 <ref type="bibr" target="#b25">[26]</ref>. For MNIST, our Q-CNN approach achieves over 12× compression for two neural networks (no convolution), with lower accuracy loss than several baseline methods. For ILSVRC-12, we attempt to improve the test-phase efficiency of four convolutional networks: AlexNet <ref type="bibr" target="#b15">[16]</ref>, CaffeNet <ref type="bibr" target="#b14">[15]</ref>, CNN-S <ref type="bibr" target="#b0">[1]</ref>, and VGG-16 <ref type="bibr" target="#b26">[27]</ref>. Generally, Q-CNN achieves 4× acceleration and 15× compression (sometimes higher) for each network, with less than 1% drop in the top-5 classification accuracy. Moreover, we implement the quantized CNN model on mobile devices, and dramatically improve the test-phase efficiency, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The main contributions of this paper can be summarized as follows:</p><p>• We propose a unified Q-CNN framework to accelerate and compress convolutional networks. We demonstrate that better quantization can be learned by minimizing the estimation error of each layer's response.</p><p>• We propose an effective training scheme to suppress the accumulative error while quantizing the whole convolutional network.</p><p>• Our Q-CNN framework achieves 4 ∼ 6× speed-up and 15 ∼ 20× compression, while the classification accuracy loss is within one percentage. Moreover, the quantized CNN model can be implemented on mobile devices and classify an image within one second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head><p>During the test phase of convolutional networks, the computation overhead is dominated by convolutional layers; meanwhile, the majority of network parameters are stored in fully-connected layers. Therefore, for better testphase efficiency, it is critical to speed-up the convolution computation and compress parameters in fully-connected layers.</p><p>Our observation is that the forward-passing process of both convolutional and fully-connected layers is dominated by the computation of inner products. More formally, we consider a convolutional layer with input feature maps S ∈ R ds×ds×Cs and response feature maps T ∈ R dt×dt×Ct , where d s , d t are the spatial sizes and C s , C t are the number of feature map channels. The response at the 2-D spatial position p t in the c t -th response feature map is computed as:</p><formula xml:id="formula_0">T pt (c t ) = (p k ,ps) W ct,p k , S ps<label>(1)</label></formula><p>where W ct ∈ R d k ×d k ×Cs is the c t -th convolutional kernel and d k is the kernel size. We use p s and p k to denote the 2-D spatial positions in the input feature maps and convolutional kernels, and both W ct,p k and S ps are C s -dimensional vectors. The layer response is the sum of inner products at all positions within the d k × d k receptive field in the input feature maps.</p><p>Similarly, for a fully-connected layer, we have:</p><formula xml:id="formula_1">T (c t ) = W ct , S<label>(2)</label></formula><p>where S ∈ R Cs and T ∈ R Ct are the layer input and layer response, respectively, and W ct ∈ R Cs is the weighting vector for the c t -th neuron of this layer.</p><p>Product quantization <ref type="bibr" target="#b13">[14]</ref> is widely used in approximate nearest neighbor search, demonstrating better performance than hashing-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The idea is to decompose the feature space as the Cartesian product of multiple subspaces, and then learn sub-codebooks for each subspace. A vector is represented by the concatenation of subcodewords for efficient distance computation and storage.</p><p>In this paper, we leverage product quantization to implement the efficient inner product computation. Let us consider the inner product computation between x, y ∈ R D . At first, both x and y are split into M sub-vectors, denoted as x (m) and y (m) . Afterwards, each x (m) is quantized with a sub-codeword from the m-th sub-codebook, then we have</p><formula xml:id="formula_2">y, x = m y (m) , x (m) ≈ m y (m) , c (m) km<label>(3)</label></formula><p>which transforms the O(D) inner product computation to M addition operations (M ≤ D), if the inner products between each sub-vector y (m) and all the sub-codewords in the m-th sub-codebook have been computed in advance.</p><p>Quantization-based approaches have been explored in several works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12]</ref>. These approaches mostly focus on compressing parameters in fully-connected layers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>, and none of them can provide acceleration for the test-phase computation. Furthermore, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> require the network parameters to be re-constructed during the testphase, which limit the compression to disk storage instead of memory consumption. On the contrary, our approach offers simultaneous acceleration and compression for both convolutional and fully-connected layers, and can reduce the run-time memory consumption dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Quantized CNN</head><p>In this section, we present our approach for accelerating and compressing convolutional networks. Firstly, we introduce an efficient test-phase computation process with the network parameters quantized. Secondly, we demonstrate that better quantization can be learned by directly minimizing the estimation error of each layer's response. Finally, we analyze the computation complexity of our quantized CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quantizing the Fully-connected Layer</head><p>For a fully-connected layer, we denote its weighting matrix as W ∈ R Cs×Ct , where C s and C t are the dimensions of the layer input and response, respectively. The weighting vector W ct is the c t -th column vector in W .</p><p>We evenly split the C s -dimensional space (where</p><formula xml:id="formula_3">W ct lies in) into M subspaces, each of C ′ s = C s /M dimen- sions. Each W ct is then decomposed into M sub-vectors, denoted as W (m)</formula><p>ct . A sub-codebook can be learned for each subspace after gathering all the sub-vectors within this subspace. Formally, for the m-th subspace, we optimize:</p><formula xml:id="formula_4">min D (m) ,B (m) D (m) B (m) − W (m) 2 F s.t. D (m) ∈ R C ′ s ×K , B (m) ∈ {0, 1} K×Ct<label>(4)</label></formula><p>where is an indicator vector (only one non-zero entry), specifying which subcodeword is used to quantize the corresponding sub-vector. The optimization can be solved via k-means clustering.</p><formula xml:id="formula_5">W (m) ∈ R</formula><p>The layer response is approximately computed as:</p><formula xml:id="formula_6">T (ct) = m W (m) ct , S (m) ≈ m D (m) B (m) ct , S (m) = m D (m) km(ct) , S (m)<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">B (m)</formula><p>ct is the c t -th column vector in B (m) , and S (m) is the m-th sub-vector of the layer input. k m (c t ) is the index of the sub-codeword used to quantize the sub-vector W (m) ct . In <ref type="figure" target="#fig_2">Figure 2</ref>, we depict the parameter quantization and test-phase computation process of the fully-connected layer. By decomposing the weighting matrix into M sub-matrices, M sub-codebooks can be learned, one per subspace. During the test-phase, the layer input is split into M sub-vectors, denoted as S (m) . For each subspace, we compute the inner products between S (m) and every sub-codeword in D (m) , and store the results in a look-up table. Afterwards, only M addition operations are required to compute each response. As a result, the overall time complexity can be reduced from</p><formula xml:id="formula_8">O(C s C t ) to O(C s K + C t M ).</formula><p>On the other hand, only sub-codebooks and quantization indices need to be stored, which can dramatically reduce the storage consumption. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quantizing the Convolutional Layer</head><p>Unlike the 1-D weighting vector in the fully-connected layer, each convolutional kernel is a 3-dimensional tensor:</p><formula xml:id="formula_9">W ct ∈ R d k ×d k ×Cs .</formula><p>Before quantization, we need to determine how to split it into sub-vectors, i.e. apply subspace splitting to which dimension. During the test phase, the input feature maps are traversed by each convolutional kernel with a sliding window in the spatial domain. Since these sliding windows are partially overlapped, we split each convolutional kernel along the dimension of feature map channels, so that the pre-computed inner products can be reused at multiple spatial locations. Specifically, we learn the quantization in each subspace by:</p><formula xml:id="formula_10">min D (m) ,{B (m) p k } p k D (m) B (m) p k − W (m) p k 2 F s.t. D (m) ∈ R C ′ s ×K , B (m) p k ∈ {0, 1} K×Ct<label>(6)</label></formula><p>where</p><formula xml:id="formula_11">W (m) p k ∈ R C ′</formula><p>s ×Ct contains the m-th sub-vectors of all convolutional kernels at position p k . The optimization can also be solved by k-means clustering in each subspace.</p><p>With the convolutional kernels quantized, we approximately compute the response feature maps by:</p><formula xml:id="formula_12">T pt (c t ) = (p k ,ps) m W (m) ct,p k , S (m) ps ≈ (p k ,ps) m D (m) B (m) ct,p k , S (m) ps = (p k ,ps) m D (m) km(ct,p k ) , S (m) ps<label>(7)</label></formula><p>where</p><formula xml:id="formula_13">S (m)</formula><p>ps is the m-th sub-vector at position p s in the input feature maps, and k m (c t , p k ) is the index of the subcodeword to quantize the m-th sub-vector at position p k in the c t -th convolutional kernel.</p><p>Similar to the fully-connected layer, we pre-compute the look-up tables of inner products with the input feature maps. Then, the response feature maps are approximately computed with <ref type="formula" target="#formula_12">(7)</ref>, and both the time and storage complexity can be greatly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Quantization with Error Correction</head><p>So far, we have presented an intuitive approach to quantize parameters and improve the test-phase efficiency of convolutional networks. However, there are still two critical drawbacks. First, minimizing the quantization error of model parameters does not necessarily give the optimal quantized network for the classification accuracy. In contrast, minimizing the estimation error of each layer's response is more closely related to the network's classification performance. Second, the quantization of one layer is independent of others, which may lead to the accumulation of error when quantizing multiple layers. The estimation error of the network's final response is very likely to be quickly accumulated, since the error introduced by the previous quantized layers will also affect the following layers.</p><p>To overcome these two limitations, we introduce the idea of error correction into the quantization of network parameters. This improved quantization approach directly minimizes the estimation error of the response at each layer, and can compensate the error introduced by previous layers. With the error correction scheme, we can quantize the network with much less performance degradation than the original quantization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Error Correction for the Fully-connected Layer</head><p>Suppose we have N images to learn the quantization of a fully-connected layer, and the layer input and response of image I n are denoted as S n and T n . In order to minimize the estimation error of the layer response, we optimize:</p><formula xml:id="formula_14">min {D (m) },{B (m) } n T n − m (D (m) B (m) ) T S (m) n 2 F<label>(8)</label></formula><p>where the first term in the Frobenius norm is the desired layer response, and the second term is the approximated layer response computed via the quantized parameters.</p><p>A block coordinate descent approach can be applied to minimize this objective function. For the m-th subspace, its residual error is defined as:</p><formula xml:id="formula_15">R (m) n = T n − m ′ =m (D (m ′ ) B (m ′ ) ) T S (m ′ ) n (9)</formula><p>and then we attempt to minimize the residual error of this subspace, which is:</p><formula xml:id="formula_16">min D (m) ,B (m) n R (m) n − (D (m) B (m) ) T S (m) n 2 F<label>(10)</label></formula><p>and the above optimization can be solved by alternatively updating the sub-codebook and sub-codeword assignment. Update D (m) . We fix the sub-codeword assignment B (m) , and define L k = {c t |B (m) (k, c t ) = 1}. The optimization in (10) can be re-formulated as:</p><formula xml:id="formula_17">min {D (m) k } n,k ct∈L k [R (m) n (c t ) − D (m) T k S (m) n ] 2<label>(11)</label></formula><p>which implies that the optimization over one sub-codeword does not affect other sub-codewords. Hence, for each subcodeword, we construct a least square problem from <ref type="formula" target="#formula_0">(11)</ref> to update it. Update B (m) . With the sub-codebook D (m) fixed, it is easy to discover that the optimization of each column in B (m) is mutually independent. For the c t -th column, its optimal sub-codeword assignment is given by:</p><formula xml:id="formula_18">k * m (c t ) = arg min k n [R (m) n (c t ) − D (m) T k S (m) n ] 2<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Error Correction for the Convolutional Layer</head><p>We adopt the similar idea to minimize the estimation error of the convolutional layer's response feature maps, that is:</p><formula xml:id="formula_19">min {D (m) },{B (m) p k } n,pt Tn,p t − (p k ,ps) m (D (m) B (m) p k ) T S (m) n,ps 2 F<label>(13)</label></formula><p>The optimization also can be solved by block coordinate descent. More details on solving this optimization can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Error Correction for Multiple Layers</head><p>The above quantization method can be sequentially applied to each layer in the CNN model. One concern is that the estimation error of layer response caused by the previous layers will be accumulated and affect the quantization of the following layers. Here, we propose an effective training scheme to address this issue.</p><p>We consider the quantization of a specific layer, assuming its previous layers have already been quantized. The optimization of parameter quantization is based on the layer input and response of a group of training images. To quantize this layer, we take the layer input in the quantized network as {S n }, and the layer response in the original network (not quantized) as {T n } in Eq. (8) and <ref type="bibr" target="#b12">(13)</ref>. In this way, the optimization is guided by the actual input in the quantized network and the desired response in the original network. The accumulative error introduced by the previous layers is explicitly taken into consideration during optimization. In consequence, this training scheme can effectively suppress the accumulative error for the quantization of multiple layers.</p><p>Another possible solution is to adopt back-propagation to jointly update the sub-codebooks and sub-codeword assignments in all quantized layers. However, since the subcodeword assignments are discrete, the gradient-based optimization can be quite difficult, if not entirely impossible. Therefore, back-propagation is not adopted here, but could be a promising extension for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Computation Complexity</head><p>Now we analyze the test-phase computation complexity of convolutional and fully-connected layers, with or without parameter quantization. For our proposed Q-CNN model, the forward-passing through each layer mainly consists of two procedures: pre-computation of inner products, and approximate computation of layer response. Both subcodebooks and sub-codeword assignments are stored for the test-phase computation. We report the detailed comparison on the computation and storage overhead in <ref type="table" target="#tab_0">Table 1</ref>. </p><formula xml:id="formula_20">2 t C t d 2 k C s Q-CNN d 2 s C s K + d 2 t C t d 2 k M FCnt. CNN C s C t Q-CNN C s K + C t M Bytes Conv. CNN 4d 2 k C s C t Q-CNN 4C s K + 1 8 d 2 k M C t log 2 K FCnt. CNN 4C s C t Q-CNN 4C s K + 1 8 M C t log 2 K</formula><p>As we can see from <ref type="table" target="#tab_0">Table 1</ref>, the reduction in the computation and storage overhead largely depends on two hyperparameters, M (number of subspaces) and K (number of sub-codewords in each subspace). Large values of M and K lead to more fine-grained quantization, but is less efficient in the computation and storage consumption. In practice, we can vary these two parameters to balance the tradeoff between the test-phase efficiency and accuracy loss of the quantized CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>There have been a few attempts in accelerating the testphase computation of convolutional networks, and many are inspired from the low-rank decomposition. Denton et al. <ref type="bibr" target="#b6">[7]</ref> presented a series of low-rank decomposition designs for convolutional kernels. Similarly, CP-decomposition was adopted in <ref type="bibr" target="#b16">[17]</ref> to transform a convolutional layer into multiple layers with lower complexity. Zhang et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> considered the subsequent nonlinear units while learning the low-rank decomposition. <ref type="bibr" target="#b17">[18]</ref> applied group-wise pruning to the convolutional tensor to decompose it into the multiplications of thinned dense matrices. Recently, fixed-point based approaches are explored in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. By representing the connection weights (or even network activations) with fixed-point numbers, the computation can greatly benefit from hardware acceleration.</p><p>Another parallel research trend is to compress parameters in fully-connected layers. Ciresan et al. <ref type="bibr" target="#b2">[3]</ref> randomly remove connection to reduce network parameters. Matrix factorization was adopted in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> to decompose the weighting matrix into two low-rank matrices, which demonstrated that significant redundancy did exist in network parameters. Hinton et al. <ref type="bibr" target="#b7">[8]</ref> proposed to use dark knowledge (the response of a well-trained network) to guide the training of a much smaller network, which was superior than directly training. By exploring the similarity among neurons, Srinivas et al. <ref type="bibr" target="#b27">[28]</ref> proposed a systematic way to remove redundant neurons instead of network connections. In <ref type="bibr" target="#b29">[30]</ref>, multiple fully-connected layers were replaced by a single "Fastfood" layer, which can be trained in an end-to-end style with convolutional layers. Chen et al. <ref type="bibr" target="#b1">[2]</ref> randomly grouped connection weights into hash buckets, and then fine-tuned the network with back-propagation. <ref type="bibr" target="#b11">[12]</ref> combined pruning, quantization, and Huffman coding to achieve higher compression rate. Gong et al. <ref type="bibr" target="#b10">[11]</ref> adopted vector quantization to compress the weighing matrix, which was actually a special case of our approach (apply Q-CNN without error correction to fully-connected layers only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate our quantized CNN framework on two image classification benchmarks, MNIST <ref type="bibr" target="#b19">[20]</ref> and ILSVRC-12 <ref type="bibr" target="#b25">[26]</ref>. For the acceleration of convolutional layers, we compare with: For all above baselines, we use their reported results under the same setting for fair comparison. We report the theoretical speed-up for more consistent results, since the realistic speed-up may be affected by various factors, e.g. CPU, cache, and RAM. We compare the theoretical and realistic speed-up in Section 5.4, and discuss the effect of adopting the BLAS library for acceleration.</p><formula xml:id="formula_21">• CPD</formula><p>Our approaches are denoted as "Q-CNN" and "Q-CNN (EC)", where the latter one adopts error correction while the former one does not. We implement the optimization process of parameter quantization in MATLAB, and fine-tune the resulting network with Caffe <ref type="bibr" target="#b14">[15]</ref>. Additional results of our approach can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on MNIST</head><p>The MNIST dataset contains 70k images of hand-written digits, 60k used for training and 10k for testing. To evaluate the compression performance, we pre-train two neural networks, one is 3-layer and another one is 5-layer, where each hidden layer contains 1000 units. Different compression techniques are then adopted to compress these two network, and the results are as depicted in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref>. Comparison on the compression rates and classification error on MNIST, based on a 3-layer network (784-1000-10) and a 5-layer network (784-1000-1000-1000-10). In our Q-CNN framework, the trade-off between accuracy and efficiency is controlled by M (number of subspaces) and K (number of sub-codewrods in each subspace). Since M = C s /C ′ s is determined once C ′ s is given, we tune (C ′ s , K) to adjust the quantization precision. In Table 2, we set the hyper-parameters as C ′ s = 4 and K = 32. From <ref type="table">Table 2</ref>, we observe that our Q-CNN (EC) approach offers higher compression rates with less performance degradation than all baselines for both networks. The error correction scheme is effective in reducing the accuracy loss, especially for deeper networks (5-layer). Also, we find the performance of both Q-CNN and Q-CNN (EC) quite stable, as the standard deviation of five random runs is merely 0.05%. Therefore, we report the single-run performance in the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on ILSVRC-12</head><p>The ILSVRC-12 benchmark consists of over one million training images drawn from 1000 categories, and a disjoint validation set of 50k images. We report both the top-1 and top-5 classification error rates on the validation set, using single-view testing (central patch only).</p><p>We demonstrate our approach on four convolutional networks: AlexNet <ref type="bibr" target="#b15">[16]</ref>, CaffeNet <ref type="bibr" target="#b14">[15]</ref>, CNN-S <ref type="bibr" target="#b0">[1]</ref>, and VGG-16 <ref type="bibr" target="#b26">[27]</ref>. The first two models have been adopted in several related works, and therefore are included for comparison. CNN-S and VGG-16 use a either wider or deeper structure for better classification accuracy, and are included here to prove the scalability of our approach. We compare all these networks' computation and storage overhead in <ref type="table" target="#tab_3">Table 3</ref>, together with their classification error rates on ILSVRC-12. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Quantizing the Convolutional Layer</head><p>To begin with, we quantize the second convolutional layer of AlexNet, which is the most time-consuming layer during the test-phase. In <ref type="table">Table 4</ref>, we report the performance under several (C ′ s , K) settings, comparing with two baseline methods, CPD <ref type="bibr" target="#b16">[17]</ref> and GBD <ref type="bibr" target="#b17">[18]</ref>. <ref type="table">Table 4</ref>. Comparison on the speed-up rates and the increase of top-1/5 error rates for accelerating the second convolutional layer in AlexNet, with or without fine-tuning (FT). The hyper-parameters of Q-CNN, C ′ s and K, are as specified in the "Para." column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Para From <ref type="table">Table 4</ref>, we discover that with a large speed-up rate (over 4×), the performance loss of both CPD and GBD become severe, especially before fine-tuning. The naive parameter quantization method also suffers from the similar problem. By incorporating the idea of error correction, our Q-CNN model achieves up to 6× speed-up with merely 0.6% drop in accuracy, even without fine-tuning. The accuracy loss can be further reduced after fine-tuning the subsequent layers. Hence, it is more effective to minimize the estimation error of each layer's response than minimize the quantization error of network parameters.</p><p>Next, we take one step further and attempt to speed-up all the convolutional layers in AlexNet with Q-CNN (EC).  <ref type="table" target="#tab_5">Table 5</ref>, we observe that the loss in accuracy grows mildly than the single-layer case. The speed-up rates reported here are consistently smaller than those in <ref type="table">Table 4</ref>, since the acceleration effect is less significant for some layers (i.e. "conv 4" and "conv 5"). For AlexNet, our Q-CNN model (C ′ s = 8, K = 128) can accelerate the computation of all the convolutional layers by a factor of 4.27×, while the increase in the top-1 and top-5 error rates are no more than 2.5%. After fine-tuning the remaining fully-connected layers, the performance loss can be further reduced to less than 1%.</p><p>In <ref type="table" target="#tab_5">Table 5</ref>, we also report the comparison against LANR <ref type="bibr" target="#b30">[31]</ref> on VGG-16. For the similar speed-up rate (4×), their approach outperforms ours in the top-5 classification error (an increase of 0.95% against 1.83%). After fine-tuning, the performance gap is narrowed down to 0.35% against 0.45%. At the same time, our approach offers over 23× compression of parameters in convolutional layers, much larger than theirs 2.7× compression 2 . Therefore, our approach is effective in accelerating and compressing networks with many convolutional layers, with only minor performance loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Quantizing the Fully-connected Layer</head><p>For demonstration, we first compress parameters in a single fully-connected layer. In CaffeNet, the first fully-connected layer possesses over 37 million parameters (9216 × 4096), more than 60% of whole network parameters. Our Q-CNN approach is adopted to quantize this layer and the results are as reported in <ref type="table" target="#tab_6">Table 6</ref>. The performance loss of our Q-CNN model is negligible (within 0.4%), which is much smaller than baseline methods (DPP and SVD). Furthermore, error correction is effective in preserving the classification accuracy, especially under a higher compression rate. Now we evaluate our approach's performance for compressing all the fully-connected layers in CaffeNet in Table 7. The third layer is actually the combination of 1000 classifiers, and is more critical to the classification accuracy. Hence, we adopt a much more fine-grained hyper-parameter <ref type="bibr" target="#b1">2</ref> The compression effect of their approach was not explicitly discussed in the paper; we estimate the compression rate based on their description. setting (C ′ s = 1, K = 16) for this layer. Although the speed-up effect no longer exists, we can still achieve around 8× compression for the last layer. From <ref type="table" target="#tab_7">Table 7</ref>, we discover that with less than 1% drop in accuracy, Q-CNN achieves high compression rates (12 ∼ 20×), much larger than that of SVD</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Quantizing the Whole Network</head><p>So far, we have evaluated the performance of CNN models with either convolutional or fully-connected layers quantized. Now we demonstrate the quantization of the whole network with a three-stage strategy. Firstly, we quantize all the convolutional layers with error correction, while fullyconnected layers remain untouched. Secondly, we fine-tune fully-connected layers in the quantized network with the ILSVRC-12 training set to restore the classification accuracy. Finally, fully-connected layers in the fine-tuned network are quantized with error correction. We report the performance of our Q-CNN models in <ref type="table" target="#tab_8">Table 8</ref>. For convolutional layers, we let C ′ s = 8 and K = 128 for AlexNet, CaffeNet, and CNN-S, and let C ′ s = 6 and K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speedup for each network. Then we vary the hyper-parameter settings in fully-connected layers for different compression levels. For the former two networks, we achieve 18× compression with about 1% loss in the top-5 classification accuracy. For CNN-S, we achieve 5.78× speed-up and 20.16× compression, while the top-5 classification accuracy drop is merely 0.85%. The result on VGG-16 is even more encouraging: with 4.06× speed-up and 20.34×, the increase of top-5 error rate is only 0.58%. Hence, our proposed Q-CNN framework can improve the efficiency of convolutional networks with minor performance loss, which is acceptable in many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on Mobile Devices</head><p>We have developed an Android application to fulfill CNN-based image classification on mobile devices, based on our Q-CNN framework. The experiments are carried out on a Huawei R Mate 7 smartphone, equipped with an 1.8GHz Kirin 925 CPU. The test-phase computation is carried out on a single CPU core, without GPU acceleration.</p><p>In <ref type="table" target="#tab_9">Table 9</ref>, we compare the computation efficiency and classification accuracy of the original and quantized CNN models. Our Q-CNN framework achieves 3× speed-up for AlexNet, and 4× speed-up for CNN-S. What's more, we compress the storage consumption by 20 ×, and the re- quired run-time memory is only one quarter of the original model. At the same time, the loss in the top-5 classification accuracy is no more than 1%. Therefore, our proposed approach improves the run-time efficiency in multiple aspects, making the deployment of CNN models become tractable on mobile platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Theoretical vs. Realistic Speed-up</head><p>In <ref type="table" target="#tab_0">Table 10</ref>, we compare the theoretical and realistic speed-up on AlexNet. The BLAS <ref type="bibr" target="#b28">[29]</ref> library is used in Caffe <ref type="bibr" target="#b14">[15]</ref> to accelerate the matrix multiplication in convolutional and fully-connected layers. However, it may not always be an option for mobile devices. Therefore, we measure the run-time speed under two settings, i.e. with BLAS enabled or disabled. The realistic speed-up is slightly lower with BLAS on, indicating that Q-CNN does not benefit as much from BLAS as that of CNN. Other optimization techniques, e.g. SIMD, SSE, and AVX <ref type="bibr" target="#b3">[4]</ref>, may further improve our realistic speed-up, and shall be explored in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a unified framework to simultaneously accelerate and compress convolutional neural networks. We quantize network parameters to enable efficient test-phase computation. Extensive experiments are conducted on MNIST and ILSVRC-12, and our approach achieves outstanding speed-up and compression rates, with only negligible loss in the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work was supported in part by National Natural Science Foundation of China (Grant No. 61332016), and 863 program (Grant No. 2014AA015105).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison on the efficiency and classification accuracy between the original and quantized AlexNet [16] and CNN-S [1] on a Huawei R Mate 7 smartphone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>consists of the m-th sub-vectors of all weighting vectors. The sub-codebook D (m) contains K sub-codewords, and each column in B (m)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The parameter quantization and test-phase computation process of the fully-connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Comparison on the computation and storage overhead of convolutional and fully-connected layers.</figDesc><table>FLOPs 

Conv. 
CNN 
d 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>[17]: CP-Decomposition; • GBD [18]: Group-wise Brain Damage; • LANR [31]: Low-rank Approximation of Non-linear Responses.</figDesc><table>and for the compression of fully-connected layers, we com-
pare with the following approaches: 

• RER [3]: Random Edge Removal; 
• LRD [6]: Low-Rank Decomposition; 
• DK [8]: Dark Knowledge; 
• HashNet [2]: Hashed Neural Nets; 
• DPP [28]: Data-free Parameter Pruning; 
• SVD [7]: Singular Value Decomposition; 
• DFC [30]: Deep Fried Convnets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>CNN (EC) 12.1× 1.39% 13.4× 1.19%</figDesc><table>Method 
3-layer 
5-layer 
Compr. 
Error 
Compr. 
Error 
Original 
-
1.35% 
-
1.12% 
RER [3] 
8× 
2.19% 
8× 
1.24% 
LRD [6] 
8× 
1.89% 
8× 
1.77% 
DK [8] 
8× 
1.71% 
8× 
1.26% 
HashNets [2] 
8× 
1.43% 
8× 
1.22% 
Q-CNN 
12.1× 1.42% 13.4× 1.34% 
Q-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Comparison on the test-phase computation overhead 
(FLOPs), storage consumption (Bytes), and classification error 
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16. 
Model 
FLOPs 
Bytes 
Top-1 Err. Top-5 Err. 
AlexNet 
7.29e+8 
2.44e+8 
42.78% 
19.74% 
CaffeNet 
7.27e+8 
2.44e+8 
42.53% 
19.59% 
CNN-S 
2.94e+9 
4.12e+8 
37.31% 
15.82% 
VGG-16 1.55e+10 5.53e+8 
28.89% 
10.05% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers in AlexNet and VGG-16.</figDesc><table>Model 
Method 
Para. Speed-up Compression 
Top-1 Err. ↑ 
Top-5 Err. ↑ 
No FT 
FT 
No FT 
FT 

AlexNet 
Q-CNN 
(EC) 

4/64 
3.32× 
12.46× 
1.33% 
-
0.94% 
-
6/64 
4.32× 
18.55× 
2.32% 
-
1.90% 
-
6/128 
3.71× 
15.25× 
1.44% 0.13% 1.16% 0.36% 
8/128 
4.27× 
20.26× 
2.25% 0.99% 1.64% 0.60% 

VGG-16 
LANR [31] 
-
4.00× 
2.73× 
-
-
0.95% 0.35% 
Q-CNN (EC) 6/128 
4.06× 
23.68× 
3.04% 1.06% 1.83% 0.45% 

We fix the quantization hyper-parameters (C 

′ 

s , K) across all 
layers. From </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 .</head><label>6</label><figDesc>Comparison on the compression rates and the increase of top-1/5 error rates for compressing the first fully-connected layer in CaffeNet, without fine-tuning.</figDesc><table>Method 
Para. 
Compression 
Top-1 Err. ↑ 
Top-5 Err. ↑ 

DPP 

-
1.19× 
0.16% 
-
-
1.47× 
1.76% 
-
-
1.91× 
4.08% 
-
-
2.75× 
9.68% 
-

SVD 

-
1.38× 
0.03% 
-0.03% 
-
2.77× 
0.07% 
0.07% 
-
5.54× 
0.36% 
0.19% 
-
11.08× 
1.23% 
0.86% 

Q-CNN 

2/16 
15.06× 
0.19% 
0.19% 
3/16 
21.94× 
0.35% 
0.28% 
3/32 
16.70× 
0.18% 
0.12% 
4/32 
21.33× 
0.28% 
0.16% 

Q-CNN 
(EC) 

2/16 
15.06× 
0.10% 
0.07% 
3/16 
21.94× 
0.18% 
0.03% 
3/32 
16.70× 
0.14% 
0.11% 
4/32 
21.33× 
0.16% 
0.12% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 7 .</head><label>7</label><figDesc>Comparison on the compression rates and the increase of top-1/5 error rates for compressing all the fully-connected layers in CaffeNet. Both SVD and DFC are fine-tuned, while Q-CNN and Q-CNN (EC) are not fine-tuned.</figDesc><table>Method 
Para. 
Compression 
Top-1 Err. ↑ 
Top-5 Err. ↑ 

SVD 
-
1.26× 
0.14% 
-
-
2.52× 
1.22% 
-

DFC 
-
1.79× 
-0.66% 
-
-
3.58× 
0.31% 
-

Q-CNN 

2/16 
13.96× 
0.28% 
0.29% 
3/16 
19.14× 
0.70% 
0.47% 
3/32 
15.25× 
0.44% 
0.34% 
4/32 
18.71× 
0.75% 
0.59% 

Q-CNN 
(EC) 

2/16 
13.96× 
0.31% 
0.30% 
3/16 
19.14× 
0.59% 
0.47% 
3/32 
15.25× 
0.31% 
0.27% 
4/32 
18.71× 
0.57% 
0.39% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 8 .</head><label>8</label><figDesc>The speed-up/compression rates and the increase of top- 1/5 error rates for the whole CNN model. Particularly, for the quantization of the third fully-connected layer in each network, we let C ′ s = 1 and K = 16.</figDesc><table>Model 
Para. 
Speed-up 
Compression 
Top-1/5 Err. ↑ 
Conv. 
FCnt. 

AlexNet 
8/128 
3/32 
4.05× 
15.40× 
1.38% / 0.84% 
8/128 
4/32 
4.15× 
18.76× 
1.46% / 0.97% 

CaffeNet 
8/128 
3/32 
4.04× 
15.40× 
1.43% / 0.99% 
8/128 
4/32 
4.14× 
18.76× 
1.54% / 1.12% 

CNN-S 
8/128 
3/32 
5.69× 
16.32× 
1.48% / 0.81% 
8/128 
4/32 
5.78× 
20.16× 
1.64% / 0.85% 

VGG-16 
6/128 
3/32 
4.05× 
16.55× 
1.22% / 0.53% 
6/128 
4/32 
4.06× 
20.34× 
1.35% / 0.58% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 9 .</head><label>9</label><figDesc>Comparison on the time, storage, memory consumption, and top-5 classification error rates of the original and quantized AlexNet and CNN-S.</figDesc><table>Model 
Time 
Storage 
Memory 
Top-5 Err. 

AlexNet 
CNN 
2.93s 
232.56MB 
264.74MB 
19.74% 
Q-CNN 
0.95s 
12.60MB 
74.65MB 
20.70% 

CNN-S 
CNN 
10.58s 
392.57MB 
468.90MB 
15.82% 
Q-CNN 
2.61s 
20.13MB 
129.49MB 
16.68% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 10 .</head><label>10</label><figDesc>Comparison on the theoretical and realistic speed-up on AlexNet (CPU only, single-threaded). Here we use the ATLAS library, which is the default BLAS choice in Caffe [15].</figDesc><table>BLAS 
FLOPs 
Time (ms) 
Speed-up 
CNN 
Q-CNN 
CNN 
Q-CNN 
Theo. 
Real. 
Off 
7.29e+8 
1.75e+8 
321.10 
75.62 
4.15× 
4.25× 
On 
167.79 

4 

55.35 
3.03× 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">FLOPs: number of FLoating-point OPerations required to classify one image with the convolutional network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">and DFC (&lt; 4×). Again, Q-CNN with error correction consistently outperforms the naive Q-CNN approach as adopted in [11]. 3 In Table 6, SVD means replacing the weighting matrix with the multiplication of two low-rank matrices; in Table 7, SVD means fine-tuning the network after the low-rank matrix decomposition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is Caffe's run-time speed. The code for the other three settings is on https://github.com/jiaxiang-wu/quantized-cnn.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-performance neural networks for visual object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1102.0183</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Intel architecture instruction set extensions programming reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Corporation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-02" />
			<publisher>Intel Corporation</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training deep neural networks with low precision multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling the knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Geoffrey Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<idno>abs/1412.6115</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>abs/1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011-01" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fast convnets using groupwise brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1506.02515</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online sketching hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2503" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hashing for distributed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1642" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassncer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning ordinal discriminative features for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2570" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1603.05279</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data-free parameter pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="31" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Minimizing development and maintenance costs in supporting persistently optimized BLAS. Software: Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-02" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="101" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1412.7149</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1505.06798</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1984" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
