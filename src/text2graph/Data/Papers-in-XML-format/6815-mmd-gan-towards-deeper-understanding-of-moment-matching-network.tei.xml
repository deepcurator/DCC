<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MMD GAN: Towards Deeper Understanding of Moment Matching Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>2 AI Foundations</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>2 AI Foundations</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>2 AI Foundations</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
							<email>bapoczos@cs.cmu.educhengyu@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>2 AI Foundations</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MMD GAN: Towards Deeper Understanding of Moment Matching Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>( ⇤ denotes equal contribution)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak ⇤ topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA and LSUN, the performance of MMD GAN significantly outperforms GMMN, and is competitive with other representative GAN works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The essence of unsupervised learning models the underlying distribution P X of the data X . Deep generative model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> uses deep learning to approximate the distribution of complex datasets with promising results. However, modeling arbitrary density is a statistically challenging task <ref type="bibr" target="#b2">[3]</ref>. In many applications, such as caption generation <ref type="bibr" target="#b3">[4]</ref>, accurate density estimation is not even necessary since we are only interested in sampling from the approximated distribution.</p><p>Rather than estimating the density of P X , Generative Adversarial Network (GAN) <ref type="bibr" target="#b4">[5]</ref> starts from a base distribution P Z over Z, such as Gaussian distribution, then trains a transformation network g ✓ such that P ✓ ⇡ P X , where P ✓ is the underlying distribution of g ✓ (z) and z ⇠ P Z . During the training, GAN-based algorithms require an auxiliary network f to estimate the distance between P X and P ✓ . Different probabilistic (pseudo) metrics have been studied <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> under GAN framework.</p><p>Instead of training an auxiliary network f for measuring the distance between P X and P ✓ , Generative moment matching network (GMMN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> uses kernel maximum mean discrepancy (MMD) <ref type="bibr" target="#b10">[11]</ref>, which is the centerpiece of nonparametric two-sample test, to determine the distribution distances. During the training, g ✓ is trained to pass the hypothesis test (minimize MMD distance). <ref type="bibr" target="#b10">[11]</ref> shows even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the empirical performance of GMMN does not meet its theoretical properties. There is no promising empirical results comparable with GAN on challenging benchmarks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Computationally, it also requires larger batch size than GAN needs for training, which is considered to be less efficient <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8]</ref> In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels instead of fixed Gaussian kernels to have better hypothesis testing power. The main contributions of this work are:</p><p>• In Section 2, we prove that training g ✓ via MMD with learned kernels is continuous and differentiable, which guarantees the model can be trained by gradient descent. Second, we prove a new distance measure via kernel learning, which is a sensitive loss function to the distance between P X and P ✓ (weak ⇤ topology). Empirically, the loss decreases when two distributions get closer.</p><p>• In Section 3, we propose a practical realization called MMD GAN that learns generator g ✓ with the adversarially trained kernel. We further propose a feasible set reduction to speed up and stabilize the training of MMD GAN.</p><p>• In Section 5, we show that MMD GAN is computationally more efficient than GMMN, which can be trained with much smaller batch size. We also demonstrate that MMD GAN has promising results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To our best knowledge, we are the first MMD based work to achieve comparable results with other GAN works on these datasets.</p><p>Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasserstein GAN <ref type="bibr" target="#b7">[8]</ref> is the special case of the proposed MMD GAN under certain conditions. The unified view shows more connections between moment matching and GAN, which can potentially inspire new algorithms based on well-developed tools in statistics <ref type="bibr" target="#b14">[15]</ref>. Our experiment code is available at https://github.com/OctoberChang/MMD-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GAN, Two-Sample Test and GMMN</head><p>Assume we are given data {x</p><formula xml:id="formula_0">i } n i=1</formula><p>, where x i 2 X and x i ⇠ P X . If we are interested in sampling from P X , it is not necessary to estimate the density of P X . Instead, Generative Adversarial Network (GAN) <ref type="bibr" target="#b4">[5]</ref> trains a generator g ✓ parameterized by ✓ to transform samples z ⇠ P Z , where z 2 Z, into g ✓ (z) ⇠ P ✓ such that P ✓ ⇡ P X . To measure the similarity between P X and P ✓ via their samples {x} n i=1 and {g ✓ (z j )} n j=1 during the training, <ref type="bibr" target="#b4">[5]</ref> trains the discriminator f parameterized by for help. The learning is done by playing a two-player game, where f tries to distinguish x i and g ✓ (z j ) while g ✓ aims to confuse f by generating g ✓ (z j ) similar to x i . On the other hand, distinguishing two distributions by finite samples is known as Two-Sample Test in statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) <ref type="bibr" target="#b10">[11]</ref>. Given two distributions P and Q, and a kernel k, the square of MMD distance is defined as</p><formula xml:id="formula_1">M k (P, Q) = kµ P µ Q k 2 H = E P [k(x, x 0 )] 2E P,Q [k(x, y)] + E Q [k(y, y 0 )]. Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then M k (P, Q) = 0 iff P = Q. GMMN: One example of characteristic kernel is Gaussian kernel k(x, x 0 ) = exp(kx x 0 k 2</formula><p>). Based on Theorem 1, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> propose generative moment-matching network (GMMN), which trains g ✓ by</p><formula xml:id="formula_2">min ✓ M k (P X , P ✓ ),<label>(1)</label></formula><p>with a fixed Gaussian kernel k rather than training an additional discriminator f as GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MMD with Kernel Learning</head><p>In practice we use finite samples from distributions to estimate MMD distance. Given</p><formula xml:id="formula_3">X = {x 1 , · · · , x n } ⇠ P and Y = {y 1 , · · · , y n } ⇠ Q, one estimator of M k (P, Q) iŝ M k (X, Y ) = 1 n 2 X i6 =i 0 k(x i , x 0 i ) 2 n 2 X i6 =j k(x i , y j ) + 1 n 2 X j6 =j 0 k(y j , y 0 j ).</formula><p>Because of the sampling variance,M (X, Y ) may not be zero even when P = Q. We then conduct hypothesis test with null hypothesis H 0 : P = Q. For a given allowable probability of false rejection ↵, we can only reject H 0 , which imply P 6 = Q, ifM (X, Y ) &gt; c ↵ for some chose threshold c ↵ &gt; 0. Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to <ref type="bibr" target="#b10">[11]</ref> for more details.</p><p>Intuitively, if kernel k cannot result in high MMD distance M k (P, Q) when P 6 = Q,M k (P, Q) has more chance to be smaller than c ↵ . Then we are unlikely to reject the null hypothesis H 0 with finite samples, which implies Q is not distinguishable from P. Therefore, instead of training g ✓ via (1) with a pre-specified kernel k as GMMN, we consider training g ✓ via</p><formula xml:id="formula_4">min ✓ max k2K M k (P X , P ✓ ),<label>(2)</label></formula><p>which takes different possible characteristic kernels k 2 K into account. On the other hand, we could also view (2) as replacing the fixed kernel k in (1) with the adversarially learned kernel arg max k2K M k (P X , P ✓ ) to have stronger signal where P 6 = P ✓ to train g ✓ . We refer interested readers to <ref type="bibr" target="#b15">[16]</ref> for more rigorous discussions about testing power and increasing MMD distances.</p><p>However, it is difficult to optimize over all characteristic kernels when we solve <ref type="bibr" target="#b1">(2)</ref>. By <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> if f is a injective function and k is characteristic, then the resulted kernelk = k f , wherek(x,</p><formula xml:id="formula_5">x 0 ) = k(f (x), f(x 0 )</formula><p>) is still characteristic. If we have a family of injective functions parameterized by , which is denoted as f , we are able to change the objective to be</p><formula xml:id="formula_6">min ✓ max M k f (P X , P ✓ ),<label>(3)</label></formula><p>In this paper, we consider the case that combining Gaussian kernels with injective functions f , wherẽ</p><formula xml:id="formula_7">k(x, x 0 ) = exp( kf (x) f (x) 0 k 2 )</formula><p>. One example function class of f is {f |f (x) = x, &gt; 0}, which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed in Section 3. Next, we abuse the notation M f (P, Q) to be MMD distance given the composition kernel of Gaussian kernel and f in the following. Note that <ref type="bibr" target="#b17">[18]</ref> considers the linear combination of characteristic kernels, which can also be incorporated into the discussed composition kernels. A more general kernel is studied in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Properties of MMD with Kernel Learning</head><p>[8] discuss different distances between distributions adopted by existing deep learning algorithms, and show many of them are discontinuous, such as Jensen-Shannon divergence <ref type="bibr" target="#b4">[5]</ref> and Total variation <ref type="bibr" target="#b6">[7]</ref>, except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training. From (3), we train g ✓ via minimizing max M f (P X , P ✓ ). Next, we show max M f (P X , P ✓ ) also enjoys the advantage of being a continuous and differentiable objective in ✓ under mild assumptions. Assumption 2. g : Z ⇥ R m ! X is locally Lipschitz, where Z ✓ R d . We will denote g ✓ (z) the evaluation on (z, ✓) for convenience. Given f and a probability distribution P z over Z, g satisfies Assumption 2 if there are local Lipschitz constants L(✓, z) for f g, which is independent of , such that E z⇠Pz [L(✓, z)] &lt; +1. Theorem 3. The generator function g ✓ parameterized by ✓ is under Assumption 2. Let P X be a fixed distribution over X and Z be a random variable over the space Z. We denote</p><formula xml:id="formula_8">P ✓ the distribution of g ✓ (Z), then max M f (P X , P ✓ )</formula><p>is continuous everywhere and differentiable almost everywhere in ✓.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If g</head><p>✓ is parameterized by a feed-forward neural network, it satisfies Assumption 2 and can be trained via gradient descent as well as propagation, since the objective is continuous and differentiable followed by Theorem 3. More technical discussions are shown in Appendix B.</p><formula xml:id="formula_9">Theorem 4. (weak ⇤ topology) Let {P n } be a sequence of distributions. Considering n ! 1, under mild Assumption, max M f (P X , P n ) ! 0 () P n D ! P X , where D ! means converging in distribution [3].</formula><p>Theorem 4 shows that max M f (P X , P n ) is a sensible cost function to the distance between P X and P n . The distance is decreasing when P n is getting closer to P X , which benefits the supervision of the improvement during the training. All proofs are omitted to Appendix A. In the next section, we introduce a practical realization of training g ✓ via optimizing min</p><formula xml:id="formula_10">✓ max M f (P X , P ✓ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MMD GAN</head><p>To approximate (3), we use neural networks to parameterized g ✓ and f with expressive power. For g ✓ , the assumption is locally Lipschitz, where commonly used feed-forward neural networks satisfy this constraint. Also, the gradient 5 ✓ (max f g ✓ ) has to be bounded, which can be done by clipping <ref type="bibr" target="#b7">[8]</ref> or gradient penalty <ref type="bibr" target="#b19">[20]</ref>. The non-trivial part is f has to be injective. For an injective function f , there exists an function f 1 such that f 1 (f (x)) = x, 8x 2 X and f 1 (f (g(z))) = g(z), 8z 2 Z 1 , which can be approximated by an autoencoder. In the following, we denote = { e , d } to be the parameter of discriminator networks, which consists of an encoder f e , and train the corresponding decoder f d ⇡ f 1 to regularize f . The objective (3) is relaxed to be</p><formula xml:id="formula_11">min ✓ max M f e (P(X ), P(g ✓ (Z))) E y2X [g(Z) ky f d (f e (y))k 2 .<label>(4)</label></formula><p>Note that we ignore the autoencoder objective when we train ✓, but we use (4) for a concise presentation. We note that the empirical study suggests autoencoder objective is not necessary to lead the successful GAN training as we will show in Section 5, even though the injective property is required in Theorem 1.</p><p>The proposed algorithm is similar to GAN <ref type="bibr" target="#b4">[5]</ref>, which aims to optimize two neural networks g ✓ and f in a minmax formulation, while the meaning of the objective is different. In <ref type="bibr" target="#b4">[5]</ref>, f e is a discriminator (binary) classifier to distinguish two distributions. In the proposed algorithm, distinguishing two distribution is still done by two-sample test via MMD, but with an adversarially learned kernel parametrized by f e . g ✓ is then trained to pass the hypothesis test. More connection and difference with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but one can easily extend to other Lipschitz approximations, such as gradient penalty <ref type="bibr" target="#b19">[20]</ref>.</p><p>Algorithm 1: MMD GAN, our proposed algorithm. input :↵ the learning rate, c the clipping parameter, B the batch size, n c the number of iterations of discriminator per generator update. initialize generator parameter ✓ and discriminator parameter ; while ✓ has not converged do for t = 1, . . . , n c do Sample a minibatches {x</p><formula xml:id="formula_12">i } B i=1 ⇠ P(X ) and {z j } B j=1 ⇠ P(Z) g r M f e (P(X ), P(g ✓ (Z))) E y2X [g(Z) ky f d (f e (y))k 2 + ↵ · RMSProp( , g ) clip( , c, c) Sample a minibatches {x i } B i=1 ⇠ P(X ) and {z j } B j=1 ⇠ P(Z) g ✓ r ✓ M f e (P(X ), P(g ✓ (Z))) ✓ ✓ ↵ · RMSProp(✓, g ✓ )</formula><p>Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN, the other way to see the proposed MMD GAN is viewing f e as a feature transformation function, and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of the autoencoder). The optimization is finding a manifold with stronger signals for MMD two-sample test. From this perspective, <ref type="bibr" target="#b8">[9]</ref> is the special case of MMD GAN if f e is the identity mapping function. In such circumstance, the kernel two-sample test is conducted in the original data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feasible Set Reduction</head><p>Theorem 5. For any f , there exists f</p><formula xml:id="formula_13">0 such that M f (P r , P ✓ ) = M f 0 (P r , P ✓ ) and E x [f (x)] ⌫ E z [f 0 (g ✓ (z))]</formula><p>. With Theorem 5, we could reduce the feasible set of during the optimization by solving</p><formula xml:id="formula_14">min ✓ max M f (P r , P ✓ ) s.t. E[f (x)] ⌫ E[f (g ✓ (z))]</formula><p>1 Note that injective is not necessary invertible.</p><p>which the optimal solution is still equivalent to solving (2).</p><p>However, it is hard to solve the constrained optimization problem with backpropagation. We relax the constraint by ordinal regression <ref type="bibr" target="#b20">[21]</ref> to be</p><formula xml:id="formula_15">min ✓ max M f (P r , P ✓ ) + min E[f (x)] E[f (g ✓ (z))]</formula><p>, 0 , which only penalizes the objective when the constraint is violated. In practice, we observe that reducing the feasible set makes the training faster and stabler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>There has been a recent surge on improving GAN <ref type="bibr" target="#b4">[5]</ref>. We review some related works here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection with WGAN:</head><p>If we composite f with linear kernel instead of Gaussian kernel, and restricting the output dimension h to be 1, we then have the objective</p><formula xml:id="formula_16">min ✓ max kE[f (x)] E[f (g ✓ (z))]k 2 .<label>(5)</label></formula><p>Parameterizing f and g ✓ with neural networks and assuming 9 0 2 such f 0 = f , 8 , recovers Wasserstein GAN (WGAN) <ref type="bibr" target="#b7">[8]</ref> 2 . If we treat f (x) as the data transform function, WGAN can be interpreted as first-order moment matching (linear kernel) while MMD GAN aims to match infinite order of moments with Gaussian kernel form Taylor expansion <ref type="bibr" target="#b8">[9]</ref>. Theoretically, Wasserstein distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, <ref type="bibr" target="#b21">[22]</ref> show neural networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we demonstrate matching high-order moments benefits the results. <ref type="bibr" target="#b22">[23]</ref> also propose McGAN that matches second order moment from the primal-dual norm perspective. However, the proposed algorithm requires matrix (tensor) decompositions because of exact moment matching <ref type="bibr" target="#b23">[24]</ref>, which is hard to scale to higher order moment matching. On the other hand, by giving up exact moment matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions are in Appendix B.3.</p><p>Difference from Other Works with Autoencoders: Energy-based GANs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> also utilizes the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes the reconstruction error of real samples x while maximize the reconstruction error of generated samples g ✓ (z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing the reconstruction errors of both real samples x and generated samples g ✓ (z). Also, <ref type="bibr" target="#b7">[8]</ref> show EBGAN approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD distance. The other line of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9]</ref> aims to match the AE codespace f (x), and utilize the decoder f dec (·). <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref> match the distribution of f (x) and z via different distribution distances and generate data (e.g. image) by f dec (z). <ref type="bibr" target="#b8">[9]</ref> use MMD to match f (x) and g(z), and generate data via f dec (g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z) directly as GAN. <ref type="bibr" target="#b26">[27]</ref> is similar to MMD GAN but it considers KL-divergence without showing continuity and weak ⇤ topology guarantee as we prove in Section 2.</p><p>Other GAN Works: In addition to the discussed works, there are several extended works of GAN. <ref type="bibr" target="#b27">[28]</ref> proposes using the linear kernel to match first moment of its discriminator's latent features. <ref type="bibr" target="#b13">[14]</ref> considers the variance of empirical MMD score during the training. Also, <ref type="bibr" target="#b13">[14]</ref> only improves the latent feature matching in <ref type="bibr" target="#b27">[28]</ref> by using kernel MMD, instead of proposing an adversarial training framework as we studied in Section 2. <ref type="bibr" target="#b28">[29]</ref> uses Wasserstein distance to match the distribution of autoencoder loss instead of data. One can consider to extend <ref type="bibr" target="#b28">[29]</ref> to higher order matching based on the proposed MMD GAN. A parallel work <ref type="bibr" target="#b29">[30]</ref> use energy distance, which can be treated as MMD GAN with different kernel. However, there are some potential problems of its critic. More discussion can be referred to <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We train MMD GAN for image generation on the MNIST <ref type="bibr" target="#b31">[32]</ref>, CIFAR-10 <ref type="bibr" target="#b32">[33]</ref>, CelebA <ref type="bibr" target="#b12">[13]</ref>, and LSUN bedrooms <ref type="bibr" target="#b11">[12]</ref> datasets, where the size of training instances are 50K, 50K, 160K, 3M</p><p>respectively. All the samples images are generated from a fixed noise random vectors and are not cherry-picked.</p><p>Network architecture: In our experiments, we follow the architecture of DCGAN <ref type="bibr" target="#b33">[34]</ref> to design g ✓ by its generator and f by its discriminator except for expanding the output layer of f to be h dimensions.</p><p>Kernel designs: The loss function of MMD GAN is implicitly associated with a family of characteristic kernels. Similar to the prior MMD seminal papers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, we consider a mixture of K RBF kernels k(x, x</p><formula xml:id="formula_17">0 ) = P K q=1 k q (x, x 0</formula><p>) where k q is a Gaussian kernel with bandwidth parameter q . Tuning kernel bandwidth q optimally still remains an open problem. In this works, we fixed K = 5 and q to be {1, 2, 4, 8, 16} and left the f to learn the kernel (feature representation) under these q . Hyper-parameters: We use RMSProp <ref type="bibr" target="#b34">[35]</ref> with learning rate of 0.00005 for a fair comparison with WGAN as suggested in its original paper <ref type="bibr" target="#b7">[8]</ref>. We ensure the boundedness of model parameters of discriminator by clipping the weights point-wisely to the range [ 0.01, 0.01] as required by Assumption 2. The dimensionality h of the latent space is manually set according to the complexity of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and LSUN bedrooms. The batch size is set to be B = 64 for all datasets. We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-10. We consider two variants for GMMN. The first one is original GMMN, which trains the generator by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then fix the autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in the code space. We call it as GMMN-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Qualitative Analysis</head><p>The results are pictured in <ref type="figure" target="#fig_0">Figure 1</ref>. Both GMMN-D and GMMN-C are able to generate meaningful digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary and shape of the digits in <ref type="figure" target="#fig_0">Figure 1a</ref> and 1b are often irregular and non-smooth. In contrast, the sample digits in <ref type="figure" target="#fig_0">Figure 1c</ref> are more natural with smooth outline and sharper strike. For CIFAR-10 dataset, both GMMN variants fail to generate meaningful images, but resulting some low level visual features. We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms, thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural images with sharp boundary and high diversity. The results in <ref type="figure" target="#fig_0">Figure 1</ref> confirm the success of the proposed adversarial learned kernels to enrich statistical testing power, which is the key difference between GMMN and MMD GAN.</p><p>If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates that the proposed MMD GAN can be trained more efficiently than GMMN with smaller batch size.</p><p>Comparisons with GANs: There are several representative extensions of GANs. We consider recent state-of-art WGAN <ref type="bibr" target="#b7">[8]</ref> based on DCGAN structure <ref type="bibr" target="#b33">[34]</ref>, because of the connection with MMD GAN discussed in Section 4. The results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For MNIST, the digits generated from WGAN in <ref type="figure" target="#fig_1">Figure 2a</ref> are more unnatural with peculiar strikes. In Contrary, the digits from MMD GAN in <ref type="figure" target="#fig_1">Figure 2d</ref> enjoy smoother contour. Furthermore, both WGAN and MMD GAN generate diversified digits, avoiding the mode collapse problems appeared in the literature of training GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN. Specifically, we observe varied poses, expressions, genders, skin colors and light exposure in <ref type="figure" target="#fig_1">Figure  2b</ref> and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences between the samples generated from MMD GAN and WGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Analysis</head><p>To quantitatively measure the quality and diversity of generated samples, we compute the inception score <ref type="bibr" target="#b27">[28]</ref> on CIFAR-10 images. The inception score is used for GANs to measure samples quality and diversity on the pretrained inception model <ref type="bibr" target="#b27">[28]</ref>. Models that generate collapsed samples have a relatively low score. <ref type="table">Table 1</ref> lists the results for 50K samples generated by various unsupervised generative models trained on CIFAR-10 dataset. The inception scores of <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28]</ref> are directly derived from the corresponding references.</p><p>Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our score is better than other GAN techniques except for DFM <ref type="bibr" target="#b35">[36]</ref>. This seems to confirm empirically that higher order of moment matching between the real data and fake sample distribution benefits generating more diversified sample images. Also note DFM appears compatible with our method and combing training techniques in DFM is a possible avenue for future work.</p><p>Method Scores ± std.</p><p>Real data 11.95 ± .20 DFM <ref type="bibr" target="#b35">[36]</ref> 7.72 ALI <ref type="bibr" target="#b36">[37]</ref> 5.34 Improved GANs <ref type="bibr" target="#b27">[28]</ref> 4  <ref type="figure">Figure 3</ref>: Computation time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Stability of MMD GAN</head><p>We further illustrate how the MMD distance correlates well with the quality of the generated samples. <ref type="figure" target="#fig_2">Figure 4</ref> plots the evolution of the MMD GAN estimate the MMD distance during training for MNIST, CelebA and LSUN datasets. We report the average of theM f (P X , P ✓ ) with moving average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We observe during the whole training process, samples generated from the same noise vector across iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and backgrounds will evolve.) This qualitative observation indicates valuable stability of the training process. The decreasing curve with the improving quality of images supports the weak ⇤ topology shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In <ref type="figure" target="#fig_2">Figure  4b</ref>, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Computation Issue</head><p>We conduct time complexity analysis with respect to the batch size B. The time complexity of each iteration is O(B) for WGAN and O(KB 2 ) for our proposed MMD GAN with a mixture of K RBF kernels. The quadratic complexity O(B 2 ) of MMD GAN is introduced by computing kernel matrix, which is sometimes criticized for being inapplicable with large batch size in practice. However, we point that there are several recent works, such as EBGAN <ref type="bibr" target="#b6">[7]</ref>, also matching pairwise relation between samples of batch size, leading to O(B 2 ) complexity as well.</p><p>Empirically, we find that under GPU environment, the highly parallelized matrix operation tremendously alleviated the quadratic time to almost linear time with modest B. <ref type="figure">Figure 3</ref> compares the computational time per generator iterations versus different B on Titan X. When B = 64, which is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for training GMMN in its references <ref type="bibr" target="#b8">[9]</ref>, the time per iteration becomes 4.431 and 8.565 seconds, respectively. This result coheres our argument that the empirical computational time for MMD GAN is not quadratically expensive compared to WGAN with powerful GPU parallel computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Better Lipschitz Approximation and Necessity of Auto-Encoder</head><p>Although we used weight-clipping for Lipschitz constraint in Assumption 2, one can also use other approximations, such as gradient penalty <ref type="bibr" target="#b19">[20]</ref>. On the other hand, in Algorithm 1, we present an algorithm with auto-encoder to be consistent with the theory that requires f to be injective. However, we observe that it is not necessary in practice. We show some preliminary results of training MMD GAN with gradient penalty and without the auto-encoder in <ref type="figure">Figure 5</ref>. The preliminary study indicates that MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One potential future work is conducting more thorough empirical comparison studies between different approximations.</p><p>(a) Cifar10, Giter = 300K (b) CelebA, Giter = 300K <ref type="figure">Figure 5</ref>: MMD GAN results using gradient penalty <ref type="bibr" target="#b19">[20]</ref> and without auto-encoder reconstruction loss during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We introduce a new deep generative model trained via MMD with adversarially learned kernels. We further study its theoretical properties and propose a practical realization MMD GAN, which can be trained with much smaller batch size than GMMN and has competitive performances with state-of-theart GANs. We can view MMD GAN as the first practical step forward connecting moment matching network and GAN. One important direction is applying developed tools in moment matching <ref type="bibr" target="#b14">[15]</ref> on general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect WGAN and MMD GAN by first-order and infinite-order moment matching. <ref type="bibr" target="#b23">[24]</ref> shows finite-order moment matching (⇠ 5) achieves the best performance on domain adaption. One could extend MMD GAN to this by using polynomial kernels. Last, in theory, an injective mapping f is necessary for the theoretical guarantees. However, we observe that it is not mandatory in practice as we show in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by parameterizing with neural networks, which worth more study as a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD GAN with batch size B = 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training curves and generative samples at different stages of training. We can see a clear correlation between lower distance and better sample quality.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31">st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Theoretically, they are not equivalent but the practical neural network approximation results in the same algorithm.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments. This work is supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-1563887.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">All of statistics: a concise course in statistical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Energy-based Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Gintare Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generative models and model criticism via optimized maximum mean discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyajit</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09522</idno>
		<title level="m">Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel mean embedding of distributions: A review and beyonds</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kernel choice and classifiability for rkhs embeddings of probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Support vector learning for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Obermayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00573</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mcgan: Mean and covariance feature matching gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
		<idno>1702.08398</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arxiv pre-print</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generative adversarial networks as variational training of energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogério</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1611.01799</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial generator-encoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The cramer distance as a solution to biased wasserstein gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10743</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<ptr target="https://medium.com/towards-data-science/notes-on-the-cramer-gan-752abd505c00" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2017" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hilbert space embeddings and metrics on probability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R G</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanckriet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
