<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressed Sensing using Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
						</author>
						<title level="a" type="main">Compressed Sensing using Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model G : R k ! R n . Our main theorem is that, if G is L-Lipschitz, then roughly O(k log L) random Gaussian measurements suffice for an`2/`2 recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use 5-10x fewer measurements than Lasso for the same accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Compressive or compressed sensing is the problem of reconstructing an unknown vector x ⇤ 2 R n after observing m &lt; n linear measurements of its entries, possibly with added noise: y = Ax ⇤ + ⌘, where A 2 R m⇥n is called the measurement matrix and ⌘ 2 R m is noise. Even without noise, this is an underdetermined system of linear equations, so recovery is impossible unless we make an assumption on the structure of the unknown vector x ⇤ . We need to assume that the unknown vector is "natural," or "simple," in some applicationdependent way.</p><p>The most common structural assumption is that the vector x ⇤ is k-sparse in some known basis (or approximately k-sparse). Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x ⇤ if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted Eigenvalue Condition (REC) <ref type="bibr" target="#b35">(Tibshirani, 1996;</ref><ref type="bibr" target="#b6">Candes et al., 2006;</ref><ref type="bibr" target="#b13">Donoho, 2006;</ref><ref type="bibr" target="#b5">Bickel et al., 2009</ref>). The problem is also called high-dimensional sparse linear regression and there is vast literature on establishing conditions for different recovery algorithms, different assumptions on the design of A and generalizations of RIP and REC for other structures, see e.g. <ref type="bibr" target="#b5">(Bickel et al., 2009;</ref><ref type="bibr" target="#b33">Negahban et al., 2009;</ref><ref type="bibr" target="#b0">Agarwal et al., 2010;</ref><ref type="bibr" target="#b30">Loh &amp; Wainwright, 2011;</ref><ref type="bibr" target="#b2">Bach et al., 2012)</ref>. This significant interest is justified since a large number of applications can be expressed as recovering an unknown vector from noisy linear measurements. For example, many tomography problems can be expressed in this framework: x ⇤ is the unknown true tomographic image and the linear measurements are obtained by x-ray or other physical sensing system that produces sums or more general linear projections of the unknown pixels. Compressed sensing has been studied extensively for medical applications including computed tomography (CT) <ref type="bibr" target="#b7">(Chen et al., 2008)</ref>, rapid MRI <ref type="bibr" target="#b31">(Lustig et al., 2007)</ref> and neuronal spike train recovery <ref type="bibr" target="#b20">(Hegde et al., 2009</ref>). Another impressive application is the "single pixel camera" <ref type="bibr" target="#b14">(Duarte et al., 2008)</ref>, where digital micro-mirrors provide linear combinations to a single pixel sensor that then uses compressed sensing reconstruction algorithms to reconstruct an image. These results have been extended by combining sparsity with additional structural assumptions <ref type="bibr" target="#b4">(Baraniuk et al., 2010;</ref><ref type="bibr" target="#b21">Hegde et al., 2015)</ref>, and by generalizations such as translating sparse vectors into low-rank matrices <ref type="bibr" target="#b33">(Negahban et al., 2009;</ref><ref type="bibr" target="#b2">Bach et al., 2012;</ref><ref type="bibr" target="#b16">Foygel &amp; Mackey, 2014)</ref>. These results can improve performance when the structural assumptions fit the sensed signals. Other works perform "dictionary learning," seeking overcomplete bases where the data is more sparse (see <ref type="bibr" target="#b8">(Chen &amp; Needell, 2016)</ref> and refer-ences therein).</p><p>In this paper instead of relying on sparsity, we use structure from a generative model. Recently, several neural network based generative models such as variational autoencoders (VAEs) <ref type="bibr" target="#b26">(Kingma &amp; Welling, 2013)</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b18">(Goodfellow et al., 2014)</ref> have found success at modeling data distributions. In these models, the generative part learns a mapping from a low dimensional representation space z 2 R k to the high dimensional sample space G(z) 2 R n . While training, this mapping is encouraged to produce vectors that resemble the vectors in the training dataset. We can therefore use any pre-trained generator to approximately capture the notion of a vector being "natural" in our domain: the generator defines a probability distribution over vectors in sample space and tries to assign higher probability to more likely vectors, for the dataset it has been trained on. We expect that vectors "natural" to our domain will be close to some point in the support of this distribution, i.e., in the range of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contributions:</head><p>We present an algorithm that uses generative models for compressed sensing. Our algorithm simply uses gradient descent to optimize the representation z 2 R k such that the corresponding image G(z) has small measurement error kAG(z) yk 2 2 . While this is a nonconvex objective to optimize, we empirically find that gradient descent works well, and the results can significantly outperform Lasso with relatively few measurements.</p><p>We obtain theoretical results showing that, as long as gradient descent finds a good approximate solution to our objective, our output G(z) will be almost as close to the true x ⇤ as the closest possible point in the range of G.</p><p>The proof is based on a generalization of the Restricted Eigenvalue Condition (REC) that we call the SetRestricted Eigenvalue Condition (S-REC). Our main theorem is that if a measurement matrix satisfies the S-REC for the range of a given generator G, then the measurement error minimization optimum is close to the true x ⇤ . Furthermore, we show that random Gaussian measurement matrices satisfy the S-REC condition with high probability for large classes of generators. Specifically, for d-layer neural networks such as VAEs and GANs, we show that O(kd log n) Gaussian measurements suffice to guarantee good reconstruction with high probability. One result, for ReLU-based networks, is the following:</p><formula xml:id="formula_0">Theorem 1.1. Let G : R k ! R n</formula><p>be a generative model from a d-layer neural network using ReLU activations. Let A 2 R m⇥n be a random Gaussian matrix for m = O(kd log n), scaled so A i,j ⇠ N (0, 1/m). For any x ⇤ 2 R n and any observation y = Ax ⇤ + ⌘, let b z minimize ky AG(z)k 2 to within additive ✏ of the optimum. Then</p><formula xml:id="formula_1">with 1 e ⌦(m) probability, kG(b z) x ⇤ k 2  6 min z ⇤ 2R k kG(z ⇤ ) x ⇤ k 2 + 3k⌘k 2 + 2✏.</formula><p>In the error bound above, the first two terms are the minimum possible error of any vector in the range of the generator and the norm of the noise; these are necessary for such a technique, and have direct analogs in standard compressed sensing guarantees. The third term ✏ comes from gradient descent not necessarily converging to the global optimum; empirically, ✏ does seem to converge to zero, and one can check post-observation that this is small by computing the upper bound ky AG(b z)k 2 . While the above is restricted to ReLU-based neural networks, we also show similar results for arbitrary LLipschitz generative models, for m ⇡ O(k log L). Typical neural networks have poly(n)-bounded weights in each layer, so L  n O(d) , giving for any activation, the same O(kd log n) sample complexity as for ReLU networks.</p><formula xml:id="formula_2">Theorem 1.2. Let G : R k ! R n be an L-Lipschitz func- tion. Let A 2 R m⇥n be a random Gaussian matrix for m = O(k log Lr ), scaled so A i,j ⇠ N (0, 1/m). For any x ⇤ 2 R n and any observation y = Ax ⇤ + ⌘, let b z minimize ky AG(z)k</formula><p>2 to within additive ✏ of the optimum over vectors with kb zk 2  r. Then with 1 e ⌦(m) probability,</p><formula xml:id="formula_3">kG(b z) x ⇤ k 2  6 min z ⇤ 2R k kz ⇤ k2r kG(z ⇤ ) x ⇤ k 2 +3k⌘k 2 +2✏+2 .</formula><p>The downside is two minor technical conditions: we only optimize over representations z with kzk bounded by r, and our error gains an additive term. Since the dependence on these parameters is log(rL/ ), and L is something like n O(d) , we may set r = n O(d) and = 1/n O(d) while only losing constant factors, making these conditions very mild. In fact, generative models normally have the coordinates of z be independent uniform or Gaussian, so kzk ⇡ p k ⌧ n d , and a constant signal-to-noise ratio would have k⌘k 2 ⇡ kx ⇤ k ⇡ p n 1/n d . We remark that, while these theorems are stated in terms of Gaussian matrices, the proofs only involve the distributional Johnson-Lindenstrauss property of such matrices. Hence the same results hold for matrices with subgaussian entries or fast-JL matrices <ref type="bibr" target="#b1">(Ailon &amp; Chazelle, 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Algorithm</head><p>All norms are 2-norms unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let x</head><p>⇤ 2 R n be the vector we wish to sense. Let A 2 R m⇥n be the measurement matrix and ⌘ 2 R m be the noise vector. We observe the measurements y = Ax ⇤ + ⌘. Given y and A, our task is to find a reconstructionx close to x ⇤ .</p><p>A generative model is given by a deterministic function G : R k ! R n , and a distribution P Z over z 2 R k . To generate a sample from the generator, we can draw z ⇠ P Z and the sample then is G(z). Typically, we have k ⌧ n, i.e. the generative model maps from a low dimensional representation space to a high dimensional sample space.</p><p>Our approach is to find a vector in representation space such that the corresponding vector in the sample space matches the observed measurements. We thus define the objective to be</p><formula xml:id="formula_4">loss(z) = kAG(z) yk 2<label>(1)</label></formula><p>By using any optimization procedure, we can minimize loss(z) with respect to z. In particular, if the generative model G is differentiable, we can evaluate the gradients of the loss with respect to z using backpropagation and use standard gradient based optimizers. If the optimization procedure terminates atẑ, our reconstruction for x ⇤ is G(ẑ). We define the measurement error to be kAG(ẑ) yk 2 and the reconstruction error to be kG(ẑ) x ⇤ k 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Several recent lines of work explore generative models for reconstruction. The first line of work attempts to project an image on to the representation space of the generator. These works assume full knowledge of the image, and are special cases of the linear measurements framework where the measurement matrix A is identity. Excellent reconstruction results with SGD in the representation space to find an image in the generator range have been reported by <ref type="bibr" target="#b28">(Lipton &amp; Tripathi, 2017)</ref> with stochastic clipping and <ref type="bibr" target="#b10">(Creswell &amp; Bharath, 2016)</ref> with logistic measurement loss. A different approach is introduced in <ref type="bibr" target="#b15">(Dumoulin et al., 2016)</ref> and <ref type="bibr" target="#b11">(Donahue et al., 2016)</ref>. In their method, a recognition network that maps from the sample space vector x to the representation space vector z is learned jointly with the generator in an adversarial setting.</p><p>A second line of work explores reconstruction with structured partial observations. The inpainting problem consists of predicting the values of missing pixels given a part of the image. This is a special case of linear measurements where each measurement corresponds to an observed pixel. The use of generative models for this task has been studied in <ref type="bibr" target="#b39">(Yeh et al., 2016)</ref>, where the objective is taken to be a sum of L 1 error in measurements and a perceptual loss term given by the discriminator. Super-resolution is a related task that attempts to increase the resolution of an image. We can view the observations as local spatial averages of the unknown higher resolution image and hence cast this as another special case of linear measurements. For prior work on super-resolution see e.g. <ref type="bibr" target="#b38">(Yang et al., 2010;</ref><ref type="bibr" target="#b12">Dong et al., 2016;</ref><ref type="bibr" target="#b23">Kim et al., 2016)</ref> and references therein.</p><p>We also take note of the related work of <ref type="bibr" target="#b17">(Gilbert et al., 2017)</ref> that connects model-based compressed sensing with the invertibility of Convolutional Neural Networks, Bayesian compressed sensing <ref type="bibr" target="#b22">(Ji et al., 2008)</ref> and compressive sensing using Gaussian mixture models <ref type="bibr" target="#b37">(Yang et al., 2014)</ref>.</p><p>A related result appears in <ref type="bibr" target="#b3">(Baraniuk &amp; Wakin, 2009)</ref>, which studies the measurement complexity of an RIP condition for smooth manifolds. This is analogous to our S-REC for the range of G, but the range of G is neither smooth (because of ReLUs) nor a manifold (because of self-intersection). Their recovery result was extended in <ref type="bibr" target="#b19">(Hegde &amp; Baraniuk, 2012)</ref> to unions of two manifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Results</head><p>We begin with a brief review of the Restricted Eigenvalue Condition (REC) in standard compressed sensing. The REC is a sufficient condition on A for robust recovery to be possible. The REC essentially requires that all "approximately sparse" vectors are far from the nullspace of the matrix A. More specifically, A satisfies REC for a constant &gt; 0 if for all approximately sparse vectors x,</p><formula xml:id="formula_5">kAxk kxk.<label>(2)</label></formula><p>It can be shown that this condition is sufficient for recovery of sparse vectors using Lasso. If one examines the structure of Lasso recovery proofs, a key property that is used is that the difference of any two sparse vectors is also approximately sparse (for sparsity up to 2k). This is a coincidence that is particular to sparsity. By contrast, the difference of two vectors "natural" to our domain may not itself be natural. The condition we need is that the difference of any two natural vectors is far from the nullspace of A.</p><p>We propose a generalized version of the REC for a set S ✓ R n of vectors, the Set-Restricted Eigenvalue Condition (S-REC):</p><formula xml:id="formula_6">Definition 1. Let S ✓ R n . For some parameters &gt; 0, 0, a matrix A 2 R m⇥n is said to satisfy the S-REC(S, , ) if 8 x 1 , x 2 2 S, kA(x 1 x 2 )k kx 1 x 2 k .</formula><p>There are two main differences between the S-REC and the standard REC in compressed sensing. First, the condition applies to differences of vectors in an arbitrary set S of "natural" vectors, rather than just the set of approximately k-sparse vectors in some basis. This will let us apply the definition to S being the range of a generative model.</p><p>Second, we allow an additive slack term . This is necessary for us to achieve the S-REC when S is the output of general Lipschitz functions. Without it, the S-REC depends on the behavior of S at arbitrarily small scales. Since there are arbitrarily many such local regions, one cannot guarantee the existence of an A that works for all these local regions. Fortunately, as we shall see, poor behavior at a small scale will only increase our error by O( ).</p><p>The S-REC definition requires that for any two vectors in S, if they are significantly different (so the right hand side is large), then the corresponding measurements should also be significantly different (left hand side). Hence we can hope to approximate the unknown vector from the measurements, if the measurement matrix satisfies the S-REC.</p><p>But how can we find such a matrix? To answer this, we present two lemmas showing that random Gaussian matrices of relatively few measurements m satisfy the S-REC for the outputs of large and practically useful classes of generative models G : R k ! R n .</p><p>In the first lemma, we assume that the generative model</p><formula xml:id="formula_7">G(·) is L-Lipschitz, i.e., 8 z 1 , z 2 2 R k , we have kG(z 1 ) G(z 2 )k  Lkz 1 z 2 k.</formula><p>Note that state of the art neural network architectures with linear layers, (transposed) convolutions, max-pooling, residual connections, and all popular non-linearities satisfy this assumption. In Lemma 8.5 in the Appendix we give a simple bound on L in terms of parameters of the network; for typical networks this is n O(d) . We also require the input z to the generator to have bounded norm. Since generative models such as VAEs and GANs typically assume their input z is drawn with independent uniform or Gaussian inputs, this only prunes an exponentially unlikely fraction of the possible outputs.</p><formula xml:id="formula_8">Lemma 4.1. Let G : R k ! R n be L-Lipschitz. Let B k (r) = {z | z 2 R k , kzk  r} be an L 2 -norm ball in R k . For ↵ &lt; 1, if m = ⌦ ✓ k ↵ 2 log Lr ◆ ,</formula><p>then a random matrix A 2 R m⇥n with IID entries such that</p><formula xml:id="formula_9">A ij ⇠ N 0, 1 m satisfies the S-REC(G(B k (r)), 1 ↵, ) with 1 e ⌦(↵ 2 m)</formula><p>probability. All proofs, including this one, are deferred to Appendix A.</p><p>Note that even though we proved the lemma for an L 2 ball, the same technique works for any compact set.</p><p>For our second lemma, we assume that the generative model is a neural network such that each layer is a composition of a linear transformation followed by a pointwise non-linearity. Many common generative models have such architectures. We also assume that all non-linearities are piecewise linear with at most two pieces. The popular ReLU or LeakyReLU non-linearities satisfy this assumption. We do not make any other assumption, and in particular, the magnitude of the weights in the network do not affect our guarantee. Lemma 4.2. Let G : R k ! R n be a d-layer neural network, where each layer is a linear transformation followed by a pointwise non-linearity. Suppose there are at most c nodes per layer, and the non-linearities are piecewise linear with at most two pieces, and let</p><formula xml:id="formula_10">m = ⌦ ✓ 1 ↵ 2 kd log c ◆ for some ↵ &lt; 1. Then a random matrix A 2 R m⇥n with IID entries A ij ⇠ N(0, 1 m ) satisfies the S-REC(G(R k ), 1 ↵, 0) with 1 e ⌦(↵ 2 m)</formula><p>probability. To show Theorems 1.1 and 1.2, we just need to show that the S-REC implies good recovery. In order to make our error guarantee relative to`2 error in the image space R n , rather than in the measurement space R m , we also need that A preserves norms with high probability <ref type="bibr" target="#b9">(Cohen et al., 2009</ref>). Fortunately, Gaussian matrices (or other distributional JL matrices) satisfy this property. Lemma 4.3. Let A 2 R m⇥n by drawn from a distribution that (1) satisfies the S-REC(S, , ) with probability 1 p and (2) has for every fixed x 2 R n , kAxk  2kxk with probability 1 p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For any x</head><p>⇤ 2 R n and noise ⌘, let y = Ax Then,</p><formula xml:id="formula_11">kb x x ⇤ k  ✓ 4 + 1 ◆ min x2S kx ⇤ xk + 1 (2k⌘k + ✏ + )</formula><p>with probability 1 2p. Combining Lemma 4.1, Lemma 4.2, and Lemma 4.3 gives Theorems 1.1 and 1.2. In our setting, S is the range of the generator, and b x in the theorem above is the reconstruction G(b z) returned by our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Models</head><p>In this section we describe the generative models used in our experiments. We used two image datasets and two different generative model types (a VAE and a GAN). This provides some evidence that our approach can work with many types of models and datasets.</p><p>In our experiments, we found that it was helpful to add a regularization term L(z) to the objective to encourage the optimization to explore more in the regions that are preferred by the respective generative models (see comparison to unregularized versions in <ref type="figure" target="#fig_1">Fig. 1</ref>). Thus the objective function we use for minimization is</p><formula xml:id="formula_12">kAG(z) yk 2 + L(z).</formula><p>Both VAE and GAN typically imposes an isotropic Gaussian prior on z. Thus kzk 2 is proportional to the negative log-likelihood under this prior. Accordingly, we use the following regularizer:</p><formula xml:id="formula_13">L(z) = kzk 2 ,<label>(3)</label></formula><p>where measures the relative importance of the prior as compared to the measurement error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">MNIST with VAE</head><p>The MNIST dataset consists of about 60, 000 images of handwritten digits, where each image is of size 28⇥28 ( <ref type="bibr" target="#b27">LeCun et al., 1998)</ref>. Each pixel value is either 0 (background) or 1 (foreground). No pre-processing was performed. We trained VAE on this dataset. The input to the VAE is a vectorized binary image of input dimension 784. We set the size of the representation space k = 20. The recognition network is a fully connected 784 500 500 20 network. The generator is also fully connected with the architecture 20 500 500 784. We train the VAE using the Adam optimizer <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014</ref>) with a mini-batch size 100 and a learning rate of 0.001. We use = 0.1 in Eqn. (3).</p><p>The digit images are reasonably sparse in the pixel space. Thus, as a baseline, we use the pixel values directly for sparse recovery using Lasso. We set shrinkage parameter to be 0.1 for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">CelebA with DCGAN</head><p>CelebA is a dataset of more than 200, 000 face images of celebrities <ref type="bibr" target="#b29">(Liu et al., 2015)</ref>. The input images were cropped to a 64 ⇥ 64 RGB image, giving 64 ⇥ 64 ⇥ 3 = 12288 inputs per image. Each pixel value was scaled so that all values are between [ 1, 1]. We trained a DCGAN <ref type="bibr" target="#b34">(Radford et al., 2015;</ref><ref type="bibr" target="#b24">Kim, 2017)</ref> on this dataset. We set the input dimension k = 100 and use a standard normal distribution. The architecture follows that of <ref type="bibr" target="#b34">(Radford et al., 2015)</ref>. The model was trained by one update to the discriminator and two updates to the generator per cycle. Each update used the Adam optimizer <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref> with minibatch size 64, learning rate 0.0002 and 1 = 0.5. We use = 0.001 in Eqn. (3).</p><p>For baselines, we perform sparse recovery using Lasso on the images in two domains: (a) 2D Discrete Cosine Transform (2D-DCT) and (b) 2D Daubechies-1 Wavelet Transform (2D-DB1). While we provide Gaussian measurements of the original pixel values, the L 1 penalty is on either the DCT coefficients or the DB1 coefficients of each color channel of an image. For all experiments, we set the shrinkage parameter to be 0.1 and 0.00001 respectively for 2D-DCT, and 2D-DB1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Reconstruction from Gaussian measurements</head><p>We take A to be a random matrix with IID Gaussian entries with zero mean and standard deviation of 1/m. Each entry of noise vector ⌘ is also an IID Gaussian random variable. We compare performance of different sensing algorithms qualitatively and quantitatively. For quantitative comparison, we use the reconstruction error = kx x ⇤ k 2 , wherex is an estimate of x ⇤ returned by the algorithm. In all cases, we report the results on a held out test set, unseen by the generative model at training time.</p><p>MNIST: The standard deviation of the noise vector is set such that p E[k⌘k 2 ] = 0.1. We use Adam optimizer <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref>, with a learning rate of 0.01. We do 10 random restarts with 1000 steps per restart and pick the reconstruction with best measurement error.</p><p>In <ref type="figure" target="#fig_1">Fig. 1a</ref>, we show the reconstruction error as we change the number of measurements both for Lasso and our algorithm. We observe that our algorithm is able to get low errors with far fewer measurements. For example, our algorithm's performance with 25 measurements matches Lasso's performance with 400 measurements. <ref type="figure">Fig. 2a</ref> shows sample reconstructions by Lasso and our algorithm.</p><p>However, our algorithm is limited since its output is constrained to be in the range of the generator. After 100 measurements, our algorithm's performance saturates, and additional measurements give no additional performance. Since Lasso has no such limitation, it eventually surpasses our algorithm, but this takes more than 500 measurements of the 784-dimensional vector. We expect that a more powerful generative model with representation dimension k &gt; 20 can make better use of additional measurements.</p><p>celebA: The standard deviation of the noise vector is set such that p E[k⌘k 2 ] = 0.01. We use Adam optimizer <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref>, with a learning rate of 0.1. We do 2 random restarts with 500 update steps per restart and pick the reconstruction with best measurement error.</p><p>In <ref type="figure" target="#fig_1">Fig. 1b, we</ref> show the reconstruction error as we change the number of measurements both for Lasso and our algorithm. In <ref type="figure">Fig. 3</ref> we show sample reconstructions by Lasso and our algorithm. We observe that our algorithm is able to produce reasonable reconstructions with as few as 500 measurements, while the output of the baseline algorithms is quite blurry. Similar to the results on MNIST, if we continue to give more measurements, our algorithm saturates, and for more than 5000 measurements, Lasso gets a better reconstruction. We again expect that a more powerful generative model with k &gt; 100 would perform better in the high-measurement regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Super-resolution</head><p>Super-resolution is the task of constructing a high resolution image from a low resolution version of the same image. This problem can be thought of as special case of our general framework of linear measurements, where the measurements correspond to local spatial averages of the pixel values. Thus, we try to use our recovery algorithm to perform this task with measurement matrix A tailored to give only the relevant observations. We note that this measurement matrix may not satisfy the S-REC condition (with good constants and ), and consequently, our theorems may not be applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST:</head><p>We construct a low resolution image by spatial 2 ⇥ 2 pooling with a stride of 2 to produce a 14 ⇥ 14 image. These measurements are used to reconstruct the original 28 ⇥ 28 image. <ref type="figure">Fig. 2b</ref> shows reconstructions produced by our algorithm on images from a held out test set. We observe sharp reconstructions which closely match the fine structure in the ground truth.</p><p>celebA: We construct a low resolution image by spatial 4⇥ 4 pooling with a stride of 4 to produce a 16 ⇥ 16 image. These measurements are used to reconstruct the original 64 ⇥ 64 image. In <ref type="figure">Fig. 4</ref> we show results on images from a held out test set. We see that our algorithm is able to fill in the details to match the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Understanding sources of error</head><p>Although better than baselines, our method still admits some error. This error can be decomposed into three components: (a) Representation error: the unknown image is far from the range of the generator (b) Measurement error: The finite set of random measurements do not contain all the information about the unknown image (c) Optimization error: The optimization procedure did not find the best z.</p><p>In this section we present some experiments that suggest that the representation error is the dominant term. In our first experiment, we ensure that the representation error is zero, and try to minimize the sum of other two errors. In this setting, we observe that the reconstructions are almost perfect. In the second experiment, we ensure that the measurement error is zero, and try to minimize the sum of other two. Here, we observe that the total error obtained is very close to the total error in our reconstruction experiments (Sec. 6.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1.">SENSING IMAGES FROM RANGE OF GENERATOR</head><p>Our first approach is to sense an image that is in the range of the generator. More concretely, we sample a z ⇤ from P Z . Then we pass it through the generator to get x</p><formula xml:id="formula_14">⇤ = G(z ⇤ )</formula><p>. Now, we pretend that this is a real image and try to sense that. This method eliminates the representation error and allows us to check if our gradient based optimization procedure is able to find z ⇤ by minimizing the objective.</p><p>In <ref type="figure" target="#fig_4">Fig. 6a</ref> and <ref type="figure" target="#fig_4">Fig. 6b</ref>, we show the reconstruction error for images in the range of the generators trained on MNIST and celebA datasets respectively. We see that we get almost perfect reconstruction with very few measurements. This suggests that objective is being properly minimized and we indeed getẑ close to z ⇤ . i.e. the sum of optimization error and the measurement error is small in the absence of the (a) We show original images (top row) and reconstructions by Lasso (middle row) and our algorithm (bottom row).</p><p>(b) We show original images (top row), low resolution version of original images (middle row) and reconstructions (last row).    representation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">QUANTIFYING REPRESENTATION ERROR</head><p>We saw that in absence of the representation error, the overall error is small. However from <ref type="figure" target="#fig_1">Fig. 1</ref>, we know that the overall error is still non-zero. So, in this experiment, we seek to quantify the representation error, i.e., how far are the real images from the range of the generator?</p><p>From the previous experiment, we know that theẑ recovered by our algorithm is close to z ⇤ , the best possible value, if the image being sensed is in the range of the generator. Based on this, we make an assumption that this property is also true for real images. With this assumption, we get an estimate to the representation error as follows: We sample real images from the test set. Then we use the full image in our algorithm, i.e., our measurement matrix A is identity. This eliminates the measurement error. Using these measurements, we get the reconstructed image G(ẑ) through our algorithm. The estimated representation error is then kG(ẑ) x ⇤ k 2 . We repeat this procedure several times over randomly sampled images from our dataset and report average representation error values. The task of finding the closest image in the range of the generator has been studied in prior work <ref type="bibr" target="#b10">(Creswell &amp; Bharath, 2016;</ref><ref type="bibr" target="#b15">Dumoulin et al., 2016;</ref><ref type="bibr" target="#b11">Donahue et al., 2016)</ref>.</p><p>On the MNIST dataset, we get average per pixel representation error of 0.005. The recovered images are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. In contrast, with only 100 Gaussian measurements, we get a per pixel reconstruction error of about 0.009. On the celebA dataset, we get average per pixel representation error of 0.020. The recovered images are shown in <ref type="figure">Fig. 5</ref>. In contrast, with only 500 Gaussian measurements, we get a per pixel reconstruction error of about 0.028.</p><p>This suggests that the representation error is the major component of the total error, and thus a more flexible generative model can help reduce it on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We demonstrate how to perform compressed sensing using generative models from neural nets. These models can represent data distributions more concisely than standard sparsity models, while their differentiability allows for fast signal reconstruction. This will allow compressed sensing applications to make significantly fewer measurements.</p><p>Our theorems and experiments both suggest that, after relatively few measurements, the signal reconstruction gets close to the optimal within the range of the generator. To reach the full potential of this technique, one should use larger generative models as the number of measurements increase. Whether this can be expressed more concisely than by training multiple independent generative models of different sizes is an open question.</p><p>Generative models are an active area of research with ongoing rapid improvements. Because our framework applies to general generative models, this improvement will immediately yield better reconstructions with fewer measurements. We also believe that one could also use the performance of generative models for our task as one benchmark for the quality of different models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>⇤+</head><label></label><figDesc>⌘. Let b x approximately minimize ky Axk over x 2 S, i.e., ky Ab xk  min x2S ky Axk + ✏.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. We compare the performance of our algorithm with baselines. We show a plot of per pixel reconstruction error as we vary the number of measurements. The vertical bars indicate 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .Figure 4 .Figure 5 .</head><label>2345</label><figDesc>Figure 2. Results on MNIST. Reconstruction with 100 measurements (left) and Super-resolution (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Reconstruction error for images in the range of the generator. The vertical bars indicate 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results on the representation error experiments on MNIST. Top row shows original images and the bottom row shows closest images found in the range of the generator.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Philipp Krähenbühl for helpful discussions. This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP W911NF-14-1-0258, and the William Hartwig fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Martin J. Fast global convergence rates of gradient methods for high-dimensional statistical recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wainwright</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The fast johnsonlindenstrauss transform and approximate nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="302" to="322" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimization with sparsityinducing penalties. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Random projections of smooth manifolds. Foundations of computational mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="51" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-based compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmay</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1982" to="2001" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsybakov</forename><surname>Ritov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1705" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Terence. Stable signal recovery from incomplete and inaccurate measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on pure and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1207" to="1223" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prior image constrained compressed sensing (piccs): a method to accurately reconstruct dynamic ct images from highly undersampled projection data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="660" to="663" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compressed sensing and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deanna</forename><surname>Needell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposia in Applied Mathematics</title>
		<meeting>Symposia in Applied Mathematics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressed sensing and best k-term approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dahmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Inverting the generator of a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anthony</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Change</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Single-pixel imaging via compressive sampling. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Takbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dharmpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">N</forename><surname>Laska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishmael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Corrupted sensing: Novel guarantees for separating structured signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rina</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lester</forename><surname>Mackey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1223" to="1247" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kibok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honglak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08664</idno>
		<title level="m">Towards understanding the invertibility of convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Signal recovery on incoherent manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmay</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7204" to="7214" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compressive sensing recovery of spike trains using a structured sparsity model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmay</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPARS&apos;09-Signal Processing with Adaptive Sparse Structured Representations</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A nearly-linear time framework for graph-structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmay</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="928" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shihao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2346" to="2356" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A tensorflow implementation of &quot;deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://github.com/carpedm20/DCGAN-tensorflow" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Precise recovery of latent vectors from generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04782</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Ling</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wainwright</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2726" to="2734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sparse mri: The application of compressed sensing for rapid mr imaging. Magnetic resonance in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauly</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lectures on discrete geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matoušek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A unified framework for highdimensional analysis of m-estimators with decomposable regularizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Negahban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sahand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravikumar</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1348" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soumith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Introduction to the non-asymptotic analysis of random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.3027</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video compressive sensing using gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xuejun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Llull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4863" to="4878" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semantic image inpainting with perceptual and contextual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teck</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawajohnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07539</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
