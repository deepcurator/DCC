<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Black-box Adversarial Attacks with Limited Queries and Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessy</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">Black-box Adversarial Attacks with Limited Queries and Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partialinformation setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural network-based image classifiers are susceptible to adversarial examples, minutely perturbed inputs that fool classifiers <ref type="bibr" target="#b27">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b2">Biggio et al., 2013)</ref>. These adversarial examples can potentially be exploited in the real world <ref type="bibr" target="#b14">(Kurakin et al., 2016;</ref><ref type="bibr" target="#b0">Athalye et al., 2017;</ref><ref type="bibr" target="#b26">Sharif et al., 2017;</ref><ref type="bibr" target="#b8">Evtimov et al., 2017)</ref>. For many commercial or proprietary systems, adversarial examples must be considered under a limited threat model. This has motivated black-box attacks that do not require access to the gradient of the classifier.</p><p>One approach to attacking a classifier in this setting trains * Equal contribution 1 Massachusetts Institute of Technology 2 LabSix. Correspondence to: LabSix &lt;team@labsix.org&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s). a substitute network to emulate the original network and then attacks the substitute with first-order white-box methods <ref type="bibr" target="#b22">(Papernot et al., 2016a;</ref>. Recent works note that adversarial examples for substitute networks do not always transfer to the target model, especially when conducting targeted attacks <ref type="bibr" target="#b19">Narodytska &amp; Kasiviswanathan, 2017)</ref>. These works instead construct adversarial examples by estimating the gradient through the classifier with coordinate-wise finite difference methods.</p><p>We consider additional access and resource restrictions on the black-box model that characterize restrictions in realworld systems. These restrictions render targeted attacks with prior methods impractical or infeasible. We present new algorithms for generating adversarial examples that render attacks in the proposed settings tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Definitions</head><p>At a high level, an adversarial example for a classifier is an input that is slightly perturbed to cause misclassification.</p><p>Prior work considers various threat models <ref type="bibr" target="#b23">(Papernot et al., 2016b;</ref><ref type="bibr" target="#b4">Carlini &amp; Wagner, 2017)</ref>. In this work, we consider ∞ -bounded perturbation that causes targeted misclassification (i.e. misclassification as a given target class). Thus, the task of the adversary is: given an input x, target class y adv , and perturbation bound , find an input x adv such that ||x adv − x|| ∞ &lt; and x adv is classified as y adv .</p><p>All of the threat models considered in this work are additional restrictions on the black-box setting:</p><p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type="bibr" target="#b15">Liu et al., 2017;</ref><ref type="bibr" target="#b11">Hayes &amp; Danezis, 2017)</ref>. In this model, the adversary can supply any input x and receive the predicted class probabilities, P (y|x) for all classes y. This setting does not allow the adversary to analytically compute the gradient ∇P (y|x) as is doable in the white-box case.</p><p>We introduce the following threat models as more limited variants of the black-box setting that reflect access and resource restrictions in real-world systems:</p><p>1. Query-limited setting. In the query-limited setting, the attacker has a limited number of queries to the classifier. In this setting, we are interested in queryefficient algorithms for generating adversarial examples. A limit on the number of queries can be a result of limits on other resources, such as a time limit if inference time is a bottleneck or a monetary limit if the attacker incurs a cost for each query.</p><p>Example. The Clarifai NSFW (Not Safe for Work) detection API 1 is a binary classifier that outputs P (N SF W |x) for any image x and can be queried through an API. However, after the first 2500 predictions, the Clarifai API costs upwards of $2.40 per 1000 queries. This makes a 1-million query attack, for example, cost $2400.</p><p>2. Partial-information setting.</p><p>In the partialinformation setting, the attacker only has access to the probabilities P (y|x) for y in the top k (e.g. k = 5) classes {y 1 , . . . , y k }. Instead of a probability, the classifier may even output a score that does not sum to 1 across the classes to indicate relative confidence in the predictions.</p><p>Note that in the special case of this setting where k = 1, the attacker only has access to the top label and its probability-a partial-information attack should succeed in this case as well.</p><p>Example. The Google Cloud Vision API 2 (GCV) only outputs scores for a number of the top classes (the number varies between queries). The score is not a probability but a "confidence score" (that does not sum to one).</p><p>3. Label-only setting.</p><p>In the label-only setting, the adversary does not have access to class probabilities or scores. Instead, the adversary only has access to a list of k inferred labels ordered by their predicted probabilities. Note that this is a generalization of the decision-only setting defined in <ref type="bibr" target="#b3">Brendel et al. (2018)</ref>, where k = 1, and the attacker only has access to the top label. We aim to devise an attack that works in this special case but can exploit extra information in the case where k &gt; 1.</p><p>Example. Photo tagging apps such as Google Photos 3 add labels to user-uploaded images. However, no "scores" are assigned to the labels, and so an attacker can only see whether or not the classifier has inferred a given label for the image (and where that label appears in the ordered list).</p><p>1 https://clarifai.com/models/ nsfw-image-recognition-modele9576d86d2004ed1a38ba0cf39ecb4b1</p><p>2 https://cloud.google.com/vision/ 3 https://photos.google.com/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>Query-efficient adversarial examples. Previous methods using substitute networks or coordinate-wise gradient estimation for targeted black-box attacks require on the order of millions of queries to attack an ImageNet classifier. Low throughput, high latency, and rate limits on commercially deployed black-box classifiers heavily impact the feasibility of current approaches to black-box attacks on real-world systems.</p><p>We propose the variant of NES described in <ref type="bibr" target="#b25">Salimans et al. (2017)</ref> (inspired by <ref type="bibr" target="#b29">Wierstra et al. (2014)</ref>) as a method for generating adversarial examples in the query-limited setting. We use NES as a black-box gradient estimation technique and employ PGD (as used in white-box attacks) with the estimated gradient to construct adversarial examples.</p><p>We relate NES in this special case with the finite difference method over Gaussian bases, providing a theoretical comparison with previous attempts at black-box adversarial examples. The method does not require a substitute network and is 2-3 orders of magnitude more query-efficient than previous methods based on gradient estimation such as <ref type="bibr" target="#b6">Chen et al. (2017)</ref>. We show that our approach reliably produces targeted adversarial examples in the blackbox setting.</p><p>Adversarial examples with partial information. We present a new algorithm for attacking neural networks in the partial-information setting. The algorithm starts with an image of the target class and alternates between blending in the original image and maximizing the likelihood of the target class. We show that our method reliably produces targeted adversarial examples in the partial-information setting, even when the attacker only sees the top probability. To our knowledge, this is the first attack algorithm proposed for this threat model.</p><p>We use our method to perform the first targeted attack on the Google Cloud Vision API, demonstrating the applicability of the attack on large, commercial systems: the GCV API is an opaque (no published enumeration of labels), partial-information (queries return only up to 10 classes with uninterpretable "scores"), several-thousand-way commercial classifier.</p><p>Adversarial examples with scoreless feedback. Often, in deployed machine learning systems, even the score is hidden from the attacker. We introduce an approach for producing adversarial examples even when no scores of any kind are available. We assume the adversary only receives the top k sorted labels when performing a query. We integrate noise robustness as a proxy for classification score into our partial-information attack to mount a targeted attack in the label-only setting. We show that even in the decision-only setting, where k = 1, we can mount a successful attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>We outline the key components of our approach for conducting an attack in each of the proposed threat models. We begin with a description of our application of Natural Evolutionary Strategies <ref type="bibr" target="#b29">(Wierstra et al., 2014)</ref> to enable query-efficient generation of black-box adversarial examples. We then show the need for a new technique for attack in the partial-information setting, and we discuss our algorithm for such an attack. Finally, we describe our method for attacking a classifier with access only to a sorted list of the top k labels (k ≥ 1). We have released full source code for the attacks we describe 4 .</p><p>We define some notation before introducing the approach. The projection operator</p><formula xml:id="formula_0">Π [x− ,x+ ] (x )</formula><p>is the ∞ projection of x onto an -ball around x. When x is clear from context, we abbreviate this as Π (x ), and in pseudocode we denote this projection with the function CLIP(x , x − , x + ). We define the function rank(y|x) to be the smallest k such that y is in the top-k classes in the classification of x. We use N and U to represent the normal and uniform distributions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Query-Limited Setting</head><p>In the query-limited setting, the attacker has a query budget L and aims to cause targeted misclassification in L queries or less. To attack this setting, we can use "standard" first-order techniques for generating adversarial examples <ref type="bibr" target="#b9">Goodfellow et al. (2015)</ref>; <ref type="bibr" target="#b23">Papernot et al. (2016b)</ref>; <ref type="bibr" target="#b16">Madry et al. (2017);</ref><ref type="bibr" target="#b4">Carlini &amp; Wagner (2017)</ref>, substituting the gradient of the loss function with an estimate of the gradient, which is approximated by querying the classifier rather than computed by autodifferentiation. This idea is used in <ref type="bibr" target="#b6">Chen et al. (2017)</ref>, where the gradient is estimated via pixel-by-pixel finite differences, and then the CW attack <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017</ref>) is applied. In this section, we detail our algorithm for efficiently estimating the gradient from queries, based on the Natural Evolutionary Strategies approach of <ref type="bibr" target="#b29">Wierstra et al. (2014)</ref>, and then state how the estimated gradient is used to generate adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">NATURAL EVOLUTIONARY STRATEGIES</head><p>To estimate the gradient, we use NES <ref type="bibr" target="#b29">(Wierstra et al., 2014)</ref>, a method for derivative-free optimization based on the idea of a search distribution π(θ|x). Rather than maximizing an objective function F (x) directly, NES maxi-mizes the expected value of the loss function under the search distribution. This allows for gradient estimation in far fewer queries than typical finite-difference methods. For a loss function F (·) and a current set of parameters x, we have from <ref type="bibr" target="#b29">Wierstra et al. (2014)</ref>:</p><formula xml:id="formula_1">E π(θ|x) [F (θ)] = F (θ)π(θ|x) dθ ∇ x E π(θ|x) [F (θ)] = ∇ x F (θ)π(θ|x) dθ = F (θ)∇ x π(θ|x) dθ = F (θ) π(θ|x) π(θ|x) ∇ x π(θ|x) dθ = π(θ|x)F (θ)∇ x log (π(θ|x)) dθ = E π(θ|x) [F (θ)∇ x log (π(θ|x))]</formula><p>In a manner similar to that in <ref type="bibr" target="#b29">Wierstra et al. (2014)</ref>, we choose a search distribution of random Gaussian noise around the current image x; that is, we have θ = x + σδ, where δ ∼ N (0, I). Like Salimans et al. <ref type="formula">(2017)</ref>, we employ antithetic sampling to generate a population of δ i values: instead of generating n values δ i ∼ N (0, I), we sample Gaussian noise for i ∈ {1, . . . , n 2 } and set δ j = −δ n−j+1 for j ∈ {( n 2 + 1), . . . , n}. This optimization has been empirically shown to improve performance of NES. Evaluating the gradient with a population of n points sampled under this scheme yields the following variancereduced gradient estimate:</p><formula xml:id="formula_2">∇E[F (θ)] ≈ 1 σn n i=1 δ i F (θ + σδ i )</formula><p>Finally, we perform a projected gradient descent update <ref type="bibr" target="#b16">(Madry et al., 2017)</ref> with momentum based on the NES gradient estimate.</p><p>The special case of NES that we have described here can be seen as a finite-differences estimate on a random Gaussian basis. <ref type="bibr" target="#b10">Gorban et al. (2016)</ref> shows that for an n-dimensional space and N randomly sampled Gaussian vectors v 1 . . . v N , we can lower bound the probability that N random Gaussians are c-orthogonal:</p><formula xml:id="formula_3">N ≤ −e c 2 n 4 ln (p) 1 2 =⇒ P v i · v j ||v i ||||v j || ≤ c ∀ (i, j) ≥ p</formula><p>Considering a matrix Θ with columns δ i , NES gives the projection Θ(∇F ), so we can use standard results from concentration theory to analyze our estimate. A more complex treatment is given in <ref type="bibr" target="#b7">Dasgupta et al. (2006)</ref>, but using Algorithm 1 NES Gradient Estimate Input: Classifier P (y|x) for class y, image x Output: Estimate of ∇P (y|x) Parameters: Search variance σ, number of samples n,</p><formula xml:id="formula_4">image dimensionality N g ← 0 n for i = 1 to n do u i ← N (0 N , I N ·N ) g ← g + P (y|x + σ · u i ) · u i g ← g − P (y|x − σ · u i ) · u i end for return 1 2nσ g</formula><p>a straightforward application of the Johnson-Lindenstrauss Theorem, we can upper and lower bound the norm of our estimated gradient ∇ in terms of the true gradient ∇. As σ → 0, we have that:</p><formula xml:id="formula_5">P (1−δ)||∇|| 2 ≤ || ∇|| 2 ≤ (1+δ)||∇|| 2 ≥ 1 − 2p</formula><p>where 0 &lt; δ &lt; 1 and</p><formula xml:id="formula_6">N = O(−δ −2 log(p))</formula><p>More rigorous analyses of these "Gaussian-projected finite difference" gradient estimates and bounds <ref type="bibr" target="#b20">(Nesterov &amp; Spokoiny, 2017)</ref> detail the algorithm's interaction with dimensionality, scaling, and various other factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">QUERY-LIMITED ATTACK</head><p>In the query-limited setting, we use NES as an unbiased, efficient gradient estimator, the details of which are given in Algorithm 1. Projected gradient descent (PGD) is performed using the sign of the estimated gradient:</p><formula xml:id="formula_7">x (t) = Π [x0− ,x0+ ] (x (t−1) − η · sign(g t ))</formula><p>The algorithm takes hyperparameters η, the step size, and N , the number of samples to estimate each gradient. In the query-limited setting with a query limit of L, we use N queries to estimate each gradient and perform L N steps of PGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Partial-Information Setting</head><p>In the partial-information setting, rather than beginning with the image x, we instead begin with an instance x 0 of the target class y adv , so that y adv will initially appear in the top-k classes.</p><p>At each step t, we then alternate between:</p><p>(1) projecting onto ∞ boxes of decreasing sizes t centered at the original image x 0 , maintaining that the adversarial class remains within the top-k at all times:</p><formula xml:id="formula_8">t = min s.t. rank y adv |Π (x (t−1) ) &lt; k Algorithm 2 Partial Information Attack Input: Initial image x, Target class y adv , Classifier P (y|x) : R n × Y → [0, 1] k (access to probabilities for y in top k), image x Output:</formula><p>Adversarial image x adv with ||x adv −x|| ∞ ≤ Parameters: Perturbation bound adv , starting perturbation 0 , NES Parameters (σ, N, n), epsilon decay δ , maximum learning rate η max , minimum learning rate</p><formula xml:id="formula_9">η min ← 0 x adv ← image of target class y adv x adv ← CLIP(x adv , x − , x + ) while &gt; adv or max y P (y|x) = y adv do g ← NESESTGRAD(P (y adv |x adv )) η ← η max x adv ← x adv − ηg while not y adv ∈ TOP-K(P (·|x adv )) do if η &lt; η min then ← + δ δ ← δ /2 x adv ← x adv break end if η ← η 2 x adv ← CLIP(x adv − ηg, x − , x + )</formula><p>end while x adv ←x adv ← − δ end while return x adv (2) perturbing the image to maximize the probability of the adversarial target class,</p><formula xml:id="formula_10">x (t) = arg max x P (y adv |Π t−1 (x ))</formula><p>We implement this iterated optimization using backtracking line search to find t that maintains the adversarial class within the top-k, and several iterations of projected gradient descent (PGD) to find x (t) . Pseudocode is shown in Algorithm 2. Details regarding further optimizations (e.g. learning rate adjustment) can be found in our source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Label-Only Setting</head><p>Now, we consider the setting where we only assume access to the top-k sorted labels. As previously mentioned, we explicitly include the setting where k = 1 but aim to design an algorithm that can incorporate extra information when k &gt; 1.</p><p>The key idea behind our attack is that in the absence of output scores, we find an alternate way to characterize the success of an adversarial example. First, we define the dis-cretized score R(x (t) ) of an adversarial example to quantify how adversarial the image is at each step t simply based on the ranking of the adversarial label y adv :</p><formula xml:id="formula_11">R(x (t) ) = k − rank(y adv |x (t) )</formula><p>As a proxy for the softmax probability, we consider the robustness of the adversarial image to random perturbations (uniformly chosen from a ∞ ball of radius µ), using the discretized score to quantify adversariality:</p><formula xml:id="formula_12">S(x (t) ) = E δ∼U [−µ,µ] [R(x (t) + δ)]</formula><p>We estimate this proxy score with a Monte Carlo approximation:</p><formula xml:id="formula_13">S(x (t) ) = 1 n n i=1 R(x (t) + µδ i )</formula><p>A visual representation of this process is given in <ref type="figure" target="#fig_0">Figure 1</ref>. We proceed to treat S(x) as a proxy for the output probabilities P (y adv |x) and use the partial-information technique we introduce in Section 2.2 to find an adversarial example using an estimate of the gradient ∇ x S(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>We evaluate the methods proposed in Section 2 on their effectiveness in producing targeted adversarial examples in the three threat models we consider: query-limited, partialinformation, and label-only. First, we present our evaluation methodology. Then, we present evaluation results for our three attacks. Finally, we demonstrate an attack against a commercial system: the Google Cloud Vision (GCV) classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methodology</head><p>We evaluate the effectiveness of our attacks against an ImageNet classifier. We use a pre-trained InceptionV3 network  that has 78% top-1 accuracy,  <ref type="table">Table 1</ref>. Quantitative analysis of targeted = 0.05 adversarial attacks in three different threat models: query-limited (QL), partialinformation (PI), and label-only (LO). We perform attacks over 1000 randomly chosen test images (100 for label-only) with randomly chosen target classes. For each attack, we use the same hyperparameters across all images. Here, we report the overall success rate (percentage of times the adversarial example was classified as the target class) and the median number of queries required.</p><p>and for each attack, we restrict our access to the classifier according to the threat model we are considering.</p><p>For each evaluation, we randomly choose 1000 images from the ImageNet test set, and we randomly choose a target class for each image. We limit ∞ perturbation to = 0.05. We use a fixed set of hyperparameters across all images for each attack algorithm, and we run the attack until we produce an adversarial example or until we time out at a chosen query limit (e.g. L = 10 6 for the query-limited threat model).</p><p>We measure the success rate of the attack, where an attack is considered successful if the adversarial example is classified as the target class and considered unsuccessful otherwise (whether it's classified as the true class or any other incorrect class). This is a strictly harder task than producing untargeted adversarial examples. We also measure the number of queries required for each attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation on ImageNet</head><p>In our evaluation, we do not enforce a particular limit on the number of queries as there might be in a real-world attack. Instead, we cap the number of queries at a large number, measure the number of queries required for each attack, and present the distribution of the number of queries required. For both the the partial-information attack and the label-only attack, we consider the special case where k = 1, i.e. the attack only has access to the top label. Note that in the partial-information attack the adversary also has access to the probability score of the top label. <ref type="table">Table 1</ref> summarizes evaluation results our attacks for the three different threat models we consider, and <ref type="figure" target="#fig_1">Figure 2</ref> shows the distribution of the number of queries. <ref type="figure" target="#fig_2">Figure 3</ref> shows a sample of the adversarial examples we produced. <ref type="table">Table 2</ref> gives our hyperparameters; for each attack, we use the same set of hyperparameters across all images.    <ref type="table">Table 2</ref>. Hyperparameters used for evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Real-world attack on Google Cloud Vision</head><p>To demonstrate the relevance and applicability of our approach to real-world systems, we attack the Google Cloud Vision (GCV) API, a publicly available computer vision suite offered by Google. We attack the most general object labeling classifier, which performs n-way classification on images. Attacking GCV is considerably more challenging than attacking a system in the typical black-box setting because of the following properties:</p><p>• The number of classes is large and unknown -a full enumeration of labels is unavailable.</p><p>• The classifier returns "confidence scores" for each label it assigns to an image, which seem to be neither probabilities nor logits.</p><p>• The classifier does not return scores for all labels, but instead returns an unspecified-length list of labels that varies based on image.</p><p>This closely mirrors our partial-information threat model, with the additional challenges that a full list of classes is unavailable and the length of the results is unspecified and varies based on the input. Despite these challenges, we succeed in constructing targeted adversarial examples against this classifier. <ref type="figure" target="#fig_4">Figure 4</ref> shows an unperturbed image being correctly labeled as several skiing-related classes, including "skiing" and "ski." We run our partial-information attack to force this image to be classified as "dog" (an arbitrarily chosen target class). Note that the label "dog" does not appear in the output for the unperturbed image. Using our partialinformation algorithm, we initialize our attack with a photograph of a dog (classified by GCV as a dog) and successfully synthesize an image that looks like the skiers but is  classified as "dog," as shown in <ref type="figure" target="#fig_5">Figure 5</ref> 5 . <ref type="bibr" target="#b1">Biggio et al. (2012)</ref> and <ref type="bibr" target="#b27">Szegedy et al. (2013)</ref> discovered that machine learning classifiers are vulnerable to adversarial examples. Since then, a number of techniques have been developed to generate adversarial examples in the whitebox case <ref type="bibr" target="#b9">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b4">Carlini &amp; Wagner, 2017;</ref><ref type="bibr" target="#b18">Moosavi-Dezfooli et al., 2016;</ref><ref type="bibr" target="#b17">Moosavi-Dezfooli et al., 2017;</ref><ref type="bibr" target="#b11">Hayes &amp; Danezis, 2017)</ref>, where an attacker has full access to the model parameters and architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>In this section, we focus on prior work that specifically address the black-box case and practical attack settings more generally and compare them to our contributions. Throughout this section, it is useful to keep in the mind the axes for comparison: (1) white-box vs. black-box; (2) access to train-time information + query access vs. only query access; (3) the scale of the targeted model and the dataset it was trained on (MNIST vs. CIFAR-10 vs. ImageNet); (4) untargeted vs. targeted.</p><p>5 https://www.youtube.com/watch?v= 1h9bU7WBTUg demonstrates our algorithm transforming the image of a dog into an image of the skier while retaining the original classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Black-box adversarial attacks</head><p>Several papers have investigated practical black-box attacks on real-world systems such as speech recognition systems <ref type="bibr" target="#b5">(Carlini et al., 2016)</ref>, malware detectors <ref type="bibr" target="#b13">(Hu &amp; Tan, 2017;</ref><ref type="bibr" target="#b30">Xu et al., 2016)</ref>, and face recognition systems <ref type="bibr" target="#b26">(Sharif et al., 2017)</ref>. Current black-box attacks use either substitute networks or gradient estimation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">BLACK-BOX ATTACKS WITH SUBSTITUTE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NETWORKS</head><p>One approach to generating adversarial examples in the black-box case is with a substitute model, where an adversary trains a new model with synthesized data labeled by using the target model as an oracle. Adversarial examples can then be generated for the substitute with whitebox methods, and they will often transfer to the target model, even if it has a different architecture or training dataset <ref type="bibr" target="#b27">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2015)</ref>. <ref type="bibr" target="#b22">Papernot et al. (2016a;</ref> have successfully used this method to attack commercial classifiers like the Google Cloud Prediction API, the Amazon Web Services Oracle, and the MetaMind API, even evading various defenses against adversarial attacks. A notable subtlety is that the Google Cloud Vision API 6 we attack in this work is not the same as the Google Cloud Prediction API 7 (now the Google Cloud Machine Learning Engine) attacked in <ref type="bibr" target="#b22">Papernot et al. (2016a;</ref>. Both systems are black-box, but the Prediction API is intended to be trained with the user's own data, while the Cloud Vision API has been trained on large amounts of Google's own data and works "out-of-the-box." In the black-box threat model we consider in our work, the adversary does not have access to the internals of the model architecture and has no knowledge of how the model was trained or what datasets were used. <ref type="bibr" target="#b22">Papernot et al. (2016a;</ref> trained the Cloud Prediction API with small datasets like MNIST and successfully demonstrated an untargeted attack. As <ref type="bibr" target="#b15">Liu et al. (2017)</ref> demonstrated, it is more difficult to transfer targeted adversarial examples with or without their target labels, particularly when attacking models trained on large datasets like ImageNet. Using ensemble-based methods, <ref type="bibr" target="#b15">Liu et al. (2017)</ref> overcame these limitations to attack the Clarifai API. Their threat model specifies that the adversary does not have any knowledge of the targeted model, its training process, or training and testing data, matching our definition of black-box. While Liu et al.'s substitute network attack does not require any queries to the target model (the models in the ensemble are all trained on ImageNet), only 18% of the targeted adversarial examples generated by the ensemble model are transferable in the Clarifai attack. In contrast, our method needs to query the model many times to perform a similar attack but has better guarantees that an adversarial example will be generated successfully (94% even in the partial-information case, and over 99% in the standard black-box setting). <ref type="bibr" target="#b6">Chen et al. (2017)</ref> explore black-box gradient estimation methods as an alternative to substitute networks, where we have noted that transferability is not always reliable. They work under the same threat model, restricting an adversary solely to querying the target model as an oracle. As they note, applying zeroth order optimization naively in this case is not a tractable solution, requiring 2 × 299 × 299 × 3 = 536406 queries to estimate the gradients with respect to all pixels. To resolve this problem, they devise an iterative coordinate descent procedure to decrease the number of evaluations needed and successfully perform untargeted and targeted attacks on MNIST and CIFAR-10 and untargeted attacks on ImageNet. Although we do not provide a direct comparison due to the incompability of the 2 and ∞ metric as well as the fixed-budget nature of the optimization algorithm in <ref type="bibr" target="#b6">Chen et al. (2017)</ref>, our method takes far fewer queries to generate imperceptible adversarial examples. <ref type="bibr" target="#b19">Narodytska &amp; Kasiviswanathan (2017)</ref> propose a blackbox gradient estimation attack using a local-search based technique, showing that perturbing only a small fraction of pixels in an image is often sufficient for it to be misclassified. They successfully perform targeted black-box attacks on an ImageNet classifier with only query access and additionally with a more constrained threat model where an adversary only has access to a "proxy" model. For the most successful misclassification attack on CIFAR-10 (70% success) the method takes 17,000 queries on average. Targeted adversarial attacks on ImageNet are not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">BLACK-BOX ATTACKS WITH GRADIENT ESTIMATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adversarial attacks with limited information</head><p>Our work is concurrent with <ref type="bibr" target="#b3">Brendel et al. (2018)</ref>, which also explores the label-only case using their "Boundary Attack," which is similar to our two-step partial information algorithm. Starting with an image of the target adversarial class, they alternate between taking steps on the decision boundary to maintain the adversarial classification of the image and taking steps towards the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Other adversarial attacks</head><p>Several notable works in adversarial examples use similar techniques but with different adversarial goals or threat models. <ref type="bibr" target="#b30">Xu et al. (2016)</ref> explore black-box adversarial examples to fool PDF malware classifiers. To generate an adversarial PDF, they start with an instance of a malicious PDF and use genetic algorithms to evolve it into a PDF that is classified as benign but preserves its malicious behavior. This attack is similar in spirit to our partial-information algorithm, although our technique (NES) is more similar to traditional gradient-based techniques than evolutionary algorithms, and we consider multiway image classifiers under a wider set of threat models rather than binary classifiers for PDFs. <ref type="bibr" target="#b21">Nguyen et al. (2014)</ref> is another work that uses genetic algorithms and gradient ascent to produce images that fool a classifier, but their adversarial goal is different: instead of aiming to make a interpretable image of some class (e.g. skiiers) be misclassified as another class (e.g. a dog), they generate entirely unrecognizable images of noise or abstract patterns that are classified as a paricular class. Another work generates adversarial examples by inverting the image instead of taking local steps; their goal is to show that CNNs do not generalize to inverted images, rather than to demonstrate a novel attack or to consider a new threat model <ref type="bibr" target="#b12">(Hosseini et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our work defines three new black-box threat models that characterize many real world systems: the query-limited setting, partial-information setting, and the label-only setting. We introduce new algorithms for attacking classifiers under each of these threat models and show the effectiveness of these algorithms by attacking an ImageNet classifier. Finally, we demonstrate targeted adversarial examples for the Google Cloud Vision API, showing that our methods enable black-box attacks on real-world systems in challenging settings. Our results suggest that machine learning systems remain vulnerable even with limited queries and information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An illustration of the derivation of the proxy scoreŜ in the label-only setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The distribution of the number of queries required for the query-limited (top) and partial-information with k = 1 (bottom) attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. = 0.05 targeted adversarial examples for the InceptionV3 network. The top row contains unperturbed images, and the bottom row contains corresponding adversarial examples (with randomly chosen target classes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The Google Cloud Vision Demo labeling on the unperturbed image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The Google Cloud Vision Demo labeling on the adversarial image generated with ∞ bounded perturbation with = 0.1: the image is labeled as the target class.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/labsix/limitedblackbox-attacks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://cloud.google.com/vision/ 7 https://cloud.google.com/prediction/docs/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We wish to thank Nat Friedman and Daniel Gross for providing compute resources for this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.07397" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<idno>978-1-4503- 1285-1</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3042573.3042761" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML&apos;12</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning, ICML&apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1467" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.04248" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security &amp; Privacy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hidden voice commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th USENIX Security Symposium (USENIX Security 16</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoo</surname></persName>
		</author>
		<idno type="doi">10.1145/3128572.3140448</idno>
		<ptr target="http://doi.acm.org/10.1145/3128572.3140448" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec &apos;17</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security, AISec &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A concentration theorem for projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Robust physical-world attacks on machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno>abs/1707.08945</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximation with random bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename><surname>Tyukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Sofeikov</surname></persName>
		</author>
		<idno type="doi">10.1016/j.ins.2015.09.021</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2015.09.021" />
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Machine learning as an adversarial service: Learning black-box adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Danezis</surname></persName>
		</author>
		<idno>abs/1708.05207</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the limitation of convolutional neural networks in recognizing negative images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poovendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="352" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Black-box attacks against RNN based malware detection algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<idno>abs/1705.08131</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.02533" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.06083" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="86" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple black-box adversarial perturbations for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Narodytska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Kasiviswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random gradient-free minimization of convex functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Spokoiny</surname></persName>
		</author>
		<idno type="doi">10.1007/s10208-015-9296-2</idno>
		<ptr target="https://doi.org/10.1007/s10208-015-9296-2" />
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="527" to="566" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clune</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>abs/1412.1897</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on Security &amp; Privacy</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno type="doi">10.1145/3052973.3053009</idno>
		<ptr target="http://doi.acm.org/10.1145/3052973.3053009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS &apos;17</title>
		<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1703.03864</idno>
		<ptr target="http://arxiv.org/abs/1703.03864" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial generative nets: Neural network attacks on state-of-theart face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1312.6199" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.00567" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural evolution strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Glasmachers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>1532- 4435</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=2627435.2638566" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="949" to="980" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatically evading classifiers: A case study on pdf malware classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adversarial Attacks with Limited Queries and Information</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Network and Distributed System Security Symposium (NDSS)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
