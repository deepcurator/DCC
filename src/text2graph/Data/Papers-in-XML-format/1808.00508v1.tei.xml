<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Arithmetic Logic Units</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Trask</surname></persName>
							<email>atrask@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
							<email>felixhill@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
							<email>reedscot@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
							<email>jwrae@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<email>pblunsom@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Arithmetic Logic Units</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to represent and manipulate numerical quantities is apparent in the behavior of many species, from insects to mammals to humans, suggesting that basic quantitative reasoning is a general component of intelligence <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>While neural networks can successfully represent and manipulate numerical quantities given an appropriate learning signal, the behavior that they learn does not generally exhibit systematic generalization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. Specifically, one frequently observes failures when quantities that lie outside the numerical range used during training are encountered at test time, even when the target function is simple (e.g., it depends only on aggregating counts or linear extrapolation). This failure pattern indicates that the learned behavior is better characterized by memorization than by systematic abstraction. Whether input distribution shifts that trigger extrapolation failures are of practical concern depends on the environments where the trained models will operate. However, considerable evidence exists showing that animals as simple as bees demonstrate systematic numerical extrapolation <ref type="bibr">[? 7]</ref>, suggesting that systematicity in reasoning about numerical quantities is ecologically advantageous.</p><p>In this paper, we develop a new module that can be used in conjunction with standard neural network architectures (e.g., LSTMs or convnets) but which is biased to learn systematic numerical computation. Our strategy is to represent numerical quantities as individual neurons without a nonlinearity. To these single-value neurons, we apply operators that are capable of representing simple functions (e.g., +, −, ×, etc.). These operators are controlled by parameters which determine the inputs and operations used to create each output. However, despite this combinatorial character, they are differentiable, making it possible to learn them with backpropagation <ref type="bibr" target="#b23">[24]</ref>.</p><p>We experiment across a variety of task domains (synthetic, image, text, and code), learning signals (supervised and reinforcement learning), and structures (feed-forward and recurrent). We find that our proposed model can learn functions over representations that capture the underlying numerical nature of the data and generalize to numbers that are several orders of magnitude larger than those observed during training. We also observe that our module exhibits a superior numeracy bias relative to linear layers, even when no extrapolation is required. In one case our model exceeds a state-of-the-art image counting network by an error margin of 54%. Notably, the only modification we made over the previous state-of-the-art was the replacement of its last linear layer with our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Numerical Extrapolation Failures in Neural Networks</head><p>To illustrate the failure of systematicity in standard networks, we show the behavior of various MLPs trained to learn the scalar identity function, which is the most straightforward systematic relationship possible. The notion that neural networks struggle to learn identity relations is not new <ref type="bibr" target="#b13">[14]</ref>. We show this because, even though many of the architectures evaluated below could theoretically represent the identity function, they typically fail to acquire it. In <ref type="figure" target="#fig_0">Figure 1</ref>, we show the nature of this failure (experimental details and more detailed results in Appendix A). We train an autoencoder to take a scalar value as input (e.g., the number 3), encode the value within its hidden layers (distributed representations), then reconstruct the input value as a linear combination of the last hidden layer (3 again). Each autoencoder we train is identical in its parameterization (3 hidden layers of size 8), tuning (10,000 iterations, learning rate of 0.01, squared error loss), and initialization, differing only on the choice of nonlinearity on hidden layers. For each point in <ref type="figure" target="#fig_0">Figure 1</ref>, we train 100 models to encode numbers between −5 and 5 and average their ability to encode numbers between −20 and 20. We see that even over a basic task using a simple architecture, all nonlinear functions fail to learn to represent numbers outside of the range seen during training. The severity of this failure directly corresponds to the degree of non-linearity within the chosen activation function. Some activations learn to be highly linear (such as PReLU) which reduces error somewhat, but sharply non-linear functions such as sigmoid and tanh fail consistently. Thus, despite the fact that neural networks are capable of representing functions that extrapolate, in practice we find that they fail to learn to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Neural Accumulator &amp; Neural Arithmetic Logic Unit</head><p>Here we propose two models that are able to learn to represent and manipulate numbers in a systematic way. The first supports the ability to accumulate quantities additively, a desirable inductive bias for linear extrapolation. This model forms the basis for a second model, which supports multiplicative extrapolation. This model also illustrates how an inductive bias for arbitrary arithmetic functions can be effectively incorporated into an end-to-end model.</p><p>Our first model is the neural accumulator (NAC), which is a special case of a linear (affine) layer whose transformation matrix W consists just of −1's, 0's, and 1's; that is, its outputs are additions or subtractions (rather than arbitrary rescalings) of rows in the input vector. This prevents the layer from changing the scale of the representations of the numbers when mapping the input to the output, meaning that they are consistent throughout the model, no matter how many operations are chained together. We improve the inductive bias of a simple linear layer by encouraging 0's, 1's, and −1's within W in the following way.</p><p>Since a hard constraint enforcing that every element of W be one of {−1, 0, 1} would make learning hard, we propose a continuous and differentiable parameterization of W in terms of unconstrained parameters: W = tanh(Ŵ) σ(M). This form is convenient for learning with gradient descent The Neural Accumulator (NAC) is a linear transformation of its inputs. The transformation matrix is the elementwise product of tanh(Ŵ) and σ(M). The Neural Arithmetic Logic Unit (NALU) uses two NACs with tied weights to enable addition/subtraction (smaller purple cell) and multiplication/division (larger purple cell), controlled by a gate (orange cell).</p><p>and produces matrices whose elements are guaranteed to be in [−1, 1] and biased to be close to −1, 0, or 1. <ref type="bibr" target="#b0">1</ref> The model contains no bias vector, and no squashing nonlinearity is applied to the output.</p><p>While addition and subtraction enable many useful systematic generalizations, a similarly robust ability to learn more complex mathematical functions, such as multiplication, may be be desirable. <ref type="figure">Figure 2</ref> describes such a cell, the neural arithmetic logic unit (NALU), which learns a weighted sum between two subcells, one capable of addition and subtraction and the other capable of multiplication, division, and power functions such as √ x. Importantly, the NALU demonstrates how the NAC can be extended with gate-controlled sub-operations, facilitating end-to-end learning of new classes of numerical functions. As with the NAC, there is the same bias against learning to rescale during the mapping from input to output.</p><p>The NALU consists of two NAC cells (the purple cells) interpolated by a learned sigmoidal gate g (the orange cell), such that if the add/subtract subcell's output value is applied with a weight of 1 (on), the multiply/divide subcell's is 0 (off) and vice versa. The first NAC (the smaller purple subcell) computes the accumulation vector a, which stores results of the NALU's addition/subtraction operations; it is computed identically to the original NAC, (i.e., a = Wx). The second NAC (the larger purple subcell) operates in log space and is therefore capable of learning to multiply and divide, storing its results in m:</p><formula xml:id="formula_0">NAC: a = Wx W = tanh(Ŵ) σ(M) NALU: y = g a + (1 − g) m m = exp W(log(|x| + )), g = σ(Gx)</formula><p>where prevents log 0. Altogether, this cell can learn arithmetic functions consisting of multiplication, addition, subtraction, division, and power functions in a way that extrapolates to numbers outside of the range observed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Numerical reasoning is central to many problems in intelligence and by extension is an important topic in deep learning <ref type="bibr" target="#b4">[5]</ref>. A widely studied task is counting objects in images <ref type="bibr">[2, 4? , 25, 31, 33]</ref>. These models generally take one of two approaches: 1) using a deep neural network to segment individual instances of a particular object and explicitly counting them in a post-processing step or 2) learning end-to-end to predict object counts via a regression loss. Our work is more closely related to the second strategy.</p><p>Other work more explicitly attempts to model numerical representations and arithmetic functions within the context of learning to execute small snippets of code <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23]</ref>. Learning to count within a bounded range has also been included in various question-answer tasks, notably the BaBI tasks <ref type="bibr" target="#b28">[29]</ref>, and many models successfully learn to do so <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>. However, to our knowledge, no tasks of this kind explicitly require counting beyond the range observed during training.</p><p>One can also view our work as advocating a new context for linear activations within deep neural networks. This is related to recent architectural innovations such as ResNets <ref type="bibr" target="#b13">[14]</ref>, Highway Networks <ref type="bibr" target="#b25">[26]</ref>, and DenseNet <ref type="bibr" target="#b14">[15]</ref>, which also advocate for linear connections to reduce exploding/vanishing gradients and promote better learning bias. Such connections improved performance, albeit with additional computational overhead due to the increased depth of the resulting architectures.</p><p>Our work is also in line with a broader theme in machine learning which seeks to identify, in the form of behavior-governing equations, the underlying structure of systems that extrapolate well to unseen parts of the space <ref type="bibr" target="#b2">[3]</ref>. This is a strong trend in recent neural network literature concerning the systematic representation of concepts within recurrent memory, allowing for functions over these concepts to extrapolate to sequences longer than observed during training. The question of whether and how recurrent networks generalize to sequences longer than they encountered in training has been of enduring interest, especially since well-formed sentences in human languages are apparently unbounded in length, but are learned from a limited sample <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>. Recent work has also focused on augmenting LSTMs with systematic external memory modules, allowing them to generalize operations such as sorting <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>, again with special interest in generalization to sequences longer than observed during training through systematic abstraction.</p><p>Finally, the cognitive and neural bases of numerical reasoning in humans and animals have been intensively studied; for a popular overview see Dehaene <ref type="bibr" target="#b4">[5]</ref>. Our models are reminiscient of theories which posit that magnitudes are represented as continuous quantities manipulated by accumulation operations <ref type="bibr">[? ]</ref>, and, in particular, our single-neuron representation of number recalls Gelman and Gallistel's posited "numerons"-individual neurons that represent numbers <ref type="bibr" target="#b7">[8]</ref>. However, across many species, continuous quantities appear to be represented using an approximate representation where acuity decreases with magnitude <ref type="bibr" target="#b21">[22]</ref>, quite different from our model's constant precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The experiments in this paper test numeric reasoning and extrapolation in a variety of settings. We study the explicit learning of simple arithmetic functions directly from numerical input, and indirectly from image data. We consider temporal domains: the translation of text to integer values, and the evaluation of computer programs containing conditional logic and arithmetic. These supervised tasks are supplemented with a reinforcement learning task which implicitly involves counting to keep track of time. We conclude with the previously-studied MNIST parity task where we obtain state-of-the-art prediction accuracy and provide an ablation study to understand which components of NALU provide the most benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simple Function Learning Tasks</head><p>In these initial synthetic experiments, we demonstrate the ability of NACs and NALUs to learn to select relevant inputs and apply different arithmetic functions to them, which are the key functions they are designed to be able to solve (below we will use these as components in more complex architectures). We have two task variants: one where the inputs are presented all at once as a single vector (the static tasks) and a second where inputs are presented sequentially over time (the recurrent tasks). Inputs are randomly generated, and for the target, two values (a and b) are computed as a sum over regular parts of the input. An operation (e.g., a × b) is then computed providing the training (or evaluation) target. The model is trained end-to-end by minimizing the squared loss, and evaluation looks at performance of the model on held-out values from within the training range (interpolation) or on values from outside of the training range (extrapolation). Experimental details and more detailed results are in Appendix B.</p><p>On the static task, for baseline models we compare the NAC and NALU to MLPs with a variety of standard nonlinearities as well as a linear model. We report the baseline with the best median held-out performance, which is the Relu6 activation <ref type="bibr" target="#b16">[17]</ref>. Results using additional nonlinearities are also in Appendix B. For the recurrent task, we report the performance of an LSTM and the best performing RNN variant from among several common architectures, an RNN with ReLU activations (additional recurrent baselines also in Appendix B).   <ref type="table">Table 1</ref>: Interpolation and extrapolation error rates for static and recurrent tasks. Scores are scaled relative to a randomly initialized model for each task such that 100.0 is equivalent to random, 0.0 is perfect accuracy, and &gt;100 is worse than a randomly initialized model. Raw scores in Appendix B. <ref type="table">Table 1</ref> summarizes results and shows that while several standard architectures succeed at these tasks in the interpolation case, none of them succeed at extrapolation. However, in both interpolation and extrapolation, the NAC succeeds at modeling addition and subtraction, whereas the more flexible NALU succeeds at multiplicative operations as well (except for division in the recurrent task 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MNIST Counting and Arithmetic Tasks</head><p>In the previous synthetic task, both inputs and outputs were provided in a generalization-ready representation (as floating point numbers), and only the internal operations and representations had to be learned in a way that generalized. In this experiment, we discover whether backpropagation can learn the representation of non-numeric inputs to NACs/NALUs.</p><p>In these tasks, a recurrent network is fed a series of 10 randomly chosen MNIST digits and at the end of the series it must output a numerical value about the series it observed. <ref type="bibr" target="#b2">3</ref> In the MNIST Digit Counting task, the model must learn to count how many images of each type it has seen (a 10-way regression), and in the MNIST Digit Addition task, it must learn to compute the sum of the digits it observed (a linear regression). Each training series is formed using images from the MNIST digit training set, and each testing series from the MNIST test set. Evaluation occurs over held-out sequences of length 10 (interpolation), and two extrapolation lengths: 100 and 1000. Although no direct supervision of the convnet is provided, we estimate how well it has learned to distinguish digits by passing in a test sequences of length 1 (also from the MNIST test dataset) and estimating the accuracy based on the count/sum. Parameters are initialized randomly and trained by backpropagating the mean squared error against the target count vector or the sum. <ref type="table">Table 2</ref> shows the results for both tasks. As we saw before, standard architectures succeed on held-out sequences in the interpolation length, but they completely fail at extrapolation. Notably, the RNN-tanh and RNN-ReLU models also fail to learn to interpolate to shorter sequences than seen during training. However, the NAC and NALU both extrapolate and interpolate well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language to Number Translation Tasks</head><p>Neural networks have also been quite successful in working with natural language inputs, and LSTMbased models are state-of-the-art in many tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref>. However, much like other numerical input, it is not clear whether representations of number words are learned in a systematic way. To test this, we created a new translation task which translates a text number expression (e.g., five hundred and fifteen) into a scalar representation (515).  <ref type="table">Table 2</ref>: Accuracy of the MNIST Counting &amp; Addition tasks for series of length 1, 10, 100, and 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head><p>We trained and tested using numbers from 0 to 1000. The training set consists of the numbers 0-19 in addition to a random sample from the rest of the interval, adjusted to make sure that each unique token is present at least once in the training set. There are 169 examples in the training set, 200 in validation, and 631 in the test test. All networks trained on this dataset start with a token embedding layer, followed by encoding through an LSTM, and then a linear layer, NAC, or NALU.   <ref type="figure">Figure 3</ref>: Intermediate NALU predictions on previously unseen queries.</p><p>We observed that both baseline LSTM variants overfit severely to the 169 training set numbers and generalize poorly. The LSTM + NAC performs poorly on both training and test sets. The LSTM + NALU achieves the best generalization performance by a wide margin, suggesting that the multiplier is important for this task.</p><p>We show in <ref type="figure">Figure 3</ref> the intermediate states of the NALU on randomly selected test examples. Without supervision, the model learns to track sensible estimates of the unknown number up to the current token. This allows the network to predict given tokens it has never seen before in isolation, such as e.g. eighty, since it saw eighty one, eighty four and eighty seven during training. <ref type="bibr" target="#b3">4</ref> The and token can be exploited to form addition expressions (see last example), even though these were not seen in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Program Evaluation</head><p>Evaluating a program requires the control of several logical and arithmetic operations and internal book-keeping of intermediate values. We consider the two program evaluation tasks defined in <ref type="bibr" target="#b31">[32]</ref>. The first consists of simply adding two large integers, and the latter involves evaluating programs containing several operations (if statements, +, −). We focus on extrapolation: can the network learn a solution that generalizes to larger value ranges? We investigate this by training with two-digit input integers pulled uniformly from [0, 100) and evaluating on random integers with three and four digits.</p><p>Following the setup of <ref type="bibr" target="#b31">[32]</ref> we report the the percentage of matching digits between the rounded prediction from the model and target integer, however we handle numeric input differently. Instead of passing the integers character-by-character, we pass the full integer value at a single time step and regress the output with an RMSE loss. Our model setup consists of a NALU that is "configured by" an LSTM, that is, its parametersŴ,M, andĜ are learned functions of the LSTM output h t at each timestep. Thus, the LSTM learns to control the NALU, dependent upon operations seen. We compare to three popular RNNs (UGRNN, LSTM and DNC) and observe in both addition (Supplementary <ref type="figure">Figure 6</ref>) and program evaluation ( <ref type="figure" target="#fig_3">Figure 4</ref>) that all models are able to solve the task at a fixed input domain, however only the NALU is able to extrapolate to larger numbers. In this case we see extrapolation is stable even when the domain is increased by two orders of magnitude. In all experiments thus far, our models have been trained to make numeric predictions. However, as discussed in the introduction, systematic numeric computation appears to underlie a diverse range of (natural) intelligent behaviors. In this task, we test whether a NAC can be used "internally" by an RL-trained agent to develop more systematic generalization to quantitative changes in its environment. We developed a simple grid-world environment task in which an agent is given a time (specified as a real value) and receives a reward if is arrives at a particular location at (and not before) that time. As illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>, each episode in this task begins (t = 0) with the agent and a single target red square randomly positioned in a 5 × 5 grid-world. At each timestep, the agent receives as input a 56 × 56 pixel representation of the state of the (entire) world, and must select a single discrete action from {UP, DOWN, LEFT, RIGHT, PASS}. At the start of the episode, the agent also receives a numeric (integer) instruction T , which communicates the exact time the agent must arrive at its destination (the red square).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Learning to Track Time in a Grid-World Environment</head><p>To achieve the maximum episode reward m, the agent must select actions and move around so as to first step onto the red square precisely when t = T . Training episodes end either when the agent reaches the red square or after timing out (t = L). We first trained a conventional A3C agent <ref type="bibr" target="#b20">[21]</ref> with a recurrent (LSTM) core memory, modified so that the instruction T was communicated to the agent via an additional input unit concatenated to the output of the agent's convnet visual module before being passed to the agent's LSTM core memory. We also trained a second variant of the same architecture where the instruction was passed both directly to the LSTM memory and passed through an NAC and back into the LSTM. Both agents were trained on episodes where T ∼ U{5, 12} (eight being the lowest value of T such that reaching the target destination when t = T is always possible). Both agents quickly learned to master the training episodes. However, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the agent with the NAC performed well on the task for T ≤ 19, whereas performance of the standard A3C agent deteriorated for T &gt; 13.</p><p>It is instructive to also consider why both agents eventually fail. As would be predicted by consideration of extrapolation error observed in previous models, for stimuli greater than 12 the baseline agent behaves as if the stimulus were still 12, arriving at the destination at t = 12 (too early) and thus receiving incrementally less reward with larger stimuli. In contrast, for stimuli greater than 20, the agent with NAC never arrives at the destination. Note that in order to develop an agent that could plausibly follow both numerical and non-numeric (e.g. linguistic or iconic) instructions, the instruction stimulus was passed both directly to the agent's core LSTM and first through the NAC. We hypothesize that the more limited extrapolation (in terms of orders of magnitude) of the NAC here relative with other uses of the NAC was caused by the model still using the LSTM to encode numeracy to some degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">MNIST Parity Prediction Task &amp; Ablation Study</head><p>Layer Configuration Test Acc.</p><p>Seguí et al. <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_1">Wx + b 85.1 Ours: Wx + b 88.1 σ(W)x + b 60.0 tanh(W)x + b 87.6 Wx + 0 91.4 σ(W)x + 0 62.5 tanh(W)x + 0 88.7 NAC: (tanh(Ŵ) σ(M))x + 0</formula><p>93.1 <ref type="table">Table 4</ref>: An ablation study between an affine layer and a NAC on the MNIST parity task.</p><p>Thus far, we have emphasized the extrapolation successes; however, our results indicate that the NAC layer often performs extremely well at interpolation. In our final task, the MNIST parity task <ref type="bibr" target="#b24">[25]</ref>, we look explicitly at interpolation. Also, in this task, neither the input nor the output is directly provided as a number, but it implicitly invovles reasoning about numeric quantities. In these experiments, the NAC or its variants replace the last linear layer in the model proposed by Seguí et al. <ref type="bibr" target="#b24">[25]</ref>, where it connects output of the convnet to the prediction softmax layer. Since the original model had an affine layer here, and a NAC is a constrained affine layer, we look systematically at the importance of each constraint. <ref type="table">Table 4</ref> summarizes the performance of the variant models. As we see, removing the bias and applying nonlinearities to the weights significantly increases the accuracy of the end-toend model, even though the majority of the parameters are not in the NAC itself. The NAC reduces the error of the previous best results by 54%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Current approaches to modeling numeracy in neural networks fall short because numerical representations fail to generalize outside of the range observed during training. We have shown how the NAC and NALU can be applied to rectify these two shortcomings across a wide variety of domains, facilitating both numerical representations and functions on numerical representations that generalize outside of the range observed during training. However, it is unlikely that NAC or NALU will be the perfect solution for every task. Rather, they exemplify a general design strategy for creating models that have biases intended for a target class of functions. This design strategy is enabled by the single-neuron number representation we propose, which allows arbitrary (differentiable) numerical functions to be added to the module and controlled via learned gates, as the NALU has exemplified between addition/subtraction and multiplication/division.  <ref type="table">Table 5</ref>: Mean Absolute Reconstruction Error. 500 is equivalent to simply predicting 0 for every test datapoint. % Error is simply Error divided by 500. Scores represent averages over 100 models reconstructing all integer values from -1000 to 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Learning the Identity Function</head><p>In <ref type="table">Table 5</ref>, we show the average error results for each MLP trained on the identity function on the range from −5 to 5 when evaluated on numbers ranging from −1000 to 1000. Pictured on the right is a small graph demonstrating the plotted shape for each nonlinearity. The important thing to note is that nonlinearities which are sharply nonlinear exhibit greater extrapolation error than those which are only mildly nonlinear. An error score of 500 is equivalent to simply predicting 0 for each target prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Synthetic Arithmetic Tasks</head><p>In the static tasks, a vector x ∈ R 100 is given and the target is a scalar y whose value is formed by first taking two random (but consistent) subsections and summing them a = n i=m x i and b = q j=p x j . The target y is then the result of applying different arithmetic functions to a and b. The process must be learned end-to-end by each model. This task is challenging because the test set comes in two forms. The first is the interpolation test set, which never proposes an example to the model requiring a, b, or y to represent a number greater than exceeded during training. The extrapolation test set, however, always includes at least one value of a, b, or y that is greater than observed during training in each example.</p><p>In order to test the recurrent variant of the NAC, we propose second set of tasks which are only a slight modification of the first. Instead of the first step requiring sums over an input vector x ∈ R 100 , the sum is computed over a timeseries where each step x t ∈ R 10 , a = T t n i=m x t,i , and b = T t q j=p x t,j . Training and interpolation testing occur over sequences of length 10. Testing occurs over sequences of length 1000 (forcing a, b, and y to take on values that are much larger than observed during training). All baseline experiments simply use an MLP with one hidden layer containing a non-linearity. For comparison, we stack two NALUs end-to-end. All models use a hidden layer size of 2, the minimally required size to solve the task.</p><p>In <ref type="table" target="#tab_7">Table 7</ref>, we show the raw scores for MLPs trained with a wide variety of non-linearities over the hidden layer. As the scores for each task vary significantly, we also show scores for a randomly initialized model in the left column (headed "Random") for context.       <ref type="table">Table 9</ref>: Recurrent use of NALU and NAC compared to modern recurrent architectures, evaluated using Mean Squared Error relative to a randomly initialized LSTM. 100.0 is equivalent to random. &gt;100.0 is worse than random. 0 is perfect accuracy. Raw scores in appendix.</p><p>In <ref type="table" target="#tab_7">Table 7</ref>, we observe that with very strong supervision over a simple task, most nonlinearities cannot learn functions requiring numeracy that generalize outside of the range observed during training. Note the consistency in these results with those in <ref type="table">Table 5</ref>; common non-linearities with a very small output range catastrophically fail to extrapolate in any case, even with strong supervision, notably including Sigmoid and Tanh which are ubiquitous in recurrent neural networks, leading to results in <ref type="table">Table 9</ref>.</p><p>In <ref type="table">Table 9</ref>, we observe the accuracy of various baselines and our models when used recurrently. The tanh and relu in <ref type="table">Table 9</ref> refer to vanilla RNNs with tanh and relu respectively applied to hidden states. All models successfully learn to interpolate over learned functions of the input. However, the when attempting to extrapolate a function learned over series of length 10 to series of length 1000, the NAC and NALU significantly outperform the baselines. Division, however, was quite challenging to extrapolate and no models were able to solve the task in a way that extrapolates. Baselines generally predicted a fixed range. However, the NAC and NALU underestimated the denominator, leading to quite significant error during extrapolation.</p><p>While these simple experiments are quite numerous, they exist to make clear a simple idea. Across a wide variety of nonlinearities and recurrent combinations of nonlinearities, most architectures can fit a training dataset requiring arithmetic; many can even learn to generalize to a test set if the numbers are within the same bounded range. However, modern neural architectures are ill-equipped to learn arithmetic in a systematic way. We did observe a few narrow exceptions in <ref type="table" target="#tab_7">Table 7</ref>, but we will later show that even these do not hold when the supervised signal is more realistic (such as when this model is merely a piece of a large end-to-end architecture). It is the systematic numerical representations present in NAC and NALU that create the desirable learning bias leading to accurate extrapolation. This will continue to be the key to systematic extrapolation in future experiments as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MNIST Counting</head><p>We report the performance of MNIST counting and addition for the full set of models considered in <ref type="table" target="#tab_11">Table 10</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Language To Number Translation Tasks</head><p>For the LSTM, we tried both summing the preceding states as output instead of using the final state and found this to be slightly advantageous to simply outputting the final state, as shown in <ref type="table" target="#tab_13">Table 11</ref>. This summed state LSTM is what we use for comparison with the NAC and NALU in table 3. We train all models for 300K steps of gradient descent on the whole training set using Adam.</p><p>We selected the best model by validation loss over layer sizes {16, 32}, learning rates {0.01, 0.001}, and 10 initializations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Program Evaluation</head><p>The addition task, which is the simpler variant of the two program evaluation tasks considered, is simple for all models to solve. However all models fail to generalize except for the recurrent NALU. The UGRNN proves much better than the LSTM and DNC, likely due to the simple linear update of the state.  <ref type="figure">Figure 6</ref>: Summing a sequence of two random integers with extrapolation to larger values. All models are averaged over 10 independent runs, 2σ confidence bands are displayed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MLPs learn the identity function only for the range of values they are trained on. The mean error ramps up severely both below and above the range of numbers seen during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: The Neural Accumulator (NAC) is a linear transformation of its inputs. The transformation matrix is the elementwise product of tanh(Ŵ) and σ(M). The Neural Arithmetic Logic Unit (NALU) uses two NACs with tied weights to enable addition/subtraction (smaller purple cell) and multiplication/division (larger purple cell), controlled by a gate (orange cell).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Simple program evaluation with extrapolation to larger values. All models are averaged over 10 independent runs, 2σ confidence bands are displayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (above) Frames from the gridworld time tracking task. The agent (gray) must move to the destination (red) at a specified time. (below) NAC improves extrapolation ability learned by A3C agents for the dating task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Digit Counting Test</figDesc><table>MNIST Digit Addition Test 
Classification Mean Absolute Error 
Classification 
Mean Absolute Error 
Seq Len 
1 
10 
100 
1000 
1 
10 
100 
1000 
LSTM 
98.29% 
0.79 18.2 198.5 
0.0% 
14.8 800.8 8811.6 
GRU 
99.02% 
0.73 18.0 198.3 
0.0% 
1.75 771.4 8775.2 
RNN-tanh 
38.91% 
1.49 18.4 198.7 
0.0% 
2.98 20.4 
200.7 
RNN-ReLU 
9.80% 
0.66 39.8 1.4e10 
88.18% 
19.1 182.1 1171.0 
NAC 
99.23% 
0.12 0.76 
3.32 
97.6% 
1.42 7.88 
57.3 
NALU 
97.6% 
0.17 0.93 
4.18 
77.7% 
5.11 26.8 
248.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Mean absolute error (MAE) comparison on translating number strings to scalars. LSTM + NAC/NALU means a single LSTM layer followed by NAC or NALU, respectively.</figDesc><table>"three 
hundred and 
thirty 
four" 

3.05 
299.9 301.3 330.1 
334 

"seven hundred and 
two" 

6.98 
699.9 701.3 702.2 

"eighty eight" 

79.6 
88 

"twenty seven 
and 
eighty" 

18.2 
27.0 
29.1 106.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 8</head><label>8</label><figDesc>normalizes all scores reported by dividing them by this "Random" column, making them more easily comparable.</figDesc><table>Tasks Test Random Tanh Sigmoid Relu6 Softsign SELU ELU ReLU Crelu None 
NAC 
NALU 

a + b 
I 
8.659 
.0213 
.0086 
.0144 
.0229 
.0031 .0191 .0040 .0020 .0017 &lt;.0001 &lt;.0001 
E 
120.9 
52.37 
51.14 
51.56 
47.78 
.0126 .1064 .0225 .0127 .0013 &lt;.0001 .0012 

a − b 
I 
6.478 
.0344 
.0183 
.0010 
.0616 
.0046 .0510 .0133 .0035 .0007 &lt;.0001 .0001 
E 
42.14 
11.54 
9.627 
12.22 
9.734 
5.60 
3.458 6.108 .0090 .0007 &lt;.0001 &lt;.0001 

a  *  b 
I 
233.9 
21.82 
12.06 
7.579 
20.07 
11.99 5.928 6.084 36.28 48.91 
50.02 &lt;.0001 
E 
3647 
971.3 
608.9 
366.6 
998.6 
380.7 187.8 183.0 197.9 1076 
1215 
&lt;.0001 

a 
b 

I 
1.175 
.0514 
.0421 
.0499 
.0558 
.1218 .1233 .1233 .1011 .4113 
.4363 
.0625 
E 
41.56 
14.65 
13.34 
15.46 
13.55 
7.219 7.286 7.291 6.746 21.75 
25.48 
.2920 

a 

2 

I 
151.7 
.3327 
.0672 
1.027 
1.479 
6.462 .1744 .7829 .1593 6.495 
34.09 &lt;.0001 
E 
4674 
2168 
2142 
2195 
1938 
1237 406.6 569.4 408.4 1172 
2490 
&lt;.0001 
√ 
a 
I 
1.476 
.0124 
.0026 
.0080 
.0101 
.0046 .0055 .0265 .0027 .0329 
.0538 &lt;.0001 
E 
4.180 
.4702 
.3571 
.4294 
.3237 
.0739 .1006 .5066 .0622 .8356 
.6876 &lt;.0001 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 :</head><label>6</label><figDesc>Raw Mean Squared Error for all arithmetic tasks across activation functions. I/E refers to interpolation/extrapolation test sets respectively. Losses on the left refer to the decoder function where a is the sum (a scalar) over one random subset of the input matrix and b is the sum (a scalar) over another subset. The model must correctly predict the output of the function for each grid.</figDesc><table>Tasks Relu6 Softsign Tanh Sigmoid SELU ELU ReLU Crelu None NAC NALU 
Interpolation Test Error -Relative to Random Initialization Baseline 
a + b 
0.2 
0.3 
0.2 
0.1 
0.0 
0.2 
0.0 
0.0 
0.0 
0.0 
0.0 
a − b 
0.0 
1.0 
0.5 
0.3 
0.1 
0.8 
0.2 
0.1 
0.0 
0.0 
0.0 
a × b 
3.2 
8.6 
9.3 
5.2 
5.1 
2.5 
2.6 
15.5 
20.9 
21.4 
0.0 
a/b 
4.2 
4.7 
4.4 
3.58 
10.4 
10.5 
10.5 
8.6 
35.0 
37.1 
5.3 
a 

2 

0.7 
1.0 
0.2 
0.0 
4.3 
0.1 
0.5 
0.1 
4.3 
22.4 
0.0 
√ 
a 
0.5 
0.7 
31.6 
24.2 
0.3 
0.4 
1.8 
0.2 
2.2 
3.6 
0.0 
Extrapolation Test Error -Relative to Random Initialization Baseline 
a + b 
42.6 
39.5 
43.3 
42.3 
0.0 
0.0 
0.0 
0.0 
0.0 
0.0 
0.0 
a − b 
29.0 
23.1 
27.3 
22.8 
13.3 
8.2 
14.5 
0.0 
0.0 
0.0 
0.0 
a × b 
10.1 
27.4 
26.6 
16.7 
10.4 
5.1 
5.0 
5.4 
29.5 
33.3 
0.0 
a/b 
37.2 
32.6 
35.3 
32.1 
17.4 
17.5 
17.5 
16.2 
52.3 
61.3 
0.7 
a 

2 

47.0 
41.5 
46.4 
45.8 
26.5 
8.7 
12.2 
8.7 
25.1 
53.3 
0.0 
√ 
a 
10.3 
7.7 
11.2 
8.5 
1.7 
2.4 
12.1 
1.5 
20.0 
16.4 
0.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Static (non-recurrent) arithmetic error rates. Lower is better. Best models in bold. Scores 
relative to one randomly initialized model for each task. 100.0 is equivalent to random. 0.0 is perfect 
accuracy. Raw scores are in the Appendix. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Mean Squared Error Loss values for all recurrent arithmetic tasks across baseline and proposed models. T/I/E refers to final training loss, interpolation loss, and extrapolation loss respectively. Best scores in bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 10 :</head><label>10</label><figDesc>Accuracy of the MNIST Counting &amp; Addition tasks for series of length 1, 10, 100, and 1000.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 11 :</head><label>11</label><figDesc>Mean absolute error (MAE) comparison on translating number strings to scalars with LSTM state aggregation methods. Summing states improved generalization slightly, but</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The stable points {−1, 0, 1} correspond to the saturation points of either σ or tanh.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Division is much more challenging to extrapolate. While our models limit numbers using nonlinearities, our models are still able to represent numbers that are very, very small. Division allows such small numbers to be in the denominator, greatly amplifying even small drifts in extrapolation ability. 3 The input to the recurrent networks is the output the convnet in https://github.com/pytorch/ examples/tree/master/mnist.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note slight accumulation for the word and owing to the spurious correlation between the use of the word and a subsequent increase in the target value.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We thank Edward Grefenstette for suggesting the name Neural ALU and for valuable discussion involving numerical tasks. We also thank Stephen Clark, whose presentation on neural counting work directly inspired this one. Finally, we also thank Karl Moritz Hermann, John Hale, Richard Evans, David Saxton, and Angeliki Lazaridou for valuable comments and discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="504" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discovering governing equations from data by sparse identification of nonlinear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Steven L Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Nathan</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3932" to="3937" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang-Sheng John</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Dehaene</surname></persName>
		</author>
		<title level="m">The Number Sense: How the Mind Creates Mathematics</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: a critical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenon</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="71" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding numbers in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Randy</forename><surname>Gallistel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The child&apos;s understanding of number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rochel</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Randy</forename><surname>Gallistel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<pubPlace>Harvard</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LSTM recurrent networks learn simple context-free and context-sensitive languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1340" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural turing machines. CoRR, abs/1410</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.5401" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5401</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page">471</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<ptr target="http://arxiv.org/abs/1603.05027" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
		<ptr target="http://arxiv.org/abs/1608.06993" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Convolutional deep belief networks on CIFAR-10. Unpublished manuscript</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Algebraic Mind: Integrating Connectionism and Cognitive Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tuning curves for approximate numerosity in the human intraparietal sulcus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>Izard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><forename type="middle">Le</forename><surname>Bihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Dehaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="547" to="555" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reed and Nando de Freitas. Neural programmer-interpreters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to count with deep object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santi</forename><surname>Seguí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Vitrià</surname></persName>
		</author>
		<idno>abs/1505.08082</idno>
		<ptr target="http://arxiv.org/abs/1505.08082" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<ptr target="http://arxiv.org/abs/1505.00387" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the practical computational power of finite precision RNNs for language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1502.05698</idno>
		<ptr target="http://arxiv.org/abs/1502.05698" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sumit Chopra, and Antoine Bordes. Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods in biomechanics and biomedical engineering: Imaging &amp; Visualization</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="292" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to execute. CoRR, abs/1410</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.4615" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4615</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
