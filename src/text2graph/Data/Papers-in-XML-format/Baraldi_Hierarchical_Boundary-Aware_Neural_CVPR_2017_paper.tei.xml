<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Boundary-Aware Neural Encoder for Video Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costantino</forename><surname>Grana</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Boundary-Aware Neural Encoder for Video Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatically describing a video in natural language is an important challenge for computer vision and machine learning. This task, called video captioning, is a crucial achievement towards machine intelligence and also the support of a number of potential applications. Indeed, bringing together vision and language, video captioning can be leveraged for video retrieval, to enhance content search on video sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a video.</p><p>Before targeting videos, captioning has been tackled for images, where the task was that of generating a single sentence which described a static visual content <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b44">46]</ref>. Later, image captioning approaches have been extended to short videos with a single action, object, or scene, initially using very similar approaches to image captioning, and then with solutions to account for the temporal evolution of the video <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">50]</ref>. After having been applied to highly constrained or user generated videos <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref>,  We propose a novel video encoding network which can adaptively modify its structure to improve video captioning. Our Time Boundary-aware LSTM cell (depicted with dashed rectangles) extends the standard LSTM unit by adding a trainable boundary detector (BD), which can alter the temporal connections of the network depending on the input video.</p><p>video captioning is moving to more complex and structured kinds of video, thanks to the spread of movie description datasets <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>So far, video captioning algorithms have relied on the use of Recurrent Neural Networks or Long Short-Term Memory (LSTM) <ref type="bibr" target="#b11">[12]</ref> layers, which can naturally deal with sequences of frames and, in principle, learn long-range temporal patterns. However, it has been proved that LSTMs show good learning capabilities on sequences which are between 30 and 80 frames long <ref type="bibr" target="#b49">[51]</ref>, shorter than the ones used in video captioning. Furthermore, the plain nature of recurrent networks can not deal with the layered structure of videos. This is the case of edited video, such as movies. Long edited video can be segmented into short scenes, using Descriptive Video Services or with deep learning techniques <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>; however video scenes contain several shots that, although temporally consistent, have a different appearance. As an example, in <ref type="figure" target="#fig_1">Figure 1</ref> two shots of a dialogue are depicted. In this case we want to prevent the network from mixing the memory of the two shots; conversely, if the network could be aware of the presence of a temporal boundary, it could reset its internal status creating a new output independent to the one of the previous shot. This also applies to user-generated video, where events can be composed by a sequence of actions in a single shot (e.g. a player runs and shoots the ball). An effective encoder should consider the temporal dependencies both intra-action and interactions.</p><p>In this paper, we propose a novel video encoding scheme for video captioning capable of identifying temporal discontinuities, like action or appearance changes, and exploiting them to get a better representation of the video. <ref type="figure" target="#fig_1">Figure 1</ref> shows the hierarchical structure of our sequenceto-sequence architecture: frames, described by features computed by a CNN, enter into our time boundary-aware LSTM. The awareness of the presence of an appearance or action discontinuity automatically modifies the connectivity through time of the LSTM layer: the result is a variable length and adaptive encoding of the video, whose length and granularity depends on the input video itself. The outputs of the first boundary-aware layer are encoded through an additional recurrent layer into a fixed length vector, which is then used for generating the final caption through a Gated Recurrent Unit (GRU) layer. The contributions of the paper are summarized below.</p><p>• We present a new time boundary-aware LSTM cell:</p><p>it can discover discontinuities in the input video and enables the encoding layer to modify its temporal connectivity, resetting its internal state and memory if required. The proposed cell incorporates a boundary detection module and encodes content and temporal structure in a trainable end-to-end layer.</p><p>• The time boundary-aware LSTM is used to build a hierarchical encoder for video captioning: to the best of our knowledge, this is the first proposal of a video captioning network which can learn to adapt its structure to input data.</p><p>• We test our approach on three large-scale movie description and video captioning datasets: M-VAD <ref type="bibr" target="#b38">[39]</ref>, MPII-MD <ref type="bibr" target="#b29">[30]</ref>, and MSVD <ref type="bibr" target="#b5">[6]</ref>. Our results significantly improve the state-of-the art on movie description, being competitive also on short user-generated video. We also investigate boundaries learned by our encoder, and show that it can discover appropriate decompositions of the input video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Early captioning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref> were based on the identification of (subject, verb, object) triplets with visual classifiers, and captions were generated through a language model which fitted predicted triplets to predefined sentence templates. Of course, template-based sentences can not satisfy the richness of natural language, and have limited ability to generalize to unseen data. For these reasons, research on image and video captioning has soon moved to the use of recurrent networks, which, given a vectored description of a visual content, could naturally deal with sequences of words <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In one of the first approaches to video captioning with recurrent networks, Venugopalan et al. <ref type="bibr" target="#b42">[44]</ref> used CNN features extracted from single frames, mean pooled them to represent the entire video, and then fed the resulting vector to a LSTM layer <ref type="bibr" target="#b11">[12]</ref> for the generation of the caption. The major drawback of this method is that it ignored the sequential nature of video, reducing the task of video captioning to a mere extension of image captioning. Therefore, many following works tried to develop more appropriate video encoding strategies. Donahue et al. <ref type="bibr" target="#b8">[9]</ref>, for example, used a LSTM network to sequentially encode the input video, and then employed CRFs to get semantic tuples of activity, object, tool and location. A final LSTM layer translated the semantic tuple into a sentence.</p><p>Venugopalan et al. <ref type="bibr" target="#b41">[43]</ref> proposed a completely neural architecture addressing both the video encoding stage and sentence decoding. They used a stacked LSTM to read the sequence of video frames, and a second LSTM, conditioned on the last hidden state of the first, to generate the corresponding caption. Interestingly, the LSTM parameters used in the two stages were shared. That was the first time the so-called sequence to sequence approach, already applied to machine translation <ref type="bibr" target="#b34">[35]</ref>, was used for video captioning. Other works have then followed this kind of approach, either by incorporating attentive mechanisms <ref type="bibr" target="#b47">[49]</ref> in the sentence decoder, by building a common visual-semantic emebedding <ref type="bibr" target="#b22">[23]</ref>, or by adding external knowledge with language models <ref type="bibr">[42]</ref> or visual classifiers <ref type="bibr" target="#b28">[29]</ref>.</p><p>Recently, researches improved both the components of the encoder-decoder approach by significantly changing their structure. Yu et al. <ref type="bibr" target="#b48">[50]</ref> focused on the sentence decoder, and proposed a hierarchical model containing a sentence and a paragraph generator: short sentences are produced by a Gated Recurrent Unit (GRU) layer <ref type="bibr" target="#b6">[7]</ref> conditioned on video features, while another recurrent layer is in charge of generating paragraphs by combining sentence vectors and contextual information. The paragraph generator can therefore captures inter-sentence dependencies and generate a sequence of related and consecutive sentences. In this paper, as in their proposal, we adopt a final GRU layer for the generation of the caption.  <ref type="figure">Figure 2</ref>. Comparison between a standard LSTM encoder and the Time Boundary-aware LSTM network, and schema of the Boundaryaware LSTM cell. The proposed video encoder can learn to modify its temporal connections according to appearance or action changes which are found in the video: when a boundary is detected, the state of the LSTM is reinitialized and a representation of the ended segment is given to the output. Red dashed boxes represent LSTM units with reset state, black boxes stand for LSTM unit with modified states.</p><formula xml:id="formula_0">… (a) Traditional LSTM network … s t =1 s t =1 (b)</formula><p>In contrast, Pan et al. <ref type="bibr" target="#b21">[22]</ref> targeted the video encoding stage, by proposing a hierarchical recurrent video encoder. Their proposal tries to abstract visual features at different time scales and granularities, by processing frames of the video in a way similar to a convolutional operation applied in the time dimension. A LSTM is applied to small overlapped video chunks, in a sliding window fashion: this results in a sequence of vectors, which are then forwarded to a second recurrent layer, or processed by the decoder LSTM through a soft attention mechanism <ref type="bibr" target="#b0">[1]</ref>.</p><p>Also in this paper, we focus on the video encoding stage. However, instead of building an hand-crafted variation of the plain LSTM layer as in <ref type="bibr" target="#b21">[22]</ref>, we propose a recurrent network which can learn to adapt its temporal structure to input data. Our strategy, contrary to the sliding window approach, also ensures that the cell memory encoding each chunk always contains homogeneous information. The idea of leveraging segment-level features has been investigated in natural language processing <ref type="bibr" target="#b7">[8]</ref>, action recognition <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19]</ref> and event detection <ref type="bibr" target="#b45">[47]</ref>. Our network is the first proposal which exploits temporal segments in video captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given an input video, we propose a recurrent video encoder which takes as input a sequence of visual features (x 1 , x 2 , ..., x n ) and outputs a sequence of vectors (s 1 , s 2 , ..., s m ) as the representation for the whole video. In our encoder, the connectivity schema of the layer varies with respect to both the current input and the hidden state, so it is thought as an activation instead of being a non learnable hyperparameter.</p><p>To this aim, we define a time boundary-aware recurrent cell, which can modify the layer connectivity through time: when an appearance or action change is estimated, the hidden state and the cell memory are reinitialized, and at the end of a segment the hidden state of the layer is given to the output, as a summary of the detected segment. This ensures that the input data following a time boundary are not influenced by those seen before the boundary, and generates a hierarchical representation of the video in which each chunk is composed by homogeneous frames. Figures 2a and 2b show the temporal connections determined by the boundary detector in a sample case, compared to those of a plain LSTM encoder.</p><p>The proposed time boundary-aware recurrent cell is built on top of a Long Short-Term Memory (LSTM) unit, which has been shown to be particularly suited to video encoding, since it is known to learn patterns with wide temporal dependencies. At its core there is a memory cell c t which maintains the history of the inputs observed up to a timestep. Update operations on the memory cell are modulated by three gates i t , f t and o t , which are all computed as a combination of the current input x t and of the previous hidden state h t−1 , followed by a sigmoid activation. The input gate i t controls how the current input should be added to the memory cell; the forget gate f t is used to control what the cell will forget from the previous memory c t−1 , and the out-put gate o t controls whether the current memory cell should be passed as output.</p><p>At each timestep, we select whether to transfer the hidden state and memory cell content to the next timestep or to reinitialize them, interrupting the seamless update and processing of the input sequence. This depends on a time boundary detection unit, which allows our encoder to independently process variable length chunks of the input video. The boundaries of each chunk are given by a learnable function which depends on the input, and are not set in advance.</p><p>Formally, the boundary detector s t ∈ {0, 1} is computed as a linear combination of the current input and of the hidden state, followed by a function τ which is the composition of a sigmoid and a step function:</p><formula xml:id="formula_1">s t = τ (v T s · (W si x t + W sh h t−1 + b s ))<label>(1)</label></formula><formula xml:id="formula_2">τ (x) = 1, if σ(x) &gt; 0.5 0, otherwise<label>(2)</label></formula><p>where v T s is a learnable row vector and W sh , b s are learned weights and biases.</p><p>Given the current boundary detection s t , before applying the memory unit update equations, the following substitutions are applied to transfer or reinitialize the network hidden state and memory cell at the beginning of a new segment, according to s t :</p><formula xml:id="formula_3">h t−1 ← h t−1 · (1 − s t ) (3) c t−1 ← c t−1 · (1 − s t ).<label>(4)</label></formula><p>The resulting state and memory are now employed to recompute the gates values, which will in turn be used for advancing to the next time step. The encoder produces an output only at the end of a segment. If s t = 1, indeed, the hidden state of timestep t − 1 is passed to the next layer. Many LSTM architectures have been proposed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref>, and all are slightly different in their structure and activation functions, even though they all share the presence of additive memory cells and gates. In our case, we apply the following equations <ref type="bibr" target="#b11">[12]</ref> </p><formula xml:id="formula_4">i t = σ(W ix x t + W ih h t−1 + b i ) (5) f t = σ(W f x x t + W f h h t−1 + b f ) (6) g t = φ(W gx x t + W gh h t−1 + b g ) (7) c t = f t ⊙ c t−1 + i t ⊙ g t (8) o t = φ(W f x x t + W f h h t−1 + b f ) (9) h t = o t ⊙ φ(c t )<label>(10)</label></formula><p>where ⊙ denotes the element-wise Hadamard product, σ is the sigmoid function, φ is the hyperbolic tangent tanh, W * are learned weight matrices and b * are learned biases vectors. The internal state h and memory cell c are initialized to zero. <ref type="figure">Figure 2c</ref> shows a schema of the proposed time boundary-aware cell. A recurrent layer which follows the equations reported above will produce a variable length set of outputs (s 1 , s 2 , ..., s m ), where m is the number of detected segments. Each of these outputs conceptually summarizes the content of a detected segment inside the video. This set of outputs is passed to another recurrent layer, thus building a hierarchical representation of the video. To this end, we fed the output of the boundary-aware encoder to an additional LSTM layer, whose last hidden state can be used as the feature vector for the entire video.</p><p>Existing approaches to video encoding add more nonlinearity to LSTM architectures by stacking together more layers <ref type="bibr" target="#b41">[43]</ref>, or by building a hierarchical architecture in which a lower level encodes fixed length chunks, while a higher level is in charge of composing these encoded chunks to get the final video representation <ref type="bibr" target="#b21">[22]</ref>. Our proposal, while keeping a completely neural architecture, enables the encoder to both produce variable length chunks, based on the input data characteristics, and to encode them in a hierarchical structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training</head><p>Due to the presence of a binary variable which influences the temporal structure of the video encoder, special training expedients are required.</p><p>First of all, the boundary detector s t is treated at training time as a stochastic neuron <ref type="bibr" target="#b26">[27]</ref>. In particular, we introduce a stochastic version of function τ (x) (Eq. 2), in which its ouput is sampled from a uniform distribution conditioned on σ(x). Formally, during the forward pass of the training phase, τ is computed as <ref type="formula" target="#formula_1">(11)</ref> where U [0, 1] is a uniform distribution over the interval [0, 1] and 1 is the indicator function. This ensures that s t is stochastic, and its probability of being 0 or 1 is proportional to the value of a sigmoid applied to the input of τ .</p><formula xml:id="formula_5">τ (x) = 1 σ(x)&gt;z , with z ∼ U [0, 1] , forward pass</formula><p>In the backward pass, since the derivative of the step function is zero almost anywhere, the standard back propagation would no longer be applicable. To solve this issue, we employ an estimator of the step function as suggested by Bengio et al. <ref type="bibr" target="#b4">[5]</ref>. The idea is that discrete operations can be used in the forward pass if a differentiable approximation is used in the backward one. In our case, we approximate the step function with the identity function, which has shown good performances <ref type="bibr" target="#b4">[5]</ref>. Being τ the composition of a sigmoid and a step function, the derivative of τ used in backward is simply the derivative of the sigmoid function.</p><formula xml:id="formula_6">∂τ ∂x (x) = σ(x)(1 − σ(x)), backward pass<label>(12)</label></formula><p>At test time, the deterministic version of the step function (Eq. 2) is used. In this way the number of detected segments is stochastic during training and deterministic during test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sentence generation</head><p>Once the representation of the video has been computed, the description of the video is generated through a decoder network, following the encoder-decoder scheme <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Given a video vector v and a sentence (y 0 , y 1 , ..., y T ), encoded with one-hot vectors (1-of-N encoding, where N is the size of the vocabulary), our decoder is conditioned step by step on the first t words of the caption and on the corresponding video descriptor, and is trained to produce the next word of the caption. The objective function which we optimize is the log-likelihood of correct words over the sequence</p><formula xml:id="formula_7">max w T t=1</formula><p>log Pr(y t |y t−1 , y t−2 , ..., y 0 , v)</p><p>where w are all the parameters of the encoder-decoder model. The probability of a word is modeled via a softmax layer applied on the output of the decoder. To reduce the dimensionality of the decoder, a linear embedding transformation is used to project one-hot word vectors into the input space of the decoder and, viceversa, to project the output of the decoder to the dictionary space.</p><p>Pr(y t |y t−1 , y t−2 , ...,</p><formula xml:id="formula_9">y 0 , v) ∝ exp(y T t W p p t )<label>(14)</label></formula><p>W p is a matrix for transforming the decoder output space to the word space and p t is the output of the decoder, computed with a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref> layer. The output of the GRU layer, at each timestep, is modeled via two sigmoid gates: a reset gate (r t ), which determines whether the previous hidden state should be dropped to generate the next outputs, and an update gate (z t ) which controls how much information of the previous hidden state should be preserved:</p><formula xml:id="formula_10">z t = σ(W zy W w y t + W zv v + W zh p t−1 + b i ) (15) r t = σ(W ry W w y t + W rv v + W rh p t−1 + b f ). (16)</formula><p>Exploiting the values of the above gates, the output of the decoder GRU is computed as:</p><formula xml:id="formula_11">h t = φ(W hy W w y t + W hv v + W hh (r t ⊙ p t−1 ) + b f )<label>(17)</label></formula><formula xml:id="formula_12">p t = (1 − z t ) ⊙h t−1 + z t ⊙ p t<label>(18)</label></formula><p>where W * and b * are learned weights and biases and W w transforms the one-hot encoding of words to a dense lower dimensional embedding. Again, ⊙ denotes the elementwise product, σ is the sigmoid function and φ is the hyperbolic tangent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>Evaluation is carried out on three large-scale datasets for video captioning, one containing user-generated videos, and the other two specifically built for movie description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Montreal Video Annotation dataset (M-VAD) The Montreal Video Annotation dataset <ref type="bibr" target="#b38">[39]</ref> is a large-scale video description dataset based on Descriptive Video Service (DVS). DVS, or Audio Descriptions, are audio tracks describing the visual elements of a movie, produced to help visually impaired people. The dataset consists of 84.6 hours of video from 92 Hollywood movies, for a total of 46,523 video clips, each automatically aligned with with a single description. We use the standard splits provided in <ref type="bibr" target="#b38">[39]</ref>, which consists of 36,921 training samples, 4,651 validation samples and 4,951 test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII Movie Description dataset (MPII-MD)</head><p>The MPII Movie Description dataset <ref type="bibr" target="#b29">[30]</ref> has been built in a way similar to M-VAD, even though in this case the alignment between video snippets and descriptions is more correct, since it has been manually corrected. The dataset contains a parallel corpus of over 68K sentences and video snippets from 94 HD movies, obtained from scripts and Audio Descriptions. Following the splits provided by the authors, the dataset contains 56,861 train samples, 4,930 validation samples and 6,584 test samples.</p><p>Microsoft Video Description Corpus (MSVD) The Microsoft Video Description Corpus <ref type="bibr" target="#b5">[6]</ref> contains 2,089 Youtube video clips, labeled with 85K English descriptions collected by Amazon Mechanical Turkers. The dataset was initially conceived to contain multi-lingual descriptions; however, we only consider captions in the English language. As done in previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">44]</ref>, we split the dataset in contiguous groups of videos by index number: 1,200 for training, 100 for validation and 670 for test. This dataset mainly contains short video clips with a single action, an is therefore less appropriate than M-VAD and MPII-MD to evaluate the effectiveness of our method in identifying the video structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>We employ four popular metrics for evaluation: BLEU <ref type="bibr" target="#b23">[24]</ref>, ROUGE L <ref type="bibr" target="#b19">[20]</ref>, METEOR <ref type="bibr" target="#b1">[2]</ref> and CIDEr <ref type="bibr" target="#b40">[41]</ref>. BLEU is a form of precision of word n-grams between predicted and ground-truth sentences. As done in previous works, we evaluate our predictions with BLEU using four-grams. ROUGE L computes an F-measure with a recall bias using a longest common subsequence technique. METEOR, instead, scores captions by aligning them to one or more ground truths. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases, therefore METEOR is more semantically adequate than BLEU and ROUGE L . CIDEr, finally, computes the average cosine similarity between n-grams found in the generated caption and those found in reference sentences, weighting them using TF-IDF. The authors of CIDEr <ref type="bibr" target="#b40">[41]</ref> reported that CIDEr and METEOR are always more accurate, especially when the number of reference captions is low.</p><p>To ensure a fair evaluation, we use the Microsoft CoCo evaluation toolkit <ref type="bibr" target="#b0">1</ref> to compute all scores, as done in previous video captioning works <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Preprocessing and training details</head><p>We extract static appearance as well as motion features from input videos of all datasets. To encode video appearance, we use the ResNet50 model <ref type="bibr" target="#b13">[14]</ref> trained on the Imagenet dataset <ref type="bibr" target="#b30">[31]</ref>, and compute a descriptor every 5 frames. For motion, we employ the C3D network <ref type="bibr" target="#b39">[40]</ref> (trained on the Sports-1M dataset <ref type="bibr" target="#b16">[17]</ref>): this model outputs a fixed length feature vector every 16 frames, which encodes motion features computed around the middle frame of the window. To maintain the same granularity used for appearance, we sample 16 frames long, partially overlapped, windows with a stride of 5 frames. In both cases, we use the activations from the penultimate layer of the network, which leads to a 2,048+4,096-dimensional feature vector. Instead of directly inputting visual features into our model, we learn a linear embedding as the input of the model.</p><p>Ground truth descriptions are converted to lower case and tokenized after having removed punctuation characters. We retain only words which appear at least five times in a dataset. This yields a vocabulary of 6,090 words for the M-VAD dataset, 7,198 words for MPII-MD and 4,215 words for MSVD. During training, we add a begin-of-sentence &lt;BOS&gt; tag at the beginning of the caption, and end-ofsentence tag &lt;EOS&gt; at its end, so that our model can deal with captions with variable length. At test time, the decoder RNN is given a &lt;BOS&gt; tag as input for the first timestep, then the most probable word according to the predicted distribution is sampled and given as input for the next timestep, until a &lt;EOS&gt; tag is predicted.</p><p>Training is performed by minimizing the log-likelihood loss with the Adadelta optimizer, with a learning rate of 1.0 and decay parameters ρ = 0.95 and ǫ = 10 × 10 −7 , which generally show good performance. We set the minibatch size to 128. To regularize the training and avoid overfitting, we apply the well known regularization technique Dropout <ref type="bibr" target="#b33">[34]</ref> with retain probability 0.5 on the input and output of the encoding LSTMs, as suggested by Zaremba et al. <ref type="bibr" target="#b24">[25]</ref>.</p><p>Embeddings for video features and words have all size 512, while the size of all recurrent hidden state is empirically set to 1024. Regarding initializiation, we used the 1 https://github.com/tylin/coco-caption Model METEOR SA-GoogleNet+3D-CNN <ref type="bibr" target="#b47">[49]</ref> 4.1 HRNE <ref type="bibr" target="#b21">[22]</ref> 5.8 S2VT-RGB(VGG) <ref type="bibr" target="#b41">[43]</ref> 6.7 HRNE with attention <ref type="bibr" target="#b21">[22]</ref> 6.  gaussian initialization suggested by Glorot et al. <ref type="bibr" target="#b10">[11]</ref> for weight matrices applied to inputs, and orthogonal initialization for weight matrices applied to internal states. Embedding matrices were also initialized according to <ref type="bibr" target="#b10">[11]</ref>, and all biases were initialized to zero. We train the model for 100 epochs, or until the loss improvement over the validation set stops. The source code of model has been written using Theano, and is made publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with the state of the art</head><p>On the M-VAD dataset we compare our method with four recent proposals: Temporal attention (SA) <ref type="bibr" target="#b47">[49]</ref>, S2VT <ref type="bibr" target="#b41">[43]</ref>, HRNE <ref type="bibr" target="#b21">[22]</ref>, and the approach from Venugopalan et al. <ref type="bibr">[42]</ref>. SA employed a LSTM decoder with a temporal attention mechanism over features extracted from GoogleNet <ref type="bibr" target="#b35">[36]</ref> and from a 3D spatio-temporal CNN. S2VT, instead, used stacked LSTMs both for the encoder and the decoder stage, and frame-level features extracted from the VGG model. HRNE runs a LSTM on short video chunks, in a sliding window fashion, and the decoder selectively attends to the resulting set of vectors, optionally through a soft attention mechanism; the approach from Venugopalan et al. <ref type="bibr">[42]</ref>, finally, focuses on the language model by adding knowledge from text corpora to the S2VT architecture. <ref type="table">Table 1</ref> shows the results on this dataset. As done in most GT: She gets out. LSTM encoder: Someone stops. BA encoder (ours): Someone gets out of the car.</p><p>GT: Shakes his head. LSTM encoder: Someone gives her gaze. BA encoder (ours): Someone looks at someone who shakes his head.</p><p>GT: He slows down in front of one house with a garage and box tree on the front.</p><p>LSTM encoder: Someone gets out of the car and walks out of the house. BA encoder (ours): Someone drives up to the house. of the previous video captioning works, we use METEOR as the main comparison metric. Firstly, to investigate the role of the boundary-aware encoder, we compare its performance against that of a single LSTM layer and that of a 2-layers LSTM encoder, trained using the same features and same hyperparameters. In this case, the last hidden state is used as the video vector for the GRU decoder. These baselines achieve a 6.7% METEOR, while using the proposed encoder significantly increases performance, yielding to a 7.3% METEOR which corresponds to an improvement of 0.6%. This result also outperforms the most recent state-of-the-art method by a margin of On the MPII-MD dataset, we again consider Temporal attention (SA) <ref type="bibr" target="#b47">[49]</ref>, S2VT <ref type="bibr" target="#b41">[43]</ref>, as well as the approach from Venugopalan et al. <ref type="bibr">[42]</ref>. We also include two other references, which are applicable to this dataset: the Statistical Machine Translation (SMT) approach in <ref type="bibr" target="#b29">[30]</ref> and the work by Rohrbach et al. <ref type="bibr" target="#b28">[29]</ref>, which exploits visual classifiers trained on visual labels extracted from captions.</p><p>The performance of these approaches and that of our solution is reported in <ref type="table" target="#tab_2">Table 2</ref>. We observe that our approach is able to exceed the current state of the art on the CIDEr and ROUGE L metrics, while we achieve almost the same performance of the semantic approach of <ref type="bibr" target="#b28">[29]</ref> according to BLEU-4 and METEOR, without exploiting the semantics of captions and building concept classifiers. For reference, <ref type="bibr" target="#b41">[43]</ref> reported a 7.1% METEOR on this dataset. As for the M-VAD dataset, we also compare our solution to the baseline with a single LSTM layer: in this case, the improvement of the boundary-aware encoder is 0.6% METEOR.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref> we present a few examples of descriptions generated by our model on clips from the M-VAD and MPII-MD. We notice that the results obtained with the Boundary-aware encoder are generally better than those of the plain LSTM encoder, which is consistent with the results reported in <ref type="table" target="#tab_2">Table 1 and 2.</ref> As an additional test, we apply our method on MSVD, a common dataset for video captioning in which the hierarchical video structure is absent. The purpose, in this case, is to investigate whether our strategy impacts negatively when there is no structure in the video.</p><p>We compare our approach on MSVD with five state of the art approaches for video captioning: Temporal attention (SA) <ref type="bibr" target="#b47">[49]</ref>, LSTM-YT <ref type="bibr" target="#b42">[44]</ref>, S2VT <ref type="bibr" target="#b41">[43]</ref>, LSTM-E <ref type="bibr" target="#b22">[23]</ref> and HRNE <ref type="bibr" target="#b21">[22]</ref>. LSTM-YT used a mean pool strategy on frame-level CNN features to encode the input video, while the caption was generated by a LSTM layer. LSTM-E, instead, proposed a visual-semantic embedding in which video descriptors and captions were projected, by maximizing distances between the projection of a video and that of its corresponding captions. As it can be noticed in <ref type="table">Table 3</ref>, our method improves over plain techniques and can achieve competitive results. It is also worth noting that the attentive mechanism used in <ref type="bibr" target="#b21">[22]</ref> could be integrated in our method, and potentially improve performance. <ref type="figure" target="#fig_4">Figure 4</ref> reports some sample results on MSVD, comparing captions generated by our approach to those from the state of the art approach in <ref type="bibr" target="#b21">[22]</ref>. As it can be seen, even though our method has not been conceived for videos lacking structure, it is still capable of generating accurate captions even in some difficult cases.  <ref type="table">Table 3</ref>. Experiment results on the MSVD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of learned boundaries</head><p>We collect statistics on the behavior of the boundary detector, which is the key component that rules the temporal structure of the video encoder. <ref type="figure" target="#fig_5">Figure 5</ref> shows the distribution of the number and position of detected cuts on the M-VAD and MPII-MD datasets. As it can be observed, in the vast majority of the videos less than three boundaries are detected. This result is in contrast with the approach of <ref type="bibr" target="#b21">[22]</ref>, in which the video was purposely segmented in very small chunks. Looking at the position of cuts, we also observe a linear growth in the probability of having a cut between the 20% and 80% of the duration of the video, so the more the video advances, the more the need of a cut increases. Two peaks can also be noticed, at the very beginning and ending of the video; this is due the fact that in the M-VAD and MPII-MD datasets videos are not precisely aligned with their captions, so the ends of the video are often uncorrelated with the main content of the video.</p><p>To confirm the effectiveness of the position of detected segments, we trained our network by forcing the encoder to split the input video in equally spaced chunks, maintaining the same number of segments detected by the original Boundary-aware encoder. This resulted in a reduction of 0.2% METEOR on M-VAD, and 0.5% METEOR on MPII-MD.</p><p>We also compare the boundaries found by our neural model with those found by an off-the-shelf open source shot detector <ref type="bibr" target="#b3">[4]</ref>. Among all detected boundaries on the M-VAD and MPII-MD datasets, 33.7% of them were found to be less than 15 frames far from a shot boundary. This confirms that the proposed LSTM cell can identify camera changes and appearance variations, but also detects more soft boundaries which do not correspond to shots.</p><p>Finally, we investigate how the the proposed video encoder would perform using shot boundaries detected with <ref type="bibr" target="#b3">[4]</ref> instead of those learned by the boundary detector. Results are reported in <ref type="table" target="#tab_2">Tables 1 and 2</ref>. On the M-VAD dataset, using shot boundaries resulted in a 7.1% METEOR, which is 0.2% below the performance of the Boundaryaware encoder, while on the MPII-MD dataset, we observed a 6.6% METEOR, which again is below the result reported by our complete model. This confirms that, even though shots give a reasonable decomposition of the video, learned GT: A woman dips a shrimp in batter. HRNE <ref type="bibr" target="#b21">[22]</ref>: A woman is cooking. BA encoder (ours): A woman is adding ingredients to a bowl of food.</p><p>GT: A boy is playing a guitar. HRNE <ref type="bibr" target="#b21">[22]</ref>: A man is playing a guitar. BA encoder (ours): A boy is playing guitar.</p><p>GT: A dog is swimming in a pool. HRNE <ref type="bibr" target="#b21">[22]</ref>: A dog is swimming. BA encoder (ours): A dog is swimming in the pool.  boundaries are definitely more effective and yield to better captioning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed a novel boundary-aware video encoder for the task of video captioning, which achieves competitive results across popular benchmarks. Our method can discover the hierarchical structure of the video, and modify the temporal connections of a recurrent layer accordingly. We believe that the proposed architecture is generic and could be employed in other video-related applications, such as video classification and action detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. We propose a novel video encoding network which can adaptively modify its structure to improve video captioning. Our Time Boundary-aware LSTM cell (depicted with dashed rectangles) extends the standard LSTM unit by adding a trainable boundary detector (BD), which can alter the temporal connections of the network depending on the input video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example results on the M-VAD and MPII-MD dataset. Blue vertical lines represent an activation of the boundary detector in the LSTM cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For reference, our method achieves a 0.9% BLEU-4, 17.1% ROUGE L and 10.4% CIDER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example results on the MSVD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Distribution of the number and position of detected cuts on the M-VAD and MPII-MD datasets. The dashed green line in the right plot shows the distribution of cuts with respect to their relative position inside the video (where 0 represents the beginning and 1 represents the end of a video) obtained with an histogram with 100 bins, while the solid blue line is obtained by fitting a polynomial with degree 10 on the histogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Experiment results on the MPII-MD dataset.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://imagelab.ing.unimore.it/video_ captioning</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work has been partially founded by the project "Città educante" (CTN01 00034 393801) of the National Technological Cluster on Smart Communities (cofunded by the Italian Ministry of Education, University and Research -MIUR). We acknowledge the CINECA award under the ISCRA initiative, for the availability of high performance computing resources and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deep siamese network for scene detection in broadcast videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1199" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shot and scene detection via hierarchical clustering for re-using broadcast video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="801" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR W&amp;CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010)</title>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating natural-language video descriptions using text-mined knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical mid-level action elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4552" to="4560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a contextual multi-thread model for movie/tv scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="884" to="897" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
	<note>14th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Techniques for learning binary stochastic feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training recurrent networks by evolino</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gagliolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="757" to="779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical sequence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3562" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1218" to="1227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Using descriptive video services to create a large data source for video annotation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01070</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving lstm-based video description with linguistic knowledge mined from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh ; S. Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Conference on Empirical Methods in Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Translating videos to natural language using deep recurrent neural networks. North American Chapter of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
