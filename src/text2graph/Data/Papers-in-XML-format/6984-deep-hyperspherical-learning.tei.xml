<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Hyperspherical Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ming</forename><surname>Zhang</surname></persName>
							<email>ymzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingguo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Minnesota</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
							<email>tourzhao@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
							<email>lsong@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Hyperspherical Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss -a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, deep convolutional neural networks have led to significant breakthroughs on many vision problems such as image classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6]</ref>, segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1]</ref>, object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>, etc. While showing stronger representation power over many conventional hand-crafted features, CNNs often require a large amount of training data and face certain training difficulties such as overfitting, vanishing/exploding gradient, covariate shift, etc. The increasing depth of recently proposed CNN architectures have further aggravated the problems.</p><p>To address the challenges, regularization techniques such as dropout <ref type="bibr" target="#b8">[9]</ref> and orthogonality parameter constraints <ref type="bibr" target="#b20">[21]</ref> have been proposed. Batch normalization <ref type="bibr" target="#b7">[8]</ref> can also be viewed as an implicit regularization to the network, by normalizing each layer's output distribution. Recently, deep residual learning <ref type="bibr" target="#b5">[6]</ref> emerged as a promising way to overcome vanishing gradients in deep networks. However, <ref type="bibr" target="#b19">[20]</ref> pointed out that residual networks (ResNets) are essentially an exponential ensembles of shallow networks where they avoid the vanishing/exploding gradient problem but do not provide direct solutions. As a result, training an ultra-deep network still remains an open problem. Besides vanishing/exploding gradient, network optimization is also very sensitive to initialization. Finding better initializations is thus widely studied <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref>. In general, having a large parameter space is double-edged considering the benefit of representation power and the associated training difficulties. Therefore, proposing better learning frameworks to overcome such challenges remains important.</p><p>In this paper, we introduce a novel convolutional learning framework that can effectively alleviate training difficulties, while giving better performance over dot product based convolution. Our idea 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. is to project parameter learning onto unit hyperspheres, where layer activations only depend on the geodesic distance between kernels and input signals 1 instead of their inner products. To this end, we propose the SphereConv operator as the basic module for our network layers. We also propose softmax losses accordingly under such representation framework. Specifically, the proposed softmax losses supervise network learning by also taking the SphereConv activations from the last layer instead of inner products. Note that the geodesic distances on a unit hypersphere is the angles between inputs and kernels. Therefore, the learning objective is essentially a function of the input angles and we call it generalized angular softmax loss in this paper. The resulting architecture is the hyperspherical convolutional network (SphereNet), which is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Our key motivation to propose SphereNet is that angular information matters in convolutional representation learning. We argue this motivation from several aspects: training stability, training efficiency, and generalization power. SphereNet can also be viewed as an implicit regularization to the network by normalizing the activation distributions. The weight norm is no longer important since the entire network operates only on angles. And as a result, the 2 weight decay is also no longer needed in SphereNet. SphereConv to some extent also alleviates the covariate shift problem <ref type="bibr" target="#b7">[8]</ref>. The output of SphereConv operators are bounded from âˆ’1 to 1 (0 to 1 if considering ReLU), which makes the variance of each output also bounded.</p><p>Our second intuition is that angles preserve the most abundant discriminative information in convolutional learning. We gain such intuition from 2D Fourier transform, where an image is decomposed by the combination of a set of templates with magnitude and phase information in 2D frequency domain. If one reconstructs an image with original magnitudes and random phases, the resulting images are generally not recognizable. However, if one reconstructs the image with random magnitudes and original phases. The resulting images are still recognizable. It shows that the most important structural information in an image for visual recognition is encoded by phases. This fact inspires us to project the network learning into angular space. In terms of low-level information, SphereConv is able to preserve the shape, edge, texture and relative color. SphereConv can learn to selectively drop the color depth but preserve the RGB ratio. Thus the semantic information of an image is preserved.</p><p>SphereNet can also be viewed as a non-trivial generalization of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>. By proposing a loss that discriminatively supervises the network on a hypersphere, <ref type="bibr" target="#b10">[11]</ref> achieves state-of-the-art performance on face recognition. However, the rest of the network remains a conventional convolution network. In contrast, SphereNet not only generalizes the hyperspherical constraint to every layer, but also to different nonlinearity functions of input angles. Specifically, we propose three instances of SphereConv operators: linear, cosine and sigmoid. The sigmoid SphereConv is the most flexible one with a parameter controlling the shape of the angular function. As a simple extension to the sigmoid SphereConv, we also present a learnable SphereConv operator. Moreover, the proposed generalized angular softmax (GA-Softmax) loss naturaly generalizes the angular supervision in <ref type="bibr" target="#b10">[11]</ref> using the SphereConv operators. Additionally, the SphereConv can serve as a normalization method that is comparable to batch normalization, leading to an extension to spherical normalization (SphereNorm).</p><p>SphereNet can be easily applied to other network architectures such as GoogLeNet <ref type="bibr" target="#b18">[19]</ref>, VGG <ref type="bibr" target="#b17">[18]</ref> and ResNet <ref type="bibr" target="#b5">[6]</ref>. One simply needs to replace the convolutional operators and the loss functions with the proposed SphereConv operators and hyperspherical loss functions. In summary, SphereConv can be viewed as an alternative to the original convolution operators, and serves as a new measure of correlation. SphereNet may open up an interesting direction to explore the neural networks. We ask the question whether inner product based convolution operator is an optimal correlation measure for all tasks? Our answer to this question is likely to be "no".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hyperspherical Convolutional Operator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition</head><p>The convolutional operator in CNNs is simply a linear matrix multiplication, written as F(w, x) = w x + b F where w is a convolutional filter, x denotes a local patch from the bottom feature map and b F is the bias. The matrix multiplication here essentially computes the similarity between the local patch and the filter. Thus the standard convolution layer can be viewed as patch-wise matrix multiplication. Different from the standard convolutional operator, the hyperspherical convolutional (SphereConv) operator computes the similarity on a hypersphere and is defined as:</p><formula xml:id="formula_0">Fs(w, x) = g(Î¸ (w,x) ) + bF s ,<label>(1)</label></formula><p>where Î¸ (w,x) is the angle between the kernel parameter w and the local patch x. g(Î¸ (w,x) ) indicates a function of Î¸ (w,x) (usually a monotonically decreasing function), and b Fs is the bias. To simplify analysis and discussion, the bias terms are usually left out. The angle Î¸ (w,x) can be interpreted as the geodesic distance (arc length) between w and x on a unit hypersphere. In contrast to the convolutional operator that works in the entire space, SphereConv only focuses on the angles between local patches and the filters, and therefore operates on the hypersphere space. In this paper, we present three specific instances of the SphereConv Operator. To facilitate the computation, we constrain the output of SphereConv operators to [âˆ’1, 1] (although it is not a necessary requirement). Linear SphereConv. In linear SphereConv operator, g is a linear function of Î¸ (w,x) , with the form:</p><formula xml:id="formula_1">g(Î¸ (w,x) ) = aÎ¸ (w,x) + b,<label>(2)</label></formula><p>where a and b are parameters for the linear SphereConv operator. In order to constrain the output range to [0, 1] while Î¸ (w,x) âˆˆ [0, Ï€], we use a = âˆ’ 2 Ï€ and b = 1 (not necessarily optimal design).  Cosine SphereConv. The cosine SphereConv operator is a nonlinear function of Î¸ (w,x) , with its g being the form of</p><formula xml:id="formula_2">g(Î¸ (w,x) ) = cos(Î¸ (w,x) ),<label>(3)</label></formula><p>which can be reformulated as</p><formula xml:id="formula_3">w T x w 2 x 2</formula><p>. Therefore, it can be viewed as a doubly normalized convolutional operator, which bridges the SphereConv operator and convolutional operator.</p><p>Sigmoid SphereConv. The Sigmoid SphereConv operator is derived from the Sigmoid function and its g can be written as</p><formula xml:id="formula_4">g(Î¸ (w,x) ) = 1 + exp(âˆ’ Ï€ 2k ) 1 âˆ’ exp(âˆ’ Ï€ 2k ) Â· 1 âˆ’ exp Î¸ (w,x) k âˆ’ Ï€ 2k 1 + exp Î¸ (w,x) k âˆ’ Ï€ 2k ,<label>(4)</label></formula><p>where k &gt; 0 is the parameter that controls the curvature of the function. While k is close to 0, g(Î¸ (w,x) ) will approximate the step function. While k becomes larger, g(Î¸ (w,x) ) is more like a linear function, i.e., the linear SphereConv operator. Sigmoid SphereConv is one instance of the parametric SphereConv family. With more parameters being introduced, the parametric SphereConv can have richer representation power. To increase the flexibility of the parametric SphereConv, we will discuss the case where these parameters can be jointly learned via back-prop later in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimization</head><p>The optimization of the SphereConv operators is nearly the same as the convolutional operator and also follows the standard back-propagation. Using the chain rule, we have the gradient of the SphereConv with respect to the weights and the feature input:</p><formula xml:id="formula_5">âˆ‚g(Î¸ (w,x) ) âˆ‚w = âˆ‚g(Î¸ (w,x) ) âˆ‚Î¸ (w,x) Â· âˆ‚Î¸ (w,x) âˆ‚w , âˆ‚g(Î¸ (w,x) ) âˆ‚x = âˆ‚g(Î¸ (w,x) ) âˆ‚Î¸ (w,x) Â· âˆ‚Î¸ (w,x) âˆ‚x .<label>(5)</label></formula><p>For different SphereConv operators, both</p><formula xml:id="formula_6">âˆ‚Î¸ (w,x) âˆ‚w and âˆ‚Î¸ (w,x) âˆ‚x</formula><p>are the same, so the only difference lies in the</p><formula xml:id="formula_7">âˆ‚g(Î¸ (w,x) ) âˆ‚Î¸ (w,x)</formula><p>part. For</p><formula xml:id="formula_8">âˆ‚Î¸ (w,x)</formula><p>âˆ‚w , we have</p><formula xml:id="formula_9">âˆ‚Î¸ (w,x) âˆ‚w = âˆ‚ arccos w T x w 2 x 2 âˆ‚w , âˆ‚Î¸ (w,x) âˆ‚x = âˆ‚ arccos w T x w 2 x 2 âˆ‚x ,<label>(6)</label></formula><p>which are straightforward to compute and therefore neglected here. Because</p><formula xml:id="formula_10">âˆ‚g(Î¸ (w,x) ) âˆ‚Î¸ (w,x)</formula><p>for the linear SphereConv, the cosine SphereConv and the Sigmoid SphereConv are a, âˆ’ sin(Î¸ (w,x) ) and <ref type="bibr" target="#b1">2</ref> respectively, all these partial gradients can be easily computed.</p><formula xml:id="formula_11">âˆ’2 exp(Î¸ (w,x) /kâˆ’Ï€/2k) k(1+exp(Î¸ (w,x) /kâˆ’Ï€/2k))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Theoretical Insights</head><p>We provide a fundamental analysis for the cosine SphereConv operator in the case of linear neural network to justify that the SphereConv operator can improve the conditioning of the problem. In specific, we consider one layer of linear neural network, where the observation is F = U * V * (ignore the bias), U * âˆˆ R nÃ—k is the weight, and V * âˆˆ R mÃ—k is the input that embeds weights from previous layers. Without loss of generality, we assume the columns satisfying U i,: 2 = V j,: 2 = 1 for all i = 1, . . . , n and j = 1, . . . , m, and consider</p><formula xml:id="formula_12">min U âˆˆR nÃ—k ,V âˆˆR mÃ—k G(U , V ) = 1 2 F âˆ’ U V 2 F .<label>(7)</label></formula><p>This is closely related with the matrix factorization and <ref type="formula" target="#formula_12">(7)</ref> can be also viewed as the expected version for the matrix sensing problem <ref type="bibr" target="#b9">[10]</ref>. The following lemma demonstrates a critical scaling issue of <ref type="formula" target="#formula_12">(7)</ref> for U and V that significantly deteriorate the conditioning without changing the objective of <ref type="formula" target="#formula_12">(7)</ref>. Lemma 1. Consider a pair of global optimal points U ,</p><formula xml:id="formula_13">V satisfying F = U V and Tr(V V âŠ— I n ) â‰¤ Tr(U U âŠ— I m ). For any real c &gt; 1, let U = cU and V = V /c, then we have Îº(âˆ‡ 2 G( U , V )) = â„¦(c 2 Îº(âˆ‡ 2 G(U , V ))), where Îº = Î»max</formula><p>Î»min is the restricted condition number with Î» max being the largest eigenvalue and Î» min being the smallest nonzero eigenvalue.</p><p>Lemma 1 implies that the conditioning of the problem <ref type="formula" target="#formula_12">(7)</ref> at a unbalanced global optimum scaled by a constant c is â„¦(c 2 ) times larger than the conditioning of the problem at a balanced global optimum. Note that Î» min = 0 may happen, thus we consider the restricted condition here. Similar results hold beyond global optima. This is an undesired geometric structure, which further leads to slow and unstable optimization procedures, e.g., using stochastic gradient descent (SGD). This motivates us to consider the SphereConv operator discussed above, which is equivalent to projecting data onto the hypersphere and leads to a better conditioned problem.</p><p>Next, we consider our proposed cosine SphereConv operator for one-layer of the linear neural network. Based on our previous discussion on SphereConv, we consider an equivalent problem:</p><formula xml:id="formula_14">min U âˆˆR nÃ—k ,V âˆˆR mÃ—k GS(U , V ) = 1 2 F âˆ’ D U U V D V 2 F ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_15">D U = diag 1 U1,: 2 , . . . , 1 Un,: 2 âˆˆ R nÃ—n and D V = diag 1 V1,: 2 , . . . , 1 Vm,: 2 âˆˆ R</formula><p>mÃ—m are diagonal matrices. We provide an analogous result to Lemma 1 for (8) .</p><p>Lemma 2. For any real c &gt; 1, let U = cU and V = V /c, then we have</p><formula xml:id="formula_16">Î» i (âˆ‡ 2 G S ( U , V )) = Î» i (âˆ‡ 2 G S (U , V )) for all i âˆˆ [(n + m)k] = {1, 2, . . . , (n + m)k} and Îº(âˆ‡ 2 G( U , V )) = Îº(âˆ‡ 2 G(U , V ))</formula><p>, where Îº is defined as in Lemma 1.</p><p>We have from Lemma 2 that the issue of increasing condition caused by the scaling is eliminated by the SphereConv operator in the entire parameter space. This enhances the geometric structure over (7), which further results in improved convergence of optimization procedures. If we extend the result from one layer to multiple layers, the scaling issue propagates. Roughly speaking, when we train N layers, in the worst case, the conditioning of the problem can be c N times worse with a scaling factor c &gt; 1. The analysis is similar to the one layer case, but the computation of the Hessian matrix and associated eigenvalues are much more complicated. Though our analysis is elementary, we provide an important insight and a straightforward illustration of the advantage for using the SphereConv operator. The extension to more general cases, e..g, using nonlinear activation function (e.g., ReLU), requires much more sophisticated analysis to bound the eigenvalues of Hessian for objectives, which is deferred to future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discussion</head><p>Comparison to convolutional operators. Convolutional operators compute the inner product between the kernels and the local patches, while the SphereConv operators compute a function of the angle between the kernels and local patches. If we normalize the convolutional operator in terms of both w and x, then the normalized convolutional operator is equivalent to the cosine SphereConv operator. Essentially, they use different metric spaces. Interestingly, SphereConv operators can also be interpreted as a function of the Geodesic distance on a unit hypersphere. Extension to fully connected layers. Because the fully connected layers can be viewed as a special convolution layer with the kernel size equal to the input feature map, the SphereConv operators could be easily generalized to the fully connected layers. It also indicates that SphereConv operators could be used not only to deep CNNs, but also to linear models like logistic regression, SVM, etc.</p><p>Network Regularization. Because the norm of weights is no longer crucial, we stop using the 2 weight decay to regularize the network. SphereNets are learned on hyperspheres, so we regularize the network based on angles instead of norms. To avoid redundant kernels, we want the kernels uniformly spaced around the hypersphere, but it is difficult to formulate such constraints. As a tradeoff, we encourage the orthogonality. Given a set of kernels W where the i-th column W i is the weights of the i-th kernel, the network will also minimize W W âˆ’ I 2 F where I is an identity matrix. Determining the optimal SphereConv. In practice, we could treat different types of SphereConv as a hyperparameter and use the cross validation to determine which SphereConv is the most suitable one. For sigmoid SphereConv, we could also use the cross validation to determine its hyperparameter k. In general, we need to specify a SphereConv operator before using it, but prefixing a SphereConv may not be an optimal choice (even using cross validation). What if we treat the hyperparameter k in sigmoid SphereConv as a learnable parameter and use the back-prop to learn it? Following this idea, we further extend sigmoid SphereConv to a learnable SphereConv in the next subsection. SphereConv as normalization. Because SphereConv could partially address the covariate shift, it could also serve as a normalization method similar to batch normalization. Differently, SphereConv normalizes the network in terms of feature map and kernel weights, while batch normalization is for the mini-batches. Thus they do not contradict with each other and can be used simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Extension: Learnable SphereConv and SphereNorm</head><p>Learnable SphereConv. It is a natrual idea to replace the current prefixed SphereConv with a learnable one. There will be plenty of parametrization choices for the SphereConv to be learnable, and we present a very simple learnable SphereConv operator based on the sigmoid SphereConv. Because the sigmoid SphereConv has a hyperparameter k, we could treat it as a learnable parameter that can be updated by back-prop. In back-prop, k is updated using k t+1 = k t + Î· âˆ‚L âˆ‚k where t denotes the current iteration index and âˆ‚L âˆ‚k can be easily computed by the chain rule. Usually, we also require k to be positive. The learning of k is in fact similar to the parameter learning in PReLU <ref type="bibr" target="#b4">[5]</ref>. SphereNorm: hyperspherical learning as a normalization method. Similar to batch normalization (BatchNorm), we note that the hyperspherical learning can also be viewed as a way of normalization, because SphereConv constrain the output value in [âˆ’1, 1] ([0, 1] after ReLU). Different from BatchNorm, SphereNorm normalizes the network based on spatial information and the weights, so it has nothing to do with the mini-batch statistic. Because SphereNorm normalize both the input and weights, it could avoid covariate shift due to large weights and large inputs while BatchNorm could only prevent covariate shift caused by the inputs. In such sense, it will work better than BatchNorm when the batch size is small. Besides, SphereConv is more flexible in terms of design choices (e.g. linear, cosine, and sigmoid) and each may lead to different advantages.</p><p>Similar to BatchNorm, we could use a rescaling strategy for the SphereNorm. Specifically, we rescale the output of SphereConv via Î²F s (w, x) + Î³ where Î² and Î³ are learned by back-prop (similar to BatchNorm, the rescaling parameters can be either learned or prefixed). In fact, SphereNorm does not contradict with the BatchNorm at all and can be used simultaneously with BatchNorm. Interestingly, we find using both is empirically better than using either one alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Objective on Hyperspheres</head><p>For learning on hyperspheres, we can either use the conventional loss function such as softmax loss, or use some loss functions that are tailored for the SphereConv operators. We present some possible choices for these tailored loss functions. Weight-normalized Softmax Loss. The input feature and its label are denoted as x i and y i , respectively. The original softmax loss can be written where N is the number of training samples and f j is the score of the j-th class (j âˆˆ [1, K], K is the number of classes). The class score vector f is usually the output of a fully connected layer W , so we have f j = W j x i + b j and f yi = W yi x i + b yi in which x i , W j , and W yi are the i-th training sample, the j-th and y i -th column of W respectively. We can rewrite L i as</p><formula xml:id="formula_17">as L = 1 N i L i = 1 N i âˆ’ log</formula><formula xml:id="formula_18">Li = âˆ’ log e W y i x i +by i j e W j x i +b j = âˆ’ log e Wy i x i cos(Î¸ y i ,i )+by i j e W j x i cos(Î¸ j,i )+b j ,<label>(9)</label></formula><p>where Î¸ j,i (0 â‰¤ Î¸ j,i â‰¤ Ï€) is the angle between vector W j and x i . The decision boundary of the original softmax loss is determined by the vector f . Specifically in the binary-class case, the decision boundary of the softmax loss is</p><formula xml:id="formula_19">W 1 x + b 1 = W 2 x + b 2 .</formula><p>Considering the intuition of the SphereConv operators, we want to make the decision boundary only depend on the angles. To this end, we normalize the weights ( W j = 1) and zero out the biases (b j = 0), following the intuition in <ref type="bibr" target="#b10">[11]</ref> (sometimes we could keep the biases while data is imbalanced). The decision boundary becomes x cos(Î¸ 1 ) = x cos(Î¸ 2 ). Similar to SphereConv, we could generalize the decision boundary to x g(Î¸ 1 ) = x g(Î¸ 2 ), so the weight-normalized softmax (W-Softmax) loss can be written as</p><formula xml:id="formula_20">Li = âˆ’ log e x i g(Î¸ y i ,i ) j e x i g(Î¸ j,i ) ,<label>(10)</label></formula><p>where g(Â·) can take the form of linear SphereConv, cosine SphereConv, or sigmoid SphereConv. Thus we also term these three difference weight-normalized loss functions as linear W-Softmax loss, cosine W-Softmax loss, and sigmoid W-Softmax loss, respectively. Generalized Angular Softmax Loss. Inspired by <ref type="bibr" target="#b10">[11]</ref>, we use a multiplicative parameter m to impose margins on hyperspheres. We propose a generalized angular softmax (GA-Softmax) loss which extends the W-Softmax loss to a loss function that favors large angular margin feature distribution. In general, the GA-Softmax loss is formulated as</p><formula xml:id="formula_21">Li = âˆ’ log e x i g(mÎ¸ y i ,i ) e x i g(mÎ¸ y i ,i ) + j =y i e x i g(Î¸ j,i ) ,<label>(11)</label></formula><p>where g(Â·) could also have the linear, cosine and sigmoid form, similar to the W-Softmax loss. We can see A-Softmax loss <ref type="bibr" target="#b10">[11]</ref> is exactly the cosine GA-Softmax loss and W-Softmax loss is the special case (m = 1) of GA-Sofmtax loss. Note that we usually require Î¸ j,i âˆˆ [0, Ï€ m ], because cos(Î¸ j,i ) is only monotonically decreasing in [0, Ï€]. To address this, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> construct a monotonically decreasing function recursively using the [0, Ï€ m ] part of cos(mÎ¸ j,i ). Although it indeed partially addressed the issue, it may introduce a number of saddle points (w.r.t. W ) in the loss surfaces. Originally, âˆ‚g âˆ‚Î¸ will be close to 0 only when Î¸ is close to 0 and Ï€. However, in L-Softmax <ref type="bibr" target="#b11">[12]</ref> or A-Softmax (cosine GA-Softmax), it is not the case. âˆ‚g âˆ‚Î¸ will be 0 when Î¸ = kÎ¸ m , k = 0, Â· Â· Â· , m. It will possibly cause instability in training. The sigmoid GA-Softmax loss also has similar issues. However, if we use the linear GA-Softmax loss, this problem will be automatically solved and the training will possibly become more stable in practice. There will also be a lot of choices of g(Â·) to design a specific GA-Sofmtax loss, and each one has different optimization dynamics. The optimal one may depend on the task itself (e.g. cosine GA-Softmax has been shown effective in deep face recognition <ref type="bibr" target="#b10">[11]</ref>). Discussion of Sphere-normalized Softmax Loss. We have also considered the sphere-normalized softmax loss (S-Softmax), which simultaneously normalizes the weights (W j ) and the feature x. It seems to be a more natural choice than W-Softmax for the proposed SphereConv and makes the entire framework more unified. In fact, we have tried this and the empirical results are not that good, because the optimization seems to become very difficult. If we use the S-Softmax loss to train a network from scratch, we can not get reasonable results without using extra tricks, which is the reason we do not use it in this paper. For completeness, we give some discussions here. Normally, it is very difficult to make the S-Softmax loss value to be small enough, because we normalize the features to unit hypersphere. To make this loss work, we need to either normalize the feature to a value much larger than 1 (hypersphere with large radius) and then tune the learning rate or first train the network with the softmax loss from scratch and then use the S-Softmax loss for finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We will first perform comprehensive ablation study and exploratory experiments for the proposed SphereNets, and then evaluate the SphereNets on image classification. For the image classification task, we perform experiments on CIFAR10 (only with random left-right flipping), CIFAR10+ (with full data augmentation), CIFAR100 and large-scale Imagenet 2012 datasets <ref type="bibr" target="#b16">[17]</ref>. General Settings. For CIFAR10, CIFAR10+ and CIFAR100, we follow the same settings from <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. For Imagenet 2012 dataset, we mostly follow the settings in <ref type="bibr" target="#b8">[9]</ref>. We attach more details in Appendix B. For fairness, batch normalization and ReLU are used in all methods if not specified. All the comparisons are made to be fair. Compared CNNs have the same architecture with SphereNets. Training. Appendix A gives the network details. For CIFAR-10 and CIFAR-100, we use the ADAM, starting with the learning rate 0.001. The batch size is 128 if not specified. The learning rate is divided by 10 at 34K, 54K iterations and the training stops at 64K. For both A-Softmax and GA-Softmax loss, we use m = 4. For Imagenet-2012, we use the SGD with momentum 0.9. The learning rate starts with 0.1, and is divided by 10 at 200K and 375K iterations. The training stops at 550K iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study and Exploratory Experiments</head><p>We perform comprehensive Ablation and exploratory study on the SphereNet and evaluate every component individually in order to analyze its advantages. We use the 9-layer CNN as default (if not specified) and perform the image classification on CIFAR-10 without any data augmentation. Comparison of different loss functions. We first evaluate all the SphereConv operators with different loss functions. All the compared SphereConv operators use the 9-layer CNN architecture in the experiment. From the results in <ref type="table">Table 1</ref>, one can observe that the SphereConv operators consistently outperforms the original convolutional operator. For the compared loss functions except A-Softmax and GA-Softmax, the effect on accuracy seems to less crucial than the SphereConv operators, but sigmoid W-Softmax is more flexible and thus works slightly better than the others. The sigmoid SphereConv operators with a suitably chosen parameter also works better than the others. Note that, W-Softmax loss is in fact comparable to the original softmax loss, because our SphereNet optimizes angles and the W-Softmax is derived from the original softmax loss. Therefore, it is fair to compare the SphereNet with W-Softmax and CNN with softmax loss. From <ref type="table">Table 1</ref>, we can see SphereConv operators are consistently better than the covolutional operators. While we use a large-margin loss function like the A-Softmax <ref type="bibr" target="#b10">[11]</ref> and the proposed GA-Softmax, the accuracy can be further boosted. One may notice that A-Softmax is actually cosine GA-Softmax. The superior performance of A-Softmax with SphereNet shows that our architecture is more suitable for the learning of angular loss. Moreover, our proposed large-margin loss (linear GA-Softmax) performs the best among all these compared loss functions.</p><p>Comparison of different network architectures. We are also interested in how our SphereConv operators work in different architectures. We evaluate all the proposed SphereConv operators with the same architecture of different layers and a totally different architecture (ResNet). Our baseline CNN architecture follows the design of VGG network <ref type="bibr" target="#b17">[18]</ref> only with different convolutional layers. For fair comparison, we use cosine W-Softmax for all SphereConv operators and original softmax for original convolution operators. From the results in <ref type="table" target="#tab_2">Table 2</ref>, one can see that SphereNets greatly outperforms the CNN baselines, usually with more than 1% improvement. While applied to ResNet, our SphereConv operators also work better than the baseline. Note that, we use the similar ResNet architecture from the CIFAR-10 experiment in <ref type="bibr" target="#b5">[6]</ref>. We do not use data augmentation for CIFAR-10 in this experiment, so the ResNet accuracy is much lower than the reported one in <ref type="bibr" target="#b5">[6]</ref>. Our results on different network architectures show consistent and significant improvement over CNNs.   Comparison of different width (number of filters). We evaluate the SphereNet with different number of filters. <ref type="figure" target="#fig_4">Fig. 3(c)</ref> shows the convergence of different width of SphereNets. 16/32/48 means conv1.x, conv2.x and conv3.x have 16, 32 and 48 filters, respectively. One could observe that while the number of filters are small, SphereNet performs similarly to CNNs (slightly worse). However, while we increase the number of filters, the final accuracy will surpass the CNN baseline even faster and more stable convergence performance. With large width, we find that SphereNets perform consistently better than CNN baselines, showing that SphereNets can make better use of the width. Learning without ReLU. We notice that SphereConv operators are no longer a matrix multiplication, so it is essentially a non-linear function. Because the SphereConv operators already introduce certain non-linearity to the network, we evaluate how much gain will such non-linearity bring. Therefore, we remove the ReLU activation and compare our SphereNet with the CNNs without ReLU. The results are given in <ref type="table" target="#tab_3">Table 3</ref>. All the compared methods use 18-layer CNNs (with BatchNorm). Although removing ReLU greatly reduces the classification accuracy, our SphereNet still outperforms the CNN without ReLU by a significant margin, showing its rich non-linearity and representation power. Convergence. One of the most significant advantages of SphereNet is its training stability and convergence speed. We evaluate the convergence with two different architectures: CNN-9 and ResNet-32. For fair comparison, we use the original softmax loss for all compared methods (including SphereNets). ADAM is used for the stochastic optimization and the learning rate is the same for all networks. From <ref type="figure" target="#fig_4">Fig. 3(a)</ref>, the SphereResNet converges significantly faster than the original ResNet baseline in both CIFAR-10 and CIFAR-10+ and the final accuracy are also higher than the baselines. In <ref type="figure" target="#fig_4">Fig. 3(b)</ref>, we evaluate the SphereNet with and without orthogonality constraints on kernel weights. With the same network architecture, SphereNet also converges much faster and performs better than the baselines. The orthogonality constraints also can bring performance gains in some cases. Generally from <ref type="figure" target="#fig_4">Fig. 3</ref>, one could also observe that the SphereNet converges fast and very stably in every case while the CNN baseline fluctuates in a relative wide range.</p><p>Optimizing ultra-deep networks. Partially because of the alleviation of the covariate shift problem and the improvement of conditioning, our SphereNet is able to optimize ultra-deep neural networks without using residual units or any form of shortcuts. For SphereNets, we use the cosine SphereConv operator with the cosine W-Softmax loss. We directly optimize a very deep plain network with 69 stacked convolutional layers. From <ref type="figure" target="#fig_4">Fig. 3(d)</ref>, one can see that the convergence of SphereNet is much easier than the CNN baseline and the SphereNet is able to achieve nearly 90% final accuracy. Although the learnable SphereConv is not a main theme of this paper, we still run some preliminary evaluations on it. For the proposed learnable sigmoid SphereConv, we learn the parameter k independently for each filter. It is also trivial to learn it in a layer-shared or network-shared fashsion. With the same 9-layer architecture used in Section 4.2, the learnable SphereConv (with cosine W-Softmax loss) achieves 91.64% on CIFAR-10 (without full data augmentation), while the best sigmoid SphereConv (with cosine W-Softmax loss) achieves 91.22%. In <ref type="figure" target="#fig_5">Fig. 4</ref>, we also plot the frequency histogram of k in Conv1.1 (64 filters), Conv2.1 (96 filters) and Conv3.1 (128 filters) of the final learned SphereNet. <ref type="figure" target="#fig_5">From Fig. 4</ref>, we observe that each layer learns different distribution of k. The first convolutional layer (Conv1.1) tends to uniformly distribute k into a large range of values from 0 to 1, potentially extracting information from all levels of angular similarity. The fourth convolutional layer (Conv2.1) tends to learn more concentrated distribution of k than Conv1.1, while the seventh convolutional layer (Conv3.1) learns highly concentrated distribution of k which is centered around 0.8. Note that, we initialize all k with a constant 0.5 and learn them with the back-prop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Preliminary Study towards Learnable SphereConv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of SphereNorm</head><p>From Section 4.2, we could clearly see the convergence advantage of SphereNets. In general, we can view the SphereConv as a normalization method (comparable to batch normalization) that can be applied to all kinds of networks. This section evaluates the challenging scenarios where the minibatch size is small (results under 128 batch size could be found in Section 4.2) and we use the same   We first evaluate the SphereNet in a classic image classification task. We use the CIFAR-10+ and CI-FAR100 datasets and perform random flip (both horizontal and vertical) and random crop as data augmentation (CIFAR-10 with full data augmentation is denoted as CIFAR-10+). We use the ResNet-32 as a baseline architecture.  <ref type="table" target="#tab_5">Table 4</ref>, we could see the SphereNet outperforms a lot of current state-of-the-art methods and is even comparable to the ResNet-1001 which is far deeper than ours. This experiment further validates our idea that learning on a hyperspheres constrains the parameter space to a more semantic and label-related one. We evaluate SphereNets on large-scale Imagenet-2012 dataset. We only use the minimum data augmentation strategy in the experiment (details are in Appendix B). For the ResNet-18 baseline and SphereResNet-18, we use the same filter numbers in each layer. We develop two types of SphereResNet-18, termed as v1 and v2 respectively. In SphereResNet-18-v2, we do not use SphereConv in the 1 Ã— 1 shortcut convolutions which are used to match the number of channels. In SphereResNet-18-v1, we use SphereConv in the 1 Ã— 1 shortcut convolutions. <ref type="figure" target="#fig_8">Fig. 6</ref> shows the single crop validation error over iterations. One could observe that both SphereResNets converge much faster than the ResNet baseline, while SphereResNet-18-v1 converges the fastest but yields a slightly worse yet comparable accuracy. SphereResNet-18-v2 not only converges faster than ResNet-18, but it also shows slightly better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Future Work</head><p>Our work still has some limitations: (1) SphereNets have large performance gain while the network is wide enough. If the network is not wide enough, SphereNets still converge much faster but yield slightly worse (still comparable) recognition accuracy. <ref type="formula" target="#formula_1">(2)</ref> The computation complexity of each neuron is slightly higher than the CNNs. (3) SphereConvs are still mostly prefixed. Possible future work includes designing/learning a better SphereConv, efficiently computing the angles to reduce computation complexity, applications to the tasks that require fast convergence (e.g. reinforcement learning and recurrent neural networks), better angular regularization to replace orthogonality, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Deep hyperspherical convolutional network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SphereConv operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Testing accuracy over iterations. (a) ResNet vs. SphereResNet. (b) Plain CNN vs. plain SphereNet. (c) Different width of SphereNet. (d) Ultra-deep plain CNN vs. ultra-deep plain SphereNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Frequency histogram of k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence under different mini-batch size on CIFAR-10 dataset (Same setting as Section 4.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>For the SphereNet of the same architecture, we evaluate sigmoid SphereConv operator (k = 0.3) with sigmoid W-Softmax (k = 0.3) loss (S-SW), lin- ear SphereConv operator with linear W-Softmax loss (L-LW), cosine SphereConv operator with cosine W-Softmax loss (C-CW) and sigmoid SphereConv operator (k = 0.3) with GA-Softmax loss (S-G). In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Validation error (%) on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) with different network architectures.</figDesc><table>SphereConv Operator 
Acc. (%) 

Sigmoid (0.1) 
86.29 
Sigmoid (0.3) 
85.67 
Sigmoid (0.7) 
85.51 
Linear 
85.34 
Cosine 
85.25 
CNN w/o ReLU 
80.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Acc. w/o ReLU.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>9 -</head><label>9</label><figDesc>layer CNN as in Section 4.2. To be simple, we use the cosine SphereConv as SphereNorm. The softmax loss is used in both CNNs and SphereNets. From Fig. 5, we could observe that SphereNorm achieves the final accuracy similar to BatchNorm, but SphereNorm converges faster and more stably. SphereNorm plus the orthogonal constraint helps convergence a little bit and rescaled SphereNorm does not seem to work well. While BatchNorm and SphereNorm are used together, we obtain the fastest convergence and the highest final accuracy, showing excellent compatibility of SphereNorm. 4.5 Image Classification on CIFAR-10+ and CIFAR-100</figDesc><table>Method 
CIFAR-10+ 
CIFAR-100 

ELU [2] 
94.16 
72.34 
FitResNet (LSUV) [14] 
93.45 
65.72 
ResNet-1001 [7] 
95.38 
77.29 

Baseline ResNet-32 (softmax) 
93.26 
72.85 
SphereResNet-32 (S-SW) 
94.47 
76.02 
SphereResNet-32 (L-LW) 
94.33 
75.62 
SphereResNet-32 (C-CW) 
94.64 
74.92 
SphereResNet-32 (S-G) 
95.01 
76.39 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Acc. (%) on CIFAR-10+ &amp; CIFAR-100.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Without loss of generality, we study CNNs here, but our method is generalizable to any other neural nets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Zhen Liu (Georgia Tech) for helping with the experiments and providing suggestions. This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS. Xingguo Li is supported by doctoral dissertation fellowship from University of Minnesota. Yan-Ming Zhang is supported by the National Natural Science Foundation of China under Grant 61773376.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-ArnÃ©</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarvis</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.09296</idno>
		<title level="m">Symmetry, saddle points, and global geometry of nonconvex matrix factorization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eigenvalue perturbation bounds for hermitian block tridiagonal matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Nakatsukasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01827</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
