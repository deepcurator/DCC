<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pohlen</surname></persName>
							<email>tobias.pohlen@rwth-aachen.dehermans</email>
							<affiliation key="aff0">
								<orgName type="institution">Visual Computing Institute RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Visual Computing Institute RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Visual Computing Institute RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Visual Computing Institute RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have seen an increasing interest in self driving cars and in driver assistance systems. A crucial aspect of autonomous driving is to acquire a comprehensive understanding of the surroundings in which a car is moving. Semantic image segmentation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b32">33]</ref>, the task of assigning a set of predefined class labels to image pixels, is an important tool for modeling the complex relationships of the semantic entities usually found in street scenes, such as cars, pedestrians, road, or sidewalks. In automotive scenarios it is used in various ways, e.g. as a pre-processing step to discard image regions that are unlikely to contain objects of . Example output and the abstract structure of our fullresolution residual network. The network has two processing streams. The residual stream (blue) stays at the full image resolution, the pooling stream (red) undergoes a sequence of pooling and unpooling operations. The two processing streams are coupled using full-resolution residual units (FRRUs).</p><p>interest <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15]</ref>, to improve object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b57">58]</ref>, or in combination with 3D scene geometry <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35]</ref>. Many of those applications require precise region boundaries <ref type="bibr" target="#b19">[20]</ref>. In this work, we therefore pursue the goal of achieving high-quality semantic segmentation with precise boundary adherence.</p><p>Current state-of-the-art approaches for image segmentation all employ some form of fully convolutional network (FCN) <ref type="bibr" target="#b37">[38]</ref> that takes the image as input and outputs a probability map for each class. Many papers rely on network architectures that have already been proven successful for image classification such as variants of the ResNet <ref type="bibr" target="#b24">[25]</ref> or the VGG architecture <ref type="bibr" target="#b49">[50]</ref>. Starting from pre-trained nets, where a large number of weights for the target task can be pre-set by an auxiliary classification task, reduces training time and often yields superior performance compared to training a network from scratch using the (possibly limited amount of) data of the target application. However, a main limitation of using such pre-trained networks is that they severely restrict the design space of novel approaches, since new network elements such as batch normalization <ref type="bibr" target="#b26">[27]</ref> or new activation functions often cannot be added into an existing architecture.</p><p>When performing semantic segmentation using FCNs, a common strategy is to successively reduce the spatial size of the feature maps using pooling operations or strided convolutions. This is done for two reasons: First, it significantly increases the size of the receptive field and second, it makes the network robust against small translations in the image. While pooling operations are highly desirable for recognizing objects in images, they significantly deteriorate localization performance of the networks when applied to semantic image segmentation. Several approaches exist to overcome this problem and obtain pixel-accurate segmentations. Noh et al. <ref type="bibr" target="#b40">[41]</ref> learn a mirrored VGG network as a decoder, Yu and Koltun <ref type="bibr" target="#b54">[55]</ref> introduce dilated convolutions to reduce the pooling factor of their pre-trained network. Ghiasi et al. <ref type="bibr" target="#b19">[20]</ref> use multi-scale predictions to successively improve their boundary adherence. An alternative approach used by several methods is to apply post-processing steps such as CRF-smoothing <ref type="bibr" target="#b29">[30]</ref>.</p><p>In this paper, we propose a novel network architecture that achieves state-of-the-art segmentation performance without the need for additional post-processing steps and without the limitations imposed by pre-trained architectures. Our proposed ResNet-like architecture unites strong recognition performance with precise localization capabilities by combining two distinct processing streams. One stream undergoes a sequence of pooling operations and is responsible for understanding large-scale relationships of image elements; the other stream carries feature maps at the full image resolution, resulting in precise boundary adherence. This idea is visualized in <ref type="figure" target="#fig_0">Figure 1</ref>, where the two processing streams are shown in blue and red. The blue residual stream reflects the high-resolution stream. It can be combined with classical residual units (left and right), as well as with our new full-resolution residual units (FRRU). The FRRUs from the red pooling lane act as residual units for the blue stream, but also undergo pooling operations and carry high-level information through the network. This results in a network that successively combines and computes features at two resolutions. This paper makes the following contributions: (i) We propose a novel network architecture geared towards precise semantic segmentation in street scenes which is not limited to pre-trained architectures and achieves state-ofthe-art results. (ii) We propose to use two processing streams to realize strong recognition and strong localization performance: One stream undergoes a sequence of pooling operations while the other stream stays at the full image resolution. (iii) In order to foster further research in this area, we published our code and the trained models on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The dramatic performance improvements from using CNNs for semantic segmentation have brought about an increasing demand for such algorithms in the context of autonomous driving scenarios. As a large amount of annotated data is crucial in order to train such deep networks, multiple new datasets have been released to encourage further research in this area, including Synthia <ref type="bibr" target="#b44">[45]</ref>, Virtual KITTI <ref type="bibr" target="#b17">[18]</ref>, and Cityscapes <ref type="bibr" target="#b10">[11]</ref>. In this work, we focus on Cityscapes, a recent large-scale dataset consisting of real-world imagery with well-curated annotations. Given their success, we will constrain our literature review to deep learning based semantic segmentation approaches and deep learning network architectures.</p><p>Semantic Segmentation Approaches. Over the last years, the most successful semantic segmentation approaches have been based on convolutional neural networks (CNNs). Early approaches constrained their output to a bottom-up segmentation followed by a CNN based region classification <ref type="bibr" target="#b53">[54]</ref>. Rather than classifying entire regions in the first place, the approach by Farabet et al. performs pixel-wise classification using CNN features originating from multiple scales, followed by aggregation of these noisy pixel predictions over superpixel regions <ref type="bibr" target="#b15">[16]</ref>.</p><p>The introduction of so-called fully convolutional networks (FCNs) for semantic image segmentation by Long et al. <ref type="bibr" target="#b37">[38]</ref> opened a wide range of semantic segmentation research using end-to-end training <ref type="bibr" target="#b12">[13]</ref>. Long et al. further reformulated the popular VGG architecture <ref type="bibr" target="#b49">[50]</ref> as a fully convolutional network (FCN), enabling the use of pretrained models for this architecture. To improve segmentation performance at object boundaries, skip connections were added which allow information to propagate directly from early, high-resolution layers to deeper layers.</p><p>Pooling layers in FCNs fulfill a crucial role in order to increase the receptive field size of later units and with it the classification performance. However, they have the downside that the resulting network outputs are at a lower resolution. To overcome this, various strategies have been proposed. Some approaches extract features from intermediate layers via some sort of skip connections <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7]</ref>. Noh et al. propose an encoder/decoder network <ref type="bibr" target="#b40">[41]</ref>. The encoder computes low-dimensional feature representations via a sequence of pooling and convolution operations. The decoder, which is stacked on top of the encoder, then learns an upscaling of these low-dimensional features via subsequent unpooling and deconvolution operations <ref type="bibr" target="#b55">[56]</ref>. Similarly, Badrinarayanan et al. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> use convolutions instead of deconvolutions in the decoder network. In contrast, our approach preserves high-resolution information throughout the entire network by keeping a separate high-resolution processing stream.</p><p>Many approaches apply smoothing operations to the output of a CNN in order to obtain more consistent predictions. Most commonly, conditional random fields (CRFs) <ref type="bibr" target="#b29">[30]</ref> are applied on the network output <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref>. More recently, some papers approximate the mean-field inference of CRFs using specialized network architectures <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b36">37]</ref>. Other approaches to smoothing the network predictions include domain transform <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> and superpixel-based smoothing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. Our approach is able to swiftly combine high-and low-resolution information, resulting in already smooth output predictions. Experiments with additional CRF smoothing therefore did not result in significant performance improvements.</p><p>Network architectures. Since the success of the AlexNet architecture <ref type="bibr" target="#b30">[31]</ref> in the ImageNet Large-Scale Visual Classification Challenge (ILSVRC) <ref type="bibr" target="#b46">[47]</ref>, the vision community has seen several milestones with respect to CNN architectures. The network depth has been constantly increased, first with the popular VGG net <ref type="bibr" target="#b49">[50]</ref>, then by using batch normalization with GoogleNet <ref type="bibr" target="#b50">[51]</ref>. Lately, many computer vision applications have adopted the ResNet architecture <ref type="bibr" target="#b24">[25]</ref>, which often leads to signification performance boosts compared to earlier network architectures. All of these developments show how important a proper architecture is. However, so far most of these networks have been specifically tailored towards the task of classification, in many cases including a pre-training step on ILSVRC. As a result, some of their design choices may contribute to a suboptimal performance when performing pixel-to-pixel tasks such as semantic segmentation. In contrast, our proposed architecture has been specifically designed for segmentation tasks and reaches competitive performance on the Cityscapes dataset without requiring ILSVRC pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architectures for Segmentation</head><p>Feed-Forward Networks. Until recently, the majority of feedforward networks, such as the VGG-variants <ref type="bibr" target="#b49">[50]</ref>, were composed of a linear sequence of layers. Each layer in such a network computes a function F and the output x n of the n-th layer is computed as</p><formula xml:id="formula_0">x n = F(x n−1 ; W n )<label>(1)</label></formula><p>where W n are the parameters of the layer (see 2a). We refer to this class of network architectures as traditional feedforward networks.</p><p>Residual Networks (ResNets). He et al. observed that deepening traditional feedforward networks often results in an increased training loss <ref type="bibr" target="#b24">[25]</ref>. In theory, however, the training loss of a shallow network should be an upper bound on the training loss of a corresponding deep network. This is due to the fact that increasing the depth by adding layers strictly increases the expressive power of the model.  <ref type="figure" target="#fig_2">Figure 2b</ref>, the output x n of the n-th RU in a ResNet is computed as</p><formula xml:id="formula_1">x n = x n−1 + F(x n−1 ; W n )<label>(2)</label></formula><p>where F(x n−1 ; W n ) is the residual, which is parameterized by W n . Thus, instead of computing the output x n directly, F only computes a residual that is added to the input x n−1 . One commonly refers to this design as skip connection, because there is a connection from the input x n−1 to the output x n that skips the actual computation F. It has been empirically observed that ResNets have superior training properties over traditional feedforward networks. This can be explained by an improved gradient flow within the network. In order to understand this, consider the n-th and m-th residual units in a ResNet where m &gt; n (i.e., the m-th unit is closer to the output layer of the network). By applying the recursion (2) several times, He et al. showed in <ref type="bibr" target="#b25">[26]</ref> that the output of the m-th residual unit admits a representation of the form</p><formula xml:id="formula_2">x m = x n + m−1 i=n F(x i ; W i+1 ).<label>(3)</label></formula><p>Furthermore, if l is the loss that is used to train the network, we can use the chain rule of calculus and express the derivative of the loss l with respect to the output x n of the n-th RU as</p><formula xml:id="formula_3">∂l ∂x n = ∂l ∂x m ∂x m ∂x n = ∂l ∂x m + ∂l ∂x m m−1 i=n ∂F(x i ; W i+1 ) ∂x n .<label>(4)</label></formula><p>Thus, we find</p><formula xml:id="formula_4">∂l ∂W n = ∂l ∂x n ∂x n ∂W n = ∂x n ∂W n ∂l ∂x m + ∂l ∂x m m−1 i=n ∂F(x i ; W i+1 ) ∂x n .<label>(5)</label></formula><p>We see that the weight updates depend on two sources of information, . While the amount of information that is contained in the latter may depend crucially on the depth n, the former allows a gradient flow that is independent of the depth. Hence, gradients can flow unhindered from the deeper unit to the shallower unit. This makes training even extremely deep ResNets possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full-Resolution Residual Networks (FRRNs).</head><p>In this paper, we unify the two above-mentioned principles of network design and propose full-resolution residual networks (FRRNs) that exhibit the same superior training properties as ResNets but have two processing streams. The features on one stream, the residual stream, are computed by adding successive residuals, while the features on the other stream, the pooling stream, are the direct result of a sequence of convolution and pooling operations applied to the input.</p><p>Our design is motivated by the need to have networks that can jointly compute good high-level features for recognition and good low-level features for localization. Regardless of the specific network design, obtaining good highlevel features requires a sequence of pooling operations. The pooling operations reduce the size of the feature maps and increase the network's receptive field, as well as its robustness against small translations in the image. While this is crucial to obtaining robust high-level features, networks that employ a deep pooling hierarchy have difficulties tracking low-level features, such as edges and boundaries, in deeper layers. This makes them good at recognizing the elements in a scene but bad at localizing them to pixel accuracy. On the other hand, a network that does not employ any pooling operations behaves the opposite way. It is good at localizing object boundaries, but performs poorly at recognizing the actual objects. By using the two processing streams together, we are able to compute both kinds of features simultaneously. While the residual stream of an FRRN computes successive residuals at the full image resolution, allowing low level features to propagate effortlessly through the network, the pooling stream undergoes a sequence of pooling and unpooling operations resulting in good high-level features. <ref type="figure" target="#fig_0">Figure 1</ref> visualizes the concept of having two distinct processing streams.</p><p>An FRRN is composed of a sequence of full-resolution residual units <ref type="bibr">(FRRUs)</ref>. Each FRRU has two inputs and two outputs, because it simultaneously operates on both streams. <ref type="figure" target="#fig_2">Figure 2c</ref> shows the structure of an FRRU. Let z n−1 be the residual input to the n-th FRRU and let y n−1 be its pooling input. Then the outputs are computed as</p><formula xml:id="formula_5">z n = z n−1 + H(y n−1 , z n−1 ; W n )<label>(6)</label></formula><formula xml:id="formula_6">y n = G(y n−1 , z n−1 ; W n ),<label>(7)</label></formula><p>where W n are the parameters of the functions G and H, respectively. If G ≡ 0, then an FRRU corresponds to an RU since it disregards the pooling input y n , and the network effectively becomes an ordinary ResNet. On the other hand, if H ≡ 0, then the output of an FRRU only depends on its input via the function G. Hence, no residuals are computed and we obtain a traditional feedforward network. By carefully constructing G and H, we can combine the two network principles.</p><p>In order to show that FRRNs have similar training characteristics as ResNets, we adapt the analysis presented in <ref type="bibr" target="#b25">[26]</ref> to our case. Using the same recursive argument as before, we find that for m &gt; n, z m has the representation</p><formula xml:id="formula_7">z m = z n + m−1 i=n H(y i , z i ; W i+1 ).<label>(8)</label></formula><p>We can then express the derivative of the loss l with respect to the weights W n as</p><formula xml:id="formula_8">∂l ∂W n = ∂l ∂z n ∂z n ∂W n + ∂l ∂y n ∂y n ∂W n = ∂z n ∂W n ∂l ∂z m + ∂l ∂z m m−1 i=n ∂H(y i , z i ; W i+1 ) ∂z n + ∂l ∂y n ∂y n ∂W n .<label>(9)</label></formula><p>Hence, the weight updates depend on three sources of information. Analogous to the analysis of ResNets, the two sources ∂H(yi,zi;Wi+1) ∂zn depend crucially on the depth n, while the term ∂l ∂zm is independent of the depth. Thus, we achieve a depth-independent gradient flow for all parameters that are used by the residual function H. If we use some of these weights in order to compute the output of G, all weights of the unit benefit from the improved gradient flow. This is most easily achieved by reusing the output of G in order to compute H. However, we note that other designs are possible. <ref type="figure" target="#fig_4">Figure 3</ref> shows our proposed FRRU design. The unit first concatenates the two incoming streams by using a pooling layer in order to reduce the size of the residual stream. Then the concatenated features are fed through two convolution units. Each convolution unit consists of a 3 × 3 convolution layer followed by a batch normalization layer <ref type="bibr" target="#b26">[27]</ref> and a ReLU activation function. The result of the second convolution unit is used in two ways. First, it forms the pooling stream input of the next FRRU in the network and second it is the basis for the computed residual. To this end, we first adjust the number of feature channels using a 1 × 1 convolution and then upscale the spatial dimensions using an unpooling layer. Because the features might have to be upscaled significantly (e.g., by a factor of 16), we found that simply upscaling by repeating the entries along the spatial dimensions performed superior to bilinear interpolation. In <ref type="figure" target="#fig_4">Figure 3</ref>, the inner red box corresponds to the function G while the outer blue box corresponds to the function H. We can see that the output of G is used in order to compute H, because the red box is entirely contained within the blue box. As shown above, this design choice results in superior gradient flow properties for all weights of the unit. <ref type="table">Table 1</ref> shows the two network architectures that we used in order to assess our approach's segmentation performance. The proposed architectures are based on several principles employed by other authors. We follow Noh et al. <ref type="bibr" target="#b40">[41]</ref> and use an encoder/decoder formulation. In the encoder, we reduce the size of the pooling stream using max pooling operations. The pooled feature maps are then successively upscaled using bilinear interpolation in the decoder. Furthermore, similar to Simonyan and Zisserman <ref type="bibr" target="#b49">[50]</ref>, we define a number of base channels that we double after each pooling operation (up to a certain upper limit). Instead of choosing 64 base channels as in VGG net, we use 48 channels in order to have a manageable number of trainable parameters. Depending on the input image resolution, we use FRRN A or FRRN B to keep the relative size of the receptive fields consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training Procedure</head><p>Following Wu et al., we train our network by minimizing a bootstrapped cross-entropy loss <ref type="bibr" target="#b51">[52]</ref>. Let c be the number of classes, y 1 , ..., y N ∈ {1, ..., c} be the target class labels for the pixels 1, ..., N , and let p i,j be the posterior class probability for class j and pixel i. Then, the bootstrapped  cross-entropy loss over K pixels is defined as</p><formula xml:id="formula_9">l = − 1 K N i=1 1[p i,yi &lt; t K ] log p i,yi ,<label>(10)</label></formula><p>where 1[x] = 1 iff x is true and t k ∈ R is chosen such that |{i ∈ {1, ..., N } : p i,yi &lt; t k }| = K. The threshold parameter t k can easily be determined by sorting the predicted log probabilities and choosing the K + 1-th one as threshold. <ref type="figure">Figure 4</ref> visualizes the concept. Depending on the number of pixels K that we consider, we select misclassified pixels or pixels where we predict the correct label with a small probability. We minimize the loss using ADAM <ref type="bibr" target="#b27">[28]</ref>. Because each FRRU processes features at the full image resolution, training a full-resolution residual network is very memory intensive. Recall that in order for the backpropagation algorithm <ref type="bibr" target="#b45">[46]</ref> to work, the entire forward pass has to be stored in memory. If the memory required to store the forward pass for a given network exceeds the available GPU memory, we can no longer use the standard backpropagation algorithm. In order to alleviate this problem,  <ref type="figure">Figure 4</ref>. Pixels used by the bootstrapped cross-entropy loss for varying values of K. The images and ground truth annotations originate from the twice-subsampled Cityscapes validation set <ref type="bibr" target="#b10">[11]</ref>. Pixels that are labeled void are not considered for the bootstrapping process.</p><p>we partition the computation graph into several subsequent blocks by manually placing cut points in the graph. We then compute the derivatives for each block individually. To this end, we perform one (partial) forward pass per block and only store the feature maps for the block whose derivatives are computed given the derivative of the subsequent block. This simple scheme allows us to manually control a spacetime trade-off. The idea of recomputing some intermediate results on demand is also used in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b9">[10]</ref>. Note that these memory limitations only apply during training. During testing, there is no need to store results of each operation in the network and our architecture's memory footprint is comparable to that of a ResNet encoder/decoder architecture. We made code for the gradient computation for arbitrary networks publicly available in Theano/Lasagne. In order to reduce overfitting, we used two methods of data augmentation: translation augmentation and gamma augmentation. The former method randomly translates an image and its annotations. In order to keep consistent image dimensions, we have to pad the translated images and annotations. To this end, we use reflection padding on the image and constant padding with void labels on the annotations. Our second method of data augmentation is gamma augmentation. We use a slightly modified gamma augmentation method detailed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>We implemented our models using Theano/Lasagne <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. We evaluate our approach on the recently released Cityscapes benchmark <ref type="bibr" target="#b10">[11]</ref> containing images recorded in 50 different cities. This benchmark provides 5,000 images with high-quality annotations split up into a training, validation, and test set (2,975, 500, and 1,525 images, respectively). The dense pixel annotations span 30 classes frequently occurring in urban street scenes, out of which 19 are used for actual training and evaluation. Annotations for the test set remain private and comparison to other methods is performed via a dedicated evaluation server.</p><p>We report the results of our FRRNs for two settings: FRRN A trained on quarter-resolution (256 × 512) Cityscapes images; and FRRN B trained on half-resolution (512 × 1024). We then upsample our predictions using bilinear interpolation in order to report scores at the full image resolution of 1024 × 2048 pixels. Directly training at the full Cityscapes resolution turned out to be too memory intensive with our current design. However, as our experimental results will show, even when trained only on halfresolution images, our FRRN B's results are competitive with the best published methods trained on full-resolution data. Unless specified otherwise, the reported results are based on the Cityscapes test set. Qualitative results are shown in <ref type="figure">Figure 6</ref>, in the supplementary material, and in our result video 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Residual Network Baseline</head><p>Our network architecture can be described as a ResNet <ref type="bibr" target="#b24">[25]</ref> encoder/decoder architecture, where the residuals remain at the full input resolution throughout the network. A natural baseline is thus a traditional ResNet encoder/decoder architecture with long-range skip connections <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>. In fact, such an architecture resembles a single deep hourglass module in the stacked hourglass network architecture <ref type="bibr" target="#b39">[40]</ref>. This baseline differs from our proposed architecture in two important ways: While the feature maps on our residual stream are processed by each FRRU, the feature maps on the long-range skip connections are not processed by intermediate layers. Furthermore, long-range skip connections are scale dependent, meaning that features at one scale travel over a different skip connection than features at another scale. This is in contrast to our network design, where the residual stream can carry upscaled features from several pooling stages simultaneously.</p><p>In order to illustrate the benefits of our approach over the natural baseline, we converted the architecture FRRN A <ref type="table">(Table 1a)</ref>   <ref type="table">Table 2</ref>. IoU scores from the Cityscapes test set. We highlight the best published baselines for the different sampling rates. (Additional anonymous submissions exist as concurrent work.) Bold numbers represent the best, italic numbers the second best score for a class. We also indicate the subsampling factor used on the input images, whether additional coarsely annotated data was used, and whether the model was initialized with pre-trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>6 vs. 17.7×10 6 ). This is due to the concatenated features in FRRUs and the additional 1×1 convolutions that connect the pooling to the residual stream.</p><p>We train both networks on the quarter-resolution Cityscapes dataset for 45,000 iterations at a batch size of 3. We use a learning rate of 10 −3 for the first 35,000 iterations and then reduce it to 10 −4 for the following 10,000 iterations. Both networks converged within these iterations. The FRRN A resulted in a validation set mean IoU score of 65.7% while the ResNet baseline only achieved 62.8%, showing a significant advantage of our FRRNs. Training FRRN B is performed in a similar fashion. Detailed training curves are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Evaluation</head><p>Overview. In <ref type="table">Table 2</ref> we compare our method to the best (published) performers on the Cityscapes leader board, namely LRR <ref type="bibr" target="#b19">[20]</ref>, Adelaide <ref type="bibr" target="#b22">[23]</ref>, and Dilation <ref type="bibr" target="#b54">[55]</ref>. Note that our network performs on par with the very complex and well engineered system by <ref type="bibr">Ghiasi et al. (LRR)</ref>. Among the top performers on Cityscapes, only ENet refrain from using a pre-trained network. However, they target real-time performance and trade accuracy for speed. Thus, they do not obtain top scores. To the best of our knowledge, we are the first to show that it is possible to obtain state-of-the-art results even without pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subsampling</head><p>Factor. An interesting observation that we made on the Cityscapes test set is a correlation between the subsampling factor and the test performance. This correlation can be seen in <ref type="figure">Figure 5</ref> where we show the scores of several approaches currently listed on the leader board against their respective subsampling factors. Unsurprisingly, most of the best performers operate on the fullresolution input images. Throughout our experiments, we consistently outperformed other approaches who trained on Mean IoU Score (%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subsampling factor</head><p>Published Unpublished LRR <ref type="bibr" target="#b19">[20]</ref> Adelaide <ref type="bibr" target="#b33">[34]</ref> Dilation <ref type="bibr" target="#b54">[55]</ref> ENet <ref type="bibr" target="#b43">[44]</ref> SegNet <ref type="bibr" target="#b1">[2]</ref> DeepLab <ref type="bibr" target="#b42">[43]</ref> FRRN A/B <ref type="figure">Figure 5</ref>. Comparison of the mean IoU scores of all approaches on the leader board of the Cityscapes segmentation benchmark based on the subsampling factor of the images that they were trained on.</p><p>the same image resolutions. Even though we only train on half-resolution images, <ref type="figure">Figure 5</ref> clearly shows we can match the current published state-of-the-art (LRR <ref type="bibr" target="#b19">[20]</ref>). We expect that further improvements can be obtained by switching to full-resolution training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Boundary Adherence</head><p>Due to several pooling operations (and subsequent upsampling) in many of today's FCN architectures, boundaries are often overly smooth, resulting in lost details and edge-bleeding. This leads to suboptimal scores, but it also makes the output of a semantic segmentation approach harder to use without further post-processing. Since inaccurate boundaries are often not apparent from the standard evaluation metric scores, a typical approach is a trimap evaluation in order to quantify detailed boundary adherence <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20]</ref>. During trimap evaluation, all predictions are ignored if they do not fall within a certain radius r of a ground truth label boundary. <ref type="figure">Figure 7</ref> visualizes our trimap evaluation performed on the validation set for varying trimap widths r between 1 and 80 pixels. We compare to LRR <ref type="bibr" target="#b19">[20]</ref> and Dilation <ref type="bibr" target="#b54">[55]</ref>  <ref type="figure">Figure 6</ref>. Qualitative comparison on the Cityscapes validation set. Interesting cases are the fence in the first row, the truck in the second row, or the street light poles in the last row. An interesting failure case is shown in the third row: all methods struggle to find the correct sidewalk boundary, however our network makes a clean and reasonable prediction. Please consult the supplemental material for more qualitative results. Dilation <ref type="bibr" target="#b54">[55]</ref> LRR <ref type="bibr" target="#b19">[20]</ref> FRRN B <ref type="figure">Figure 7</ref>. The trimap evaluation on the validation set. The solid lines show the mean IoU score of our approach and two top performing methods that released their code. The dashed lines show the mean IoU score when using the 7 Cityscapes category labels. models available. We see that our approach outperforms the competition consistently for all radii r. Furthermore, it should be noted that the method of <ref type="bibr" target="#b19">[20]</ref> is based on an architecture specifically designed for clean boundaries. Our method achieves better boundary adherence, both numerically and qualitatively (see <ref type="figure">Figure 6)</ref>, with a much simpler architecture and without ImageNet pre-training. Often one can boost both the numerical score and the boundary adherence by using a fully connected CRF as post-processing step. We tried to apply a fully connected CRF with Gaussian kernel, as introduced by Krähenbühl and Koltun <ref type="bibr" target="#b29">[30]</ref>. We used the standard appearance and smoothness kernels and tuned parameters on the validation set by running several thousand Hyperopt iterations <ref type="bibr" target="#b4">[5]</ref>. Applying this post processing step yielded a marginal increase in the average IoU score of ∼ 0.5% on the validation set. This further supports the claim that our architecture natively solves problems that were conventionally addressed using costly post processing steps. Given the high computation time and low yield we decided against any postprocessing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Runtime</head><p>A forward pass of our proposed architectures FRRN A and FRRN B takes 0.166s and 0.469s on images of size 256 × 512 and 512 × 1024, respectively. This compares to 0.081s of the ResNet architecture on images of size 256 × 512. All measurements were averaged over 100 individual forward passes on an NVidia Titan X Pascal GPU. While FRRNs are indeed slower than the ResNet baseline, they are faster or as fast as other methods on the Cityscapes leaderboard that report runtimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we propose a novel network architecture for semantic segmentation in street scenes. Our architecture is clean, does not require additional post-processing, can be trained from scratch, shows superior boundary adherence, and reaches state-of-the-art results on the Cityscapes benchmark. We published code and all trained models on GitHub 2 . Since we do not incorporate design choices specifically tailored towards semantic segmentation, we believe that our architecture will also be applicable to other tasks such as stereo or optical flow where predictions are performed per pixel.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Example output and the abstract structure of our fullresolution residual network. The network has two processing streams. The residual stream (blue) stays at the full image resolution, the pooling stream (red) undergoes a sequence of pooling and unpooling operations. The two processing streams are coupled using full-resolution residual units (FRRUs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The figure compares the structures of different network design elements. (a) shows a layer in a traditional feedforward network; (b) shows a residual unit; (c) shows a full-resolution residual unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The figure shows our design of a full-resolution residual unit (FRRU). The inner red box marks the parts of the unit that are computed by the function G while the outer blue box indicates the parts that are computed by the function H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a convolution layer having m kernels each of size k × k. The notations RUm and FRRUm refer to residual units and full-resolution residual units whose convolutions have m channels, respectively. The parameter c indicates the number of classes to predict.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1. The table shows our two network designs. By conv</figDesc><table>FRRN A 

conv 

5×5 

48 + BN + ReLU 
3 × RU 48 
pooling 
stream 

residual 
stream 
max pool 
conv 

1×1 
32 

3 × FRRU 96 
max pool 
4 × FRRU 192 
max pool 
2 × FRRU 384 
max pool 
2 × FRRU 384 

unpool 
2 × FRRU 192 
unpool 
2 × FRRU 192 
unpool 
2 × FRRU 96 
unpool 
pooling 
stream 

residual 
stream 
concatenate 
3 × RU 48 

conv 

1×1 
c 

+ Bias 
Softmax 
17.7M parameters 

FRRN B 

conv 

5×5 

48 + BN + ReLU 
3 × RU 48 
pooling 
stream 

residual 
stream 
max pool 
conv 

1×1 
32 

3 × FRRU 96 
max pool 
4 × FRRU 192 
max pool 
2 × FRRU 384 
max pool 
2 × FRRU 384 
max pool 
2 × FRRU 384 
unpool 
2 × FRRU 192 
unpool 
2 × FRRU 192 
unpool 
2 × FRRU 192 
unpool 
2 × FRRU 96 
unpool 
pooling 
stream 

residual 
stream 
concatenate 
3 × RU 48 

conv 

1×1 
c 

+ Bias 
Softmax 
24.8M parameters 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>to a ResNet as follows: We first replaced all FRRUs by RUs and then added skip connections that con- nect the input of each pooling layer to the output of the corresponding unpooling layer. The resulting ResNet has slightly fewer parameters than the original FRRN (16.7 ×</figDesc><table>Method 

Subsample 

Coarse 
Pre-trained 

• 
• Mean 

Road 
Sidewalk 
Building 

Wall 
Fence 
Pole 
Traf. Light 
Traf. Sign 
Vegetation 
Terrain 
Sky 
Person 
Rider 
Car 
Truck 
Bus 
Train 
Motorcycle 

Bicycle 

SegNet [2] 

×4 

57.0 96.4 73.2 84.0 28.5 29.0 35.7 39.8 45.2 87.0 63.8 91.8 62.8 42.8 89.3 38.1 43.1 44.2 35.8 51.9 
FRRN A 

×4 

63.0 97.6 79.1 88.3 32.0 36.4 51.7 57.1 62.5 90.9 69.5 93.3 75.2 51.3 91.6 30.2 43.1 39.2 46.0 62.6 
ENet [44] 

×2 

58.3 96.3 74.2 85.0 32.2 33.2 43.5 34.1 44.0 88.6 61.4 90.6 65.5 38.4 90.6 36.9 50.5 48.1 38.8 55.4 
DeepLab [43] ×2 
64.8 97.4 78.3 88.1 47.5 44.2 29.5 44.4 55.4 89.4 67.3 92.8 71.0 49.3 91.4 55.9 66.6 56.7 48.1 58.1 
FRRN B 

×2 

71.8 98.2 83.3 91.6 45.8 51.1 62.2 69.4 72.4 92.6 70.0 94.9 81.6 62.7 94.6 49.1 67.1 55.3 53.5 69.5 
Dilation [55] ×1 
67.1 97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 55.0 93.3 45.5 53.4 47.7 52.2 66.0 
Adelaide [34] ×1 
71.6 98.0 82.6 90.6 44.0 50.7 51.1 65.0 71.7 92.0 72.0 94.1 81.5 61.1 94.3 61.1 65.1 53.8 61.6 70.6 
LRR [20] 

×1 

69.7 97.7 79.9 90.7 44.4 48.6 58.6 68.2 72.0 92.5 69.3 94.7 81.6 60.0 94.0 43.6 56.8 47.2 54.8 69.7 
LRR [20] 

×1 

71.8 97.9 81.5 91.4 50.5 52.7 59.4 66.8 72.7 92.5 70.1 95.0 81.3 60.1 94.3 51.2 67.7 54.6 55.6 69.6 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>, who made code and pre-trained</figDesc><table>Image 

Ground Truth 
Ours 
LRR [20] 
Dilation [55] 

Void 
Road 
Sidewalk 
Building 
Wall 
Fence 
Pole 
Traffic Light 
Traffic Sign 
Vegetation 

Terrain 
Sky 
Person 
Rider 
Car 
Truck 
Bus 
Train 
Motorcycle 
Bicycle 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.youtube.com/watch?v=PNzQ4PNZSzc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/TobyPDE/FRRN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was funded by the EU project STRANDS (ICT-2011-600623) and the ERC Starting Grant project CV-SUPER (ERC-2012-StG-307432).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<title level="m">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<title level="m">SegmentationNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pedestrian Detection with Depth-Guided Structure Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08358</idno>
		<title level="m">Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05895</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs. ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<title level="m">Training Deep Nets with Sublinear Memory Cost</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional Feature Masking for Joint Object and Stuff Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Lasagne: First release</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation-Based Urban Traffic Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning Hierarchical Features for Scene Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 2d-3d temporally consistent semantic segmentation of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2823" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Virtual Worlds as Proxy for Multi-Object Tracking Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain Transform for Edge-Aware Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S L</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laplacian Reconstruction and Refinement for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-class segmentation with relative location prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="316" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03401</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Memory-Efficient Backpropagation Through Time</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recognition using Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneous Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust Higher Order Potentials for Enforcing Label Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint semantic segmentation and 3d reconstruction from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">ParseNet: Looking Wider to See Better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation via Deep Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feedforward Semantic Segmentation With Zoom-Out Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Deconvolution Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-Scale Object Candidates for Generic Object Tracking in Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<title level="m">A Deep Neural Network Architecture for Real-Time Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully Connected Deep Structured Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic texton forests for Image categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Bridging Categorylevel and Instance-level Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multiple view semantic segmentation for street view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object Detection by Labeling Superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adaptive Deconvolutional Networks for Mid and High Level Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional Random Fields as Recurrent Neural Networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
