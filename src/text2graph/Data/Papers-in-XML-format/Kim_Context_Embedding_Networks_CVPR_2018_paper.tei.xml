<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Embedding Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Oisin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mac</forename><surname>Aodha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context Embedding Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large annotated datasets are a vital ingredient for training automated classification and inference systems. Labeling these datasets has been made possible by crowdsourcing services, which enable the purchasing of annotations from crowd workers. Unfortunately fine-grained categorization is very challenging for untrained workers. The alternative, obtaining annotations from experts, is equally impractical due to the fact that for many domains experts are few <ref type="bibr" target="#b23">[24]</ref>. Instead of obtaining semantic fine-grained category-level labels, one can ask workers to label images in terms of their similarities and differences. This is intuitively much easier for untrained workers because it requires the comparison of images, a task that humans are naturally good at. This approach, however, presents its own challenges: 1) different workers may use different criteria when estimating the similarity between pairs of images, and 2) workers may be influenced by the set of images that they see when making their decisions i.e. 'context'.</p><p>In <ref type="figure" target="#fig_0">Fig. 1</ref> we see an example of three different crowd Context influences similarity estimates. We hypothesize that estimating similarity according to a particular visual attribute is influenced by a combination of innate biases and the context in which these decisions are made. Compared to worker 1, worker 2 has a strong prior bias towards using the gender attribute.</p><note type="other">Gender Expr Gender Expr Gender Expr WORKER 1 Gender Expr Strength Gender Expr + Gender Expr Strength Gender Expr + Gender Expr = Gender Expr</note><p>Influenced by the context of the images worker 1 also groups based on gender. Worker 3 sees the same context as worker 1 but ultimately groups based on expression due to prior bias.</p><p>workers estimating similarity by clustering a collection of images. The workers' decision for which visual attribute they use to compare the images can be explained by two factors: 1) The workers have an innate preference towards certain attributes based on their past experiences and 2) the set of related images that a worker observes biases them towards certain attributes. We call this first bias the worker prior and the second bias the context. Our hypothesis is that different sets of images highlight different visual attributes to the workers. The majority of existing work often assumes that all workers behave in the same way <ref type="bibr" target="#b22">[23]</ref>, the list of attributes are specified in advance <ref type="bibr" target="#b24">[25]</ref>, or in addition to similarity estimates, workers also indicate which attributes they used to make their decision <ref type="bibr" target="#b20">[21]</ref>. We introduce Context Embedding Networks (CENs), an efficient end-to-end model that learns interpretable, low dimensional, image embeddings that respect the varied similarity estimates provided by different crowd workers. Our contributions are: 1) A flexible model that produces an em-bedding for a set of input images. This is achieved by modeling worker bias and image context i.e. the degree to which each worker is influenced by the attributes present in a given set of images. 2) An empirical evaluation on annotations from real crowd workers showing that CENs outperform existing approaches, producing interpretable, disentangled, low-dimensional feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning Embeddings The goal of embedding algorithms is to learn a low dimensional representation of a collection of objects (e.g. images), such that objects that are "close" in the potentially high dimensional input space are also "close" in the embedding space. Embeddings are useful for a large number of tasks from face recognition <ref type="bibr" target="#b18">[19]</ref> to estimating the clinical similarities between medical patients <ref type="bibr" target="#b33">[34]</ref>. They can be learned from pre-defined feature vectors representing the input objects <ref type="bibr" target="#b21">[22]</ref>, from similarity estimates obtained from the crowd <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>, or a combination <ref type="bibr" target="#b32">[33]</ref>. Crowdsourced annotations can come in the form of pairwise <ref type="bibr" target="#b6">[7]</ref> or relative similarity estimates <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>. Presenting workers with sets of images, as opposed to pairs or triplets, is an efficient way of acquiring estimates of similarity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>. Another approach is to learn a function that can extract meaningful features from the raw input data by training on similarity labels e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. This has the advantage of being able to also embed objects not observed at training time.</p><p>Different Notions of Similarity A limitation of the above methods is that they typically assume that objects are compared using a single similarity criteria. Given a pair or triplet of images, one estimate of similarity may be valid for one visual attribute, or trait, but invalid for another. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref> comparing faces according to gender or expression will result in a different grouping. In practice, workers may use different criteria unless they are specifically told which attribute to use. To overcome this limitation there is a body of work that attempts to learn embeddings where alternative notions of similarity are represented in the embedding space. One common approach is to instruct the workers to provide additional information regarding the attribute they used when making their decision. This information can come in multiple forms such as category labels <ref type="bibr" target="#b24">[25]</ref>, user provided text descriptions <ref type="bibr" target="#b20">[21]</ref>, or part and correspondence annotations <ref type="bibr" target="#b15">[16]</ref>.</p><p>Similar to <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b0">[1]</ref> propose a model inspired by <ref type="bibr" target="#b22">[23]</ref> that produces a separate embedding for each similarity criteria instead of learning a single embedding that tries to satisfy all constraints. In contrast, <ref type="bibr" target="#b24">[25]</ref> learn a unified embedding where alternative notions of similarity are extracted by masking different dimensions in this space. However, the visual attribute used for each similarity estimate is assumed to be known. <ref type="bibr" target="#b28">[29]</ref> also learn a weighted feature representation of the input examples but require category level labels in order to learn cross-category attributes. Their model learns a different weight vector for each triplet, resulting in a large number of parameters. <ref type="bibr" target="#b20">[21]</ref> propose a generative model for learning attributes from the crowd where workers are instructed to specify an attribute of interest via a text box and then perform similarity estimates for a set of query images based on these pre-defined attributes. The majority of these methods assume that extra information, in addition to the pairwise or triplet labels, are available to the model. We instead make use of the context information that is present in the set of images that we show to our crowd workers.</p><p>Modeling the Crowd Crowdsourcing annotations is an effective way of gathering a large amount of labeled data <ref type="bibr" target="#b11">[12]</ref>. One difficulty that arises when using such annotations is that they can be noisy, as workers behave differently. One solution to this problem is to model the ability and biases of each worker to resolve better quality annotations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3]</ref>. Specific to clustering, <ref type="bibr" target="#b6">[7]</ref> propose a Bayesian model of how workers cluster data from noisy pairwise annotations. To efficiently gather a large number of labels, workers are presented with successive grids of images and are asked to cluster the images into multiple different groups. By modeling individual workers as linear classifiers in an embedding space they allow for different worker biases. However, they assume that workers are consistent in the criteria they use when making their decisions and that it does not change over time. Our approach also learns individual worker models while also making use of the strong context information provided by the image grid.</p><p>Attribute Discovery Low dimensional, attribute based, representations of images have the benefit of being more interpretable than raw pixel information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In addition to providing semantically understandable descriptions of images, they can also be used for applications such as zero shot learning <ref type="bibr" target="#b12">[13]</ref>. Attributes can be discovered by various means, from mining noisy web images and their associated text descriptions <ref type="bibr" target="#b1">[2]</ref> to crowdsourcing <ref type="bibr" target="#b17">[18]</ref>. In this work, while we do not explicitly aim to produce 'nameable' attributes, we qualitatively observe that the embeddings that our model produces are often disentangled along the embedding dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We crowdsource the task of image similarity estimation for a dataset containing N images referenced by i, j = 1,...,N. Each crowd worker w =1 ,...,W, is presented with an image grid g =1 ,...,G, displaying a collection of images {i g } which they group into as many categories as they wish <ref type="bibr" target="#b6">[7]</ref>. A grid of S items results in (S 2 − S)/2 pairwise labels, e.g. a single grid of 24 items produces the same number of annotations as 276 individual pairs. Across grids, real workers are often inconsistent with the attributes</p><formula xml:id="formula_0">! ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ #(%) '(!) ⋮ ( ) ⋮ ⋮ #(*) #(+) ⋮ ⋮ , - , .</formula><p>Image Encoder Attribute Encoders /(, . ,, -; ( they use to cluster and the number of clusters they create. A pair of images (i g ,j g ) shown in the same grid, g, clustered by worker w is assigned a positive label l =1if they are grouped together and l =0otherwise. This results in a training set of pairwise similarity labels</p><formula xml:id="formula_2">D = {(w, g, i g ,j g ,l)|g =1,...,G}.</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Context Embedding Network (CEN)</head><p>Here we present out CEN model and define the loss function used to train it. This involves joint training of three networks which model workers, grid context, and image embedding respectively, see <ref type="figure" target="#fig_1">Fig. 2</ref>. The first two networks are referred to as attribute encoders while the third is the image encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Worker Encoder</head><p>For the workers we define an attribute encoder network q φ which takes as input a one-hot encoding (o(·)) of worker w and outputs a K dimensional worker, attribute activa-</p><formula xml:id="formula_3">tion, vector a w = q φ (o(w)) = [a w 1 ,...,a w K ]. Each a w k , for k =1 ,.</formula><p>..,K, represents the degree of prior bias towards attribute k for worker w. Once the network is trained, the output attribute activation vector models the worker's prior preferences for each visual attribute. For example, a heavily biased worker that only attends to a single attribute k * should have high activation for that particular attribute dimension a w k * . On the other hand, a worker that does not have a strong preference for any particular attribute will have weak attribute activations in all K dimensions and may be more influenced by the grid context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Context Encoder</head><p>For an image grid containing S images, we define a context encoder network p θ that takes as input a S-hot encoding (s(·)) of the grid g and outputs a K dimensional grid attribute activation vector a</p><formula xml:id="formula_4">g = p θ (s(g)) = [a g 1 ,...,a g K ]. Each a g k for k =1 ,.</formula><p>..,K represents the degree of visual prominence of attribute k for grid g. Once the network is trained, the grid attribute activation dimensions with high values should correspond to the most salient visual attributes highlighted by the input grid. Intuitively, attribute variance in the collection of images should influence which attributes are more noticeable to workers. For instance, a collection of images that is similar along all other attributes except one k * should have a peak activation at a g k * . On the other hand, if the image set varies along many different attributes, a g should be close to uniformly distributed. The attribute vectors a w and a g from the worker and context encoders are combined to produce the final attribute encoder output a m <ref type="figure" target="#fig_1">(Fig. 2</ref> Center).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Image Encoder</head><p>We seek to learn a non-linear mapping from image i to a disentangled Euclidean coordinate x i where each dimension embeds the image into a one dimensional attribute specific subspace. To achieve this we use a Siamese Network architecture for the image encoder network f ψ with shared parameters ψ that take as input a one hot encoding of image i and outputs a K dimensional embedding vector</p><formula xml:id="formula_5">x i = f ψ (o(i)) = [x i1 ,...,x iK ].</formula><p>Although our image embedding network learns an embedding for each input image directly, with enough data it is possible to learn a feature extractor from the raw images <ref type="bibr" target="#b24">[25]</ref>. Similarly, we present our model in terms of a pairwise loss, but it is also possible to use a triplet loss for the image encoder. For brevity, from this point forward we omit the one and S-hot encoding function notation o(·),s(·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning from the Crowd</head><p>By ignoring worker and context information, an embedding can be learned using Siamese networks <ref type="bibr" target="#b3">[4]</ref>, where the contrastive training loss L c is defined as</p><formula xml:id="formula_6">L c (x i ,x j )=ld(x i ,x j )+(1− l) max{0,ξ n − d(x i ,x j )},<label>(2)</label></formula><p>where d(x i ,x j )=kx i − x j k 2 is the L2 distance between image i, j in embedding space. ξ n is the negative margin which prevents over-expanding the embedding manifold, and l 2{ 0, 1} is the user provided label. This contrastive loss alone does not encourage the network to learn low dimensional attribute specific embeddings as it assumes that all crowd workers compare images using the same visual attributes. To overcome this, we weight the L2 distance metric by the attribute activation vectors a w and a g .W e hypothesize that a worker's decision to cluster along a particular attribute depends on both their prior preferences for specific visual attributes and the context highlighted by the set of images in the grid. Based on this assumption, we define three variants of the distance metric weighted by the attribute activation vectors</p><formula xml:id="formula_7">d(x i ,x j ; a w )=ka w · (x i − x j )k 2 = kq φ (w) · (f ψ (i) − f ψ (j))k 2 (3) d(x i ,x j ; a g )=ka g · (x i − x j )k 2 = kp θ (g) · (f ψ (i) − f ψ (j))k 2 (4) d(x i ,x j ; a m )=ka m · (x i − x j )k 2 (5) = k(p θ (g)+q φ (w)) · (f ψ (i) − f ψ (j))k 2 ,</formula><p>where a m = a g +a w is the mixed attribute activation vector. After exploring different non-linear methods of mixing a g and a m , we found that a simple summation sufficiently captures the relationship between the two biases. In the experiments section, we compare the performance of the above three different models. For the model in Eq. 5, biased workers should have a concentrated worker attribute activation vector a w which will dominate the mode of sum a m = a w +a g . Alternatively, workers with weak prior preferences should have low worker attribute activations a w and the grid attribute activations a g will dictate the mode. Intuitively, the attribute activation vector serves as a mask which indicates the embedding dimension that should be weighted heavily in the loss e.g. <ref type="bibr" target="#b24">[25]</ref>. By encouraging sparsity in a w and a g along with ReLU non-linearities <ref type="bibr" target="#b16">[17]</ref>, we assume that grids that were clustered along one attribute will have a uni-modal a m while grids that were clustered on a mixture of attributes will have a multi-modal a m with peaks corresponding to the attribute dimensions used.</p><p>Inspired by the dual margin contrastive loss proposed in <ref type="bibr" target="#b28">[29]</ref>, we include a positive margin term ξ p in the loss function to prevent two images from overlapping in the embedding space which could lead to over fitting. This ensures that images will be pushed closer only if their current embedding is separated by more than ξ p . We use a to denote the general attribute activation vector which can be a g ,a w , or a m depending on the model variant</p><formula xml:id="formula_8">L c (x i ,x j ; a)=l max{0,d(x i ,x j ; a) − ξ p }+ (1 − l) max{0,ξ n − d(x i ,x j ; a)}.<label>(6)</label></formula><p>A crowd worker's decision to group two images is an active decision while choosing not to group images together can be seen as a more passive decision. This can become a problem when workers group images with different levels of detail. For example, a grid of shapes containing squares, triangles, circles, and stars might be clustered into two groups, squares and non-squares, by one worker. A second worker may group the images into the four different shape types. An embedding model might incorrectly assume that a different attribute was used to separate the images, when it is in fact just a different level of granularity of 'shape' that is being used by both workers. To overcome this problem, we introduce an additional positive similarity weight γ, that captures the relative importance of the positive similarity labels compared to the dissimilarity labels</p><formula xml:id="formula_9">L c (x i ,x j ; a)=γl max{0,d(x i ,x j ; a) − ξ p }+ (1 − l) max{0,ξ n − d(x i ,x j ; a)}.<label>(7)</label></formula><p>This ensures that the model can learn the high level attributes when workers cluster with different levels of detail. In the example above, although cross category labels between circles, triangles, and stars are l =0 , the positive labels generated within each circle, triangle, and star groups agree with the positive labels generated within the non-square group thus allowing the network to learn that the high level attribute, i.e. shape, used by both workers are the same. We show the impact of γ on the performance of our CEN in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Regularization</head><p>We add L1 penalties λ 1 kak 1 to the attribute encoders to encourage sparsity in the attribute activation vector.We also regularize the embedding network with a L2 penalty λ 2 kxk 2 to encourage regularity in the latent space. The final loss function for our CENs is</p><formula xml:id="formula_10">L CEN (x i ,x j ; a)=γl max{0,d(x i ,x j ; a) − ξ p }+ (1 − l) max{0,ξ n − d(x i ,x j )}+ λ 1 kak 1 + λ 2 kx i k 2 + λ 2 kx j k 2 . (8)</formula><p>CENs require the number of dimensions K as a hyperparameter. However, we observe that by setting K to a large number and by L1 regularizing a w and a g , our model tends to only use a subset of the available embedding dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Here we show that CENs can recover meaningful lowdimensional embeddings from noisy data. Network architectures, training details, and hyperparameters tuning are described in the supplementary material. We perform experiments on the following three datasets: CELEBA contains images of different celebrity faces from which we select a random subset of 300 images <ref type="bibr" target="#b14">[15]</ref>. For this dataset we instruct workers in advance to cluster on one attribute per grid respecting four visual attributes: gender, expression, skin color, and gaze direction. Although we expect some workers to deviate from our instructions, having a definite ground truth set of attributes allow us to quantify the attribute retrieval accuracy. The CEN is unaware of the attribute selected for each grid. In total, 94 workers clustered 620 grids, yielding 170,000 similarity training pairs. RETINA is a medical dataset comprising of fundus images of the retina belonging to patients with varying degrees of diabetic retinopathy <ref type="bibr" target="#b8">[9]</ref>. The images contain a number of visual indicators for the disease such as hard exudates (yellow lesions dispersed throughout the retina). From 66 fundus images we crop out 300 image patches. These patches provide a localized view that may or may not contain indicator features of the disease. This dataset is more challenging to discover meaningful attributes as the disease indicator features are visually subtle and the images are unfamiliar to the crowd. We do not provide any instructions as to the attributes the workers should use for this dataset. 62 workers clustered 620 grids, yielding 170,000 similarity pairs. BIRDS is a larger dataset composed of 1000 bird head images made up of 16 randomly selected species from <ref type="bibr" target="#b26">[27]</ref>. We use this dataset to demonstrate the scalability of our CENs. 252 workers clustered 3,000 grids yielding 820,000 similarity labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Collection</head><p>We use Amazon Mechanical Turk's crowdsourcing platform to request crowd workers to cluster grids of images using the GUI shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Workers were presented with a 4 ⇥ 6 grid of images randomly sampled from the given dataset. Using up to ten possible groups, workers clustered images by first clicking on a group button on the right side of the page then clicking on the desired images. For each group they were asked to provide a short text description, used only for evaluation. The image, cluster, and worker ids were then converted into pairwise similarity labels (Eq. 1). Each worker clustered a minimum of ten grids in order to receive a reward, ensuring that the worker encoder network had sufficient data to learn from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baseline Comparisons</head><p>We compare results to four baseline methods and three variants of our model: Standard Siamese Network e.g. <ref type="bibr" target="#b3">[4]</ref>: Assumes that all pairwise similarity labels come from the same notion of similarity, as in Eq. 2. Standard Triplet Network e.g. <ref type="bibr" target="#b18">[19]</ref>: Learns embeddings given similarity labels of the form "A is more similar to B than C". Bayesian Crowd Clustering <ref type="bibr" target="#b6">[7]</ref>: Workers are modeled as linear classifiers in the embedding space where both an entangled image embedding and individual worker models are jointly learned with variational methods. CSN <ref type="bibr" target="#b24">[25]</ref>: Learns an entangled image embedding from similarity triplets which are disentangled by masks learned separately for each pre-known attribute. This baseline represents the situation where the similarity dimension used by the worker is known. CEN-worker encoder only: This first variant of our model uses only worker modeling to learn attribute activations which weight the embeddings as in Eq. 3. CEN-context encoder only: Here we only model context information to weight the embeddings as in Eq. 4. CEN-mixture: Our full model, incorporates both worker and context information to learn a network that weights the worker bias a w and grid context a g as in Eq. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised Attribute Retrieval</head><p>First, we evaluate whether our CEN can accurately recover the four dominant attributes present in the CELEBA dataset. For each grid g clustered by worker w, we take the mode dimension of the attribute activation vector a to be the model's prediction, a pred = argmax k a k . This is the attribute that we predict was used to cluster the set of images. Again a can be a w ,a g or a m depending on the model variant used. We then examine the annotations provided by workers for each set of grids that map to a different a pred 2{ 1, 2, 3, 4} and quantify the proportion of each attribute actually used. In <ref type="figure" target="#fig_3">Fig. 4(a)</ref> we show a confusion matrix illustrating that for each worker and grid pair, the CEN-mixture model is able to accurately predict which attribute was used. The row for gender and the first column denote the proportion of grid submissions that have a pred =1out of all the submissions that were clustered along gender. For all attributes we obtain over 85% attribute prediction accuracy. In <ref type="figure" target="#fig_3">Fig. 4(b)</ref> we plot the entropy H of the distribution p for each row of the confusion matrix where H p = − P p log p. High entropy indicates that the ground truth attributes are scattered throughout the attribute predictions and vice versa. The CEN-mixture model learns the most disentangled embeddings across the four ground truth attributes compared to its variants.</p><p>Although workers were encouraged to focus on four different attribute options for this experiment, in practice they did not abide by our instructions and the proportion of noise in the raw data is significant. For the CELEBA dataset approximately 19.1% of the HITs completed were either clustered on different attributes such as "wearing sun glasses" (see <ref type="figure">Fig. 5</ref>) or noisy submissions where images were not separated into different groups. We also observed workers using different levels of detail when clustering on the same attribute. For example, for the gaze attribute some workers labeled "looking left", "looking right", etc. To demonstrate our model's robustness, we perform all of our experiments on this raw data without filtering out annotation noise. For evaluation of the worker model learned by the worker encoder, refer to the supplementary material.  <ref type="figure">Figure 5</ref>. Cluster Names. Keywords provided by workers for CELEBA. Colored labels indicate the manual grouping performed by us (only used for evaluation). Some workers use finer grained distinctions compared to others. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the attribute specific embeddings of the four subspaces learned by the CEN for the CELEBA dataset. <ref type="figure" target="#fig_4">Fig.  6(a)</ref> shows that the embedding clearly separates the images according to gender. On the very left of the expression subspace ( <ref type="figure" target="#fig_4">Fig. 6(b)</ref>) we can see that people are smiling with teeth showing while on the right they show serious or unhappy expressions. In the middle we see ambiguous expressions. <ref type="figure" target="#fig_4">Fig. 6(c)</ref> shows the subspace embedded along the skin color attribute. On the two ends we see darker skinned and lighter skinned people. <ref type="figure" target="#fig_4">Fig. 6(d)</ref> shows the subspace for gaze direction of people, showing people that are either looking at the camera or away from it. Again, in the middle we see people wearing sunglasses or looking in ambiguous directions making it difficult to assess their gaze direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualizing Disentangled Attributes</head><p>In <ref type="figure" target="#fig_5">Fig. 7</ref> we show attribute specific embeddings learned for the RETINA dataset in which no supervision was given to the workers for which attributes to pay attention to. Here we select four dimensions that are most highly activated from the learned ten dimensional embedding vector. Other attribute dimensions attain trivial activations. This shows that our CEN is robust to value of K (please see supplementary material for robustness analysis of K). <ref type="figure" target="#fig_5">Fig. 7(a)</ref> shows the first dimension seemingly showing the presence or absence of the optic disc, a key feature of the retina. <ref type="figure" target="#fig_5">Fig. 7(b)</ref> shows the subspace which discriminates between patches with blood vessels present and those without. Blood vessels are mostly concentrated and visually prominent around the optic disc, meaning that the two attributes are highly correlated. Regardless, our CEN is capable of distinguishing between the two attributes, as we see that images displaying blood vessels without optic discs are correctly embedded in <ref type="figure" target="#fig_5">Fig. 7(a)</ref>. <ref type="figure" target="#fig_5">Fig. 7(c)</ref> plots the attribute that groups laser scars (named after consulting with an ophthalmologist) and <ref type="figure" target="#fig_5">Fig. 7(d)</ref> groups hard exudates, a key indicator for diagnosing diabetic retinopathy <ref type="bibr" target="#b10">[11]</ref>. A comparison of embedding qualities between baselines are presented in the supplementary material. <ref type="figure" target="#fig_6">Fig. 8</ref> shows a t-SNE plot of the four dimensional embedding space learned by the CEN for the BIRDS dataset.  Each ellipse center corresponds to the mean of a Gaussian distribution fit to the embedding coordinates for each ground truth species. We observe 16 compact clusters that directly correlate to the 16 ground truth species. Please refer to the supplementary materials for confusion plots of the ground truth species vs embedding clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance on Held-out Label Prediction</head><p>Here we quantify the generalization performance of the baseline methods on held-out pairwise label predictions while varying the amount of training data. We measure the accuracy of the various model's predictions on the similarity estimates for an unseen grid clustered by a known worker. For a grid input g, worker input w, and image pair i, j, the model predicts i and j to be in the same group if d&lt;(ξ n + ξ p )/2. The test set is made up of 15% of the dataset and consists only of entire grids that were not present in the training set. This allows us to measure how well our CEN generalizes to new sets of images. <ref type="figure" target="#fig_7">Fig. 9(a)</ref> shows results for the RETINA dataset. Standard Siamese Networks and Triplet Networks fail to capture the multiple attributes used to cluster the images and  have the lowest prediction accuracy of 58.1% and 58.5%.</p><p>The Bayesian Crowd Clustering model, CEN worker, and CEN grid only models attain similar prediction accuracies of 67%. For the more challenging RETINA dataset workers found it difficult to discover various attributes to cluster on and thus often fixated on a single attribute on all their HITs. However, we still benefit from modeling the context as the CEN-mixture model achieves prediction accuracy of 69.4%. The CSN model with learned masks obtains the highest accuracy of 75.5%, but it is important to note that this model was trained on triplets pre-labeled with the true similarity attributes used to cluster them. <ref type="figure" target="#fig_7">Fig. 9(b)</ref> shows the pairwise prediction accuracy for each model plotted against a varying number of training samples for the BIRDS dataset. The Bayesian Crowd Clustering model, CEN worker, and CEN grid only models attain similar prediction accuracies of 62%. The CENmixture substantially outperforms all baselines with a prediction accuracy of 70.5% which is only 3.5% below the accuracy of the CSN model which uses ground truth labels.</p><p>High variance along "Gaze" <ref type="figure" target="#fig_0">Figure 10</ref>. Synthesized image grids. Our context encoder can be used to generate collections of images that highlight specific attributes. The shown grid has high variance along the gaze direction attribute and low variance for the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Image Grid Synthesis</head><p>Being able to synthesize image grids that highlight specific attributes may be useful in active learning where the data collector seeks to obtain similarity estimates along particular visual attributes. We randomly generate ten million image grids and individually pass them through the context encoder and extract the grid attribute activation vectors a g for each grid. We take a softmax activation over the a g s and select grids that have low entropy, thus choosing grids that are highly expressive for a particular attribute. <ref type="figure" target="#fig_0">Fig. 10</ref> shows a generated grid with the lowest entropy for the gaze attribute. We see low variance among the images along other attributes such as gender and skin color, while there is high variance for 'gaze'. This suggests that in order for a grid to emphasize a particular attribute, the contained items should be similar in all but one high variance attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel deep neural network that jointly learns attribute specific embeddings, worker models, and grid context models from the crowd. By comparing to several baseline methods, we show that our model more accurately predicts the attributes used by individual workers and as a result produces better quality image embeddings.</p><p>In future we plan to incorporate relative similarity estimates and the learning of representations directly from images <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19]</ref>. Although currently we model each worker individually, in practice there may be similarity between different workers that could be discovered through clustering <ref type="bibr" target="#b9">[10]</ref>. Finally, our grid context encoder enables us to generate sets of images that highlight specific attributes. By combining this with active learning we can potentially speed up the collection of annotations from the crowd <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Context influences similarity estimates. We hypothesize that estimating similarity according to a particular visual attribute is influenced by a combination of innate biases and the context in which these decisions are made. Compared to worker 1, worker 2 has a strong prior bias towards using the gender attribute. Influenced by the context of the images worker 1 also groups based on gender. Worker 3 sees the same context as worker 1 but ultimately groups based on expression due to prior bias.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Context Embedding Networks are composed of three neural networks that are trained jointly. (Top-Left) A worker encoder network models workers' annotation behavior and (Bottom-Left) a context encoder network models the attributes highlighted by a particular set of images. Jointly, these networks are referred to as the attribute encoders and are used to weight the embeddings produced by the image encoder network (Center). (Right) Our final embedding respects similarity estimates from each worker in the same low dimensional space where each dimension corresponds to a different visual attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Data collection GUI. Workers group images they perceive to be visually similar by assigning them to different groups. They can create up to ten groups per grid of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Attribute retrieval accuracy. On the left we see the predicted embedding dimensions from the CEN-mixture model compared to the ground truth visual attributes for the CELEBA dataset. On the right, we quantify how disentangled the learned embeddings are. Lower entropy indicates models that better capture the ground truth attributes along individual embedding dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. CELEBA -Attribute specific embeddings. Each plot shows one of the four different embedding dimensions produced by the CEN-mixture mode. The vertical axis in each subplot is randomly assigned for visualization purposes. We show representative images from the embeddings space in yellow boxes. We can see that the CEN learns to disentangle the attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. RETINA -Attribute specific embeddings. Here we show a subset of four of the ten embeddings dimensions produced by the CEN-mixture model for the RETINA dataset. Dimensions correlated well with visual features of diabetic retinopathy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. BIRDS -t-SNE embedding. Here we show a t-SNE [22] plot of the four dimensional embedding produced by the full CEN model for the BIRDS dataset. Indexed ellipses are centered at the Gaussian mean of different ground truth species. Clusters correlated well with ground truth species of birds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Held-out label prediction. Prediction accuracy on held out labels for the RETINA and BIRDS datasets plotted against the amount of available data during training.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Google for supporting the Visipedia project and AWS Research Credits for their donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiview triplet embedding: Learning attributes in multiple maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lean crowdsourcing: Combining humans and machines in an online system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>IJPRAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visual Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crowdclustering</surname></persName>
		</author>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A unifying view for performance measures in multi-class prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jurman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Furlanello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1008.2908</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Diabetic Retinopathy Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaggle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/diabetic-retinopathy-detection" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustering crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kajino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diaretdb1 diabetic retinopathy database and evaluation protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V J P H</forename><surname>Kälviäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uusitalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIUA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Crowdsourcing in computer vision. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond comparing image pairs: Setwise active learning for relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Part and attribute discovery from relative annotations. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tamuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
		<title level="m">Adaptively learning the crowd kernel. ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning attributes from the crowdsourced relative labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic triplet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karaletsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crowdsourced clustering: Querying edges vs triangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Vinayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Similarity comparisons for interactive fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02534</idno>
		<title level="m">Contextual visual similarity</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cost-effective hits for relative similarity comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HCOMP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning concept embeddings with combined humanmachine expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Measuring patient similarities via a deep architecture with medical concept embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
