<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
							<email>federico.monti@usi.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
							<email>michael.bronstein@usi.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
							<email>xbresson@ntu.edu.sg</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Università della Svizzera italiana Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering NTU</orgName>
								<orgName type="institution">Università della Svizzera italiana Lugano</orgName>
								<address>
									<country>Switzerland, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationary structures on user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score matrix. Our neural network system is computationally attractive as it requires a constant number of parameters independent of the matrix size. We apply our method on several standard datasets, showing that it outperforms state-of-the-art matrix completion techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recommender systems have become a central part of modern intelligent systems. Recommending movies on Netflix, friends on Facebook, furniture on Amazon, and jobs on LinkedIn are a few examples of the main purpose of these systems. Two major approaches to recommender systems are collaborative <ref type="bibr" target="#b4">[5]</ref> and content <ref type="bibr" target="#b31">[32]</ref> filtering techniques. Systems based on collaborative filtering use collected ratings of items by users and offer new recommendations by finding similar rating patterns. Systems based on content filtering make use of similarities between items and users to recommend new items. Hybrid systems combine collaborative and content techniques.</p><p>Matrix completion. Mathematically, a recommendation method can be posed as a matrix completion problem <ref type="bibr" target="#b8">[9]</ref>, where columns and rows represent users and items, respectively, and matrix values represent scores determining whether a user would like an item or not. Given a small subset of known elements of the matrix, the goal is to fill in the rest. A famous example is the Netflix challenge <ref type="bibr" target="#b21">[22]</ref> offered in 2009 and carrying a 1M$ prize for the algorithm that can best predict user ratings for movies based on previous user ratings. The size of the Netflix matrix is 480k movies × 18k users (8.5B entries), with only 0.011% known entries. and items, respectively. Such additional information defines e.g. the notion of smoothness of the matrix and was shown beneficial for the performance of recommender systems. These approaches can be generally related to the field of signal processing on graphs <ref type="bibr" target="#b36">[37]</ref>, extending classical harmonic analysis methods to non-Euclidean domains (graphs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric deep learning.</head><p>Of key interest to the design of recommender systems are deep learning approaches. In the recent years, deep neural networks and, in particular, convolutional neural networks (CNNs) <ref type="bibr" target="#b24">[25]</ref> have been applied with great success to numerous applications. However, classical CNN models cannot be directly applied to the recommendation problem to extract meaningful patterns in users, items and ratings because these data are not Euclidean structured, i.e. they do not lie on regular lattices like images but rather irregular domains like graphs. Recent works applying deep learning to recommender systems used networks with fully connected or auto-encoder architectures <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14]</ref>. Such methods are unable to extract the important local stationary patterns from the data, which is one of the key properties of CNN architectures. New neural networks are necessary and this has motivated the recent development of geometric deep learning techniques that can mathematically deal with graph-structured data, which arises in numerous applications, ranging from computer graphics and vision <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> to chemistry <ref type="bibr" target="#b11">[12]</ref>. We recommend the review paper <ref type="bibr" target="#b5">[6]</ref> to the reader not familiar with this line of works.</p><p>The earliest attempts to apply neural networks to graphs are due to Scarselli et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> (see more recent formulation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40]</ref>). Bruna et al. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref> formulated CNN-like deep neural architectures on graphs in the spectral domain, employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator <ref type="bibr" target="#b36">[37]</ref>. Defferrard et al. <ref type="bibr" target="#b9">[10]</ref> proposed an efficient filtering scheme using recurrent Chebyshev polynomials, which reduces the complexity of CNNs on graphs to the same complexity of classical (Euclidean) CNNs. This model was later extended to deal with dynamic data <ref type="bibr" target="#b35">[36]</ref>. Kipf and Welling <ref type="bibr" target="#b20">[21]</ref> proposed a simplification of Chebychev networks using simple filters operating on 1-hop neighborhoods of the graph. Monti et al. <ref type="bibr" target="#b29">[30]</ref> introduced a spatial-domain generalization of CNNs to graphs local patch operators represented as Gaussian mixture models, showing significantly better generalization across different graphs.</p><p>Contributions. We present two main contributions. First, we introduce a new multi-graph CNN architecture that generalizes <ref type="bibr" target="#b9">[10]</ref> to multiple graphs. This new architecture is able to extract local stationary patterns from signals defined on multiple graphs simultaneously. While in this work we apply multi-graph CNNs in the context of recommender systems to the graphs of users and items, however, our architecture is generic and can be used in other applications, such as neuroscience (autism detection with network of people and brain connectivity <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref>), computer graphics (shape correspondence on product manifold <ref type="bibr" target="#b40">[41]</ref>), or social network analysis (abnormal spending behavior detection with graphs of customers and stores <ref type="bibr" target="#b38">[39]</ref>). Second, we approach the matrix completion problem as learning on user and item graphs using the new deep multi-graph CNN framework. Our architecture is based on a cascade of multi-graph CNN followed by Long Short-Term Memory (LSTM) recurrent neural network <ref type="bibr" target="#b15">[16]</ref> that together can be regarded as a learnable diffusion process that reconstructs the score matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Matrix Completion</head><p>Matrix completion problem. Recovering the missing values of a matrix given a small fraction of its entries is an ill-posed problem without additional mathematical constraints on the space of solutions. It is common to assume that the variables lie in a smaller subspace, i.e., the matrix is of low rank, min</p><formula xml:id="formula_0">X rank(X) s.t. x ij = y ij , ∀ij ∈ Ω,<label>(1)</label></formula><p>where X denotes the matrix to recover, Ω is the set of the known entries and y ij are their values. Unfortunately, rank minimization turns out to be an NP-hard combinatorial problem that is computationally intractable in practical cases. The tightest possible convex relaxation of problem <ref type="formula" target="#formula_0">(1)</ref> is to replace the rank with the nuclear norm · equal to the sum of its singular values <ref type="bibr" target="#b7">[8]</ref>,</p><formula xml:id="formula_1">min X X + µ 2 Ω • (X − Y) 2 F ;<label>(2)</label></formula><p>the equality constraint is also replaced with a penalty to make the problem more robust to noise (here Ω is the indicator matrix of the known entries Ω and • denotes the Hadamard pointwise product).</p><p>Candès and Recht <ref type="bibr" target="#b7">[8]</ref> proved that under some technical conditions the solutions of problems <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_0">(1)</ref>  </p><formula xml:id="formula_2">c ij = w c ji , w c ij = 0 if (i, j) /</formula><p>∈ E c and w c ij &gt; 0 if (i, j) ∈ E c . In our setting, the column graph could be thought of as a social network capturing relations between users and the similarity of their tastes. The row graph G r = ({1, . . . , m}, E r , W r ) representing the items similarities is defined similarly.</p><p>On each of these graphs one can construct the (normalized) graph Laplacian, an n × n symmetric positive-semidefinite matrix</p><formula xml:id="formula_3">∆ = I − D −1/2 WD −1/2 , where D = diag( j =i w ij )</formula><p>is the degree matrix. We denote the Laplacian associated with row and column graphs by ∆ r and ∆ c , respectively. Considering the columns (respectively, rows) of matrix X as vector-valued functions on the column graph G c (respectively, row graph G r ), their smoothness can be expressed as the Dirichlet norm X 2 Gr = trace(X ∆ r X) (respecitvely, X 2 Gc = trace(X∆ c X )). The geometric matrix completion problem <ref type="bibr" target="#b18">[19]</ref> thus boils down to minimizing</p><formula xml:id="formula_4">min X X 2 Gr + X 2 Gc + µ 2 Ω • (X − Y) 2 F .<label>(3)</label></formula><p>Factorized models. Matrix completion algorithms introduced in the previous section are well-posed as convex optimization problems, guaranteeing existence, uniqueness and robustness of solutions. Besides, fast algorithms have been developed for the minimization of the non-differentiable nuclear norm. However, the variables in this formulation are the full m × n matrix X, making it hard to scale up to large matrices such as the Netflix challenge.</p><p>A solution is to use a factorized representation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b32">33</ref>, 1] X = WH , where W, H are m × r and n × r matrices, respectively, with r min(m, n). The use of factors W, H reduces the number of degrees of freedom from O(mn) to O(m + n); this representation is also attractive as people often assumes the original matrix to be low-rank for solving the matrix completion problem, and rank(WH ) ≤ r by construction.</p><p>The nuclear norm minimization problem (2) can be rewritten in a factorized form as <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_5">min W,H 1 2 W 2 F + 1 2 H 2 F + µ 2 Ω • (WH − Y) 2 F .<label>(4)</label></formula><p>and the factorized formulation of the graph-based minimization problem (3) as</p><formula xml:id="formula_6">min W,H 1 2 W 2 Gr + 1 2 H 2 Gc + µ 2 Ω • (WH − Y) 2 F .<label>(5)</label></formula><p>The limitation of model <ref type="formula" target="#formula_6">(5)</ref> is that it decouples the regularization previously applied simultaneously on the rows and columns of X in (3), but the advantage is linear instead of quadratic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep learning on graphs</head><p>The key concept underlying our work is geometric deep learning, an extension of CNNs to graphs. In particular, we focus here on graph CNNs formulated in the spectral domain. A graph Laplacian admits a spectral eigendecomposition of the form ∆ = ΦΛΦ , where Φ = (φ 1 , . . . φ n ) denotes the matrix of orthonormal eigenvectors and Λ = diag(λ 1 , . . . , λ n ) is the diagonal matrix of the corresponding eigenvalues. The eigenvectors play the role of Fourier atoms in classical harmonic analysis and the eigenvalues can be interpreted as frequencies. Given a function x = (x 1 , . . . , x n ) on the vertices of the graph, its graph Fourier transform is given byx = Φ x. The spectral convolution of two functions x, y can be defined as the element-wise product of the respective Fourier transforms,</p><formula xml:id="formula_7">x y = Φ(Φ y) • (Φ x) = Φ diag(ŷ 1 , . . . ,ŷ n )x,<label>(6)</label></formula><p>by analogy to the Convolution Theorem in the Euclidean case.</p><p>Bruna et al. <ref type="bibr" target="#b6">[7]</ref> used the spectral definition of convolution (6) to generalize CNNs on graphs. A spectral convolutional layer in this formulation has the form</p><formula xml:id="formula_8">x l = ξ   q l =1 ΦŶ ll Φ x l   , l = 1, . . . , q,<label>(7)</label></formula><p>where q , q denote the number of input and output channels, respectively,Ŷ ll = diag(ŷ ll ,1 , . . . ,ŷ ll ,n ) is a diagonal matrix of spectral multipliers representing a learnable filter in the spectral domain, and ξ is a nonlinearity (e.g. ReLU) applied on the vertex-wise function values. Unlike classical convolutions carried out efficiently in the spectral domain using FFT, the computations of the forward and inverse graph Fourier transform incur expensive O(n 2 ) multiplication by the matrices Φ, Φ , as there are no FFT-like algorithms on general graphs. Second, the number of parameters representing the filters of each layer of a spectral CNN is O(n), as opposed to O(1) in classical CNNs. Third, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain, which is another important property of classical CNNs.</p><p>Henaff et al. <ref type="bibr" target="#b14">[15]</ref> argued that spatial localization can be achieved by forcing the spectral multipliers to be smooth. The filter coefficients are represented asŷ k = τ (λ k ), where τ (λ) is a smooth transfer function of frequency λ; its application to a signal x is expressed as τ (∆)x = Φ diag(τ (λ 1 ), . . . , τ (λ n ))Φ x, where applying a function to a matrix is understood in the operator sense and boils down to applying the function to the matrix eigenvalues. In particular, the authors used parametric filters of the form</p><formula xml:id="formula_9">τ θ (λ) = p j=1 θ j β j (λ),<label>(8)</label></formula><p>where β 1 (λ), . . . , β r (λ) are some fixed interpolation kernels, and θ = (θ 1 , . . . , θ p ) are p = O(1) interpolation coefficients acting as parameters of the spectral convolutional layer.</p><p>Defferrard et al. <ref type="bibr" target="#b9">[10]</ref> used polynomial filters of order p represented in the Chebyshev basis,</p><formula xml:id="formula_10">τ θ (λ) = p j=0 θ j T j (λ),<label>(9)</label></formula><p>whereλ is frequency rescaled in [−1, 1], θ is the (p+1)-dimensional vector of polynomial coefficients parametrizing the filter, and T j (λ) = 2λT j−1 (λ) − T j−2 (λ) denotes the Chebyshev polynomial of degree j defined in a recursive manner with T 1 (λ) = λ and T 0 (λ) = 1. Here,∆ = 2λ</p><formula xml:id="formula_11">−1</formula><p>n ∆ − I is the rescaled Laplacian with eigenvaluesΛ = 2λ</p><formula xml:id="formula_12">−1 n Λ − I in the interval [−1, 1].</formula><p>This approach benefits from several advantages. First, it does not require an explicit computation of the Laplacian eigenvectors, as applying a Chebyshev filter to x amounts to τ θ (∆)x = p j=0 θ j T j (∆)x; due to the recursive definition of the Chebyshev polynomials, this incurs applying the Laplacian p times. Multiplication by Laplacian has the cost of O(|E|), and assuming the graph has |E| = O(n) edges (which is the case for k-nearest neighbors graphs and most real-world networks), the overall complexity is O(n) rather than O(n 2 ) operations, similarly to classical CNNs. Moreover, since the Laplacian is a local operator affecting only 1-hop neighbors of a vertex and accordingly its pth power affects the p-hop neighborhood, the resulting filters are spatially localized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>In this paper, we propose formulating matrix completion as a problem of deep learning on user and item graphs. We consider two architectures summarized in <ref type="figure" target="#fig_2">Figures 1 and 2</ref>. The first architecture works on the full matrix model producing better accuracy but requiring higher complexity. The second architecture used factorized matrix model, offering better scalability at the expense of slight reduction of accuracy. For both architectures, we consider a combination of multi-graph CNN and RNN, which will be described in detail in the following sections. Multi-graph CNNs are used to extract local stationary features from the score matrix using row and column similarities encoded by user and item graphs. Then, these spatial features are fed into a RNN that diffuses the score values progressively, reconstructing the matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Graph CNNs</head><p>Multi-graph convolution. Our first goal is to extend the notion of the aforementioned graph Fourier transform to matrices whose rows and columns are defined on row-and column-graphs. We recall that the classical two-dimensional Fourier transform of an image (matrix) can be thought of as applying a one-dimensional Fourier transform to its rows and columns. In our setting, the analogy of the two-dimensional Fourier transform has the form</p><formula xml:id="formula_13">X = Φ r XΦ c<label>(10)</label></formula><p>where Φ c , Φ r and Λ c = diag(λ c,1 , . . . , λ c,n ) and Λ r = diag(λ r,1 , . . . , λ r,m ) denote the n × n and m × m eigenvector-and eigenvalue matrices of the column-and row-graph Laplacians ∆ c , ∆ r , respectively. The multi-graph version of the spectral convolution (6) is given by</p><formula xml:id="formula_14">X Y = Φ r (X •Ŷ)Φ c ,<label>(11)</label></formula><p>and in the classical setting can be thought as the analogy of filtering a 2D image in the spectral domain (column and row graph eigenvalues λ c and λ r generalize the x-and y-frequencies of an image).</p><p>As in <ref type="bibr" target="#b6">[7]</ref>, representing multi-graph filters as their spectral multipliersŶ would yield O(mn) parameters, prohibitive in any practical application. To overcome this limitation, we follow <ref type="bibr" target="#b14">[15]</ref>, assuming that the multi-graph filters are expressed in the spectral domain as a smooth function of both frequencies (eigenvalues λ c , λ r of the row-and column graph Laplacians) of the formŶ k,k = τ (λ c,k , λ r,k ).</p><p>In particular, using Chebychev polynomial filters of degree p, </p><formula xml:id="formula_15">1 τ Θ (λ c ,λ r ) = p j,j =0 θ jj T j (λ c )T j (λ r ),<label>(12)</label></formula><formula xml:id="formula_16">θ jj T j (∆ r )XT j (∆ c )<label>(13)</label></formula><p>incurs an O(mn) computational complexity (here, as previously,∆ c = 2λ</p><formula xml:id="formula_17">−1 c,n ∆ c − I and∆ r = 2λ −1</formula><p>r,m ∆ r − I denote the scaled Laplacians). Similarly to (7), a multi-graph convolutional layer using the parametrization of filters according to <ref type="bibr" target="#b12">(13)</ref> is applied to q input channels (m × n matrices X 1 , . . . , X q or a tensor of size m × n × q ),</p><formula xml:id="formula_18">X l = ξ   q l =1 X l Y ll   = ξ   q l =1 p j,j =0 θ jj ,ll T j (∆ r )X l T j (∆ c )   , l = 1, . . . , q, (14)</formula><p>producing q outputs (tensor of size m × n × q). Several layers can be stacked together. We call such an architecture a Multi-Graph CNN (MGCNN).</p><p>Separable convolution. A simplification of the multi-graph convolution is obtained considering the factorized form of the matrix X = WH and applying one-dimensional convolutions to the respective graph to each factor. Similarly to the previous case, we can express the filters resorting to Chebyshev polynomials,</p><formula xml:id="formula_19">w l = p j=0 θ r j T j (∆ r )w l ,h l = p j =0 θ c j T j (∆ c )h l , l = 1, . . . , r<label>(15)</label></formula><p>where w l , h l denote the lth columns of factors W, H and θ r = (θ </p><formula xml:id="formula_20">w l = ξ   q l =1 p j=0 θ r j,ll T j (∆ r )w l   ,h l = ξ   q l =1 p j =0 θ c j ,ll T j (∆ c )h l   .<label>(16)</label></formula><p>We call such an architecture a separable MGCNN or sMGCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matrix diffusion with RNNs</head><p>The next step of our approach is to feed the spatial features extracted from the matrix by the MGCNN or sMGCNN to a recurrent neural network (RNN) implementing a diffusion process that progressively reconstructs the score matrix (see <ref type="figure">Figure 3)</ref>. Modelling matrix completion as a diffusion process</p><formula xml:id="formula_21">X X (t)X(t) MGCNN RNN dX (t) X (t+1) = X (t) + dX (t)</formula><p>row+column filtering <ref type="figure">Figure 1</ref>: Recurrent MGCNN (RMGCNN) architecture using the full matrix completion model and operating simultaneously on the rows and columns of the matrix X. Learning complexity is O(mn).   <ref type="figure">Figure 3</ref>: Evolution of matrix X (t) with our architecture using full matrix completion model RMGCNN (top) and factorized matrix completion model sRMGCNN (bottom). Numbers indicate the RMS error. appears particularly suitable for realizing an architecture which is independent of the sparsity of the available information. In order to combine the few scores available in a sparse input matrix, a multilayer CNN would require very large filters or many layers to diffuse the score information across matrix domains. On the contrary, our diffusion-based approach allows to reconstruct the missing information just by imposing the proper amount of diffusion iterations. This gives the possibility to deal with extremely sparse data, without requiring at the same time excessive amounts of model parameters. See <ref type="table" target="#tab_4">Table 3</ref> for an experimental evaluation on this aspect.</p><formula xml:id="formula_22">W H H (t)H(t) W (t)W(t) GCNN RNN GCNN RNN dH (t) dW (t) W (t+1) = W (t) + dW (t) H (t+1) = H (t) + dH</formula><p>We use the classical LSTM architecture <ref type="bibr" target="#b15">[16]</ref>, which has demonstrated to be highly efficient to learn complex non-linear diffusion processes due to its ability to keep long-term internal states (in particular, limiting the vanishing gradient issue). The input of the LSTM gate is given by the static features extracted from the MGCNN, which can be seen as a projection or dimensionality reduction of the original matrix in the space of the most meaningful and representative information (the disentanglement effect). This representation coupled with LSTM appears particularly well-suited to keep a long term internal state, which allows to predict accurate small changes dX of the matrix X (or dW, dH of the factors W, H) that can propagate through the full temporal steps. <ref type="figure" target="#fig_2">Figures 1 and 2</ref> and Algorithms 1 and 2 summarize the proposed matrix completion architectures. We refer to the whole architecture combining the MGCNN and RNN in the full matrix completion setting as recurrent multi-graph CNN (RMGCNN). The factorized version with separable MGCNN and RNN is referred to as separable RMGCNN (sRMGCNN). The complexity of Algorithm 1 scales quadratically as O(mn) due to the use of MGCNN. For large matrices, Algorithm 2 that processes the rows and columns separately with standard GCNNs and scales linearly as O(m + n) is preferable.</p><p>We will demonstrate in Section 4 that the proposed RMGCNN and sRMGCNN architectures show themselves very well on different settings of matrix completion problems. However, we should note that this is just one possible configuration, which we by no means claim to be optimal. For example, in all our experiments we used only one convolutional layer; it is likely that better yet performance could be achieved with more layers.</p><formula xml:id="formula_23">Algorithm 1 (RMGCNN) input m × n matrix X (0) containing initial val- ues 1: for t = 0 : T do 2:</formula><p>Apply the Multi-Graph CNN (13) on X (t)</p><p>producing an m × n × q outputX (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for all elements (i, j) do</p><formula xml:id="formula_24">4:</formula><p>Apply RNN to q-dimx</p><formula xml:id="formula_25">(t) ij = (x (t) ij1 , . . . ,x (t)</formula><p>ijq ) producing incremental update dx (t) ij</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>end for <ref type="bibr">6:</ref> Update X (t+1) = X (t) + dX Apply the Graph CNN on H (t) producing an n × q outputH (t) .</p><p>3:</p><formula xml:id="formula_26">for j = 1 : n do 4:</formula><p>Apply RNN to q-dimh</p><formula xml:id="formula_27">(t) j = (h (t) j1 , . . . ,h (t) jq ) producing incremental update dh (t) j 5: end for 6: Update H (t+1) = H (t) + dH (t) 7:</formula><p>Repeat steps 2-6 for W</p><p>8: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Training of the networks is performed by minimizing the loss</p><formula xml:id="formula_29">(Θ, σ) = X (T ) Θ,σ 2 Gr + X (T ) Θ,σ 2 Gc + µ 2 Ω • (X (T ) Θ,σ − Y) 2 F .<label>(17)</label></formula><p>Here, T denotes the number of diffusion iterations (applications of the RNN), and we use the notation X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(T )</head><p>Θ,σ to emphasize that the matrix depends on the parameters of the MGCNN (Chebyshev polynomial coefficients Θ) and those of the LSTM (denoted by σ). In the factorized setting, we use the loss</p><formula xml:id="formula_30">(θ r , θ c , σ) = W (T ) θr,σ 2 Gr + H (T ) θc,σ 2 Gc + µ 2 Ω • (W (T ) θr,σ (H (T ) θc,σ ) − Y) 2 F<label>(18)</label></formula><p>where θ c , θ r are the parameters of the two GCNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Experimental settings. We closely followed the experimental setup of <ref type="bibr" target="#b32">[33]</ref>, using five standard datasets: Synthetic dataset from <ref type="bibr" target="#b18">[19]</ref>, MovieLens <ref type="bibr" target="#b28">[29]</ref>, Flixster <ref type="bibr" target="#b17">[18]</ref>, Douban <ref type="bibr" target="#b26">[27]</ref>, and YahooMusic <ref type="bibr" target="#b10">[11]</ref>. We used disjoint training and test sets and the presented results are reported on test sets in all our experiments. As in <ref type="bibr" target="#b32">[33]</ref>, we evaluated MovieLens using only the first of the 5 provided data splits. For Flixster, Douban and YahooMusic, we evaluated on a reduced matrix of 3000 users and items, considering 90% of the given scores as training set and the remaining as test set. Classical Matrix Completion (MC) <ref type="bibr" target="#b8">[9]</ref>, Inductive Matrix Completion (IMC) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>, Geometric Matrix Completion (GMC) <ref type="bibr" target="#b18">[19]</ref>, and Graph Regularized Alternating Least Squares (GRALS) <ref type="bibr" target="#b32">[33]</ref> were used as baseline methods. In all the experiments, we used the following settings for our RMGCNNs: Chebyshev polynomials of order p = 4, outputting k = 32-dimensional features, LSTM cells with 32 features and T = 10 diffusion steps (for both training and test). The number of diffusion steps T has been estimated on the Movielens validation set and used in all our experiments. A better estimate of T can be done by cross-validation, and thus can potentially only improve the final results. All the models were implemented in Google TensorFlow and trained using the Adam stochastic optimization algorithm <ref type="bibr" target="#b19">[20]</ref> with learning rate 10 −3 . In factorized models, ranks r = 15 and 10 was used for the synthetic and real datasets, respectively. For all methods, hyperparameters were chosen by cross-validation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic data</head><p>We start the experimental evaluation showing the performance of our approach on a small synthetic dataset, in which the user and item graphs have strong communities structure. Though rather simple, such a dataset allows to study the behavior of different algorithms in controlled settings.</p><p>The performance of different matrix completion methods is reported in <ref type="table" target="#tab_2">Table 1</ref>, along with their theoretical complexity. Our RMGCNN and sRMGCNN models achieve better accuracy than other methods with lower complexity. Different diffusion time steps of these two models are visualized in <ref type="figure">Figure 3</ref>. <ref type="figure">Figures 4 and 5</ref> depict the spectral filters learnt by MGCNN and row-and column-GCNNs.</p><p>We repeated the same experiment assuming only the column (users) graph to be given. In this setting, RMGCNN cannot be applied, while sRMGCNN has only one GCNN applied on the factor H (the other factor W is free). <ref type="table" target="#tab_3">Table 2</ref> summarizes the results of this experiment, again, showing that our approach performs the best. <ref type="table" target="#tab_4">Table 3</ref> compares our RMGCNN with more classical multilayer MGCNNs. Our recurrent solutions outperforms deeper and more complex architectures, requiring at the same time a lower amount of parameters.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real data</head><p>Following <ref type="bibr" target="#b32">[33]</ref>, we evaluated the proposed approach on the MovieLens, Flixster, Douban and YahooMusic datasets. For the MovieLens dataset we constructed the user and item (movie) graphs as unweighted 10-nearest neighbor graphs in the space of user and movie features, respectively. For Flixster, the user and item graphs were constructed from the scores of the original matrix. On this dataset, we also performed an experiment using only the users graph. For the Douban dataset, we used only the user graph (provided in the form of a social network). For the YahooMusic dataset, we used only the item graph, constructed with unweighted 10-nearest neighbors in the space of item features (artists, albums, and genres). For the latter three datasets, we used a sub-matrix of 3000 × 3000 entries for evaluating the performance. <ref type="table" target="#tab_5">Tables 4 and 5</ref>    <ref type="bibr" target="#b8">[9]</ref> 0.973 IMC <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref> 1.653 GMC <ref type="bibr" target="#b18">[19]</ref> 0.996 GRALS <ref type="bibr" target="#b32">[33]</ref> 0.945 sRMGCNN 0.929 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented a new deep learning approach for matrix completion based on multi-graph convolutional neural network architecture. Among the key advantages of our approach compared to traditional methods is its low computational complexity and constant number of degrees of freedom independent of the matrix size. We showed that the use of deep learning for matrix completion allows to beat related state-of-the-art recommender system methods. To our knowledge, our work is the first application of deep learning on graphs to this class of problems. We believe that it shows the potential of the nascent field of geometric deep learning on non-Euclidean domains, and will encourage future works in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>whereλ c ,λ r are the frequencies rescaled [−1, 1] (see Figure 4 for examples). Such filters are parametrized by a (p + 1) × (p + 1) matrix of coefficients Θ = (θ jj ), which is O(1) in the input size as in classical CNNs on images. The application of a multi-graph filter to the matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are the parameters of the row-and column-filters, respectively (a total of 2(p + 1) = O(1)). Application of such filters to W and H incurs O(m + n) complexity. Convolutional layers (14) thus take the form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Separable Recurrent MGCNN (sRMGCNN) architecture using the factorized matrix completion model and operating separately on the rows and columns of the factors W, H . Learning complexity is O(m + n).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Absolute value |τ (λ c ,λ r )| of the first ten spectral filters learnt by our MGCNN model. In each matrix, rows and columns represent frequenciesλ r andλ c of the row and column graphs, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>summarize the performance of different methods. sRMGCNN outperforms the competitors in all the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different matrix comple- tion methods using users+items graphs in terms of number of parameters (optimization variables) and computational complexity order (operations per iteration). Big-O notation is avoided for clar- ity reasons. Rightmost column shows the RMS error on Synthetic dataset.</figDesc><table>METHOD 
PARAMS NO. OP. RMSE 
GMC 
mn 
mn 
0.3693 
GRALS 
m + n 
m + n 
0.0114 
sRMGCNN 
1 
m + n 
0.0106 
RMGCNN 
1 
mn 
0.0053 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different matrix comple- tion methods using users graph only in terms of number of parameters (optimization variables) and computational complexity order (operations per iteration). Big-O notation is avoided for clar- ity reasons. Rightmost column shows the RMS error on Synthetic dataset.</figDesc><table>METHOD 
PARAMS NO. OP. RMSE 
GRALS 
m + n 
m + n 
0.0452 
sRMGCNN 
m 
m + n 
0.0362 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Reconstruction errors for the synthetic dataset between multiple convolutional layers architectures and the proposed architecture. Chebyshev polynomials of order 4 have been used for both users and movies graphs (q MGCq denotes a multi-graph convolutional layer with q input features and q output features).</figDesc><table>Method 
Params 
Architecture 
RMSE 
MGCNN 3layers 
9K 
1MGC32, 32MGC10, 10MGC1 
0.0116 
MGCNN 4layers 
53K 
1MGC32, 32MGC32 × 2, 32MGC1 0.0073 
MGCNN 5layers 
78K 
1MGC32, 32MGC32 × 3, 32MGC1 0.0074 
MGCNN 6layers 
104K 
1MGC32, 32MGC32 × 4, 32MGC1 0.0064 
RMGCNN 
9K 
1MGC32 + LSTM 
0.0053 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Performance (RMS error) of different matrix completion meth- ods on the MovieLens dataset.</figDesc><table>METHOD 
RMSE 
GLOBAL MEAN 
1.154 
USER MEAN 
1.063 
MOVIE MEAN 
1.033 
MC </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Performance (RMS error) on several datasets. For Douban and YahooMusic, a single graph (of users and items respectively) was used. For Flixster, two settings are shown: users+items graphs / only users graph.</figDesc><table>METHOD 
FLIXSTER 
DOUBAN YAHOOMUSIC 
GRALS 
1.3126 / 1.2447 
0.8326 
38.0423 
sRMGCNN 
1.1788 / 0.9258 
0.8012 
22.4149 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, we use the same degree p for row-and column frequencies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code: https://github.com/fmonti/mgcnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Song recommendation with nonnegative matrix factorization and graph total variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anisotropic diffusion descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical Analysis of Predictive Algorithms for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exact Matrix Completion via Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="111" to="119" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Yahoo! music dataset and KDD-Cup&apos;11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Cup</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0626</idno>
		<title level="m">Provable inductive matrix completion</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A matrix factorization technique with trust propagation for recommendation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Recommender Systems</title>
		<meeting>Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Matrix completion on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distance metric learning using graph convolutional networks: Application to functional brain networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A harmonic extension approach for collaborative ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<idno>abs/1602.05127</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Web Search and Data Mining</title>
		<meeting>Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DRR</title>
		<meeting>3DRR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MovieLens unplugged: experiences with an occasionally connected recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intelligent User Interfaces</title>
		<meeting>Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral graph convolutions for population-based disease prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Content-based Recommendation Systems. The Adaptive Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Billsus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Maximum-Margin Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepshop: Understanding purchase patterns via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computational Social Science</title>
		<meeting>International Conference on Computational Social Science</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Product manifold filter: Non-rigid shape correspondence via kernel density estimation in the product space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vestner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speedup matrix completion with side information: Application to multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Primal-dual algorithms for non-negative matrix factorization with the kullback-leibler divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yanez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2257" to="2261" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
