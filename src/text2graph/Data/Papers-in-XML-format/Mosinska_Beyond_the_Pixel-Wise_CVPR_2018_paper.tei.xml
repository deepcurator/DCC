<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond the Pixel-Wise Loss for Topology-Aware Delineation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Mosinska</surname></persName>
							<email>agata.mosinska@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Márquez-Neila</surname></persName>
							<email>pablo.marquezneila@epfl.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Koziński</surname></persName>
							<email>mateusz.kozinski@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">ARTORG Center for Biomedical Engineering Research</orgName>
								<orgName type="institution">University of Bern</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond the Pixel-Wise Loss for Topology-Aware Delineation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary crossentropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological impact of mistakes in the final prediction. We propose a new loss term that is aware of the higherorder topological features of linear structures. We also exploit a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step, while keeping the number of parameters and the complexity of the model constant.</p><p>When combined with the standard pixel-wise loss, both our new loss term and an iterative refinement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classifier trained with the binary cross-entropy alone. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automated delineation of curvilinear structures, such as those in <ref type="figure" target="#fig_0">Fig. 1(a, b)</ref>, has been investigated since the inception of the field of Computer Vision in the 1960s and 1970s. Nevertheless, despite decades of sustained effort, full automation remains elusive when the image data is noisy and the structures are complex. As in many other fields, the advent of Machine Learning techniques in general, and Deep Learning in particular, has produced substantial advances, in large part because learning features from the data makes them more robust to appearance variations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. * This work was supported by Swiss National Science Foundation.</p><p>† This work was funded by the ERC FastProof grant.  <ref type="bibr" target="#b20">[21]</ref> (d) Segmentation obtained after detecting membranes using our method. Our approach closes small gaps, which prevents much bigger topology mistakes.</p><p>However, all new methods focus on finding either better features to feed a classifier or more powerful deep architectures, while still using a pixel-wise loss such as binary cross-entropy for training purposes. Such loss is entirely local and does not account for the very specific and sometimes complex topology of curvilinear structures penalizing all mistakes equally regardless of their influence on geometry. As a shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(c,d) this is a major problem because small localized pixel-wise mistakes can result in large topological changes.</p><p>In this paper, we show that supplementing the usual pixel-wise loss by a topology loss that promotes results with appropriate topological characteristics yields a substantial performance increase without having to change the network architecture. In practice, we exploit the feature maps computed by a pretrained VGG19 <ref type="bibr" target="#b25">[26]</ref> to obtain high-level descriptions that are sensitive to linear structures. We use them to compare the topological properties of the ground truth and the network predictions and estimate our topology loss.</p><p>In addition to this we exploit iterative refinement freamework, which is inspired by the recurrent convolutional architecture of Pinheiro and Collobert <ref type="bibr" target="#b19">[20]</ref>. We show that, unlike in the recent methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>, sharing the same architecture and parameters across all refinement steps, instead of instantiating a new network each time, results in state of the art performance and enables keeping the number of parameters constant irrespectively of the number of iterations. This is important when only a relatively small amount of training data is available, as is often the case in biomedical and other specialized applications.</p><p>Our main contribution is therefore a demonstration that properly accounting for topology in the loss used to train the network is an important step in boosting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Detecting Linear Structures</head><p>Delineation algorithms can rely either on hand-crafted or on learned features. Optimally Oriented Flux (OOF) <ref type="bibr" target="#b11">[12]</ref> and Multi-Dimensional Oriented Flux (MDOF) <ref type="bibr" target="#b29">[30]</ref>, its extension to irregular structures, are successful examples of the former. Their great strength is that they do not require training data but at the cost of struggling with very irregular structures at different scales along with the great variability of appearances and artifacts.</p><p>In such challenging situations, learning-based methods have an edge and several approaches have been proposed over the years. For example, Haar wavelets <ref type="bibr" target="#b34">[35]</ref> or spectral features <ref type="bibr" target="#b7">[8]</ref> were used as features that were then input to the classifier. In <ref type="bibr" target="#b26">[27]</ref>, the classifier is replaced by a regressor that predicts the distance to the closest centerline, which enables estimating the width of the structures.</p><p>In more recent work, Deep Networks were successfully employed. For the purpose of road delineation, this was first done in <ref type="bibr" target="#b15">[16]</ref>, directly using image patches as input to a fully connected neural net. While the patch provided some context around the linear structures, it was still relatively small due to memory limitations. With the advent of Convolutional Neural Networks (CNNs), it became possible to use larger receptive fields. In <ref type="bibr" target="#b4">[5]</ref>, CNNs were used to extract features that could then be matched against words in a learned dictionary. The final prediction was made based on the votes from the nearest neighbors in the feature space. A fully-connected network was replaced by a CNN in <ref type="bibr" target="#b14">[15]</ref> for road detection. In <ref type="bibr" target="#b13">[14]</ref> a differentiable Intersection-overUnion loss was introduced to obtain a road segmentation, which is then used to extract graph of the road network. In the task of edge detection, nested, multiscale CNN features were utilized by Holistically-Nested Edge Detector <ref type="bibr" target="#b33">[34]</ref> to directly produce an edge map of entire image.</p><p>In the biomedical field, the VGG network <ref type="bibr" target="#b25">[26]</ref> pretrained on real images has been fine-tuned and augmented by specialized layers to extract blood vessels <ref type="bibr" target="#b12">[13]</ref>. Similarly the U-Net <ref type="bibr" target="#b20">[21]</ref>, has been shown to give excellent results for biomedical image segmentation and is currently among the methods that yield the best results for neuron boundaries detection in the ISBI'12 challenge <ref type="bibr" target="#b0">[1]</ref>.</p><p>While effective, all these approaches rely on a standard cross entropy loss for training purposes. Since they operate on individual pixels as though they were independent of each other, they ignore higher-level statistics while scoring the output. We will see in Section 4 that this is detrimental even when using an architecture designed to produce a structured output, such as the U-Net.</p><p>Of course, topological knowledge can be imposed in the output of these linear structure detectors. For example, in <ref type="bibr" target="#b31">[32]</ref>, this is done by introducing a CRF formulation whose priors are computed on higher-order cliques of connected superpixels likely to be part of road-like structures. Unfortunately, due to the huge number of potential cliques, it requires sampling and hand-designed features. Another approach to model higher-level statistics is to represent linear structures as a sequence of short linear segments, which can be accomplished using a Marked Point Process <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>. The inference involves Reversible Jump Markov Chain Monte Carlo and relies on a complex objective function. More recently, it has been shown that the delineation problem could be formulated in terms of finding an optimal subgraph in a graph of potential linear structures by solving an Integer Program <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18]</ref>. However, this requires a complex pipeline whose first step is finding points on the centerline of candidate linear structures.</p><p>Instead of encoding the topology knowledge explicitly, we propose to use higher-level features extracted using a pre-trained VGG network to score the predictions. Such feature statistics were used for image generation <ref type="bibr" target="#b3">[4]</ref> and style transfer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, both tasks for which matching output statistics is a necessity because no ground truth annotations are available. However, delineation belongs in a different category because precise per-pixel ground truth is available and strict per-pixel supervision could be considered to be the most efficient approach. We show that this is not the case and that augmenting the pixel-oriented loss with a coarser, less localized, but semantically richer loss boosts performance. Moreover, to the best of our knowledge, it is the first successful attempt of using VGG features of binary segmentation images rather than natural ones, even though VGG was pretrained on ImageNet <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recursive Refinement</head><p>Recursive refinement of a segmentation has been extensively investigated. It is usually implemented as a procedure of iterative predictions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>, sometimes at different resolutions <ref type="bibr" target="#b22">[23]</ref>. Such methods use the prediction from a previous iteration (and sometimes the image itself) as the input to a classifier that produces the next prediction. This enables the classifier to better consider the context surrounding a pixel when trying to assign a label to it and has been successfully used for delineation purposes <ref type="bibr" target="#b26">[27]</ref>.</p><p>In more recent works, the preferred approach to refinement with Deep Learning is to stack several deep modules and train them in an end-to-end fashion. For example, the pose estimation network of <ref type="bibr" target="#b18">[19]</ref> is made of eight consecutive hourglass modules and supervision is applied on the output of each one during training, which takes several days. In <ref type="bibr" target="#b24">[25]</ref> a similar idea is used to detect neuronal membranes in electron microscopy images, but due to memory size constraints the network is limited to 3 modules. In other words, even though such end-to-end solutions are convenient, the growing number of network parameters they require can become an issue when time, memory, and available amounts of training data are limited. This problem is tackled in <ref type="bibr" target="#b8">[9]</ref> by using a single network that moves its attention field within the volume to be segmented. It predicts the output for the current field of view and fills in the prediction map.</p><p>Similarly, we also use the same network to refine its prediction. In terms of network architecture, our approach is most closely related to the recurrent network for image segmentation <ref type="bibr" target="#b19">[20]</ref>, with the notable difference that, while in the existing work each recursion/refinement step is instantiated for a different scale, we instantiate our refinement modules at a fixed scale and predict jointly the probability map for the whole patch. Compared to a typical Recurrent Neural Network <ref type="bibr" target="#b6">[7]</ref>, our architecture does not have memory. Moreover, in training, we use a loss function that is a weighted sum of losses computed after each processing step. This enables us to accumulate the gradients and requires neither seeds for initialization nor processing the intermediate output contrary to <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We use the fully convolutional U-Net <ref type="bibr" target="#b20">[21]</ref> as our trainable model, as it is currently among the best and most widely used architectures for delineation and segmentation in both natural and biomedical images. The U-Net is usually trained to predict the probability of each pixel of being a linear structure using a standard pixel-wise loss. As we have already pointed out, this loss relies on local measures and does not account for the overall geometry of curvilinear structures, which is what we want to remedy.</p><p>In the remainder of this section, we first describe our topology-aware loss function, and we then introduce iterative procedure to recursively refine our predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>In the following discussion, let x 2 R H·W be the W ⇥H input image, and let y 2 {0, 1}</p><p>H·W be the corresponding ground-truth labeling, with 1 indicating pixels in the curvilinear structure and 0 indicating background pixels.</p><p>Let f be our U-Net parameterized by weights w. The output of the network is an imageŷ = f (x, w) 2 [0, <ref type="bibr" target="#b0">1]</ref> H·W . <ref type="bibr" target="#b0">1</ref> Every element ofŷ is interpreted as the probability of pixel i having label 1:</p><formula xml:id="formula_0">ŷ i ⌘ p(Y i =1| x, w), where Y i is a random Bernoulli variable Y i ⇠ Ber(ŷ i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Topology-aware loss</head><p>In ordinary image segmentation problems, the loss function used to train the network is usually the standard pixelwise binary cross-entropy (BCE):</p><formula xml:id="formula_1">L bce (x, y, w)=− X i [(1 − y i ) · log(1 − f i (x, w)) +y i · log f i (x, w)] .<label>(1)</label></formula><p>Even though the U-Net computes a structured output and considers large neighborhoods, this loss function treats every pixel independently. It does not capture the characteristics of the topology, such as the number of connected components or number of holes. This is especially important in the delineation of thin structures: as we have seen in <ref type="figure" target="#fig_0">Fig. 1(c, d)</ref>, the misclassification of a few pixels might have a low cost in terms of the pixel-wise BCE loss, but have a large impact on the topology of the predicted results. Therefore, we aim to introduce a penalty term in our loss function to account for this higher-order information. Instead of relying on a hand-designed metric, which is difficult to model and hard to generalize, we leverage the knowledge that a pretrained network contains about the structures of real-world images. In particular, we use the feature maps at several layers of a VGG19 network <ref type="bibr" target="#b25">[26]</ref> pretrained on the ImageNet dataset as a description of the higher-level features of the delineations. Our new penalty term tries to minimize the differences between the VGG19 descriptors of the ground-truth images and the corresponding predicted delineations:</p><formula xml:id="formula_2">L top (x, y, w)= N X n=1 1 M n W n H n Mn X m=1 kl m n (y) − l m n (f (x, w))k 2 2 ,<label>(2)</label></formula><p>1 For simplicity and without loss of generality, we assume that x andŷ have the same size. This is not the case in practice, and usuallyŷ corresponds to the predictions of a cropped area of x (see <ref type="bibr" target="#b20">[21]</ref> for details). ground truth where l m n denotes the m-th feature map in the n-th layer of the pretrained VGG19 network, N is the number of convolutional layers considered and M n is the number of channels in the n-th layer, each of size W n ⇥ H n . L top can be understood as a measurement of the difference between the higher-level visual features of the linear structures in the ground-truth and those in predicted image. These higherlevel features include concepts such as connectivity or holes that are ignored by the simpler pixel-wise BCE loss. <ref type="figure">Fig. 2</ref> shows examples where the pixel-wise loss is too weak to penalize properly a variety of errors that occur in the predicted delineations, while our loss L top correctly measures the topological importance of the errors in all cases: it penalizes more the mistakes that considerably change the structure of the image and those that do not resemble linear structures.</p><formula xml:id="formula_3">L top =0.2279 L top =0.7795 L top =0.2858 L top =0.9977 (a) (b) (c) (d)<label>(e)</label></formula><p>The reason behind the good performance of the VGG19 in this task can be seen in <ref type="figure">Fig. 3</ref>. Certain channels of the VGG19 layers are activated by the type of elongated structures we are interested in, while others respond strongly to small connected components. Thus, minimizing L top strongly penalizes generating small false positives, which do not exist in the ground-truth, and promotes the generation of elongated structures. On the other hand, the shape of the predictions is ignored by L bce .</p><p>In the end, we minimize</p><formula xml:id="formula_4">L(x, y, w)=L bce (x, y, w)+µL top (x, y, w) (3)</formula><p>with respect to w. µ is a scalar weighing the relative influence of both terms. We set it so that the order of magnitude of both terms is comparable. <ref type="figure" target="#fig_2">Fig. 4(a)</ref> illustrates the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Iterative refinement</head><p>The topology loss term of Eq. 2 improves the quality of the predictions. However, as we will see in Section 4, some mistakes still remain. They typically show up in the form of small gaps in lines that should be uninterrupted. We iteratively refine the predictions to eliminate such problems. At each iteration, the network takes both the input image and the prediction of the previous iteration to successively provide better predictions.</p><p>In earlier works that advocate a similarly iterative approach <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>, a different module f k is trained for each iteration k, thus increasing the number of parameters of the model and making training more demanding in terms of the amount of required labeled data. An interesting property of this iterative approach is that the correct delineation y should be the fixed point of each module f k , that is, feeding the correct delineation should return the input</p><formula xml:id="formula_5">y = f k (x ⊕ y),<label>(4)</label></formula><p>where ⊕ denotes channel concatenation and we omitted the weights of f k for simplicity. Assuming that every module f k is Lipschitz-continuous on y, 2 we know that the fixed-point iteration</p><formula xml:id="formula_6">f k (x ⊕ f k (x ⊕ f k (...)))<label>(5)</label></formula><p>converges to y. We leverage this fixed-point property to remove the necessity of training a different module at each iteration. Instead, we use the same single network f at each step of the refinement pipeline, as depicted in <ref type="figure" target="#fig_2">Fig. 4(b)</ref>. This makes our model much simpler and less demanding of labeled data for training. This approach has also been applied before in <ref type="bibr" target="#b19">[20]</ref> for image segmentation. Their application of the recurrent module was oriented towards increasing the spatial context. Rather than doing that, we keep the scale of the input to the modules fixed, in order to exploit the capacity of the network to correct its own errors. We show that it helps the network to learn a contraction map that successively improves the estimations. Our predictive model can therefore be expressed aŝ</p><formula xml:id="formula_7">y k+1 = f (x ⊕ŷ k , w),k =0,...,K − 1 ,<label>(6)</label></formula><p>where K is the total number of iterations andŷ K the final prediction. We initialize the model with an empty predictionŷ 0 = 0. Instead of minimizing only the loss for the final network output, we minimize a weighted sum of partial losses. The k-th partial model, with k  K, is the model obtained from iterating Eq. 6 k times. The k-th partial loss L k is the loss from Eq. 3 evaluated for the k-th partial model. Using this notation, we define our refinement loss as a weighted sum of the partial losses</p><formula xml:id="formula_8">L ref (x, y, w)= 1 Z K X k=1 k L k (x, y, w) ,<label>(7)</label></formula><p>with the normalization factor Z = P K k=1 k = 1 2 K(K + 1). We weigh more the losses associated with the final iterations to boost the accuracy of the final result. However, accounting for the earlier losses enables the network to learn from all the mistakes it can make along the way and increases numerical stability. It also avoids having to preprocess the predictions before re-injecting them into the computation, as in <ref type="bibr" target="#b8">[9]</ref>.</p><p>In practice, we first train a single module network, that is, for K =1 . We then increment K, retrain, and iterate. We limit K to 3 during training and testing as the results do not change significantly for larger K values. We will show that this successfully fills in small gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Data. We evaluate our approach on three datasets featuring very different kinds of linear structures:</p><p>1. Cracks: Images of cracks in road <ref type="bibr" target="#b35">[36]</ref>. It consists of 104 training and 20 test images. As can be seen in <ref type="figure" target="#fig_4">Fig. 5</ref>, the multiple shadows and cluttered background makes their detection a challenging task. Applications include quality inspection and material characterization. 2. Roads: The Massachusetts Roads Dataset <ref type="bibr" target="#b14">[15]</ref> is one of the largest publicly available collections of aerial road images, containing both urban and rural neighbourhoods, with many different kinds of roads ranging from small paths to highways. The set is split into 1108 training and 49 test images, 2 of which are shown in <ref type="figure" target="#fig_5">Fig. 6</ref> • . Additionally, in the EM dataset, we also apply elastic deformations as suggested in <ref type="bibr" target="#b20">[21]</ref> to compensate for the small amount of training data. Ground-truth of Cracks dataset consists of centerlines, so we dilate it by margin of 4 pixels to perform segmentation. We use batch normalization for faster convergence and use current batch statistics also at the test time as suggested in <ref type="bibr" target="#b2">[3]</ref>. We chose Adam <ref type="bibr" target="#b10">[11]</ref> with a learning rate of 10 −4 as our optimization method.</p><p>Pixel-wise metrics. Our algorithm outputs a probabilty map, which lends itself to evaluation in terms of precisionand recall-based metrics, such as the F1 score <ref type="bibr" target="#b22">[23]</ref> and the precision-recall break-even point <ref type="bibr" target="#b14">[15]</ref>. They are well suited for benchmarking binary segmentations, but their local character is a drawback in the presence of thin structures. Shifting a prediction even by a small distance in a direction perpendicular to the structure yields zero precision and recall, while still reasonably representing the data. We therefore evaluate the results in terms of correctness, completeness, and quality <ref type="bibr" target="#b32">[33]</ref>. They are metrics designed specifically for linear structures, which measure the similarity between predicted skeletons and ground truth-ones. They are more sensitive to precise locations or small width changes of the underlying structures. Potential shifts in centerline positions are handled by relaxing the notion of a true positive from being a precise coincidence of points to not exceeding a distance threshold. Correctness corresponds to relaxed precision, completeness to relaxed recall, and quality to intersection-over-union. We give precise definitions in appendix. In our experiments we use a threshold of 2 pixels for roads and cracks, and 1 for the neuronal membranes.</p><p>Topology-based metrics. The pixel-wise metrics are oblivious of topological differences between the predicted and ground-truth networks. A more topology-oriented set of measures was proposed in <ref type="bibr" target="#b31">[32]</ref>. It involves finding the shortest path between two randomly picked connected points in the predicted network and the equivalent path in the ground-truth network or vice-versa. If no equivalent path exists, the former is classified as infeasible. It is classified as too-long/-short if the length of the paths differ by more than 10%, and as correct otherwise. In practice, we sample 200 paths per image, which is enough for the proportion of correct, infeasible, and too-long/-short paths to stabilize. The organizers of the EM challenge use a performance metric called foreground-restricted random score, oriented at evaluating the preservation of separation between different cells. It measures the probability that two pixels belonging to the same cell in reality and in the predicted output. As  shown in <ref type="figure" target="#fig_0">Fig. 1(c,d)</ref>, this kind of metric is far more sensitive to topological perturbations than to pixel-wise errors.</p><p>Baselines and variants of the proposed method. We compare the results of our method to the following baselines:</p><p>• CrackTree [36] a crack detection method based on segmentation and subsequent graph construction • MNIH <ref type="bibr" target="#b14">[15]</ref>, a neural network for road segmentation in 64 ⇥ 64 image patches, • HED <ref type="bibr" target="#b33">[34]</ref>, a nested, multi-scale approach for edge detection, • U-Net <ref type="bibr" target="#b20">[21]</ref>, pixel labeling using the U-Net architecture with BCE loss, • CHM-LDNN <ref type="bibr" target="#b22">[23]</ref>, a multi-resolution recursive approach to delineating neuronal boundaries, • Reg-AC <ref type="bibr" target="#b26">[27]</ref>, a regression-based approach to finding centerlines and refining the results using autocontext. We reproduce the results for HED, MNIH, U-Net and Reg-AC, and report the results published in the original work for CHM-LDNN. We also perform an ablation study to isolate the individual contribution of the two main components of our approach. To this end, we compare two variants of it:</p><p>• OURS-NoRef, our approach with the topological loss of Eq. 3 but no refinement steps. To extract global features we use the channels from the VGG layers relu(conv1 2), relu(conv2 2) and relu(conv3 4), and set µ to 0.1 in Eq. 3.</p><p>• OURS, our complete, iterative method including the topological term and K =3refinement steps. It is trained using the refinement loss of Eq. 7 as explained in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Results</head><p>We start by identifying the best-performing configuration for our method. As can be seen in <ref type="table">Table 1</ref>(left), using all three first layers of the VGG network to compute the topology loss yields the best results. Similarly, we evaluated the impact of the number of improvement iterations on the resulting performance on the EM dataset, which we    <ref type="table">Table 2</ref>: Experimental results on the Roads and EM datasets.</p><p>(Left) Precision-recall break-even point (P/R) for the Roads dataset. Note the results are expressed in terms of the standard precision and recall, as opposed to the relaxed measures reported in <ref type="bibr" target="#b14">[15]</ref>. (Right) F1 scores for the EM dataset. present in <ref type="table">Table 1</ref>(right). The performance stabilizes after the third iteration. We therefore used three refinement iterations in all further experiments. Note that in <ref type="table">Table 1</ref>(right) the first iteration of OURS yields a result that is already better than OURS-NoRef. This shows that iterative training not only makes it possible to refine the results by iterating at test time, but also yields a better standalone classifier. We report results of our comparative experiments for the three datasets in <ref type="table" target="#tab_4">Tables 2, 3</ref>, and 4. Even without refinement, our topological loss outperforms all the baselines. Refinement boosts the performance yet further. The differences are greater when using the metrics specifically designed to gauge the quality of linear structures in <ref type="table" target="#tab_4">Table 3</ref> and even more when using the topology-based metrics in <ref type="table">Table 4</ref>. This confirms the hypothesis that our contributions improve the quality of the predictions mainly in its topolog-  ical aspect. The improvement in per-pixel measures, presented in <ref type="table">Table 2</ref> suggests that the improved topology is correlated with better localisation of the predictions.</p><p>Finally, we submitted our results to the ISBI challenge server for the EM task. We received a foreground-restricted random score of 0.981. This puts us in first place among algorithms relying on a single classifier without additional processing. In second place is the recent method of <ref type="bibr" target="#b24">[25]</ref>, which achieves the slightly lower score of 0.978 even though it relies on a significantly more complex classifier.    <ref type="table">Table 4</ref>: The percentage of correct, infeasible and too-long/tooshort paths sampled from predictions and ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>Figs. 5, 6, and 7 depict typical results on the three datasets. Note that adding our topology loss term and iteratively refining the delineations makes our predictions more structured and consistently eliminates false positives in the background, without losing the curvilinear structures of interest as shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. For example, in the aerial images of <ref type="figure" target="#fig_5">Fig. 6</ref>, line-like structures such as roofs and rivers are filtered out because they are not part of the training data, while the roads are not only preserved but also enhanced by closing small gaps. In the case of neuronal membranes, the additional topology term eliminates false positives corresponding to cell-like structures such as mitochondria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a new loss term that accounts for topology of curvilinear structures by exploiting their higherlevel features. We have further improved it by applying recursive refinement that does not increase the number of parameters to be learned. Our approach is generic and can be used for detection of many types of linear structures including roads and cracks in natural images and neuronal membranes in micrograms. We have relied on the U-Net to demonstrate it but it could be used in conjunction with any other network architecture. In future work, we will explore the use of adversarial networks to adapt our measure of topological similarity and learn more discriminative features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Linear structures. (a) Detected roads in an aerial image. (b) Detected cell membranes in an electron microscopy (EM) image. (c) Segmentation obtained after detecting neuronal membranes using</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The effect of mistakes on topology loss. (a) Ground truth (b)-(e) we flip 240 pixels in each prediction, so that L bce is the same for all of them, but as we see Ltop penalizes more the cases with more small mistakes, which considerably change the structure of the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Network architecture. (a) We use the U-Net for delineation purposes. During training, both its output and the ground-truth image serve as input to a pretrained VGG network. The loss Ltop is computed from the VGG responses. The loss L bce is computed pixelwise between the prediction and the ground-truth. (b) Our model iteratively applies the same U-Net f to produce progressive refinements of the predicted delineation. The final loss is a weighted sum of partial losses L k computed at the end of each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>VGG</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Cracks. From left to right: image, Reg-AC, U-Net, OURS-NoRef and OURS prediction, ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Roads. From left to right: image, MNIH, U-Net, OURS-NoRef and OURS prediction, ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: EM. From left to right: image, Reg-AC, U-Net, OURS-NoRef and OURS prediction, ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Iterative Refinement. Prediction after 1, 2 and 3 refinement iterations. The right-most image is the ground-truth. The red boxes highlight parts of the image where refinement is closing gaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Dataset Method Correct. Complet. Quality</figDesc><table>Cracks 

CrackTree [36] 
0.7900 
0.9200 
0.7392 
Reg-AC [27] 
0.1070 
0.9283 
0.1061 
U-Net [21] 
0.4114 
0.8936 
0.3924 
OURS-NoRef 
0.7955 
0.9208 
0.7446 
OURS 
0.8844 
0.9513 
0.8461 

Roads 

Reg-AC [27] 
0.2537 
0.3478 
0.1719 
MNIH [15] 
0.5314 
0.7517 
0.4521 
U-Net [21] 
0.6227 
0.7506 
0.5152 
OURS-NoRef 
0.6782 
0.7986 
0.5719 
OURS 
0.7743 
0.8057 
0.6524 

EM 

Reg-AC [27] 
0.7110 
0.6647 
0.5233 
U-Net [21] 
0.6911 
0.7128 
0.5406 
OURS-NoRef 
0.7096 
0.7231 
0.5580 
OURS 
0.7227 
0.7358 
0.5722 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Correctness, completeness and quality scores for ex- tracted centerlines.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Lipschitz continuity is a direct consequence of the assumption that every f k will always improve the prediction of the previous iteration.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Crowdsourcing the Sreation of Image Segmentation Algorithms for Connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kamentsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Uher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schindelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Frontiers in Neuroanatomy, page 142</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering LineNetworks in Images by Junction-Point Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Forstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lafarge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Dense Volumetric Segmentation from Sparse Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>3d U-Net</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating Images with Perceptual Similarity Metrics based on Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">N4-Fields: Neural Network Nearest Neighbor Fields for Image Transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image Style Transfer Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Road Centreline Extraction from High-Resolution Imagery Based on Multiscale Structural Features and Support Vector Machines. International Journal of Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Januszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maitin-Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kornfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Flood-Filling Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densecap: Fully Convolutional Localization Networks for Dense Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Three Dimensional Curvilinear Structure Detection Using Optimally Oriented Flux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Retinal Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeproadmapper: Extracting Road Topology from Aerial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyusand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urtasun</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Machine Learning for Aerial Image Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to Detect Roads in HighResolution Aerial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to Label Aerial Images from Noisy Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active Learning and Proofreading for Delineation of Curvilinear Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tarnawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks for Scenel Labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Marked point process model for curvilinear structures extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="436" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-stage Multi-recursive-input Fully Convolutional Networks for Neuronal Boundary Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiscale Centerline Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1327" to="1341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Gibbs Point Process for Road Extraction from Remotely Sensed Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="136" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-Context and Its Applications to HighLevel Vision Tasks and 3D Brain Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-Directional Oriented Flux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Glowacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Benmansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>December</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconstructing Curvilinear Networks Using Path Classifiers and Integer Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Benmansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Glowacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2515" to="2530" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A HigherOrder CRF Model for Road Network Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montoya-Zegarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Empirical Evaluation Of Automatically Extracted Road Axes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jamet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Evaluation Techniques in Computer Vision</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="172" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Holistically-Nested Edge Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Learning Based Deformable Template Matching Method for Automatic Rib Centerline Extraction and Labeling in CT Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tietjen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Puskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CrackTree: Automatic crack detection from pavement images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
