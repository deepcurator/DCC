<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Affinity Fields for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Wei</forename><surname>Ke</surname></persName>
							<email>twke@berkeley.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>zwliu@berkeley.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
							<email>stellayu@berkeley.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Affinity Fields for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>semantic segmentation</term>
					<term>affinity field</term>
					<term>adversarial learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Semantic segmentation has made much progress with increasingly powerful pixel-wise classifiers and incorporating structural priors via Conditional Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose a simpler alternative that learns to verify the spatial structure of segmentation during training only. Unlike existing approaches that enforce semantic labels on individual pixels and match labels between neighbouring pixels, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the semantic relations between neighbouring pixels in the label space. We use adversarial learning to select the optimal affinity field size for each semantic category. It is formulated as a minimax problem, optimizing our segmentation neural network in a best worst-case learning scenario. AAF is versatile for representing structures as a collection of pixel-centric relations, easier to train than GAN and more efficient than CRF without run-time inference. Our extensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par segmentation performance and robust generalization across domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation of an image refers to the challenging task of assigning each pixel a categorical label, e.g., motorcycle or person. Segmentation performance is often measured in a pixel-wise fashion, in terms of mean Intersection over Union (mIoU) across categories between the ground-truth <ref type="figure">(Fig. 1b)</ref> and the predicted label map <ref type="figure">(Fig. 1c)</ref>.</p><p>Much progress has been made on segmentation with convolutional neural nets (CNN), mostly due to increasingly powerful pixel-wise classifiers, e.g., VGG-16 <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21]</ref> and ResNet <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>, with the convolutional filters optimized by minimizing the average pixel-wise classification error over the image.</p><p>Even with big training data and with deeper and more complex network architectures, pixel-wise classification based approaches fundamentally lack the spatial discrimination power when foreground pixels and background pixels are close or mixed together: Segmentation is poor when the visual evidence for the foreground is weak, <ref type="figure">Fig. 1</ref>. We propose new pairwise pixel loss functions that capture the spatial structure of segmentation. Given an image (a), the task is to predict the ground-truth labeling (b). When a deep neural net is trained with conventional softmax cross-entropy loss on individual pixels, the predicted segmentation (c) is often based on visual appearance and oblivious of the spatial structure of each semantic class. Our work imposes an additional pairwise pixel label affinity loss (d), matching the label relations among neighouring pixels between the prediction and the ground-truth. We also learn the neighbourhood size for each semantic class, and our adaptive affinity fields result (e) picks out both large bicycle shields and thin spokes of round wheels.</p><p>e.g., glass motorcycle shields, or when the spatial structure is small, e.g., thin radial spokes of all the wheels <ref type="figure">(Fig. 1c)</ref>.</p><p>There have been two main lines of efforts at incorporating structural reasoning into semantic segmentation: Conditional Random Field (CRF) methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref> and Generative Adversarial Network (GAN) methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>1. CRF enforces label consistency between pixels measured by the similiarity in visual appearance (e.g., raw pixel value). An optimal labeling is solved via message passing algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>. CRF is employed either as a post-processing step <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6]</ref>, or as a plug-in module inside deep neural networks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref>. Aside from its time-consuming iterative inference routine, CRF is also sensitive to visual appearance changes. 2. GAN is a recent alternative for imposing structural regularity in the neural network output. Specifically, the predicted label map is tested by a discriminator network on whether it resembles ground truth label maps in the training set. GAN is notoriously hard to train, particularly prone to model instability and mode collapses <ref type="bibr" target="#b26">[27]</ref>.</p><p>We propose a simpler approach, by learning to verify the spatial structure of segmentation during training only. Instead of enforcing semantic labels on individual pixels and matching labels between neighbouring pixels using CRF or GAN, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the relations between neighbouring pixels in the label space. How the semantic label of each pixel is related to those of neighboring pixels, e.g., whether they are same or different, provides a distributed and pixel-centric description of semantic relations in the space and collectively they describe Motorcycle wheels are round with thin radial spokes. We develop new affinity field matching loss functions to learn a CNN that automatically outputs a segmentation respectful of spatial structures and small details.</p><p>The pairwise pixel affinity idea has deep roots in perceptual organization, where local affinity fields have been used to characterize the intrinsic geometric structures in early vision <ref type="bibr" target="#b25">[26]</ref>, the grouping cues between pixels for image segmentation via spectral graph partitioning <ref type="bibr" target="#b30">[31]</ref>, and the object hypothesis for non-additive score verification in object recognition at the run time <ref type="bibr" target="#b0">[1]</ref>.</p><p>Technically, affinity fields at different neighbourhood sizes encode structural relations at different ranges. Matching the affinity fields at a fixed size would not work well for all semantic categories, e.g., thin structures are needed for persons seen at a distance whereas large structures are for cows seen close-up.</p><p>One straightforward solution is to search over a list of possible affinity field sizes, and pick the one that yields the minimal affinity matching loss. However, such a practice would result in selecting trivial sizes which are readily satisfied. For example, for large uniform semantic regions, the optimal affinity field size would be the smallest neighbourhood size of 1, and any pixel-wise classification would already get them right without any additional loss terms in the label space.</p><p>We propose adversarial learning for size-adapted affinity field matching. Intuitively, we select the right size by pushing the affinity field matching with different sizes to the extreme: Minimizing the affinity loss should be hard enough to have a real impact on learning, yet it should still be easy enough for the network to actually improve segmentation towards the ground-truth, i.e., a best worst-case learning scenario. Specifically, we formulate our AAF as a minimax problem where we simultaneously maximize the affinity errors over multiple kernel sizes and minimize the overall matching loss. Consequently, our adversarial network learns to assign a smaller affinity field size to person than to cow, as the person category contains finer structures than the cow category.</p><p>Our AAF has a few appealing properties over existing approaches <ref type="table">(Table 1)</ref>.  <ref type="table">Table 1</ref>. Key differences between our method and other popular structure modeling approaches, namely CRF <ref type="bibr" target="#b14">[15]</ref> and GAN <ref type="bibr" target="#b11">[12]</ref>. The performance (% mIoU) is reported with PSPNet <ref type="bibr" target="#b35">[36]</ref> architecture on the Cityscapes <ref type="bibr" target="#b9">[10]</ref> validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>1. It provides a versatile representation that encodes spatial structural information in distributed, pixel-centric relations. 2. It is easier to train than GAN and more efficient than CRF, as AAF only impacts network learning during training, requiring no extra parameters or inference processes during testing. 3. It is more generalizable to visual domain changes, as AAF operates on the label relations not on the pixel values, capturing desired intrinsic geometric regularities despite of visual appearance variations.</p><p>We demonstrate its effectiveness and efficiency with extensive evaluations on Cityscapes <ref type="bibr" target="#b9">[10]</ref> and PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> datasets, along with its remarkable generalization performance when our learned networks are applied to the GTA5 dataset <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Most methods treat semantic segmentation as a pixel-wise classification task, and those that model structural correlations provide a small gain at a large computational cost. Semantic Segmentation. Since the introduction of fully convolutional networks for semantic segmentation <ref type="bibr" target="#b20">[21]</ref>, deeper <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref> and wider <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> network architectures have been explored, drastically improving the performance on benchmarks such as PASCAL VOC <ref type="bibr" target="#b10">[11]</ref>. For example, Wu et al. <ref type="bibr" target="#b32">[33]</ref> achieved higher segmentation accuracy by replacing backbone networks with more powerful ResNet <ref type="bibr" target="#b13">[14]</ref>, whereas Yu et al. <ref type="bibr" target="#b33">[34]</ref> tackled fine-detailed segmentation using atrous convolutions. While the performance gain in terms of mIoU is impressive, these pixel-wise classification based approaches fundamentally lack the spatial discrimination power when foreground and background pixels are close or mixed together, resulting in unnatural artifacts in <ref type="figure">Fig. 1c</ref>. Structure Modeling. Image segmentation has highly correlated outputs among the pixels. Formulating it as an independent pixel labeling problem not only makes the pixellevel classification unnecessarily hard, but also leads to artifacts and spatially incoherent results. Several ways to incorporate structure information into segmentation have been investigated <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref>. For example, Chen et al. <ref type="bibr" target="#b5">[6]</ref> utilized dense-CRF <ref type="bibr" target="#b14">[15]</ref> as post-processing to refine the final segmentation results. Zheng et al. <ref type="bibr" target="#b36">[37]</ref> and Liu et al. <ref type="bibr" target="#b18">[19]</ref> further made the CRF module differentiable within the deep neural network. Pairwise low-level image cues, such as grouping affinity <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b17">18]</ref> and contour cues <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, have also been used to encode structures. However, these methods are sensitive to visual appearance changes, or require expensive iterative inference procedures.</p><p>Our work provides another perspective to structure modeling by matching the relations between neighbouring pixels in the label space. Our segmentation network learns to verify the spatial structure of segmentation only during training; once it is trained, it is ready for deployment without run-time inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach: Adaptive Affinity Fields</head><p>We first briefly revisit the classic pixel-wise cross-entropy loss commonly used in semantic segmentation. The drawbacks of pixel-wise supervision lead to our concept of region-wise supervision. We then describe our region-wise supervision through affinity fields, and introduce an adversarial process that learns an adaptive affinity kernel size for each category. We summarize the overall AAF architecture in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From Pixel-wise Supervision to Region-wise Supervision</head><p>Pixel-wise cross-entropy loss is most often used in CNNs for semantic segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref>. It penalizes pixel-wise predictions independently and is known as a form of unary supervision. It implicitly assumes that the relationships between pixels can be learned as the effective receptive field increases with deeper layers. Given predicted categorical probabilityŷ i (l) at pixel i w.r.t. its ground truth categorical label l, the total loss is the average of cross-entropy loss at pixel i:</p><formula xml:id="formula_0">L i unary = L i cross-entropy = − logŷ i (l).</formula><p>(</p><p>Such a unary loss does not take the semantic label correlation and scene structure into account. The objects in different categories interact with each other in a certain pattern. For example, cars are usually on the road while pedestrians on the sidewalk; buildings are surrounded by the sky but never on top of it. Also, some shapes of a certain category occur more frequently, such as rectangles in trains, circles in bikes, and straight vertical lines in poles. This kind of inter-class and inner-class pixel relationships are informative and can be integrated into learning as structure reasoning. We are thus inspired to propose an additional region-wise loss to impose penalties on inconsistent unary predictions and encourage the network to learn such intrinsic pixel relationships. Region-wise supervision extends its pixel-wise counterpart from independent pixels to neighborhoods of pixels, i.e., , the region-wise loss considers a patch of predictions and ground truth jointly. Such region-wise supervision L region involves designing a specific loss function for a patch of predictions N (ŷ i ) and corresponding patch of ground truth N (y i ) centered at pixel i, where N (·) denotes the neighborhood.</p><p>The overall objective is hence to minimize the combination of unary and region losses, balanced by a constant λ:</p><formula xml:id="formula_2">S * = argmin S L = argmin S 1 n i L i unary (ŷ i , y i ) + λL i region N (ŷ i ), N (y i ) ,<label>(2)</label></formula><p>where n is the total number of pixels. We omit index i and averaging notations for simplicity in the rest of the paper. The benefits of the addition of region-wise supervision have been explored in previous works. For example, Luc et al. <ref type="bibr" target="#b21">[22]</ref> exploited GAN <ref type="bibr" target="#b11">[12]</ref> as structural priors, and Mostajabi et al. <ref type="bibr" target="#b23">[24]</ref> pre-trained an additional auto-encoder to inject structure priors into training the segmentation network. However, their approaches require much hyperparameter tuning and are prone to overfitting, resulting in very small gains over strong baseline models. Please see <ref type="table">Table 1</ref> for a comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Affinity Field Loss Function</head><p>Our affinity field loss function overcome these drawbacks and is a flexible region-wise supervision approach that is also easy to optimize.</p><p>The use of pairwise pixel affinity has a long history in image segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref>. The grouping relationships between neighbouring pixels are derived from the image and represented by a graph, where a node denotes a pixel and a weighted edge between two nodes captures the similarity between two pixels. Image segmentation then becomes a graph partitioning problem, where all the nodes are divided into disjoint sets, with maximal weighted edges within the sets and minimal weighted edges between the sets.</p><p>We define pairwise pixel affinity based not on the image, but on ground-truth label map. There are two types of label relationships between a pair of pixels: whether their labels are the same or different. If pixel i and its neighbor j have the same categorical label, we impose a grouping force which encourages network predictions at i and j to be similar. Otherwise, we impose a separating force which pushes apart their label predictions. These two forces are illustrated in <ref type="figure">Fig. 3</ref> left.</p><p>Specifically, we define a pairwise affinity loss based on KL divergence between binary classification probabilities, consistent with the cross-entropy loss for the unary label prediction term. For pixel i and its neighbour j, depending on whether two pixels belong to the same category c in the ground-truth label map y, we define a non-boundary term L ibc affinity for the grouping force and an boundary term L ibc affinity for the separating force in the prediction mapŷ:</p><formula xml:id="formula_3">L ic affinity = L ibc affinity = D KL (ŷ j (c)||ŷ i (c)) if y i (c) = y j (c) L ibc affinity = max{0, m − D KL (ŷ j (c)||ŷ i (c))} otherwise (3) D KL (·)</formula><p>is the Kullback-Leibler divergence between two Bernoulli distributions P and Q with parameters p and q respectively: D KL (P ||Q) = p log p q +p logp q for the binary distribution [p, 1 − p] and [q, 1 − q], where p, q ∈ [0, 1]. For simplicity, we abbreviate the notation as D KL (p||q).ŷ j (c) denotes the prediction probability of j in class c. The overall loss is the average of L ic affinity over all categories and pixels. Discussion 1. Our affinity loss encourages similar network predictions on two pixels of the same ground-truth label, regardless of what their actual labels are. The collection of such pairwise bonds inside a segment ensure that all the pixels achieve the same label. On the other hand, our affinity loss pushes network predictions apart on two pixels of different ground-truth labels, again regardless of what their actual labels are. The collection of such pairwise repulsion help create clear segmentation boundaries. Discussion 2. Our affinity loss may appear similar to CRF <ref type="bibr" target="#b14">[15]</ref> on the pairwise grouping or separating forces between pixels. However, a crucial difference is that CRF models require iterative inference to find a solution, whereas our affinity loss only impacts <ref type="figure">Fig. 3</ref>. Left: Our affinity field loss separates predicted probabilities across the boundary and unifies them within the segment. Right: The affinity fields can be defined over multiple ranges. Minimizing the affinity loss over different ranges results in trivial solutions which are readily satisfied. Our size-adaptive affinity field loss is achieved with adversarial learning: Maximizing the affinity loss over different kernel sizes selects the most critical range for imposing pairwise relationships in the label space, and our goal is to minimize this maximal loss -i.e., use the best worst case scenario for most effective training.</p><p>the network training with pairwise supervision. A similar perspective is metric learning with contrastive loss <ref type="bibr" target="#b8">[9]</ref>, commonly used in face identification tasks. Our affinity loss works better for segmentation tasks, because it penalizes the network predictions directly, and our pairwise supervision is in addition to and consistent with the conventional unary supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Kernel Sizes from Adversarial Learning</head><p>Region-wise supervision often requires a preset kernel size for CNNs, where pairwise pixel relationships are measured in the same fashion across all pixel locations. However, we cannot expect one kernel size fits all categories, since the ideal kernel size for each category varies with the average object size and the object shape complexity.</p><p>We propose a size-adaptive affinity field loss function, optimizing the weights over a set of affinity field sizes for each category in the loop:</p><formula xml:id="formula_4">L multiscale = c k w ck L ck region s.t. k w ck = 1 and w ck ≥ 0<label>(4)</label></formula><p>where L ck region is a region loss defined in Eqn. <ref type="formula" target="#formula_2">(2)</ref>, yet operating on a specific class channel c with kernel size k × k with a corresponding weighting w ck .</p><p>If we just minimize the affinity loss with size weighting w included, w would likely fall into a trivial solution. As illustrated in <ref type="figure">Fig 3 right</ref>, the affinity loss would be minimum if the smallest kernels are highly weighted for non-boundary terms and the largest kernels for boundary terms, since nearby pixels are more likely to belong to the same object and far-away pixels to different objects. Unary predictions based on the image would naturally have such statistics, nullifying any potential effect from our pairwise affinity supervision.</p><p>To optimize the size weighting without trivializing the affinity loss, we need to push the selection of kernel sizes to the extreme. Intuitively, we need to enforce pixels in the same segment to have the same label prediction as far as possible, and likewise to enforce pixels in different segments to have different predictions as close as possible. We use the best worst case scenario for most effective training.</p><p>We formulate the adaptive kernel size selection process as optimizing a two-player minimax game: While the segmenter should always attempt to minimize the total loss, the weighting for different kernel sizes in the loss should attempt to maximize the total loss in order to capture the most critical neighbourhood sizes. Formally, we have:</p><formula xml:id="formula_5">S * = argmin S max w L unary + L multiscale .<label>(5)</label></formula><p>For our size-adaptive affinity field learning, we separate the non-boundary term Lb ck affinity and boundary term L bck affinity in Eqn (3) since their ideal kernel sizes would be different. Our adaptive affinity field (AAF) loss becomes:</p><formula xml:id="formula_6">S * = argmin S max w L unary + L AAF ,<label>(6)</label></formula><formula xml:id="formula_7">L AAF = c k (wb ck Lb ck affinity + w bck L bck affinity ),<label>(7)</label></formula><p>s.t. 4 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We compare our proposed affinity fields and AAF with other competing methods on the PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> and Cityscapes <ref type="bibr" target="#b9">[10]</ref> datasets. PASCAL VOC 2012. PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> segmentation dataset contains 20 object categories and one background class. Following the procedure of <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b5">[6]</ref>, we use augmented data with the annotations of <ref type="bibr" target="#b12">[13]</ref>, resulting in 10,582, 1,449, and 1,456 images for training, validation and testing. Cityscapes. Cityscapes <ref type="bibr" target="#b9">[10]</ref> is a dataset for semantic urban street scene understanding. 5000 high quality pixel-level finely annotated images are divided into training, validation, and testing sets with 2975, 500, and 1525 images, respectively. It defines 19 categories containing flat, human, vehicle, construction, object, nature, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>All existing semantic segmentation works adopt pixel-wise mIoU <ref type="bibr" target="#b20">[21]</ref> as their metric.</p><p>To fully examine the effectiveness of our AAF on fine structures in particular, we also evaluate all the models using instance-wise mIoU and boundary detection metrics. Instance-wise mIoU. Since the pixel-wise mIoU metric is often biased toward large objects, we introduce the instance-wise mIoU to alleviate the bias, which allow us to evaluate fairly the performance on smaller objects. The per category instance-wise mIoU is</p><formula xml:id="formula_8">formulated asÛ c = x nc,x×Uc,x x nc,x</formula><p>, where n c,x and U c,x are the number of instances and IoU of class c in image x, respectively. Boundary detection metrics. We compute semantic boundaries using the semantic predictions and benchmark the results using the standard benchmark for contour detection proposed by <ref type="bibr" target="#b1">[2]</ref>, which summarizes the results by precision, recall, and f-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Methods of Comparison</head><p>We briefly describe other popular methods that are used for comparison in our experiments, namely, GAN's adversarial learning <ref type="bibr" target="#b11">[12]</ref>, contrastive loss <ref type="bibr" target="#b8">[9]</ref>, and CRF <ref type="bibr" target="#b14">[15]</ref>.</p><p>GAN's Adversarial Learning. We investigate a popular framework, the Generative Adversarial Networks (GAN) <ref type="bibr" target="#b11">[12]</ref>. The discriminator D in GAN works as injecting priors for region structures. The adversarial loss is formulated as</p><formula xml:id="formula_9">L i adversarial = log D(N (y i )) + log(1 − D(N (ŷ i ))).<label>(8)</label></formula><p>We simultaneously train the segmentation network S to minimize log(1 − D(N (ŷ i ))) and the discriminator to maximize L adversarial .</p><p>Pixel Embedding. We study the region-wise supervision over feature map, which is implemented by imposing the contrastive loss <ref type="bibr" target="#b8">[9]</ref> on the last convolutional layer before the softmax layer. The contrastive loss is formulated as</p><formula xml:id="formula_10">L i contrast = L iē contrast = f j − f i 2 2 if y i (c) = y j (c) L ie contrast = max{0, m − f j − f i 2 2 } otherwise, (9)</formula><p>where f i denotes L 2 -normalized feature vector at pixel i, and m is set to 0.2.</p><p>CRF-based Processing. We follow <ref type="bibr" target="#b5">[6]</ref>'s implementation by post-processing the prediction with dense-CRF <ref type="bibr" target="#b14">[15]</ref>. We set bi w to 1, bi xy std to 40, bi rgb std to 3, pos w to 1, and pos xy std to 1 for all experiments. It is worth mentioning that CRF takes additional 40 seconds to generate the final results on Cityscapes, while our proposed methods introduce no inference overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>Our implementation follows the ones of base architectures, which are PSPNet <ref type="bibr" target="#b35">[36]</ref> in most cases or FCN <ref type="bibr" target="#b20">[21]</ref>. We use the poly learning rate policy where the current learning rate equals the base one multiplied by (1 − iter max iter ) 0.9 . We set the base learning rate as 0.001. The training iterations for all experiments is 30K on VOC dataset and 90K on Cityscapes dataset while the performance can be further improved by increasing the iteration number. Momentum and weight decay are set to 0.9 and 0.0005, respectively. For data augmentation, we adopt random mirroring and random resizing between 0.5 and 2 for all datasets. We do not upscale the logits (prediction map) back to the input image resolution, instead, we follow <ref type="bibr" target="#b5">[6]</ref>'s setting by downsampling the ground-truth labels for training (output stride = 8).</p><p>PSPNet <ref type="bibr" target="#b35">[36]</ref> shows that larger "cropsize" and "batchsize" can yield better performance. In their implementation, "cropsize" can be up to 720 × 720 and "batchsize" to 16 using 16 GPUs. To speed up the experiments for validation on VOC, we downsize "cropsize" to 336 × 336 and "batchsize" to 8 so that a single GTX Titan X GPU is sufficient for training. We set "cropsize" to 480 × 480 during inference. For testing on PASCAL VOC 2012 and all experiments on Cityscapes dataset, we use 4-GPUs to train the network. On VOC dataset, we set the "batchsize" to 16 and set "cropsize" to 480 × 480. On Cityscaeps, we set the "batchsize" to 8 and "cropsize" to 720 × 720. For inference, we boost the performance by averaging scores from left-right flipped and multi-scale inputs (scales = {0.5, 0.75, 1, 1.25, 1.5, 1.75}).</p><p>For affinity fields and AAF, λ is set to 1.0 and margin m is set to 3.0. We use ResNet101 <ref type="bibr" target="#b13">[14]</ref> as the backbone network and initialize the models with weights pretrained on ImageNet <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We benchmark our proposed methods on two datasets, PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> and Cityscapes <ref type="bibr" target="#b9">[10]</ref>. All methods are evaluated by three metrics: mIoU, instance-wise mIoU and boundary detection recall. We include some visual examples to demonstrate the effectiveness of our proposed methods in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pixel-level Evaluation</head><p>Validation Results. For training on PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref>, we first train on train aug for 30K iterations and then fine-tune on train for another 30K iterations with base learning rate as 0.0001. For Cityscapes <ref type="bibr" target="#b9">[10]</ref>, we only train on finely annotated images for 90K iterations. We summarize the mIoU results on validation set in <ref type="table">Table 2 and  Table 3</ref>, respectively.</p><p>With FCN <ref type="bibr" target="#b20">[21]</ref> as base architecture, the affinity field loss and AAF improve the performance by 2.16% and 3.04% on VOC and by 1.88% and 2.37% on Cityscapes. With PSPNet <ref type="bibr" target="#b35">[36]</ref> as the base architecture, the results also improves consistently: GAN  <ref type="table">Table 4</ref>. Per-class results on Pascal VOC 2012 testing set.</p><p>loss, embedding contrastive loss, affinity field loss and AAF improve the mean IoU by 0.62%, 1.24%, 1.68% and 2.27% on VOC; affinity field loss and AAF improve by 2.00% and 2.52% on Cityscapes. It is worth noting that large improvements over PSPNet on VOC are mostly in categories with fine structures, such as "bike", "chair", "person", and "plant". Testing Results. On PASCAL VOC 2012, the training procedure for PSPNet and AAF is the same as follows: We first train the networks on train aug and then fine-tune on train val. We report the testing results on VOC 2012 and Cityscapes in <ref type="table">Table 4 and  Table 5</ref>, respectively. Our re-trained PSPnet does not reach the same performance as originally reported in the paper because we do not bootstrap the performance by finetuning on hard examples (like "bike" images), as pointed out in <ref type="bibr" target="#b6">[7]</ref>. We demonstrate that our proposed AAF achieve 82.17% and 79.07% mIoU, which is better than the PSPNet by 1.54% and 2.77% and competitive to the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Instance-level Evaluation</head><p>We measure the instance-wise mIoU on VOC and Cityscapes validation set as summarized in <ref type="table">Table 6 and Table 7</ref>, respectively In instance-wise mIoU, our AAF is higher than base architecture by 3.94% on VOC and 2.94% on Cityscapes. The improvements on  <ref type="table">Table 7</ref>. Per-class instance-wise IOU results on Cityscapes validation set.</p><p>fine-structured categories are more prominent. For example, the "bottle" is improved by 12.89% on VOC, "pole" and "tlight" is improved by 9.51% and 9.04% on Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Boundary-level Evaluation</head><p>Next, we analyze quantitatively the improvements of boundary localization. We include the boundary recall on VOC in <ref type="table" target="#tab_3">Table 8</ref> and Cityscapes in <ref type="table">Table 9</ref>. We omit the precision table due to smaller performance difference. The overall boundary recall is improved by 7.9% and 8.0% on VOC and Cityscapes, respectively. It is worth noting that the boundary recall is improved for every category. This result demonstrates that boundaries of all categories can all benefit from affinity fields and AAF. Among all, the improvements on categories with complicated boundaries, such as "bike", "bird", "boat", "chair", "person", and "plant" are significant on VOC. On Cityscapes, objects with thin structures are improved most, such as "pole", "tlight", "tsign", "person", "rider", and "bike".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Adaptive Affinity Field Size Analysis</head><p>We further analyze our proposed AAF methods on: 1) optimal affinity field size for each category, and 2) effective combinations of affinity field sizes. Optimal Adaptive Affinity Field Size. We conduct experiments on VOC with our proposed AAF on three k×k kernel sizes where k = 3, 5, 7. We report the optimal adaptive kernel size on the contour term calculated as k e c = k w eck × k, and summarized in <ref type="figure" target="#fig_2">Fig. 4</ref>. As shown, "person" and "dog" benefit from smaller kernel size (3.1 and 3.4), while "cow" and "plant"from larger kernel size (4.6 and 4.5). We display some image patches with the corresponding effective receptive field size. Combinations of Affinity Field Sizes. We explore the effectiveness of different selections of k × k kernels, where k ∈ {3, 5, 7}, for AAF. Summarized in <ref type="table" target="#tab_5">Table 10</ref>, we observe that combinations of 3 × 3 and 5 × 5 kernels have the optimal performance.  <ref type="table">Table 9</ref>. Per-class boundary recall results on Cityscapes validation set.    <ref type="table">Table 11</ref>. Per-class results on GTA5 Part 1. <ref type="figure">Fig. 5</ref>. Visual quality comparisons on the VOC 2012 <ref type="bibr" target="#b10">[11]</ref> validation set (the first four rows), Cityscapes <ref type="bibr" target="#b9">[10]</ref> validation set (the middle two rows) and GTA5 <ref type="bibr" target="#b27">[28]</ref> part 1 (the bottom row): (a) image, (b) ground truth, (c) PSPNet <ref type="bibr" target="#b35">[36]</ref>, (d) affinity fields, and (e) adaptive affinity fields (AAF).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Method overview: Learning semantic segmentation with adaptive affinity fields. The adaptive affinity fields consist of two parts: the affinity field loss with multiple kernel sizes and corresponding categorical adversarial weightings. Note that the adaptive affinity fields are only introduced during training and there is no extra computation during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>= 1 and wb ck , w bck ≥ 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Left: The optimal weightings for different kernel sizes of the edge term in AAF for each category on PASCAL VOC 2012 validation set. Right: Visualization of image patches with corresponding effective receptive field sizes, suggesting how kernel sizes capture the shape complexity in critical regions of different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Structure Guidance Training Run-time Inference Performance</figDesc><table>CRF [15] 
input image 
medium 
yes 
76.53 
GAN [12] ground-truth labels 
hard 
no 
76.20 
Our AAF 
label affinity 
easy 
no 
79.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoUPer-class results on Pascal VOC 2012 validation set. Gray colored background denotes using FCN as the base architecture. Method road swalk build. wall fence pole tlight tsign veg. terrain sky person rider car truck bus train mbike bike mIoUPer-class results on Cityscapes validation set. Gray colored background denotes using FCN as the base architecture. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU</figDesc><table>FCN 
86.95 59.25 85.18 70.33 73.92 78.86 82.30 85.64 33.57 69.34 27.41 78.04 71.45 70.45 85.54 57.42 71.55 32.48 74.91 59.10 68.91 
PSPNet 92.56 66.70 91.10 76.52 80.88 94.43 88.49 93.14 38.87 89.33 62.77 86.44 89.72 88.36 87.48 56.95 91.77 46.23 88.59 77.14 80.12 

Affinity 88.66 59.25 87.85 72.19 76.36 80.65 80.74 87.82 35.38 73.45 30.17 79.84 68.15 73.52 87.96 53.95 75.46 37.15 76.62 73.42 71.07 
AAF 88.15 67.83 87.06 72.05 76.45 85.43 80.58 88.33 35.47 72.76 31.55 79.68 67.01 77.96 88.20 50.31 73.16 42.71 78.14 73.87 71.95 
GAN 92.36 65.94 91.80 76.35 77.70 95.39 89.21 93.30 43.35 89.25 61.81 86.93 91.28 87.43 87.21 68.15 90.64 49.64 88.79 73.83 80.74 
Emb. 91.28 69.50 92.62 77.60 78.74 95.03 89.57 93.67 43.21 88.76 62.47 86.68 91.28 88.47 87.44 69.21 91.53 52.17 89.30 74.60 81.36 
Affinity 91.52 74.74 92.09 78.17 80.73 95.70 89.52 92.83 43.29 89.21 60.33 87.50 90.96 88.77 88.88 71.00 88.54 50.61 89.64 78.22 81.80 
AAF 92.97 73.68 92.49 80.51 79.73 96.15 90.92 93.42 45.11 89.00 62.87 87.97 91.32 90.28 89.30 69.05 88.92 52.81 89.05 78.91 82.39 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Method road swalk build. wall fence pole tlight tsign veg. terrain sky person rider car truck bus train mbike bike mIoUPer-class results on Cityscapes test set. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU PSPNet 87.54 53.08 83.53 76.95 45.13 87.68 68.77 89.01 39.26 88.78 51.49 88.88 84.41 85.95 77.60 48.68 86.25 54.18 88.25 66.11 73.60 Affinity 89.42 61.72 84.64 79.86 57.57 88.81 71.74 88.91 44.78 89.55 52.55 91.22 86.12 87.40 81.10 58.33 85.15 60.61 88.47 68.86 76.73 AAF 89.76 61.74 84.40 81.87 58.04 89.03 73.68 90.46 46.67 89.65 55.63 91.33 85.85 88.36 81.93 59.84 84.52 62.67 89.35 68.80 77.54Per-class instance-wise IoU results on Pascal VOC 2012 validation set. Method road swalk build. wall fence pole tlight tsign veg. terrain sky person rider car truck bus train mbike bike mIoU PSPNet 97.64 78.23 88.36 34.48 42.00 51.68 50.71 68.29 89.65 40.14 86.63 78.35 75.91 92.09 87.28 90.85 62.74 85.33 73.02 72.28 Affinity 97.73 80.51 89.32 38.21 45.89 61.31 59.75 73.41 90.62 43.22 88.20 81.18 80.29 93.24 89.60 94.10 50.69 84.76 75.59 74.61 AAF 97.86 80.40 89.44 38.38 46.33 61.19 59.75 73.55 90.63 42.51 88.48 81.27 80.08 93.18 89.47 93.73 60.74 86.40 75.84 75.22</figDesc><table>PSPNet 98.33 84.21 92.14 49.67 55.81 57.62 69.01 74.17 92.70 70.86 95.08 84.21 66.58 95.28 73.52 80.59 70.54 65.54 73.73 76.30 
AAF 98.53 85.56 93.04 53.81 58.96 65.93 75.02 78.42 93.68 72.44 95.58 86.43 70.51 95.88 73.91 82.68 76.86 68.69 76.40 79.07 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 8 .</head><label>8</label><figDesc>Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean PSPNet .694 .420 .658 .417 .624 .626 .562 .667 .297 .587 .279 .667 .608 .513 .554 .235 .547 .413 .551 .512 .527 Affinity .745 .573 .708 .524 .693 .678 .627 .690 .455 .620 .383 .732 .655 .602 .648 .370 .583 .546 .609 .635 .610 AAF .746 .559 .704 .524 .684 .675 .622 .701 .441 .612 .391 .728 .653 .595 .647 .355 .580 .547 .608 .628 .606Per-class boundary recall results on Pascal VOC 2012 validation set. Method road swalk build. wall fence pole tlight tsign veg. terrain sky person rider car truck bus train mbike bike mean PSPNet .458 .771 .584 .480 .537 .587 .649 .687 .650 .589 .587 .733 .631 .812 .577 .734 .569 .550 .697 .625 Affinity .484 .826 .686 .532 .632 .760 .769 .780 .754 .663 .655 .814 .748 .852 .627 .792 .589 .651 .798 .706 AAF .482 .826 .685 .533 .643 .756 .768 .780 .753 .645 .653 .814 .746 .851 .644 .789 .590 .642 .801 .705</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>table dog horse mbike person plant sheep sofa train tv mIoU × × 89.02 68.86 90.05 73.52 77.87 94.04 86.94 91.04 40.85 85.82 54.08 84.31 89.12 84.91 86.72 67.52 85.56 52.55 87.60 73.78 79.00 × 90.19 68.48 89.87 76.91 77.56 93.84 89.08 91.45 40.67 85.82 57.23 85.33 89.77 85.97 86.93 65.68 85.12 52.22 87.25 74.07 79.45 89.45 68.46 90.44 75.82 77.03 94.09 88.01 91.42 38.67 85.98 56.16 84.32 89.22 84.98 87.09 67.35 87.15 55.20 88.22 73.30 79.40</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 10 .</head><label>10</label><figDesc>Per-category IOU results of AAF with different combinations of kernel sizes k on VOC 2012 validation set. ' ' denotes the inclusion of respective kernel size as opposed to '×'. Method road swalk build. wall fence pole tlight tsign veg. terrain sky person rider car truck bus train mbike bike mIoU pix. acc PSPNet 61.79 34.26 37.30 13.31 18.52 26.51 31.64 17.51 55.00 8.57 82.47 42.73 49.78 69.25 34.31 18.21 25.00 33.14 6.86 35.06 68.78 Affinity 75.26 30.34 44.10 12.91 20.19 29.78 31.50 23.98 64.25 11.83 74.32 48.28 49.12 67.39 25.76 23.82 20.29 41.48 5.63 36.86 75.13 AAF 83.07 27.82 51.16 10.41 18.76 28.58 31.74 24.98 61.38 12.25 70.65 50.53 48.06 53.35 26.80 20.97 24.50 39.56 9.37 36.52 78.28</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Generalizability</head><p>We further investigate the robustness of our proposed methods on different domains. We train the networks on the Cityscapes dataset <ref type="bibr" target="#b9">[10]</ref> and test them on another dataset, Grand Theft Auto V (GTA5) <ref type="bibr" target="#b27">[28]</ref> as shown in <ref type="figure">Fig. 5</ref>. The GTA5 dataset is generated from the photo-realistic computer game-Grand Theft Auto V <ref type="bibr" target="#b27">[28]</ref>, which consists of 24,966 images with densely labelled segmentation maps compatible with Cityscapes. We test on GTA5 Part 1 (2,500 images). We summarize the performance in <ref type="table">Table 11</ref>. It is shown that without fine-tuning, our proposed AAF outperforms the PSPNet <ref type="bibr" target="#b35">[36]</ref> baseline model by 9.5% in mean pixel accuracy and 1.46% in mIoU, which demonstrates the robustness of our proposed methods against appearance variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>We propose adaptive affinity fields (AAF) for semantic segmentation, which incorporate geometric regularities into segmentation models, and learn local relations with adaptive ranges through adversarial training. Compared to other alternatives, our AAF model is 1) effective (encoding rich structural relations), 2) efficient (introducing no inference overhead), and 3) robust (not sensitive to domain changes). Our approach achieves competitive performance on standard benchmarks and also generalizes well on unseen data. It provides a novel perspective towards structure modeling in deep learning.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Grouping-based nonadditive verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">ICML</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets. In: NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep learning markov random field for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Affinity cnn: Learning pixel-centric pairwise relations for figure/ground embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regularizing deep networks by modeling and predicting label structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5629" to="5638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Early vision: From computational structure to algorithms and parallel hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="139" to="155" />
		</imprint>
	</monogr>
	<note>Computer Vision, Graphics, and Image Processing</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multiclass spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">ICCV</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
