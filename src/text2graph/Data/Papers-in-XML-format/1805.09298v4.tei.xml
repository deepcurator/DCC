<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning towards Minimum Hyperspherical Energy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
							<email>wyliu@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongmei</forename><surname>Lin</surname></persName>
							<email>rongmei.lin@emory.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Emory University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
							<email>lsong@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning towards Minimum Hyperspherical Energy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as even as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our method, by showing the superior performance with MHE regularization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent success of deep neural networks has led to its wide applications in a variety of tasks. With the over-parametrization nature and deep layered architecture, current deep networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b39">40]</ref> are able to achieve impressive performance on large-scale problems. Despite such success, having redundant and highly correlated neurons (e.g., weights of kernels/filters in convolutional neural networks (CNNs)) caused by over-parametrization presents an issue <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, which motivated a series of influential works in network compression <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1]</ref> and parameter-efficient network architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60]</ref>. These works either compress the network by pruning redundant neurons or directly modify the network architecture, aiming to achieve comparable performance while using fewer parameters. Yet, it remains an open problem to find a unified and principled theory that guides the network compression in the context of optimal generalization ability.</p><p>Another stream of works seek to further release the network generalization power by alleviating redundancy through diversification <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">34]</ref> as rigorously analyzed by <ref type="bibr" target="#b56">[57]</ref>. Most of these works address the redundancy problem by enforcing relatively large diversity between pairwise projection bases via regularization. Our work broadly falls into this category by sharing similar high-level target, but the spirit and motivation behind our proposed models are distinct. In particular, there is a recent trend of studies that feature the significance of angular learning at both loss and convolution levels <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref>, based on the observation that the angles in deep embeddings learned by CNNs tend to encode semantic difference. The key intuition is that angles preserve the most abundant and discriminative information for visual recognition. As a result, hyperspherical geodesic distances between neurons naturally plays a key role in this context, and thus, it is intuitively desired to impose discriminativeness by keeping their projections on the hypersphere as far away from each other as possible. While the concept of imposing large angular diversities was also considered in <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b33">34]</ref>, they do not consider diversity in terms of global equidistribution of embeddings on the hypersphere, which fails to achieve the state-of-the-art performances.</p><p>Given the above motivation, we draw inspiration from a well-known physics problem, called Thomson problem <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41]</ref>. The goal of Thomson problem is to determine the minimum electrostatic potential energy configuration of N mutually-repelling electrons on the surface of a unit sphere. We identify the intrinsic resemblance between the Thomson problem and our target, in the sense that diversifing neurons can be seen as searching for an optimal configuration of electron locations. Similarly, we characterize the diversity for a group of neurons by defining a generic hyperspherical potential energy using their pairwise relationship. Higher energy implies higher redundancy, while lower energy indicates that these neurons are more diverse and more uniformly spaced. To reduce the redundancy of neurons and improve the neural networks, we propose a novel minimum hyperspherical energy (MHE) regularization framework, where the diversity of neurons is promoted by minimizing the hyperspherical energy in each layer. As verified by comprehensive experiments on multiple tasks, MHE is able to consistently improve the generalization power of neural networks.</p><p>Orthonormal MHE Half-space MHE <ref type="figure">Figure 1</ref>: Orthonormal, MHE and half-space MHE regularization. The red dots denote the neurons optimized by the gradient of the corresponding regularization. The rightmost pink dots denote the virtual negative neurons. We randomly initialize the weights of 10 neurons on a 3D Sphere and optimize them with SGD.</p><p>MHE faces different situations when it is applied to hidden layers and output layers. For hidden layers, applying MHE straightforwardly may still encourage some degree of redundancy since it will produce co-linear bases pointing to opposite directions (see <ref type="figure">Fig. 1 middle)</ref>. In order to avoid such redundancy, we propose the half-space MHE which constructs a group of virtual neurons and minimize the hyperspherical energy of both existing and virtual neurons. For output layers, MHE aims to distribute the classifier neuron 1 as uniformly as possible to improve the inter-class feature separability. Different from MHE in hidden layers, classifier neurons should be distributed in the full space for the best classification performance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref>. An intuitive comparison among the widely used orthonormal regularization, the proposed MHE and half-space MHE is provided in <ref type="figure">Fig. 1</ref>. One can observe that both MHE and half-space MHE are able to uniformly distribute the neurons over the hypersphere and half-space hypershpere, respectively. In contrast, conventional orthonormal regularization tends to group neurons closer, especially when the number of neurons is greater than the dimension. MHE is originally defined on Euclidean distance, as indicated in Thomson problem. However, we further consider minimizing hyperspherical energy defined with respect to angular distance, which we will refer to as angular-MHE (A-MHE) in the following paper. In addition, we give some theoretical insights of MHE regularization, by discussing the asymptotic behavior and generalization error. Last, we apply MHE regularization to multiple vision tasks, including generic object recognition, class-imbalance learning, and face recognition. In the experiments, we show that MHE is architectureagnostic and can considerably improve the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Diversity regularization is shown useful in sparse coding <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>, ensemble learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref>, selfpaced learning <ref type="bibr" target="#b18">[19]</ref>, metric learning <ref type="bibr" target="#b55">[56]</ref> etc. Early studies in sparse coding <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> show that the generalization ability of codebook can be improved via diversity regularization, where the diversity is often modeled using the (empirical) covariance matrix. More recently, a series of studies have featured diversity regularization in neural networks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53]</ref>, where regularization is mostly achieved via promoting large angle/orthogonality, or reducing covariance between bases. Our work differs from these studies by formulating the diversity of neurons on the entire hypersphere, therefore promoting diversity from a more global, top-down perspective. Methods other than diversity-promoting regularization have been widely proposed to improve CNNs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28]</ref> and generative adversarial nets (GANs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. MHE can be regarded as a complement that can be applied on top of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Neurons towards Minimum Hyperspherical Energy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation of Minimum Hyperspherical Energy</head><p>Minimum hyperspherical energy defines an equilibrium state of the configuration of neuron's directions. We argue that the power of neural representation of each layer can be characterized by the hyperspherical energy of its neurons, and therefore a minimal energy configuration of neurons can induce better generalization. Before delving into details, we first define the hyperspherical potential energy functional for N neurons with</p><formula xml:id="formula_0">d + 1-dimension W N = {w 1 , · · · , w N ∈ R d+1 } as E s,d (ŵi| N i=1 ) = N i=1 N j=1,j =i fs ŵ i −ŵj = i =j ŵ i −ŵj −s , s &gt; 0 i =j log ŵ i −ŵj , s = 0 ,<label>(1)</label></formula><p>where · denotes Euclidean distance, f s (·) is a decreasing real-valued function, andŵ i = wi wi is the i-th neuron weight projected to the unit hypersphere</p><formula xml:id="formula_1">S d = {w ∈ R d+1 | w = 1}. We also denotê W N = {ŵ 1 , · · · ,ŵ N ∈ S d }, and E s = E s,d (ŵ i | N i=1</formula><p>) for short. There are plenty of choices for f s (·), but in this paper we use f s (z) = z −s , s &gt; 0, known as Riesz s-kernels. Particularly, as s → 0, z −s → s log(z −1 ) + 1, which is an affine transformation of log(z −1 ). It follows that optimizing the logarithmic hyperspherical energy E 0 = i =j log( ŵ i −ŵ j ) is essentially the limiting case of optimizing the hyperspherical energy E s . We therefore define f 0 (z) = log(z −1 ) for convenience.</p><p>The goal of the MHE criterion is to minimize the energy in Eq. <ref type="formula" target="#formula_0">(1)</ref> by varying the orientations of the neuron weights w 1 , · · · , w N . To be precise, we solve an optimization problem: min W N E s with s ≥ 0. In particular, when s = 0, we solve the logarithmic energy minimization problem:</p><p>arg min</p><formula xml:id="formula_2">W N E0 = arg min W N exp(E0) = arg min W N i =j ŵ i −ŵj ,<label>(2)</label></formula><p>in which we maximize the product of Euclidean distances. Note that Thomson problem corresponds to minimizing E 1 , which is a NP-hard problem. Therefore in practice we can only compute its approximate solution by heuristics. In the case of neural networks, such a differentiable objective can be directly optimized via gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Logarithmic Hyperspherical Energy as a Relaxation</head><p>Optimizing the original energy in Eq. (1) is equivalent to optimize its logarithmic form log E s . To efficiently solve this difficult optimization problem, we can instead optimize the lower bound of log E s as a surrogate energy, by applying Jensen's inequality:</p><formula xml:id="formula_3">arg min W N E log := N i=1 N j=1,j =i log fs ŵ i −ŵj<label>(3)</label></formula><p>With f s (z) = z −s , s &gt; 0, we observe that E log becomes sE 0 = −s i =j log( ŵ i −ŵ j ), which is identical to the logarithmic hyperspherical energy E 0 up to a multiplicative factor s. Hence solving the maximization of the product of distances E 0 can also be viewed as a relaxation for E s for s &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MHE as Regularization for Neural Networks</head><p>Now that we have introduced the formulation of MHE, we propose MHE regularization for neural networks. In supervised neural network learning, the entire objective function is shown as follows:</p><formula xml:id="formula_4">L = 1 m m j=1 ( w out i , xj c i=1 , yj) training data fitting + λh · L−1 j=1 1 Nj(Nj − 1) {Es}j T h : hyperspherical energy for hidden layers + λo · 1 NL(NL − 1) Es(ŵ out i | c i=1 )</formula><p>To: hyperspherical energy for output layer <ref type="bibr" target="#b3">(4)</ref> where x i is the feature of the i-th training sample entering the output layer, w out i is the classifier neuron for the i-th class in the output fully-connected layer andŵ out i denotes its normalized version. {E s } i denotes the hyperspherical energy for the neurons in the i-th layer. c is the number of classes, m is the batch size, L is the number of layers of the neural network, and N i is the number of neurons in the i-th layer.</p><formula xml:id="formula_5">E s (ŵ out i | c i=1 ) denotes the hyperspherical energy of neurons {ŵ out 1 , · · · ,ŵ out c }.</formula><p>The 2 weight decay is omitted here for simplicity, but we will use it by default in practice. An alternative interpretation of the MHE regularization from decoupled view is given in Appendix C. The different effect of the MHE regularization in hidden layers and output layers are discussed separately.</p><p>MHE for hidden layers. To make neurons in the hidden layers more discriminative and less redundant, we propose to use MHE as a form of regularization. MHE encourages the normalized neurons to be uniformly distributed on a unit hypersphere, which is partially inspired by the observation in <ref type="bibr" target="#b27">[28]</ref> that angular difference in neurons preserves semantic (label-related) information. To some extent, MHE maximizes the average angular difference between neurons (specifically, the hyperspherical energy of neurons in every hidden layer). For instance, in CNNs we minimize the hyperpsherical energy of kernels in convolutional and fully-connected layers except the output layer.</p><p>MHE for output layers. For the output layer, we propose to enhance the inter-class feature separability with MHE to learn discriminative and well-separated features. For classification tasks, MHE regularization is complementary to the softmax cross-entropy loss in CNNs. The softmax loss focuses more on the intra-class compactness, while MHE encourages the inter-class separability. Therefore, MHE on output layers can induce features with better generalization power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MHE in Half Space</head><p>Original MHE Half-space MHE Directly applying the MHE formulation may still encouter some redundancy. An example in <ref type="figure" target="#fig_0">Fig. 2</ref> </p><formula xml:id="formula_6">({ŵ k , −ŵ k }| 2Ni k=1 )</formula><p>). This half-space variant will encourage the neurons to be less correlated and less redundant, as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>. Note that, Half-space MHE can only be used in hidden layers, because the colinear neurons does not constitute redundancy in output layers, as shown in <ref type="bibr" target="#b26">[27]</ref>. Still, colinearity is not very likely in high-dimensional spaces, especially when the neurons are optimized to fit training data. This may be the reason that the original MHE regularization still improves the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">MHE beyond Euclidean Distance</head><p>The hyperspherical energy is originally defined based on the Euclidean distance on a hypersphere, which can be viewed as an angular measure. In addition to Euclidean distance, we further consider the geodesic distance on a unit hypersphere as a distance measure for neurons, which is exactly the same as the angle between neurons. Specifically, we consider to use arccos(ŵ iŵ j ) to replace ŵ i −ŵ j in hyperspherical energies. Following this idea, we propose angular MHE (A-MHE) as a simple extension, where the hyperspherical energy is rewritten as:</p><formula xml:id="formula_7">E a s,d (ŵi| N i=1 ) = N i=1 N j=1,j =i fs arccos(ŵ iŵj ) = i =j arccos(ŵ iŵj ) −s , s &gt; 0 i =j log arccos(ŵ iŵj ) , s = 0<label>(5)</label></formula><p>which can be viewed as redefining MHE based on geodesic distance on hyperspheres (i.e., angle), and can be used as an alternative to the original hyperspherical energy E s in Eq. (4). Note that, A-MHE can also be learned in full-space or half-space, leading to similar variants as original MHE. The key difference between MHE and A-MHE lies in the optimization dynamics, because their gradients w.r.t the neuron weights are quite different. A-MHE is also computationally expensive than MHE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Mini-batch Approximation for MHE</head><p>With a large number of neurons in one layer, calculating MHE can be computationally expensive as it requires computing the pair-wise distances between neurons. To address this issue, we propose to the mini-batch version of MHE to approximate the MHE (either original or half-space) objective. Mini-batch approximation for MHE on hidden layers. For hidden layers, mini-batch approximation iteratively takes a random batch of neurons as input and minimizes their hyperspherical energy as an approximation to the MHE. Note that the gradients of the mini-batch objective is an unbiased estimation of the original gradient of MHE. Data-dependent mini-batch approximation for output layers. For the output layer, the datadependent mini-batch approximation iteratively takes the classifier neurons corresponding to the classes that exist in mini-batches. It minimizes</p><formula xml:id="formula_8">1 m(N −1) m i=1 N j=1,j =yi f s ( ŵ yi −ŵ j ) in each iteration</formula><p>, where y i denotes the class label of the i-th sample in each mini-batch, m is the mini-batch size, and N is the number of neurons (in a particular layer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Discussions</head><p>Connections to scientific problems. The hyperspherical energy minimization has close relationships with scientific problems. When s = 1, Eq. (1) reduces to Thomson problem <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41]</ref> (in physics) where one needs to determine the minimum electrostatic potential energy configuration of N mutuallyrepelling electrons on a unit sphere. When s = ∞, Eq. (1) becomes Tammes problem <ref type="bibr" target="#b44">[45]</ref> (in geometry) where the goal is to pack a given number of circles on the surface of a sphere such that the minimum distance between circles is maximized. When s = 0, Eq. (1) becomes Whyte's problem where the goal is to maximize product of Euclidean distances as shown in Eq. (2). Our work aims to make use of important insights from these problems to improve neural networks.</p><p>Comparison to orthogonality/angle-promoting regularization. Promoting orthogonality or large angles between bases has been a popular choice for encouraging diversity. Probably the most related and widely used one is the orthonormal regularization which aims to minimize W W − I F , where W denotes the weights of a group of neurons with each column being one neuron and I is an identity matrix. However, these methods models diversity regularization at a more local level, while MHE regularization seeks to model the problem in a more top-down manner.</p><p>Weighted MHE? Since we use the normalized neurons for MHE, a natural question may arise: what if we use the original (i.e., unnormalized) neurons to compute MHE? This is essentially to compute a weighted MHE where the norm of the neuron is taken into consideration. Empirically, if we keep the norm of neuron weights in MHE, then the norm will simply keep increasing instead of changing its angular distance with the other neurons in order to minimize the objective. Therefore, applying weighted MHE to neural networks may reduce the recognition accuracy, further validating the importance of the hyperspherical (i.e., angular) diversity, as has been emphasized in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Insights</head><p>Different from a rigorous theoretical analysis, our goal here is to leverage existing theoretical results from <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref> and provide theoretical yet intuitive understandings about MHE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Asymptotic Behavior</head><p>This subsection shows how the hyperspherical energy behaves asymptotically. Specifically, as N → ∞, we show that the solutionŴ N is uniformly distributed on S d when the hyperspherical energy defined in Eq. (1) achieves its minimum. Definition 1 (minimal hyperspherical s-energy). We define the minimal s-energy for N points on the unit hypersphere</p><formula xml:id="formula_9">S d = {w ∈ R d+1 | w = 1} as ε s,d (N ) := inf W N ⊂S d E s,d (ŵi| N i=1 )<label>(6)</label></formula><p>where the infimum is taken over all possibleŴ N on S d . Any configuration ofŴ N to attain the infimum is called an s-extremal configuration. Usually</p><formula xml:id="formula_10">ε s,d (N ) = ∞ if N is greater than d and ε s,d (N ) = 0 if N = 0, 1.</formula><p>We discuss the asymptotic behavior (N → ∞) in three cases: 0 &lt; s &lt; d, s = d, and s &gt; d. We first write the energy integral as</p><formula xml:id="formula_11">I s (µ) = S d ×S d u − v −s dµ(u)dµ(v)</formula><p>, which is taken over all probability</p><formula xml:id="formula_12">measure µ supported on S d . With 0 &lt; s &lt; d, I s (µ) is minimal when µ is the spherical measure σ d = H d (·)| S d /H d (S d ) on S d , where H d (·) denotes the d-dimensional Hausdorff measure. When s ≥ d, I s (µ)</formula><p>becomes infinity, which therefore requires different analysis. In general, we can say all s-extremal configurations asymptotically converge to uniformly distribution on a hypersphere, as stated in Theorem 1. This asymptotic behavior has been heavily studied in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Theorem 1 (asymptotic uniform distribution on hypersphere). Any sequence of optimal s-energy</p><formula xml:id="formula_13">configurations (Ŵ N )| ∞ 2 ⊂ S</formula><p>d is asymptotically uniformly distributed on S d in the sense of the weakstar topology of measures, namely</p><formula xml:id="formula_14">1 N v∈Ŵ N δv → σ d , as N → ∞<label>(7)</label></formula><p>where δ v denotes the unit point mass at v, and σ d is the spherical measure on S d .</p><p>Theorem 2 (asymptotics of the minimal hyperspherical s-energy). We have that lim N →∞</p><formula xml:id="formula_15">ε s,d (N ) p(N )</formula><p>exists for the minimal s-energy.</p><formula xml:id="formula_16">For 0 &lt; s &lt; d, p(N ) = N 2 . For s = d, p(N ) = N 2 log N . For s &gt; d, p(N ) = N 1+s/d . Particularly if 0 &lt; s &lt; d, we have lim N →∞ ε s,d (N ) N 2 = I s (σ d ).</formula><p>Theorem 2 tells us the growth power of the minimal hyperspherical s-energy when N goes to infinity. Therefore, different potential power s leads to different optimization dynamics. In the light of the behavior of the energy integral, MHE regularization will focus more on local influence from neighborhood neurons instead of global influences from all the neurons as the power s increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalization and Optimality</head><p>As rigorously proved in <ref type="bibr" target="#b51">[52]</ref>, in one-hidden-layer ReLU neural network, the diversity of units can effectively eliminate the spurious local minima despite the non-convexity in learning dynamics of neural networks. Following such argument, our MHE regularization, which encourages the diversity of neurons, naturally matches the theorectical intuition in <ref type="bibr" target="#b51">[52]</ref>, and can effectively promote the generalization of the corresponding neural networks. This is also well verified by our comprehensive experiments in Section 5. While hyperspherical energy is minimized such that neurons become increasingly more uniformly distributed on a hypersphere, then such hyperspherical diversity is closely related to the generalization error, as shown in main results of <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applications and Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Improving Network Generalization</head><p>We use MHE to improve the generalization of CNNs and study how MHE performs in a variety of scenarios. First, we perform ablation study and some exploratory experiments on MHE. Then we apply MHE to large-scale object recognition and class-imbalance learning. For all the experiments on CIFAR-10 and CIFAR-100 in the paper, we use moderate data augmentation, following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. For ImageNet-2012, we follow the same data augmentation in <ref type="bibr" target="#b27">[28]</ref>. We train all the networks using SGD with momentum 0.9, and the network initialization follows <ref type="bibr" target="#b11">[12]</ref>. All the networks use BN <ref type="bibr" target="#b17">[18]</ref> and ReLU if not otherwise specified. Experimental details are given in each subsection and Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Ablation Study and Exploratory Experiments</head><p>Method CIFAR-10 CIFAR-100 </p><formula xml:id="formula_17">s = 2 s = 1 s = 0 s = 2 s = 1 s = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of MHE.</head><p>We first evaluate all different variants of MHE on CIFAR-10 and CIFAR-100. The compared variants include the original MHE (with the power s = 0, 1, 2) and half-space MHE (with the power s = 0, 1, 2) in both Euclidean and angular distance. In this experiment, all the methods use the CNN-9 (see Appendix A). The results in <ref type="table">Table 1</ref> show that all the variants of MHE performs consistently better than the baseline. Specifically, the half-space MHE has significant performance gain compared to the other MHE variants, and MHE in Euclidean and angular distance perform similarly. MHE with s = 2 performs best among s = 0, 1, 2. In the following experiments, we use s = 2 and Euclidean distance in both MHE and half-space MHE if not otherwise specified.    <ref type="table">Table 4</ref>: Ablation study on CIFAR-100.</p><p>Ablation study. Since the current MHE regularizes the neurons in the hidden layers and the output layer simultaneously, we perform ablation study for MHE to further investigate where the gain comes from. This experiment uses the CNN-9. The results are given in <ref type="table">Table 4</ref>. "H" means that we apply MHE to all the hidden layers, while "O" means that we apply MHE to the output layer. Because the half-space MHE can not be applied to the output layer, so there is "N/A" in the table. In general, we find that applying MHE to both the hidden layers and the output layer yields the best performance, and using MHE in the hidden layers usually produces better accuracy than using MHE in the output layer. Hyperparameter experiment. We evaluate how the selection of hyperparameter affects the performance. We experiment with different hyperparameters from 10 −2 to 10 2 on CIFAR-100 with the CNN-9. HS-MHE denotes the half-space MHE. We evaluate MHE variants by separately applying MHE to the output layer ("O"), MHE to the hidden layers ("H"), and the half-space MHE to the hidden layers ("H"). The results in <ref type="figure" target="#fig_1">Fig. 3</ref> show that the our MHE is not very hyperparametersensitive and can consistently outperform the baseline. The half-space MHE can consistently outperform the original MHE under different hyperparameter settings. Moreover, applying MHE only to hidden layers can yields better accuracy than applying MHE only to output layer.   <ref type="table" target="#tab_6">Table 5</ref>. One can observe that applying MHE to ResNet also achieves considerable improvement. Most importantly, adding MHE regularization will not affect the original settings of the architecture, and it can improve the network generalization at a very small computational cost.  We evaluate MHE on large-scale ImageNet-2012 datasets. Specifically, we perform experiment using ResNets, and then report the top-1 validation error (center crop) in <ref type="table" target="#tab_8">Table 6</ref>. From the results, we still observe that both MHE and half-space MHE yields consistently better recognition accuracy than the baseline and the orthonormal regularization (after tuning its hyperparameter). To better evaluate the consistency of MHE's performance gain, we use two ResNets with different depth: ResNet-18 and ResNet-34. On ResNet-18 and ResNet-34, both MHE and half-space MHE show better generalization power, and half-space MHE performs slightly better than full-space MHE. Because MHE aims to maximize the hyperspherical margin between different classifier neurons in the output layer, we can naturally apply MHE to class-imbalance learning where the number of training samples in different classes is imbalanced. We demonstrate the power of MHE in class-imbalance learning through a toy experiment. We first randomly throw away 98% training data for digit 0 in MNIST (only 100 samples are preserved for digit 0), and then train a 6-layer CNN on this imbalance MNIST dataset. To visualize the learned features, we set the output feature dimension as 2. The learned features and classifier neurons on training set is shown in <ref type="figure" target="#fig_2">Fig. 4</ref> where each color denotes a digit and red arrows denote the classifier neurons. <ref type="figure" target="#fig_2">From Fig. 4</ref>, one can observe that the CNN without MHE only tends to ignore the imbalanced class (digit 0) and the learned classifier neuron is highly biased to another digit. In contrast, the CNN with MHE can learn reasonably separable distribution even if digit 0 only has 2% samples compared to the other classes. Using MHE can improve the accuracy on the full testing set from 88.5% to 98%. Most importantly, the classifier neuron for digit 0 is also properly learned.  <ref type="table">Table 7</ref>: Error on imbalanced CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Large-scale Object Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Class-imbalance Learning</head><p>We experiment MHE in two data imbalance settings on CIFAR-10: 1) single class imbalance (S) -All classes have the same number of images but one single class has significantly less number, and 2) multiple class imbalance (M) -The number of images decreases as the class index decreases from 9 to 0. We use CNN-9 for all the compared regularizations. Details are provided in Appendix A. In <ref type="table">Table 7</ref>, we report the error rate on the whole testing set. In addition, we report the error rate (denoted by Err. (S)) on the imbalance class (single imbalance setting) in the full testing set. From the results, one can observe that CNN-9 with MHE is able to effectively perform recognition when classes are imbalanced. Even only given a small portion of training data in a few classes, CNN-9 with MHE can achieve competitive accuracy on the full testing set, showing MHE's superior generalization power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SphereFace+: Improving Inter-class Feature Separability via MHE for Face Recognition</head><p>We have shown that full-space MHE for output layers can encourage classifier neurons to distribute more evenly on hypersphere and improve inter-class feature separability. Intuitively, the classifier neurons serve as the approximate center for features from each class, and can therefore guide the feature learning. We also observe that open-set face recognition (e.g., face verification) requires the feature centers to be as separable as possible <ref type="bibr" target="#b25">[26]</ref>. This connection inspires us to apply MHE to face recognition. Specifically, we propose SphereFace+ by applying MHE to SphereFace <ref type="bibr" target="#b25">[26]</ref>. The objective of SphereFace, angular softmax loss ( SF ) that encourages intra-class feature compactness, is naturally complimentary to that of MHE. The objective function of SphereFace+ is defined as</p><formula xml:id="formula_18">LSF+ = 1 m m j=1 SF( w out i , xj c i=1 , yj, mSF)</formula><p>Angular softmax loss: promoting intra-class compactness</p><formula xml:id="formula_19">+ λM · 1 m(N − 1) m i=1 N j=1,j =y i fs( ŵ out y i −ŵ out j )</formula><p>MHE: promoting inter-class separability <ref type="bibr" target="#b7">(8)</ref> where c is the number of classes, m is the mini-batch size, N is the number of classifier neurons, x i the deep feature of the i-th face (y i is its groundtruth label), and w out i is the i-th classifier neuron. m SF is a hyperparameter for SphereFace, controlling the degree of intra-class feature compactness. Because the face dateset usually has thousands of identities, we will use the data-dependent minibatch approximation MHE in the output layer, as shown in Eq. <ref type="bibr" target="#b7">(8)</ref>. MHE completes a missing piece for SphereFace by promoting the inter-class separability. Our SphereFace+ consistently outperforms SphereFace, and achieves state-of-the-art performance on LFW <ref type="bibr" target="#b15">[16]</ref> and MegaFace <ref type="bibr" target="#b19">[20]</ref>    Performance under different m SF . We evaluate SphereFace+ with two different architectures (SphereFace-20 and SphereFace-64) proposed in <ref type="bibr" target="#b25">[26]</ref>. Specifically, SphereFace-20 and SphereFace-64 are 20-layer and 64-layer modified residual networks, respectively. We train our network with the publicly available CASIA-Webface dataset <ref type="bibr" target="#b57">[58]</ref>, and then test the learned model on LFW and MegaFace dataset. In MegaFace dataset, the reported accuracy indicates rank-1 identification accuracy with 1 million distractors. All the results in <ref type="table" target="#tab_11">Table 8 and Table 9</ref>   Comparison to state-of-the-art methods. We also compare our methods with some widely used loss functions. All these compared methods use SphereFace-64 network that are trained with CASIA dataset. All the results are given in <ref type="table" target="#tab_14">Table 10</ref> computed without model ensemble and PCA. Compared to the other state-of-the-art methods, SphereFace+ achieves the best accuracy on LFW dataset, while being comparable to the best accuracy on MegaFace dataset. Current state-of-the-art face recognition methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref> usually only focus on compressing the intra-class features, which makes MHE a potentially useful tool in order to further improve these face recognition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>We borrow the idea from physics and propose a novel regularization method, minimum hyperspherical energy (MHE), to encourage the diversity of neural representation.  </p><note type="other">3 × 3, 64 × 5 3 × 3, 64 3 × 3, 64 × 2 3 × 3, 64 3 × 3, 64 × 3 Conv2.x 3 × 3, 128 3 × 3, 128 × 5 3 × 3, 128 3 × 3, 128 × 2 3 × 3, 128 3 × 3, 128 × 4 Conv3.x 3 × 3, 256 3 × 3, 256 × 5 3 × 3, 256 3 × 3, 256 × 2 3 × 3, 256 3 × 3, 256 × 6</note><p>Conv4.x N/A 3 × 3, 512</p><formula xml:id="formula_20">3 × 3, 512 × 2 3 × 3, 512 3 × 3, 512 × 3</formula><p>Average Pooling General settings. The network architectures used in the paper are elaborated in <ref type="table" target="#tab_3">Table 11 Table 12</ref>. For CIFAR-10 and CIFAR-100, we use batch size 128. We start with learning rate 0.1, divide it by 10 at 20k, 30k and 37.5k iterations, and terminate training at 42.5k iterations. For ImageNet-2012, we use batch size 64 and start with learning rate 0.1. The learning rate is divided by 10 at 150k, 300k and 400k iterations, and the training is terminated at 500k iterations. Note that, for all the compared methods, we always use the best possible hyperparameters to make sure that the comparison is fair. The baseline has exactly the same architecture and training settings as the one that MHE uses, and the only difference is an additional MHE regularization. For full-space MHE in hidden layers, we set λ h as 10 for all experiments. For half-space MHE in hidden layers, we set λ h as 1 for all experiments. For MHE in output layers, we set λ o as 1 for all experiments. We use 1e − 5 for the orthonormal regularization. If not otherwise specified, standard 2 weight decay (1e − 4) is applied to all the neural network including baselines and the networks that use MHE regularization. A very minor issue for the hyperparameters λ h is that it may increase as the number of layers increases, so we can potentially further divide the hyperspherical energy for the hidden layers by the number of layers. It will probably change the current optimal hyperparameter setting by a constant multiplier. For notation simplicity, we do not explicitly write out the weight decay term in the loss function in the main paper. Note that, all the neuron weights in the neural network are not normalized, and the MHE will normalize the neuron weights while computing the regularization loss. As a result, MHE does not need to modify any component of the original neural networks, and it can simply be viewed as an extra regularization loss that can boost the performance. Class-imbalance learning. There are 50000 training images in the original CIFAR-10 dataset, with 5000 images per class. For the single class imbalance setting, we keep original images of class 1-9 and randomly throw away 90% images of class 0. The total number of training images in this setting is 45500. For the multiple class imbalance setting, we set the number of each class equals to 500 × (class_index + 1). For instance, class 0 has 500 images, class 1 has 1000 images and class 9 has 5000 images. The total number of training images in this setting is 27500.</p><p>SphereFace+. SphereFace+ uses the same face detection and alignment method <ref type="bibr" target="#b58">[59]</ref> as SphereFace <ref type="bibr" target="#b25">[26]</ref>. The testing protocol on LFW and MegaFace is also the same as SphereFace. We use exactly the same preprocessing as in the SphereFace repository. Detailed network architecture settings of SphereFace-20 and SphereFace-64 can be found in <ref type="bibr" target="#b25">[26]</ref>. Specifically, we use full-space MHE with Euclidean distance and s = 2 in the output layer. Essentially, we treat MHE as an additional loss function which aims to enlarge the inter-class angular distance of features and serves a complimentary role to the angular softmax in SphereFace. Note that, for the results of CosineFace <ref type="bibr" target="#b48">[49]</ref>, we directly use the results (with the same training settings and without using feature normalization) reported in the paper. Since ours also does not perform feature normalization, it is a fair comparison. With feature normalization, we find that the performance of SphereFace+ will also be improved significantly. However, feature normalization makes the results more tricky, because it will involve another hyperparameter that controls the projection radius of feature normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 1 and Theorem 2</head><p>Theorem 1 and Theorem 2 are natural results from classic potential theory <ref type="bibr" target="#b22">[23]</ref> and spherical configuration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref>. We discuss the asymptotic behavior (N → ∞) in three cases: 0 &lt; s &lt; d, s = d, and s &gt; d. We first write the energy integral as</p><formula xml:id="formula_21">I s (µ) = S d ×S d u − v −s dµ(u)dµ(v),<label>(9)</label></formula><p>which is taken over all probability measure µ supported on S d . With 0 &lt; s &lt; d, I s (µ) is minimal when µ is the spherical measure</p><formula xml:id="formula_22">σ d = H d (·)| S d /H d (S d ) on S d , where H d (·) denotes the d-dimensional Hausdorff measure. When s ≥ d, I</formula><p>s (µ) becomes infinity, which therefore requires different analysis.</p><p>First, the classic potential theory <ref type="bibr" target="#b22">[23]</ref> can directly give the following results for the case where 0 &lt; s &lt; d:</p><formula xml:id="formula_23">Lemma 1. If 0 &lt; s &lt; d, lim N →∞ ε s,d (N ) N 2 = I s ( H d (·)| S d H d (S d ) ),<label>(10)</label></formula><p>where I s is defined in the main paper. Moreover, any sequence of optimal hyperspherical s-enerygy</p><formula xml:id="formula_24">configurations (Ŵ N )| ∞ 2 ⊂ S d</formula><p>is asymptotically uniformly distributed in the sense that for the weakstar topology measures,</p><formula xml:id="formula_25">1 N v∈Ŵ N δv → σ d , as N → ∞ (11)</formula><p>where δ v denotes the unit point mass at v, and σ d is the spherical measure on S d .</p><p>which directly concludes Theorem 1 and Theorem 2 in the case of 0 &lt; s &lt; d.</p><p>For the case where s = d, we have from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref> the following results:</p><formula xml:id="formula_26">Lemma 2. Let B d :=B(0, 1) be the closed unit ball in R d . For s = d, lim N →∞ ε s,d (N ) N 2 log N = H d (B d ) H d (S d ) = 1 d Γ( d+1 2 ) √ πΓ( d 2 ) ,<label>(12)</label></formula><p>and any sequence (Ŵ N )| ∞ 2 ⊂ S d of optimal s-energy configurations satisfies Eq. 11.</p><p>which concludes the case of s = d. Therefore, we are left with the case where s &gt; d. For this case, we can use the results from <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_27">Lemma 3. Let A ⊂ R d be compact with H d (A) &gt; 0, andW N = {x k,N } N i=1</formula><p>be a sequence of asymptotically optimal N -point configurations in A in the sense that for some s &gt; d,</p><formula xml:id="formula_28">lim N →∞ E s (W N ) N 1+s/d = C s,d H d (A) s/d (13) or lim N →∞ E s (W N ) N 2 log N = H d (B d ) H d (A) .<label>(14)</label></formula><p>where C s,d is a finite positive constant independent of A. Let δ x be the unit point mass at the point x.</p><p>Then in the weak-star topology of measures we have</p><formula xml:id="formula_29">1 N N i=1 δ x i,N → H d (·)| A H d (A) , asN → ∞.<label>(15)</label></formula><p>The results naturally prove the case of s &gt; d. Combining these three lemmas, we have proved Theorem 1 and Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Understanding MHE from Decoupled View</head><p>Inspired by decoupled networks <ref type="bibr" target="#b24">[25]</ref>, we can view the original convolution as the multiplication of the angular function g(·) and the magnitude function h(·):</p><formula xml:id="formula_30">f (w, x) = w x · cos(θ)<label>(16)</label></formula><p>where θ is the angle between the kernel w and the input x. From the equation above, we can see that the norm of the kernel and the direction (i.e., angle) of the kernel affect the inner product similarity differently. Typically, weight decay is to regularize the kernel by minimizing its 2 norm, while there is no regularization on the direction of the kernel. Therefore, MHE is able to complete this missing piece by promoting angular diversity. By combining MHE to a standard neural networks (e.g., CNNs), the regularization term becomes</p><formula xml:id="formula_31">Lreg = λw · 1 L j=1 Nj L j=1 N j i=1 wi</formula><p>Weight decay: regularizing the magnitude of kernels</p><formula xml:id="formula_32">+ λh · L−1 j=1 1 Nj(Nj − 1) {Es}j + λo · 1 NL(NL − 1) Es(ŵ out i | c i=1 )</formula><p>MHE: regularizing the direction of kernels <ref type="bibr" target="#b16">(17)</ref> where x i is the feature of the i-th training sample entering the output layer, w From the decoupled view, we can see that MHE is actually very meaningful in regularizing the neural networks, which also serves a complementary role to weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Regularizing SphereNets with MHE</head><p>SphereNets <ref type="bibr" target="#b27">[28]</ref> are a family of network networks that learns on hyperspheres. The filters in SphereNets only focus on the hyperspherical (i.e., angular) difference. One can see that the intuition of SphereNets well matches that of MHE, so MHE can serve as a natural and effective regularization for SphereNets. Because SphereNets throw away all the magnitude information of filters, the weight decay can no longer serve as a form of regularization for SphereNets, which makes MHE a very useful regularization for SphereNets. Originally, we use the orthonormal regularization W W − I 2 F to regularize SphereNets, where W is the weight matrix of a layer with each column being a vectorized filter and I is an identity matrix. We compare MHE, half-space MHE and orthonormal regularization for SphereNets. In this section, all the SphereNets use the same architecture as the CNN-9 in <ref type="table" target="#tab_16">Table 11</ref>, the training setting is also the same as CNN-9. We only evaluate SphereNets with cosine SphereConv. Note that, s = 0 is actually the logarithmic hyperspherical energy (a relaxation of the original hyperspherical energy). From <ref type="table" target="#tab_4">Table 13</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Improving AM-Softmax with MHE</head><p>We also perform some preliminary experiments for applying MHE to additive margin softmax loss <ref type="bibr" target="#b46">[47]</ref> which is a recently proposed well-performing objective function for face recognition. The loss function of AM-Softmax is given as follows:</p><formula xml:id="formula_33">L AMS = − 1 n n i=1</formula><p>log e s· cosθ (x i ,wy i ) −mAMS e s· cosθ (x i ,wy i</p><formula xml:id="formula_34">) −mAMS + c j=1,j =yi e s·cosθ (x i ,w j )<label>(18)</label></formula><p>where y i is the label of the training sample x i , n is the mini-batch size, m AMS is the hyperparameter that controls the degree of angular margin, and θ (xi,wj ) denotes the angle between the training sample x i and the classifier neuron w j . s is the hyperparameter that controls the projection radius of feature normalization <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47]</ref>. Similar to our SphereFace+, we combine full-space MHE to the output layer to improve the inter-class feature separability. It is essentially following the same intuition of SphereFace+ by adding an additional loss function to AM-Softmax loss.</p><p>Experiments. We perform a preliminary experiment to study the benefits of MHE for improving AMSoftmax loss. We use the SphereFace-20 network and trained on CASIA-WebFace dataset (training settings are exactly the same as SphereFace+ in the main paper and <ref type="bibr" target="#b25">[26]</ref>). The hyperparameters s, m AMS for AM-Softmax loss exactly follow the best setting in <ref type="bibr" target="#b46">[47]</ref>. AM-Softmax achieves 99.26% accuracy on LFW, while combining MHE with AM-Softmax yields 99.37% accuracy on LFW. Such performance gain is actually very significant in face verification, which further validates the superiority of MHE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Improving GANs with MHE</head><p>We propose to improve the discriminator of GANs using MHE. It has been pointed out in <ref type="bibr" target="#b31">[32]</ref> that the function space from which the discriminators are learned largely affects the performance of GANs. Therefore, it is of great importance to learn a good discriminator for GANs. As a recently proposed regularization to stablize the training of GANs, spectral normalization (SN) <ref type="bibr" target="#b31">[32]</ref> encourages the Lipschitz constant of each layer's weight matrix to be one. Since MHE exhibits significant performance gain for CNNs as a regularization, we expect MHE can also improve the training of GANs by regularizing its discriminator. As a result, we perform a preliminary evaluation on applying MHE to GANs.</p><p>Specifically, for all methods except WGAN-GP <ref type="bibr" target="#b7">[8]</ref>, we use the standard objective function for the adversarial loss:</p><formula xml:id="formula_35">V (G, D) := E x∼q data (x) [log D(x)] + E z∼p(z) [log(1 − D(G(z)))],<label>(19)</label></formula><p>where z ∈ R dz is a latent variable, p(z) is the normal distribution N (0, I), and G : R dz → R d0 is a deterministic generator function. We set d z to 128 in all the experiments. For the updates of G, we used the alternate cost proposed by <ref type="bibr" target="#b5">[6]</ref> −E z∼p(z) [log(D <ref type="figure">(G(z)</ref>))] as used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b49">50]</ref>. For the updates of D, we used the original cost function defined in Eq. <ref type="bibr" target="#b18">(19)</ref>.</p><p>Recall from <ref type="bibr" target="#b31">[32]</ref> that spectral normalization normalizes the spectral norm of the weight matrix W such that it makes the Lipschitz constraint σ(W ) to be one:</p><formula xml:id="formula_36">W SN (W ) := W σ(W ) .<label>(20)</label></formula><p>We apply MHE to the discriminator of standard GANs (with the original loss function in <ref type="bibr" target="#b5">[6]</ref>) for image generation on CIFAR-10. In general, our experimental settings and training strategies (including architectures in <ref type="table" target="#tab_6">Table 15</ref>) exactly follow spectral normalization <ref type="bibr" target="#b31">[32]</ref>. For MHE, we use the half-space variant with Euclidean distance (Eq. <ref type="formula" target="#formula_0">(1)</ref>). We first experiment regularizing the discriminator using MHE alone, and it yields comparable performance to SN and orthonormal regularization. Moreover, we also regularize the discriminator simultaneously using both MHE and SN, and it can give much better results than using either SN or MHE alone. The results in <ref type="table">Table 14</ref> show that MHE is potentially very useful for training GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Network Architecture for GAN</head><p>We give the detailed network architectures in <ref type="table" target="#tab_6">Table 15</ref> that are used in our experiments for the generator and the discriminator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Comparison of Random Generated Images</head><p>We provide some randomly generated images for comparison between baseline GAN and GAN regularized by both MHE and SN. The generated images are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. We perform additional experiments on CIFAR-100 to further validate the effectiveness of MHE in class-imbalance learning. In the CNN used in the experiment, we only apply MHE (i.e., full-space MHE) to the output layer, and use MHE or half-space MHE in the hidden layers. In general, the experimental settings are the same as the main paper. We still use CNN-9 (which is a 9-layer CNN from <ref type="table" target="#tab_16">Table 11</ref>) in the experiment. Slightly differently from CIFAR-10 in the main paper, the two data imbalance settings on CIFAR-100 include 1) 10-class imbalance (denoted as Single in <ref type="table" target="#tab_8">Table 16</ref>) -All classes have the same number of images but 10 classes (index from 0 to 9) have significantly less number (only 10% training samples compared to the other normal classes), and 2) multiple class imbalance (denoted by Multiple in <ref type="table" target="#tab_8">Table 16</ref>) -The number of images decreases as the class index decreases from 99 to 0. For the multiple class imbalance setting, we set the number of each class equals to 5 × (class_index + 1). Experiment details are similar to the CIFAR-10 experiment, which is specified in Appendix A. The results in <ref type="table" target="#tab_8">Table 16</ref> show that MHE consistently improves CNNs in class-imbalance learning on CIFAR-100. In most cases, half-space MHE performs better than full-space MHE.  <ref type="table" target="#tab_8">Table 16</ref>: Error rate (%) on imbalanced CIFAR-100. The experimental settings are the same as the main paper. We supplement the 2D feature visualization on testing set in <ref type="figure" target="#fig_5">Fig. 6</ref>. The visualized features on both training set and testing set well demonstrate the superiority of MHE in class-imbalance learning. In the CNN without MHE, the classifier neuron of the imbalanced training data is highly biased towards another class, and therefore can not be properly learned. In contrast, the CNN with MHE can learn uniformly distributed classifier neurons, which greatly improves the network's generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline GAN GAN with MHE and SN Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 2D CNN Feature Visualization</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Half-space MHE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hyperparameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Class-imbalance learning on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>i is the classifier neuron for the i-th class in the output fully-connected layer andŵ out i denotes its normalized version. {E s } i denotes the hyperspherical energy for the neurons in the i-th layer. c is the number of classes, m is the batch size, L is the number of layers of the neural network, and N i is the number of neurons in the i-th layer. E s (ŵ out i | c i=1 ) denotes the hyperspherical energy of neurons {ŵ out 1 , · · · ,ŵ out c } in the output layer. λ w , λ h and λ o are weighting hyperparameters for these three regularization terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 2D CNN features with or without MHE on both training set and testing set. The features are computed by setting the output feature dimension as 2, similar to [27]. Each point denotes the 2D feature of a data point, and each color denotes a class. The red arrows are the classifier neurons of the output layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1: Testing error (%) of different MHE on CIFAR-10/100.</figDesc><table>MHE 
6.22 
6.74 
6.44 
27.15 
27.09 
26.16 
Half-space MHE 
6.28 
6.54 
6.30 
25.61 
26.30 
26.18 
A-MHE 
6.21 
6.77 
6.45 
26.17 
27.31 
27.90 
Half-space A-MHE 
6.52 
6.49 
6.44 
26.03 
26.52 
26.47 
Baseline 
7.75 
28.13 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Testing error (%) of different width on CIFAR-100.</figDesc><table>Network width. We evaluate MHE with dif-
ferent network width. We use CNN-9 as our 
base network, and change its filter number 
in Conv1.x, Conv2.x and Conv3.x (see Ap-
pendix A). We experiment different network 
width including 16/32/64, 32/64/128, 64/128/256 and 128/256/512. Results is given in Table 2. One 
can observe that both MHE and half-space MHE consistently outperforms the baseline, showing the 
stronger generalization. Interestingly, both MHE and half-space MHE have more significant gain 
while the filter number is smaller in each layer, indicating that our MHE regularization can help the 
network to make better use of the neurons. Moreover, the half-space MHE is consistently better than 
MHE, showing the necessity of reducing colinearity redundancy. 

Method 
CNN-6 
CNN-9 
CNN-15 

Baseline 
32.08 
28.13 
N/C 
MHE 
28.16 
26.75 
26.9 
Half-space MHE 
27.56 
25.96 
25.84 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Testing error (%) of different 
depth on CIFAR-100. N/C: not converged. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Error (%) of ResNet-32. MHE for ResNets. Besides the standard CNN, we also evaluate MHE on ResNet-32 to show that our MHE is architecture-agnostic and can improve accuracy on mul- tiple types of architectures. Detailed architecture settings are given in Appendix A. The results on CIFAR-10 and CIFAR-100 are given in</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Top1 error (%) on ImageNet.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head></head><label></label><figDesc>datasets.</figDesc><table>m SF 
LFW 
MegaFace 
SphereFace 
SphereFace+ 
SphereFace 
SphereFace+ 

1 
96.35 
97.15 
39.12 
45.90 
2 
98.87 
99.05 
60.48 
68.51 
3 
98.97 
99.13 
63.71 
66.89 
4 
99.26 
99.32 
70.68 
71.30 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="true"><head>Table 8 :</head><label>8</label><figDesc>Accuracy (%) on SphereFace-20 network.</figDesc><table>m SF 
LFW 
MegaFace 
SphereFace 
SphereFace+ 
SphereFace 
SphereFace+ 

1 
96.93 
97.47 
41.07 
45.55 
2 
99.03 
99.22 
62.01 
67.07 
3 
99.25 
99.35 
69.69 
70.89 
4 
99.42 
99.47 
72.72 
73.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 9 :</head><label>9</label><figDesc>Accuracy (%) on SphereFace-64 network.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head></head><label></label><figDesc>are computed without model ensemble and PCA. One can observe that SphereFace+ consistently outperforms SphereFace by a considerable margin on both LFW and MegaFace datasets under all different settings of m SF . Moreover, the performance gain generalizes across different architectures.</figDesc><table>Method 
LFW 
MegaFace 

Softmax Loss 
97.88 
54.86 
Softmax+Contrastive [43] 
98.78 
65.22 
Triplet Loss [38] 
98.70 
64.80 
L-Softmax Loss [27] 
99.10 
67.13 
Softmax+Center Loss [51] 
99.05 
65.49 
CosineFace [49, 47] 
99.10 
75.10 
SphereFace 
99.42 
72.72 
SphereFace+ (ours) 
99.47 
73.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 10 :</head><label>10</label><figDesc>Comparison to state-of-the-art.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head></head><label></label><figDesc>Neural nets with MHE show consistent improvements on performance in our experiments. Finally, MHE provides a new point of view on the role of neuron weights and may have many additional potential applications in model compression, image generation, etc.</figDesc><table>A Experimental Details 

Layer 
CNN-6 
CNN-9 
CNN-15 

Conv1.x 
[3×3, 64]×2 
[3×3, 64]×3 
[3×3, 64]×5 
Pool1 
2×2 Max Pooling, Stride 2 
Conv2.x 
[3×3, 128]×2 
[3×3, 128]×3 
[3×3, 128]×5 
Pool2 
2×2 Max Pooling, Stride 2 
Conv3.x 
[3×3, 256]×2 
[3×3, 256]×3 
[3×3, 256]×5 
Pool3 
2×2 Max Pooling, Stride 2 
Fully Connected 
256 
256 
256 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 11 :</head><label>11</label><figDesc>Our plain CNN architectures with different convolutional layers.</figDesc><table>Conv1.x, Conv2.x and Conv3.x 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 12 :</head><label>12</label><figDesc>Our ResNet architectures with different convolutional layers.</figDesc><table>Conv0.x, Conv1.x, Conv2.x, Conv3.x 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="false"><head></head><label></label><figDesc>, we observe that SphereNets with MHE can outperform both the SphereNet baseline and SphereNets with the orthonormal regularization, showing that MHE is not only effective in standard CNNs but also very suitable for SphereNets.</figDesc><table>Method 
CIFAR-10 
CIFAR-100 
s = 2 s = 1 s = 0 s = 2 
s = 1 
s = 0 
MHE 
5.71 
5.99 
5.95 
27.28 26.99 27.03 
Half-space MHE 
6.12 
6.33 
6.31 
27.17 27.77 27.46 
A-MHE 
5.91 
5.98 
6.06 
27.07 27.27 26.70 
Half-space A-MHE 
6.14 
5.87 
6.11 
27.35 27.68 27.58 
SphereNet with Orthonormal Reg. 
6.13 
27.95 
SphereNet Baseline 
6.37 
28.10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="false"><head>Table 13 :</head><label>13</label><figDesc>Testing error (%) of SphereNet with different MHE on CIFAR-10/100.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" validated="false"><head>Table 15 :</head><label>15</label><figDesc>Our CNN architectures for image Generation on CIFAR-10. The slopes of all leaky ReLU (lReLU) functions in the networks are set to 0.1. z ∈ R 128 ∼ N (0, I) dense → Mg × Mg × 512 4×4, stride=2 deconv. BN 256 ReLU 4×4, stride=2 deconv. BN 128 ReLU 4×4, stride=2 deconv. BN 64 ReLU 3×3, stride=1 conv. 3 Tanh (a) Generator (Mg = 4 for CIFAR10). RGB image x ∈ R</figDesc><table>M ×M ×3 

3×3, stride=1 conv 64 lReLU 
4×4, stride=2 conv 64 lReLU 
3×3, stride=1 conv 128 lReLU 
4×4, stride=2 conv 128 lReLU 
3×3, stride=1 conv 256 lReLU 
4×4, stride=2 conv 256 lReLU 
3×3, stride=1 conv. 512 lReLU 
dense → 1 

(b) Discriminator (M = 32 CIFAR10). 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Classifier neurons are the projection bases of the last layer (i.e., output layer) before input to softmax.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank NVIDIA corporation for donating Titan Xp GPUs to support our research. We also thank Tuo Zhao (Georgia Tech) for the valuable discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Net-trim: A layer-wise convex pruning of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Aghasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural photo editing with introspective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reducing overfitting in deep networks by decorrelating representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Note on d-extremal configurations for the sphere in r d+1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Götz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Progress in Multivariate Approximation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="159" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saff</surname></persName>
		</author>
		<idno>math-ph/0311024</idno>
		<title level="m">Minimal riesz energy point configurations for rectifiable d-dimensional manifolds</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discretizing manifolds via minimum energy points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Notices of the AMS</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1186" to="1194" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asymptotics for minimal discrete energy on the sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Kuijlaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="538" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ludmila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Foundations of modern potential theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Landkof</forename><surname>Naum Samouilovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diversity regularized ensemble pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongmei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Decoupled networks. CVPR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep hyperspherical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classification and clustering via dictionary learning with structured incoherence and shared features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Regularizing cnns with locally constrained decorrelations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pau Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reducing duplicate filters in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Deep Learning: Bridging Theory and Practice</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Distributing many points on a sphere. The mathematical intelligencer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Amo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuijlaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="5" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mathematical problems for the next century. The mathematical intelligencer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Smale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the origin of number and arrangement of the places of exit on the surface of pollen-grains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter Merkus Lambertus</forename><surname>Tammes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recueil des travaux botaniques néerlandais</title>
		<imprint>
			<date type="published" when="1930" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">on the structure of the atom: an investigation of the stability and periods of oscillation of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory of atomic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph John</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xxiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="237" to="265" />
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05599</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06369</idno>
		<title level="m">Normface: L2 hypersphere embedding for face verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09414</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03131</idno>
		<title level="m">Diverse neural network learns true target functions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01827</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning latent space models with angular constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Uncorrelation and evenness: a new diversity-promoting regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Orthogonality-promoting distance metric learning: convex relaxation and theoretical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Diversity-promoting bayesian learning of latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Table 14: Inception scores with unsupervised image generation on CIFAR-10</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
