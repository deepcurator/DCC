<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
							<email>huangrui@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Li</surname></persName>
							<email>tianyu.lizard@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Benefiting from the rapid development of deep learning methods and the easy access to a large amount of annotated face images, unconstrained face recognition techniques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> have made significant advances in recent years. Although surpassing human performance has been * These two authors contributed equally • profile image (middle) and its corresponding synthesized and ground truth frontal face. We invite the readers to guess which side is our synthesis results (please refer to Sec. 1 for the answer). The lower half shows the synthesized frontal view faces from profiles of 90</p><p>• , 75</p><p>• and 45</p><p>• respectively.</p><p>achieved on several benchmark datasets <ref type="bibr" target="#b24">[25]</ref>, pose variations are still the bottleneck for many real-world application scenarios. Existing methods that address pose variations can be divided into two categories. One category tries to adopt hand-crafted or learned pose-invariant features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>, while the other resorts to synthesis techniques to recover a frontal view image from a large pose face image and then use the recovered face images for face recognition <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. For the first category, traditional methods often make use of robust local descriptors such as Gabor <ref type="bibr" target="#b4">[5]</ref>, Haar <ref type="bibr" target="#b31">[32]</ref> and LBP <ref type="bibr" target="#b1">[2]</ref> to account for local distortions and then adopt metric learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> techniques to achieve pose invariance. In contrast, deep learning methods often handle position variances with pooling operation and employ triplet loss <ref type="bibr" target="#b24">[25]</ref> or contrastive loss <ref type="bibr" target="#b27">[28]</ref> to ensure invariance to very large intraclass variations. However, due to the tradeoff between invariance and discriminability, these approaches cannot deal with large pose cases effectively.</p><p>For the second category, earlier efforts on frontal view synthesis usually utilize 3D geometrical transformations to render a frontal view by first aligning the 2D image with either a general <ref type="bibr" target="#b11">[12]</ref> or an identity specific <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref> 3D model. These methods are good at normalizing small pose faces, but their performance decreases under large face poses due to severe texture loss. Recently, deep learning based methods are proposed to recover a frontal face in a data-driven way. For instance, Zhu et al. <ref type="bibr" target="#b41">[42]</ref> propose to disentangle identity and pose representations while learning to estimate a frontal view. Although their results are encouraging, the synthesized image sometimes lacks fine details and tends to be blurry under a large pose so that they only use the intermediate features for face recognition. The synthesized image is still not good enough to perform other facial analysis tasks, such as forensics and attribute estimation.</p><p>Moreover, from an optimization point of view, recovering the frontal view from incompletely observed profile is an ill-posed or under-defined problem, and there exist multiple solutions to this problem if no prior knowledge or constraints are considered. Therefore, the quality of recovered results heavily relies on the prior or the constraints exploited in the training process. Previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> usually adopts pairwise supervision and seldom introduce constraints in the training process, so that they tend to produce blurry results.</p><p>When human try to conduct a view synthesis process, we firstly infer the global structure (or a sketch) of a frontal face based on both our prior knowledge and the observed profile. Then our attention moves to the local areas where all facial details will be filled out. Inspired by this process, we propose a deep architecture with two pathways (TP-GAN) for frontal view synthesis. These two pathways focus on the inference of global structure and the transformation of local texture respectively. Their corresponding feature maps are then fused for further process for the generation of the final synthesis. We also make the recovery process well constrained by incorporating prior knowledge of the frontal faces' distribution with a Generative Adversarial Network (GAN) <ref type="bibr" target="#b8">[9]</ref>. The outstanding capacity of GAN in modeling 2D data distribution has significantly advanced many ill-posed low level vision problems, such as superresolution <ref type="bibr" target="#b16">[17]</ref> and inpainting <ref type="bibr" target="#b20">[21]</ref>. Particularly, drawing inspiration from the faces' symmetric structure, a symmetry loss is proposed to fill out occluded parts. Moreover, to faithfully preserve the most prominent facial structure of an individual, we adopt a perceptual loss <ref type="bibr" target="#b13">[14]</ref> in the compact feature space in addition to the pixel-wise L1 loss. Incorporating the identity preserving loss is critical for a faithful synthesis and greatly improves its potential to be applied to face analysis tasks. We show some samples generated by TP-GAN in the upper half of <ref type="figure" target="#fig_0">Fig. 1</ref> (the left side of each tuple).</p><p>The main contributions of our work lie in three folds: 1) We propose a human-like global and local aware GAN architecture for frontal view synthesis from a single image, which can synthesize photorealistic and identity preserving frontal view images even under a very large pose. 2) We combine prior knowledge from data distribution (adversarial training) and domain knowledge of faces (symmetry and identity preserving loss) to exactly recover the lost information inherent in projecting a 3D object into a 2D image space. 3) We demonstrate the possibility of a "recognition via generation" framework and outperform state-of-the-art recognition results under a large pose. Although some deep learning methods have been proposed for face synthesis, our method is the first attempt to be effective for the recognition task with synthesized faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Frontal View Synthesis</head><p>Frontal view synthesis, or termed as face normalization, is a challenging task due to its ill-posed nature. Traditional methods address this problem either with 2D/3D local texture warping <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref> or statistical modeling <ref type="bibr" target="#b23">[24]</ref>. For instance, Hassner et al. <ref type="bibr" target="#b11">[12]</ref> employ a mean 3D model for face normalization. A joint frontal view synthesis and landmark localization method is proposed in <ref type="bibr" target="#b23">[24]</ref> with a constrained low-rank minimization model. Recently, researchers employ Convolutional Neural Networks (CNN) for joint representation learning and view synthesis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Specifically, Yim et al. <ref type="bibr" target="#b37">[38]</ref> propose a multi-task CNN to predict identity preserving rotated images. Zhu et al. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> develop novel architectures and learning objectives to disentangle the identity and pose representation while estimating the frontal view. Reed et al. <ref type="bibr" target="#b21">[22]</ref> propose to use a Boltzmann machine to model factors of variation and generate rotated images via pose manifold traversal. Although it is much more convenient if the synthesized image can be directly used for facial analysis tasks, most of the previous methods mainly employ intermediate features for face recognition because they cannot faithfully produce an identity preserving synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative Adversarial Network (GAN)</head><p>As one of the most significant improvements on the research of deep generative models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, GAN <ref type="bibr" target="#b8">[9]</ref> has drawn substantial attention from both the deep learning and computer vision society. The min-max two-player game provides a simple yet powerful way to estimate target distribution and generate novel image samples <ref type="bibr" target="#b5">[6]</ref>. With its power for distribution modeling, the GAN can encourage the generated images to move towards the true image manifold and thus generates photorealistic images with plausible high frequency details. Recently, modified GAN architectures, conditional GAN <ref type="bibr" target="#b18">[19]</ref> in particular, have been successfully applied to vision tasks like image inpainting <ref type="bibr" target="#b20">[21]</ref>, super-resolution <ref type="bibr" target="#b16">[17]</ref>, style transfer <ref type="bibr" target="#b17">[18]</ref>, face attribute manipulation <ref type="bibr" target="#b25">[26]</ref> and even data augmentation for boosting classification models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>. These successful applications of GAN motivate us to develop frontal view synthesis methods based on GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The aim of frontal view synthesis is to recover a photorealistic and identity preserving frontal view image I F from a face image under a different pose, i.e. a profile image I P . To train such a network, pairs of corresponding {I F , I P } from multiple identities y are required during the training phase. Both the input I P and output I F come from a pixel space of size W × H × C with C color channel.</p><p>It's our goal to learn a synthesis function that can infer the corresponding frontal view from any given profile images. Specifically, we model the synthesis function with a two-pathway CNN G θ G that is parametrized by θ G . Each pathway contains an Encoder and a Decoder, denoted as</p><formula xml:id="formula_0">{G θ g E , G θ g D } and {G θ l E , G θ l D }</formula><p>, where g and l stand for the global structure pathway and the local texture pathway respectively. In the global pathway, the bottleneck layer, which is the output of G θ g E , is usually used for classification task <ref type="bibr" target="#b36">[37]</ref> with the cross-entropy loss L cross entropy .</p><p>The network's parameters G θ G are optimized by minimizing a specifically designed synthesis loss L syn and the aforementioned L cross entropy . For a training set with N training pairs of {I F n , I</p><p>P n }, the optimization problem can be formulated as follows:</p><formula xml:id="formula_1">θ G = 1 N argmin θ G N n=1 {L syn (G θ G (I P n ), I F n ) +αL cross entropy (G θ g E (I P n ), y n )} (1)</formula><p>where α is a weighting parameter and L syn is defined as a weighted sum of several losses that jointly constrain an image to reside in the desired manifold. We will postpone the detailed description of all the individual loss functions to Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Two Pathway Generator</head><p>The general architecture of TP-GAN is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Different from previous methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> that usually model the synthesis function with one single network, our proposed generator G θ G has two pathways, with one global network G θ g processing the global structure and four landmark located patch networks G θ l i , i ∈ {0, 1, 2, 3} attending to local textures around four facial landmarks.</p><p>We are not the first to employ the two pathway modeling strategy. Actually, this is a quite popular routine for 2D/3D local texture warping <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref> methods. Similar to the human cognition process, they usually divide the normalization of faces into two steps, with the first step to align the face globally with a 2D or 3D model and the second step to warp or render local texture to the global structure. Moreover, Mohammed et al. <ref type="bibr" target="#b19">[20]</ref> combines a global parametric model with a local non-parametric model for novel face synthesis.</p><p>Synthesizing a frontal face I F from a profile image I P is a highly non-linear transformation. Since the filters are shared across all the spatial locations of the face image, we argue that using merely a global network cannot learn filters that are suitable for both rotating a face and precisely recovering local details. Therefore, we transfer the success of the two pathway structure in traditional methods to a deep learning based framework and introduce the humanlike two pathway generator for frontal view synthesis. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, G θ g is composed of a downsampling Encoder G θ g E and an up-sampling Decoder G θ g D , extra skip layers are introduced for multi-scale feature fusion. The bottleneck layer in the middle outputs a 256-dimension feature vector v id , which is used for identity classification to allow for identity-preserving synthesis. At this bottleneck layer, as in <ref type="bibr" target="#b29">[30]</ref>, we concatenate a 100-dim Gaussian random noise to v id to model variations other than pose and identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Landmark Located Patch Network</head><p>The four input patches of the landmark located patch network G θ l are center-cropped from four facial landmarks, i.e. left eye center, right eye center, nose tip and mouth center. Each G θ l i , i ∈ {0, 1, 2, 3} learns a separate set of filters for rotating the center-cropped patch to its corresponding frontal view (after rotation, the facial landmarks are still in the center). The architecture of the landmark located patch network is also based on an encoder-decoder structure, but it has no fully connected bottleneck layer.</p><p>To effectively integrate the information from the global and local pathways, we adopt an intuitive method for feature map fusion. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we firstly fuse the output feature tensors (multiple feature maps) of four local pathways to one single feature tensor that is of the same spatial resolution as the global feature tensor. Specifically, we put each feature tensor at a "template landmark location", and then a max-out fusing strategy is introduced to reduce the stitching artifacts on the overlapping areas. Then, we simply concatenate the feature tensor from each pathway to produce a fused feature tensor and then feed it to successive convolution layers to generate the final synthesis output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Adversarial Networks</head><p>To incorporate prior knowledge of the frontal faces' distribution into the training process, we further introduce an discriminator D θ D to distinguish real frontal face images I F from synthesized frontal face images G θ G (I P ), following the work of Goodfellow et al. <ref type="bibr" target="#b8">[9]</ref>. We train D θ D and G θ G in an alternating way to optimize the following min-max problem: min</p><formula xml:id="formula_2">θ G max θ D E I F ∼P (I F ) log D θ D (I F )+ E I P ∼P (I P ) log(1 − D θ D (G θ G (I P )))<label>(2)</label></formula><p>Solving this min-max problem will continually push the output of the generator to match the target distribution of the training frontal faces, thus it encourages the synthesized image to reside in the manifold of frontal faces, leading to photorealistic synthesis with appealing high frequency details. As in <ref type="bibr" target="#b26">[27]</ref>, our D θ D outputs a 2 × 2 probability map instead of one scalar value. Each probability value now corresponds to a certain region instead of the whole face, and D θ D can specifically focus on each semantic region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Synthesis Loss Function</head><p>The synthesis loss function used in our work is a weighted sum of four individual loss functions, we will give a detailed description in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pixel-wise Loss</head><p>We adopt pixel-wise L1 loss at multiple locations to facilitate multi-scale image content consistency:</p><formula xml:id="formula_3">L pixel = 1 W × H W x=1 H y=1 |I pred x,y − I gt x,y |<label>(3)</label></formula><p>Specifically, the pixel wise loss is measured at the output of the global, the landmark located patch network and their final fused output. To facilitate a deep supervision, we also add the constraint on multi-scale outputs of the G θ g D</p><p>. Although this loss will lead to overly smooth synthesis results, it is still an essential part for both accelerated optimization and superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Symmetry Loss</head><p>Symmetry is an inherent feature of human faces. Exploiting this domain knowledge as a prior and imposing a symmetric constraint on the synthesized images may effectively alleviate the self-occlusion problem and thus greatly improve performance for large pose cases. Specifically, we define a symmetry loss in two spaces, i.e. the original pixel space and the Laplacian image space, which is robust to illumination changes. The symmetry loss of a face image takes the form:</p><formula xml:id="formula_4">L sym = 1 W/2 × H W/2 x=1 H y=1 |I pred x,y − I pred W −(x−1),y | (4)</formula><p>For simplicity, we selectively flip the input so that the occluded part are all on the right side. Besides, only the occluded part (right side) of I pred receives the symmetry (a) Profile (b) Ours (c) <ref type="bibr" target="#b29">[30]</ref> (d) <ref type="bibr" target="#b37">[38]</ref> (e) <ref type="bibr" target="#b7">[8]</ref> (f) <ref type="bibr" target="#b39">[40]</ref> (g) <ref type="bibr" target="#b11">[12]</ref> (h) Frontal loss, i.e. we explicitly pull the right side to be closer to the left. L sym 's contribution is twofold, generating realistic images by encouraging a symmetrical structure and accelerating the convergence of TP-GAN by providing additional back-propagation gradient to relieve self-occlusion for extreme poses. However, due to illumination changes or intrinsic texture difference, pixel values are not strictly symmetric most of the time. Fortunately, the pixel difference inside a local area is consistent, and the gradients of a point along all directions are largely reserved under different illuminations. Therefore, the Laplacian space is more robust to illumination changes and more indicative for face structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Adversarial Loss</head><p>The loss for distinguishing real frontal face images I F from synthesized frontal face images G θ G (I P ) is calculated as follows:</p><formula xml:id="formula_5">L adv = 1 N N n=1 − log D θ D (G θ G (I P n ))<label>(5)</label></formula><p>L adv serves as a supervision to push the synthesized image to reside in the manifold of frontal view images. It can prevent blur effect and produce visually pleasing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Identity Preserving Loss</head><p>Preserving the identity while synthesizing the frontal view image is the most critical part in developing the "recognition via generation" framework. In this work, we exploit the perceptual loss <ref type="bibr" target="#b13">[14]</ref> that is originally proposed for maintaining perceptual similarity to help our model gain the identity preserving ability. Specifically, we define the identity preserving loss based on the activations of the last two layers of the Light CNN <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_6">L ip = 2 i=1 1 W i × H i Wi x=1 Hi y=1 |F (I P ) i x,y − F (G(I pred )) i x,y |<label>(6)</label></formula><p>where W i , H i denotes the spatial dimension of the last ith layer. The identity preserving loss enforces the prediction to have a small distance with the ground-truth in the compact deep feature space. Since the Light CNN is pre-trained to classify tens of thousands of identities, it can capture the most prominent feature or face structure for identity discrimination. Therefore, it is totally viable to leverage this loss to enforce an identity preserving frontal view synthesis. L ip has better performance when used with L adv . Using L ip alone makes the results prone to annoying artifacts, because the search for a local minimum of L ip may go through a path that resides outside the manifold of natural face images. Using L adv and L ip together can ensure that the search resides in that manifold and produces photorealistic image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Overall Objective Function</head><p>The final synthesis loss function is a weighted sum of all the losses defined above:</p><formula xml:id="formula_7">L syn = L pixel + λ 1 L sym + λ 2 L adv + λ 3 L ip + λ 4 L tv (7)</formula><p>We also impose a total variation regularization L tv <ref type="bibr" target="#b13">[14]</ref> on the synthesized result to reduce spike artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Except for synthesizing natural looking frontal view images, the proposed TP-GAN also aims to generate identity preserving image for accurate face analysis with off-theshelf deep features. Therefore, in this section, we demonstrate the merits of our model on qualitative synthesis results and quantitive recognition results in Sec. 4.1 and 4.2. Sec. 4.3 presents visualization of the final deep feature representations to illustrate the effectiveness of TP-GAN. Finally, in Sec. 4.4, we conduct detailed algorithmic evaluation to demonstrate the advantages of the proposed twopathway architecture and synthesis loss function. Implementation details We use colorful images of size 128 × 128 × 3 in all our experiments for both the input   I P and the prediction I pred = G θ G (I P ). Our method is evaluated on MultiPIE <ref type="bibr" target="#b9">[10]</ref>, a large dataset with 750, 000+ images for face recognition under pose, illumination and expression changes. The feature extraction network, Light CNN, is trained on MS-Celeb-1M <ref type="bibr" target="#b10">[11]</ref> and fine-tuned on the original images of MultiPIE. Our network is implemented with Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The training of TP-GAN lasts for one day with a batch size of 10 and a learning rate of 10 −4 . In all our experiments, we empirically set α = 10 −3 , λ 1 = 0.3, λ 2 = 10 −3 , λ 3 = 3 × 10 −3 and λ 4 = 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Face Synthesis</head><p>Most of the previous work on frontal view synthesis are dedicated to address that problem within a pose range of ±60</p><p>• . Because it is commonly believed that with a pose larger than 60</p><p>• , it is difficult to faithfully recover a frontal view image. However, we will show that given enough training data and a proper architecture and loss design, it is in fact feasible to recover photorealistic frontal views from very large poses. <ref type="figure" target="#fig_3">Fig. 4</ref> shows TP-GAN's ability to recover compelling identity-preserving frontal faces from any pose (a) Ours (b) <ref type="bibr" target="#b37">[38]</ref> (c) <ref type="bibr" target="#b7">[8]</ref> (d) <ref type="bibr" target="#b39">[40]</ref> (e) <ref type="bibr" target="#b11">[12]</ref> Figure 6. Mean faces from six images (within ±45</p><p>• ) per identity.</p><p>and <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates a comparison with state-of-the-art face frontalization methods. Note that most of TP-GAN's competitors cannot deal with poses larger than 45</p><p>• , therefore, we only report their results under 30</p><p>• and 45</p><p>• .</p><p>Compared to competing methods, TP-GAN presents a good identity preserving quality while producing photorealistic synthesis. Thanks to the data-driven modeling with prior knowledge from L adv and L ip , not only the overall face structure but also the occluded ears, cheeks and forehead can be hallucinated in an identity consistent way. Moreover, it also perfectly preserves observed face attributes in the original profile image, e.g. eyeglasses and hair style, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>To further demonstrate the stable geometry shape of the syntheses across multiple poses, we show the mean image of synthesized faces from different poses in <ref type="figure">Fig. 6</ref>. The mean faces from TP-GAN preserve more texture detail and contain less blur effect, showing a stable geometry shape across multiple syntheses. Note that our method does not rely on any 3D knowledge for geometry shape estimation, the inference is made through sheer data-driven learning.</p><p>As a demonstration of our model's superior generalization ability to in the wild faces, we use images from LFW <ref type="bibr" target="#b12">[13]</ref> dataset to test a TP-GAN model trained solely on Multi-PIE. As shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, although the resultant color tone is similar to images from Multi-PIE, TP-GAN can faithfully synthesize frontal view images with both finer details and better global shapes for faces in LFW dataset compared to state-of-the-art methods like <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Identity Preserving Property</head><p>Face Recognition To quantitatively demonstrate our method's identity preserving ability, we conduct face recognition on MultiPIE with two different settings. The experiments are conducted by firstly extracting deep features with Light-CNN <ref type="bibr" target="#b34">[35]</ref> and then compare Rank-1 recognition accuracy with a cosine-distance metric. The results on the profile images I P serve as our baseline and are marked by the notation Light-CNN in all tables. It should be noted that although many deep learning methods have been proposed for frontal view synthesis, none of their synthesized images proved to be effective for recognition tasks. In a recent study on face hallucination <ref type="bibr" target="#b33">[34]</ref>, the authors show that directly using a CNN synthesized high resolution face image for recognition will certainly degenerate the performance instead of improving it. Therefore, it is of great significance to validate whether our synthesis results can boost the recognition performance (whether the "recognition via generation" procedure works).</p><p>In Setting 1, we follow the protocol from <ref type="bibr" target="#b35">[36]</ref>, and only images from session one are used. We include images with neutral expression under 20 illuminations and 11 poses within ±90</p><p>• . One gallery image with frontal view and illumination is used for each testing subject. There is no overlap between training and testing sets. <ref type="table" target="#tab_0">Table 1</ref> shows our recognition performance and the comparison with the stateof-the-art. TP-GAN consistently achieves the best performance across all angles, and the larger the angle, the greater the improvement. When compared with c-CNN Forest <ref type="bibr" target="#b35">[36]</ref>, which is an ensemble of three models, we achieve a performance boost of about 20% on large pose cases.</p><p>In Setting 2, we follow the protocol from <ref type="bibr" target="#b37">[38]</ref>, where neural expression images from all four sessions are used. One gallery image is selected for each testing identity from their first appearance. All synthesized images of MultiPIE in this paper are from the testing identities under Setting 2. The result is shown in <ref type="table" target="#tab_1">Table 2</ref>. Note that all the compared CNN based methods achieve their best performances with learned intermediate features, whereas we directly use the synthesized images following a "recognition via generation" procedure.</p><p>Gender Classification To further demonstrate the potential of our synthesized images on other facial analysis tasks, we conduct an experiment on gender classification. All the compared methods in this part also follow the "recognition via generation" procedure, where we directly use their synthesis results for gender classification. The CNN for gender classification is of the same structure as the encoder G θ g E and is trained on batch1 of the UMD <ref type="bibr" target="#b2">[3]</ref> dataset. We report the testing performance on Multi-PIE (Setting-1) in <ref type="table">Table 3</ref>. For fair comparison, we present the results on the unrotated original images in two resolutions, 128 × 128 (I P 128 ) and 60 × 60 (I P 60 ) respectively. TP-GAN's synthesis achieves a better classification accuracy than the original profile images due to normalized views. It's not surprising to see that all other compared models perform worse than the baseline, as their architectures are not designed for the gender classification task. Similar phenomenon is observed in <ref type="bibr" target="#b33">[34]</ref> where synthesized high resolution face images severely degenerate the recognition performance instead of improving it. That indicates the high risk of losing prominent facial features of I P when manipulating images in the pixel space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature Visualization</head><p>We use t-SNE <ref type="bibr" target="#b30">[31]</ref> to visualize the 256-dim deep feature on a two dimensional space. The left side of <ref type="figure" target="#fig_7">Fig. 8</ref> illustrates the deep feature space of the original profile images. It's clear that images with a large pose (90</p><p>• in particular) are not separable in the deep feature space spanned by the Light-CNN. It reveals that even though the Light-CNN is trained with millions of images, it still cannot prop-  erly deal with large pose face recognition problems. On the right side, after frontal view synthesis with our TP-GAN, the generated frontal view images can be easily classified into different groups according to their identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Algorithmic analysis</head><p>In this section, we go over different architectures and loss function combinations to gain insight into their respective roles in frontal view synthesis. Both qualitative visualization results and quantitive recognition results are reported for a comprehensive comparison.</p><p>We compare four variations of TP-GAN in this section, one for comparing the architectures and the other three for comparing the objective functions. Specifically, we train a network without the local pathway (denoted as P) as the first variant. With regards to the loss function, we keep the two-pathway architecture intact and remove one of the three losses, i.e. L ip , L adv and L sym , in each case.</p><p>Detailed recognition performance is reported in <ref type="table" target="#tab_2">Table 4</ref>. The two-pathway architecture and the identity preserving loss contribute the most for improving the recognition performance, especially on large pose cases. Although not as much apparent, both the symmetry loss and the adversarial loss help to improve the recognition performance. <ref type="figure" target="#fig_8">Fig. 9</ref> illustrates the perceptual performance of these variants. As expected, inference results without the identity preserving loss or the local pathway deviate from the true appearance seriously. And the synthesis without adversarial loss tends to be very blurry, while the result without the symmetry loss sometimes shows unnatural asymmetry effect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a global and local perception GAN framework for frontal view synthesis from a single image. The framework contains two separate pathways, modeling the out-of-plane rotation of the global structure and the non-linear transformation of the local texture respectively. To make the ill-posed synthesis problem well constrained, we further introduce adversarial loss, symmetry loss and identity preserving loss in the training process. Adversarial loss can faithfully discover and guide the synthesis to reside in the data distribution of frontal faces. Symmetry loss can explicitly exploit the symmetry prior to ease the effect of self-occlusion in large pose cases. Moreover, identity preserving loss is incorporated into our framework, so that the synthesis results are not only visually appealing but also readily applicable to accurate face recognition. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-the-art results on large pose face recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Frontal view synthesis by TP-GAN. The upper half shows the 90 • profile image (middle) and its corresponding synthesized and ground truth frontal face. We invite the readers to guess which side is our synthesis results (please refer to Sec. 1 for the answer). The lower half shows the synthesized frontal view faces from profiles of 90 • , 75 • and 45 • respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. General framework of TP-GAN. The Generator contains two pathways with each processing global or local transformations. The Discriminator distinguishes between synthesized frontal (SF) views and ground-truth (GT) frontal views. Detailed network architectures can be found in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison with state-of-the-art synthesis methods under the pose of 45 • (first two rows) and 30 • (last row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Synthesis results by TP-GAN under different poses. From left to right, the poses are 90 • , 75 • , 60 • , 45 • , 30 • and 15 • . The ground truth frontal images are provided at the last column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Challenging situations. The facial attributes, e.g. beard, eyeglasses are preserved by TP-GAN. The occluded forehead and cheek are recovered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Synthesis results on the LFW dataset. Note that TP-GAN is trained on Mulit-PIE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Feature space of the profile faces (left) and fontal view synthesized images (right). Each color represents a different identity. Each shape represent a view. The images for one identity are labeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Model comparison: synthesis results of TP-GAN and its variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Rank-1 recognition rates (%) across views and illumina- tions under Setting 1. For all the remaining tables, only methods marked with * follow the "recognition via generation" procedure while others leverage intermediate features for face recognition.</figDesc><table>Method 
±90 

• 

±75 

• 

±60 

• 

±45 

• 

±30 

• 

±15 

• 

CPF [38] 
-
-
-
71.65 
81.05 
89.45 
Hassner et al. * [12] 
-
-
44.81 
74.68 
89.59 
96.78 
HPN [7] 
29.82 
47.57 
61.24 
72.77 
78.26 
84.23 
FIP 40 [41] 
31.37 
49.10 
69.75 
85.54 
92.98 
96.30 
c-CNN Forest [36] 
47.26 
60.66 
74.38 
89.02 
94.05 
96.97 
Light CNN [35] 
9.00 
32.35 
73.30 
97.45 
99.80 
99.78 
TP-GAN* 
64.03 
84.10 
92.93 
98.58 
99.85 
99.78 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Rank-1 recognition rates (%) across views, illuminations and sessions under Setting 2.</figDesc><table>Method 
±90 

• 

±75 

• 

±60 

• 

±45 

• 

±30 

• 

±15 

• 

FIP+LDA [41] 
-
-
45.9 
64.1 
80.7 
90.7 
MVP+LDA [42] 
-
-
60.1 
72.9 
83.7 
92.8 
CPF [38] 
-
-
61.9 
79.9 
88.5 
95.0 
DR-GAN [30] 
-
-
83.2 
86.2 
90.1 
94.0 
Light CNN [35] 
5.51 
24.18 
62.09 
92.13 
97.38 
98.59 
TP-GAN* 
64.64 
77.43 
87.72 
95.38 
98.06 
98.68 

Table 3. Gender classification accuracy (%) across views and illu-
minations. 

Method 
±45 

• 

±30 

• 

±15 

• 

I 

P 
60 

85.46 
87.14 
90.05 
CPI* [38] 
76.80 
78.75 
81.55 
Amir et al. * [8] 
77.65 
79.70 
82.05 
I 

P 
128 

86.22 
87.70 
90.46 
Hassner et al. * [12] 
83.83 
84.74 
87.15 
TP-GAN* 
90.71 
89.90 
91.22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Model comparison: Rank-1 recognition rates (%) under Setting 2.</figDesc><table>Method 
±90 

• 

±75 

• 

±60 

• 

±45 

• 

±30 

• 

±15 

• 

w/o P 
44.13 
66.10 
80.64 
92.07 
96.59 
98.35 
w/o Lip 
43.23 
56.55 
70.99 
85.87 
93.43 
97.06 
w/o L adv 
62.83 
76.10 
85.04 
92.45 
96.34 
98.09 
w/o Lsym 
62.47 
75.71 
85.23 
93.13 
96.50 
98.47 
TP-GAN 
64.64 
77.43 
87.72 
95.38 
98.06 
98.68 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially funded by National Natural Science Foundation of China (Grant No. 61622310, 61473289) and the State Key Development Program (Grant No. 2016YFB1001001). We thank Xiang Wu for useful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Umdfaces: An annotated face dataset for training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01484</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by twodimensional visual cortical filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose-invariant face recognition with homography-based normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="144" to="152" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards automatic image editing: Learning to see another you</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-pie. Image Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked progressive auto-encoders (spae) for face recognition across poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining markov random fields and convolutional neural networks for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visio-lization: generating novel facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust statistical face frontalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning residual images for face attribute manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08091</idno>
		<title level="m">Deep joint face hallucination and recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02683</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional convolutional neural network for modality-aware face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weaklysupervised disentangling with recurrent transformations for 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-view perceptron: a deep model for learning face identity and view representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
