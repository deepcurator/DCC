<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SST: Single-Stream Temporal Action Proposals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
							<email>shyamal@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
							<email>victor.escorcia@kaust.edu.sa</email>
							<affiliation key="aff1">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
							<email>shencq@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff1">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SST: Single-Stream Temporal Action Proposals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Our paper presents a new approach for temporal detection   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The millions of cameras that are deployed every year generate a large amount of recorded, stored and transmitted video. In particular, a large proportion of this video depicts events about people, their activities, and behaviors. In order to effectively interpret this data, computer vision algorithms need the ability to understand and recognize human actions. This has motivated a large body of computer vision literature on the problem of action recognition in videos.</p><p>Up until recently, the vast majority of the computer vision work tackles the action recognition problem as one of video classification, where an oracle has pre-segmented videos into short clips that contain a single action. In that case, the task is reduced to classifying the video into one of the relevant actions of interest. In practice, applications (such as smart surveillance, robotics or autonomous driving) require cameras to record video streams continuously and vision algorithms to perform temporal action detection in such long streams. To achieve this, the computer vision system must simultaneously decide both the temporal interval and the category of the action as they occur.</p><p>Temporally detecting human actions can be challenging â€¦ SST sliding windows t <ref type="figure">Figure 1</ref>. We tackle the problem of temporal action localization of human actions in long, untrimmed video sequences. We introduce a new model (SST) that outputs temporal action proposals at multiple scales in a single processing stream. Our method simultaneously provides stronger action proposals while being significantly more efficient than prior work, which require constructing and processing multiple temporally overlapping sliding windows.</p><p>for computer vision algorithms. Algorithms must process very long video sequences and output the starting and ending times of each action in each video. The number and duration of actions can vary significantly, and action intervals can be very short in comparison to the video length. Recent work has leveraged temporal action proposals <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref> for efficient action detection, where the proposals identify relevant temporal windows that are then independently classified by an action classifier in a second stage. These proposals are generated by a sliding window approach, dividing the video into short overlapping temporal windows. To handle the issue of temporal variations in actions, windows are applied at multiple temporal scales <ref type="bibr" target="#b28">[29]</ref>. However, this is computationally expensive due to the exhaustive search in temporal location and scale. Alternatively, one can adopt an architecture that runs a sliding window at a fixed temporal scale but outputs proposals at varied temporal scales <ref type="bibr" target="#b8">[9]</ref>. This approach still needs to perform computations on overlapping temporal windows, which results in possibly redundant computations as each frame is processed more than once. For many practical applications that involve large scale data or real-time interactive systems, it is critical to enable very fast action localization in videos and such redundant computations can be prohibitive.</p><p>In this paper, we introduce a framework for temporal action proposals in long video sequences that only needs to process the entire video in a single pass. This means that our architecture is able to analyze videos of arbitrary length without needing to process overlapping temporal windows separately. This results in a much more efficient and effective architecture. The main contributions of our approach are: (i) We introduce a new architecture (SST) for temporal action proposals generation that runs over the video in a single pass, without the use of overlapping temporal sliding windows. We design a training scheme to enable the use of a recurrent network with very long input video sequences, without compromising model performance.</p><p>(ii) We demonstrate that our new temporal proposal generation architecture achieves state-of-the-art performance on the proposal generation task. (iii) Finally, we verify that SST proposals provide a stronger basis for temporal action localization than prior methods, and integration with existing classification stages leads to improved state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review relevant recent work in video action categorization, temporal and spatio-temporal action detection in videos, and sequence modeling with recurrent networks.</p><p>Action Categorization. A large body of research has tackled the problem of action categorization from short video clips <ref type="bibr" target="#b13">[14]</ref>. In this setting, we assume we are given short video clips with only a single action being performed in the sequence. Many approaches use a global representation <ref type="bibr" target="#b32">[33]</ref>, but some try to model the temporal structure of motions to achieve classification <ref type="bibr" target="#b9">[10]</ref>. Unfortunately, these methods do not perform well on long video sequences where actions have a relatively small temporal duration and the majority of the visual input is considered background.</p><p>Temporal Action Detection and Proposals. A number of existing methods approach the problem of temporal action localization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38]</ref>. Traditionally, temporal action detection has been tackled by densely applying action classifiers in a sliding window fashion <ref type="bibr" target="#b7">[8]</ref>. Recently, temporal action proposals <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29]</ref> have been introduced to enable more efficient application of action classifiers on a smaller number of temporal windows. The basic idea is to first generate a reduced number of candidate temporal windows, which can be achieved with dictionary learning <ref type="bibr" target="#b1">[2]</ref> or with a recurrent neural architecture <ref type="bibr" target="#b8">[9]</ref>. Then, an action classifier discriminates each window independently into one of the actions of interest. Additional processing stages can also be incorporated to refine the prediction scores with respect to the temporal bounds <ref type="bibr" target="#b28">[29]</ref>.</p><p>In particular, our proposal generation framework builds on the progress made by the Deep Action Proposals (DAPs) architecture <ref type="bibr" target="#b8">[9]</ref>. One property of DAPs is that it can retrieve action proposals of varied temporal scale by sliding a temporal window of fixed duration T . This avoids running sliding windows of multiple scales, but it still requires running an overlapping sliding window over videos longer than T . This means we need to process each input video frame multiple times, once for each window that overlaps with it. In this paper, our goal is to further reduce computation by introducing a model that processes each input frame only once and thereby processes the full video in a single pass.</p><p>Spatio-Temporal Action Detection. A related problem is that of detecting actions not only temporally but also spatially. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Here, the algorithms output spatio-temporal localization of actions. While these provide more detailed localization information, they tend to suffer from very high computational costs, which makes them difficult to use in cases where fast and efficient processing is necessary. Furthermore, it is possible that temporal proposals may actually help reduce the temporal search space for such algorithms. In this paper, we focus on detecting actions temporally, rather than spatio-temporally.</p><p>Object Detection. Object detection approaches have drastically improved their performance by adopting two key ideas: (1) the introduction of object proposals to replace sliding window detection, and (2) adoption of deep architecture as the learning machinery. Initial approaches adopted object proposal generation as a preprocessing stage that independently provided windows to an object classifier <ref type="bibr" target="#b10">[11]</ref>. However, recent frameworks have implemented the generation of object proposals with deep network architectures. An example of this is the Region Proposal Network (RPN) from <ref type="bibr" target="#b27">[28]</ref>, which directly outputs proposals on an image without multiple passes. Our approach adopts this philosophy and enables proposal generation from videos in a single pass, processing each frame only once.</p><p>Long Sequence Processing with RNNs. Recurrent Neural Networks (RNNs) have recently shown impressive performance in a variety of sequential modeling problems <ref type="bibr" target="#b19">[20]</ref>. In general, most demonstrations are limited in terms of the temporal sequence length that can be handled by RNNs at recognition time. This may be caused by the hidden state of the network becoming saturated if the input sequence is too long <ref type="bibr" target="#b20">[21]</ref>. For example, in Natural Language Processing, it is common to use the structure of text (chapters, sections, paragraphs, sentences) to break down long corpora into short but meaningful sequences <ref type="bibr" target="#b12">[13]</ref>. In the case of video, there is no prior access to equivalent semantic/syntactic structures. To handle long sequences, prior work for proposals <ref type="bibr" target="#b8">[9]</ref> adopts a windowed approach such that only short subsequences are processed by RNNs. Here, we enable the use of RNNs on very long input sequences by . Schematic illustrating our overall approach and model architecture. Here, we extract C3D features from the input video stream, with a time resolution Î´ = 16 frames for each "time step." These features are the input to the recurrent GRU-based sequence encoder model, which outputs k proposals at each time step t with a confidence vector ct, where the longest proposal is of length Î´ Â· k. Additionally, we can validate the usefulness of the top-ranked SST action proposals to the action detection task by applying a classifier model.</p><p>careful architecture and training scheme design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head><p>The main goal of our paper is to generate temporal action proposals in long untrimmed videos. Given an input video sequence, our model should produce a reduced number of temporal intervals that are likely to contain an action. It is important for temporal action proposal methods to reject many temporal intervals that contain no action, while retrieving the true action intervals with very high recall. It is also important for the retrieved action intervals to have very high temporal overlap with the correct intervals where actions are performed. Generating high overlap proposals is key to facilitate the work of the following action classification stages. Finally, it is crucial for the temporal proposals to be fast, so that the computational gains over the simple temporal sliding window approach are significant. In this section, we introduce the technical details of Single Stream Temporal Action Proposals (SST), a new model for temporal action proposals that encapsulates these three properties in an efficient and effective deep learning architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head><p>We propose a recurrent model architecture for the generation of temporal action proposals. Our model is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. In contrast with prior work, the key properties of our approach are: (i) our architecture considers the input video exactly once at multiple time-scales with no overlapping sliding windows, which results in fast runtime during inference; (ii) it considers and evaluates a large number of action proposals over densely sampled time-scales and locations, which results in the model producing proposals with high temporal overlap with the groundtruth action intervals.</p><p>Input. At inference time, our model takes as input a long, untrimmed video sequence X = {x l } L l=1 with L frames. Unlike prior work that divides the video into highly overlapping temporal windows for independent batch processing, we construct no overlapping sliding windows over the input video and process each frame once, sequentially.</p><p>Visual Encoding. The goal of the Visual Encoder module is to compute a feature representation that encapsulates the visual content of the input video. We achieve this in our framework by feeding the video input through a 3D-Convolutional (C3D) network <ref type="bibr" target="#b32">[33]</ref>. We choose C3D, as it is able to effectively capture visual and motion information over some small temporal resolution Î´ <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29]</ref>. We leverage the architecture and pre-trained weights from <ref type="bibr" target="#b32">[33]</ref> for our initialization, with a time resolution of Î´ = 16 frames. In this manner, we efficiently discretize the input stream into T = L/Î´ non-overlapping time steps. Each time step t is represented by a C3D feature encoding v t = Ï† {x i } tÂ·Î´ i=(tâˆ’1)Â·Î´+1 taken from the top layer of the C3D network and captures visual information over Î´ frames. In practice, we perform PCA to reduce the dimensionality further to improve computational performance in a similar fashion as <ref type="bibr" target="#b8">[9]</ref>.</p><p>Sequence Encoding. The goal of the Sequence Encoder module is to accumulate evidence across time as the video sequence progresses. The idea is that in order to be able to produce good proposals, the model should be able to aggregate information until it is confident that an action is taking place in the video, while simultaneously disregarding irrelevant background. At each time step t = 1, . . . , T , the module receives the corresponding encoded C3D feature vector v t as input to the recurrent sequence model.</p><p>A key property for our model is the ability to process the input video in a single pass. In order to achieve this, the recurrent model should be able to operate over the input testing video by unrolling in time over the entire duration of the video. Our training procedure in Section 3.2 is designed to facilitate the operation over long sequences at test time.</p><p>Though similar work often leverages Long Short-Term Memory (LSTM) cells for sequence encoding, we find that a Gated Recurrent Unit (GRU)-based architecture offers slightly better performance, is more robust over a wider range of hyperparameters, and has fewer parameters which means slightly faster training and test-time performance. This is consistent with empirical findings from prior work on deep recurrent models in other domains <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Thus, at each time step t, we take the hidden state h t of the final GRU layer in the recurrent stage as our sequence encoding, where h t is defined as per the formulation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>:</p><formula xml:id="formula_0">r t = Ïƒ r (W r v t + U r h tâˆ’1 + b r ) z t = Ïƒ z (W z v t + U z h tâˆ’1 + b z ) h t = tanh(W v t + r t âŠ™ (U h tâˆ’1 ) + b) h t = (1 âˆ’ z t ) âŠ™ h tâˆ’1 + z t âŠ™ h t (1)</formula><p>where âŠ™ is the Hadamard product. Additional discussion of this formulation is in our Supplementary Material.</p><p>Output. The goal of the output module is to produce confidence scores of multiple proposals at each time step t. This module takes as input the hidden representation h t calculated by the Sequence Encoding layer at time t. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, we design our architecture to score a large number of potential proposals at each time step by considering proposals of multiple time scales that end at time t. Concretely, at each time step t we output confidence scores {c j t } k j=1 that correspond to set of k proposals</p><formula xml:id="formula_1">P t = {(b tâˆ’j , b t )} k j=1</formula><p>, where the tuple (b tâˆ’1 , b t ) indicates a proposal with start and end bounds at frame b tâˆ’1 and b t , respectively. The output confidence scores are given by a fully connected layer with sigmoid nonlinearity:</p><formula xml:id="formula_2">c j t = Ïƒ o (W j o Â· h t ).<label>(2)</label></formula><p>All proposals considered at time t have a fixed ending boundary, and the model considers proposals of sizes 1, 2, . . . , k time steps, which correspond to sizes Î´, 2Î´, . . . , kÎ´ frames. Note that this is done in a single forward pass at each time step, without needing to re-run the model for each temporal scale. In this manner, our model  <ref type="figure">Figure 3</ref>. Training examples are generated densely by extracting temporal segments of length Tw in a sliding window fashion with stride s. Our dense sampling of long training instances helps enable the recurrent sequence encoder to fully unroll over very long video sequences at testing time.</p><p>considers multiple time scales with a single pass through the input video sequence. Since we consider time scales densely at each frame, the model effectively computes proposal confidences for all proposals with a time resolution of Î´ frames over the video. In a way, this is effectively pushing the sliding window to the output layer in a computationally efficient manner that does not require extra computation on overlapping temporal windows. We apply standard post-processing techniques consistent with prior literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref> to select the top proposals, such as thresholding by confidence score and non-maximum suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>The goal of the training process is to estimate the model parameters in our architecture. By design, our proposal architecture and loss function are fully differentiable, enabling training with backpropagation. We also design the training procedure so that our recurrent networks can be fully unrolled over very long input sequences at test time. This is a key property that, in conjunction with the design of our output layer, enables the model to operate without using overlapping sliding windows over the input at test time.</p><p>This constraint means that the recurrent network must be able to properly handle inference over very long input sequences. This can prove challenging since the model must disregard irrelevant background in input untrimmed video while retaining relevant context. We observe that the hidden state of related recurrent models <ref type="bibr" target="#b8">[9]</ref> tends to saturate when run over many steps, resulting in overconfident outputs.</p><p>Our strategy to improve robustness is motivated by better simulation of testing operating conditions during training. Briefly, we wish to provide the network with densely sampled, overlapping training video segments that are significantly longer than the temporal proposals we aim to detect.</p><p>We generate these training segments as follows: For each training video with L frames and length T = L/Î´ time steps, we extract segments by running sliding windows of length T w = L w /Î´ with stride s, as illustrated in <ref type="figure">Figure  3</ref>. We set L w â‰« kÎ´, so that the training instances simulate the operation of long sequences, encouraging the network to avoid saturation of the hidden state. This is in contrast to explicitly constructing training examples of length no longer than the maximum proposal length kÎ´, which is the strategy used in prior work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9]</ref>. Furthermore, our stride s is kept small, allowing for dense generation of training data.</p><p>The dense sampling of training segments also allows for each time step in the original video sequence to be considered multiple times with different contexts. For instance, consider time step t = i in <ref type="figure">Figure 3</ref>. The visual content and groundtruth observed at time t is part of several training segments X 0 , . . . , X 3 . This means that during the training process, we will be able to backpropagate the training loss at t = i within the context of four examples X 0 , . . . , X 3 . When considering X 0 , the hidden state of the sequence encoder at t = i would be h 0 t0:i ; likewise the hidden state at t = i for X 1 would be h 1 t0+s:i . In both cases, the training process would backpropagate the loss at t = i with different contexts given by the hidden state in each example, encouraging the prediction and encoding to be robust to the specific initializations of the hidden state.</p><p>Each training example is associated with groundtruth labels that indicate which temporal intervals correspond to actions in the video. The idea is that our network will classify each temporal interval in consideration as a positive or negative action proposal. For example, consider X 0 in <ref type="figure">Figure 3</ref>, which we associate with groundtruth labels Y 0 = {y t } t0+Twâˆ’1 t=t0</p><p>. At time step t, the groundtruth y t is a k dimensional vector with binary entries. The j-th entry y j t is set to 1 if the corresponding proposal interval (of scale j time steps) has a temporal Intersection-over-Union (tIoU) with the groundtruth larger than 0.5 and set to 0 otherwise.</p><p>During training, we penalize the network for errors according to a multi-label loss function. In practice, for a training video X the loss at time t is given by a weighted binary cross entropy objective:</p><formula xml:id="formula_3">L(c, t, X, y) = âˆ’ k j=1 w j 0 y j t log c j t + w j 1 (1 âˆ’ y j t ) log(1 âˆ’ c j t ),<label>(3)</label></formula><p>where the weights w j 0 , w j 1 are calculated according to the frequency of positive and negative proposals in the training set at each scale j and c is the output of the network. We add dropout and â„“ 2 regularization on the learned parameters.</p><p>Our model backpropagates at every time step t, so the total loss for all training examples X is:</p><formula xml:id="formula_4">L train = (X,y)âˆˆX Tw t=1 L(c, t, X, y).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We empirically evaluate the effectiveness of our temporal proposal method for the task of proposal generation as well as its application to temporal action detection. As our experiments will show, our proposal method achieves competitive performance at faster computing speeds. We describe our experimental settings and results here.</p><p>Dataset. To train and evaluate our model, we use the temporal action localization subset from the THUMOS'14 dataset <ref type="bibr" target="#b16">[17]</ref>, which contains 20+ hours of video with 200 validation and 213 test untrimmed video sequences. We use the validation videos as our training set, as is standard practice on this dataset. To enable direct comparisons with prior work, we adopt the experimental settings from <ref type="bibr" target="#b8">[9]</ref>.</p><p>We perform an 80%-20% split over the training examples in the dataset to cross-validate the hyperparameters for our model, ensuring that the distribution of activity classes is approximately the same. For our generalizability analysis, we leverage subsets of unseen classes in the ActivityNet dataset <ref type="bibr" target="#b2">[3]</ref>. Further details in Section 4.1.</p><p>Comparisons. We compare our SST model with Deep Action Proposals (DAPs) <ref type="bibr" target="#b8">[9]</ref>, the proposal stage of S-CNN (SCNN-prop) <ref type="bibr" target="#b28">[29]</ref>, BoFrag <ref type="bibr" target="#b24">[25]</ref>, and Sparse-Prop <ref type="bibr" target="#b1">[2]</ref>.</p><p>Implementation details. We generate training data with L w = 2048. We vary the number of recurrent layers and hidden state size, as well as number of proposals k. We implement the model and training/validation pipeline using Lasagne/Theano and Caffe, with training executed on GeForce TITAN X (Maxwell) GPUs. We optimize our model parameters with backpropagation using the adam update rule <ref type="bibr" target="#b22">[23]</ref>, with an initial learning rate of 5 Â· 10 âˆ’2 annealed after every l = 5 epochs. We include further analysis of hyperparameters, trained models, code, and sample output proposals as part of our Supplementary Material 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Temporal Proposal Generation</head><p>The task of temporal proposal generation consists of taking an input video and producing a set of temporal intervals that are likely to contain human actions. A successful proposal method should be able to retrieve action intervals with very high recall and high temporal overlap (tIoU) with the true temporal segments that correspond to actions, while only producing a small number of proposals. Additionally, it is key for the model to have fast runtime. We evaluate these three aspects of our method below.</p><p>First, we consider the ability of our model to retrieve proposals with high recall. We measure this with the proposal average recall. This is computed by extracting a fixed number of proposals and calculating the average recall over a range of tIoUs. We plot average recall against number of retrieved proposals in <ref type="figure" target="#fig_1">Figure 4</ref>(center) for tIoUs in the range 0.7 -0.95, and <ref type="figure" target="#fig_1">Figure 4</ref>(left) for tIoUs in the range 0.5 -1.0 (for consistency with <ref type="bibr" target="#b8">[9]</ref>). We observe that our model outperforms all existing state-of-the-art methods for low and  <ref type="table">Table 1</ref>. Comparison of proposal generation performance with prior state-of-the-art in terms of recall at 1000 proposals. We observe our method offers comparable performance for lower tIoU thresholds, outperforms for higher thresholds, and offers a significant boost in proposal speed. Note that we used the older GPU set-up of <ref type="bibr" target="#b8">[9]</ref> to ensure fair comparison. We discuss benchmarks on newer GPU architectures in Supplementary Material.</p><p>high number of proposals. Also, we note that when operating at the high overlap regime in <ref type="figure" target="#fig_1">Figure 4</ref>(center), our model more significantly outperforms prior work. Second, we consider the ability of the model to retrieve proposals with high tIoU overlap. <ref type="figure" target="#fig_1">Figure 4</ref>(right) plots proposal recall for our method in comparison to prior work. We note that our model performs comparably with competing approaches at the lower tIoU range, but more importantly, our method performs better at the higher tIoU regime. This is key, since it means our method can retrieve proposals that more tightly capture the true temporal action intervals.</p><p>Finally, we study the runtime speed of our method in comparison to alternative approaches in the literature. To achieve this, we measure runtime speed in frames per second (FPS). In comparison to prior work that relies on multiscale temporal sliding windows <ref type="bibr" target="#b28">[29]</ref> or single scale temporal sliding windows <ref type="bibr" target="#b8">[9]</ref>, our single-pass model achieves significantly faster processing speeds, as shown in <ref type="table">Table 1</ref>.</p><p>Robustness to Video Length. An important goal for our architecture is the ability to handle very long testing video sequences. As outlined above, the idea is that our recurrent model should be able to unroll over the entire duration of the testing video, regardless of its duration, so that the proposals are generated in a single pass through the video. We achieve this by two aspects of our model: dense predictions at each frame, and a training scheme that encourages robustness into the model with respect to video length.</p><p>We analyze the performance of our model to highlight its robustness from three perspectives. For this analysis, we select the operating point of 1000 retrieved proposals. First, we study the recall stability with respect to the temporal location of the proposal middle frame, which we plot in <ref type="figure">Figure 5(left)</ref>. Note that our model processes the entire video by unrolling a single recurrent network, so the longer the video, the more time steps the recurrent network processes. We observe that SST recall performance is stable and nearly independent of the temporal location of the proposal.</p><p>Second, we study the recall stability with respect to video length. Here, we compute recall per video and compute average recall for videos with similar length. We also observe stable behavior with respect to video length as shown in <ref type="figure">Figure 5(center)</ref>.</p><p>Finally, we analyze recall performance with respect to proposal length. We would like to analyze if longer action sequences are harder to detect than shorter actions. We plot recall against proposal length in <ref type="figure">Figure 5</ref>(right). Again, we observe that recall performance is also stable with respect to length of the groundtruth annotations we wish to localize.</p><p>We also note that some videos in the THUMOS testing set are particularly long, with durations of over 20 minutes. This corresponds to video sequences of more than 30-50 thousand frames. Our qualitative evaluation confirms our empirical observation that our network can unroll over very long testing videos without compromising its performance.</p><p>Qualitative Results. We generate sample proposals Proposal length <ref type="figure">Figure 5</ref>. We evaluate the recall stability and robustness of our model against (left) groundtruth proposal temporal position, (center) video length, and (right) groundtruth proposal length. We observe that SST is indeed able to handle long, untrimmed video sequences without compromising performance, while gaining a dramatic boost in efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ActivityNet</head><p>ActvityNet â‰¤ 1024 Method @500 @600 @500 @600 DAPs <ref type="bibr" target="#b8">[9]</ref> 0.236 0.257 0.396 0.433 SST 0.242 0.288 0.423 0.494 <ref type="table">Table 2</ref>. Summary of generalization analysis in ActivityNet in terms of average recall for @k average proposals. We observe that proposals generated by SST are comparable or outperform prior proposal methods for unseen activities. For example, our proposals offer a relative improvement of +3.1% for the general unseen dataset (group 1, col. 2), and +6.1% over DAPs segments for unseen activities that span up to 1024 frames (group 2, col. 2).</p><p>from our model in <ref type="figure">Figure 6</ref>. We see that the model localizes the groundtruth annotations well -our top confidence proposals satisfy tIoU criteria well, and our highest overlap proposals have high confidences. We observe that among false positive detections, a common case occurs where our model outputs a high-confidence "umbrella proposal" over short action sequences tightly packed together with brief periods of non-action between them (although the model does output other high-confidence proposals correctly localizing those proposals, just with slightly lower confidence). Generalizability of Proposals. Another key characteristic of action proposal approaches is their capability to generate segments for unseen action categories <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1]</ref>. Following the observations found in the analysis of object proposals <ref type="bibr" target="#b14">[15]</ref>, Escorcia et al. proposed to evaluate the average recall of temporal action proposal methods on a diverse set of action classes unseen during training <ref type="bibr" target="#b8">[9]</ref>. Thus, we can assess the generalizability of these proposal approaches. <ref type="table">Table 2</ref> summarizes the performance of our approach in this scenario. For fair comparisons, we used the same experimental protocol used in <ref type="bibr" target="#b8">[9]</ref>. We report the results in two subsets of the validation set of ActivityNet v1.2 <ref type="bibr" target="#b2">[3]</ref>. These subsets correspond to: (i) "ActivityNet" results over the validation set and (ii) "ActivityNet â‰¤ 1024" results for videos with actions from categories not present in <ref type="bibr" target="#b16">[17]</ref> on annotations that span up to 1024 frames. We observe our method exhibits a comparable or better degree of generalization than <ref type="bibr" target="#b8">[9]</ref>. Additional results and example video are Action detection (mAP) Method @50 @100 @200 @500 @1000 Sparse-prop <ref type="bibr" target="#b1">[2]</ref> 5.7 6.3 7.6 8.2 8.0 SCNN-prop <ref type="bibr" target="#b28">[29]</ref> 5.6 7.7 10.5 13.57 13.45 DAPs <ref type="bibr" target="#b8">[9]</ref> 8. Method Action detection (mAP) S-CNN (full system) <ref type="bibr" target="#b28">[29]</ref> 0.19 SST + (S-CNN classifier) 0.23 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Action Detection with SST proposals</head><p>Finally, we apply our proposal generation architecture to the task of temporal action localization. The goal is not only to localize the temporal intervals where the actions happen, but also to label the interval with the correct action category.</p><p>For direct comparison to prior state-of-the-art proposal methods, we implement the approach of Xu et al. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9]</ref>. Briefly, for each temporal segment, we encode the corresponding C3D features from the fc7 layer using VLAD. We then train a one-vs-all linear SVM classifier for each class C. We evaluate the mean AP (mAP) with 0.5 tIoU overlap threshold on the THUMOS'14 test set.</p><p>We summarize these standardized action detection results in <ref type="table">Table 3</ref>. We observe that SST consistently outperforms other proposal methods for lower number of proposals, and our mAP@200 proposals (13.94%) matches or out-  <ref type="figure">Figure 6</ref>. Qualitative results of our SST model on THUMOS'14. Time measured in seconds. (a) Proposals generated by SST on a long, untrimmed video sequence. Each groundtruth annotation is coupled with the best proposal on top. We observe that performance of the network is maintained for the full duration. (b-d) Performance of the top-ranked temporal action proposal retrieved for a given input video sequence, coupled with the nearest groundtruth annotation. SST provides tight localization bounds with high tIoU. (e-f) False-positive results for the top-ranked retrievals. In particular, (e) illustrates an issue where the model will sometimes rank "umbrella proposals" which encompass several short, consecutive action sequences with a slightly higher confidence than the individually-localized proposals.</p><p>performs all other methods, regardless of proposal number.</p><p>Finally, we demonstrate that by applying the previous state-of-the-art classification stage from <ref type="bibr" target="#b28">[29]</ref> to SST proposals, we significantly improve temporal action localization. As shown in <ref type="table" target="#tab_2">Table 4</ref>, the full S-CNN action detection architecture had the prior state-of-the-art detection performance of 0.19 mAP <ref type="bibr" target="#b28">[29]</ref>, while application of the same classifier stage to SST proposals results in 0.23 mAP. Thus, SST provides a strong base for temporal action localization. Moreover, we demonstrate the importance of improving proposal methods for overall temporal action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have introduced a new architecture, SST, for temporal action proposals that can operate on long video sequences at high processing speeds. Our approach processes input video data as a single stream, and constructs no overlapping sliding windows at evaluation time. We demonstrate that our model achieves state-of-the-art performance on the action proposals task, and, when considered as part of a full detection architecture, provides effective action localization performance with fewer proposals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Schematic illustrating our overall approach and model architecture. Here, we extract C3D features from the input video stream, with a time resolution Î´ = 16 frames for each "time step." These features are the input to the recurrent GRU-based sequence encoder model, which outputs k proposals at each time step t with a confidence vector ct, where the longest proposal is of length Î´ Â· k. Additionally, we can validate the usefulness of the top-ranked SST action proposals to the action detection task by applying a classifier model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of our proposal network with state-of-the-art proposal localization methods. SST offers strong performance against prior literature, even though it constructs no overlapping sliding windows at test time and passes through the input in a single stream. (left) SST has higher average recall and requires fewer proposals. (center) This difference is particularly visible when average recall is computed over a higher tIOU range (0.7-0.95). For clarity, we show results here for the top three proposal methods. (right) Recall @ 1K proposals vs. tIoU plot shows that SST has the largest improvements around tIoU â‰ˆ 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 3. Summary of action detection results with VLAD-based SVM classifier from [9] against number of proposals. Best value for each method shown in bold. SST outperforms prior work with fewer proposals needed, indicating that SST outputs high-quality proposals within a limited budget.</figDesc><table>4 12.1 13.9 12.5 
12.0 
SST 
10.9 13.2 13.94 13.1 
13.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Applying S-CNN classifier [29] on top of SST propos- als, we exceed the prior state-of-the-art action detection results on THUMOS'14, without any fine-tuning of the classifier. Thus, our SST approach provides a more efficient and effective way to extract precise video segments where actions are detected more accurately even with existing classifiers. available in the Supplementary Material.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please see https://github.com/shyamal-b/sst/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This research was sponsored, in part, by the Stanford AI Lab-Toyota Center for Artificial Intelligence Research, Toyota Research Institute (TRI), and by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research. This article reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. We thank our anonymous reviewers, De-An Huang, Oliver Groth, Fabian Caba, Joseph Lim, Jingwei Ji, and FeiFei Li for helpful comments and discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ActivityNet: a large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van MerriÃ«nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DAPs: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Activity representation with motion hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="238" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised sequence labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04988</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals? PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1506.02078</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bag-of-fragments: Selecting and encoding video fragments for event detection and recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cappallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Leaving some stones unturned: Dynamic feature prioritization for activity detection in streaming video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00427</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Apt: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THU-MOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A hough transform-based voting framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1507.05738</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adsc submission at thumos challenge 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
