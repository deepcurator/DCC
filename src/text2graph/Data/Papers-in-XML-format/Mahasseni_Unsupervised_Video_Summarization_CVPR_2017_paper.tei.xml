<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Video Summarization with Adversarial LSTM Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
							<email>behrooz.mahasseni@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
							<email>lamm@oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
							<email>sinisa@oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Video Summarization with Adversarial LSTM Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A wide range of applications require automated summarization of videos <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b40">42]</ref>, e.g., for saving time of human inspection, or enabling subsequent video analysis. Depending on the application, there are various distinct definitions of video summarization <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b11">12]</ref>. In this paper, we consider unsupervised video summarization, and cast it as a key frame selection problem. Given a sequence of video frames, our goal is to select a sparse subset of frames such that a representation error between the video and its summary is minimal.</p><p>Our problem statement differs from other formulations considered in the literature, for example, when a particular <ref type="bibr">(a)</ref> (b) <ref type="figure">Figure 1</ref>: (a) Overview: Our goal is to select key frames such that a distance between feature representations of the selected key frames and the video is minimized. (b) As specifying a suitable distance between deep features is difficult, we use a generative adversarial framework for optimizing the frame selector. Our approach consists of a variational auto-encoder and a generative adversarial network.</p><p>domain of videos to be summarized is a priori known (e.g., first-person videos) <ref type="bibr" target="#b17">[18]</ref>, or when ground-truth annotations of key frames are provided in training data based on attention, aesthetics, quality, landmark presence, and certain object occurrences and motions <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure">Fig. 1a</ref> shows an overview of our approach to selecting key frames from a given video. The key frame selector is learned so as to minimize a distance between features extracted from the video and the selected key frames. Following recent advances in deep learning <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b41">43]</ref>, we extract deep features from both the video and selected sequence of key frames using a cascade of a Convolutional Neural Network (CNN) -specifically GoogleNet <ref type="bibr" target="#b36">[38]</ref> -and Long Short-Term Memory Network (LSTM) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">35]</ref>. The CNN is grounded onto pixels and extracts deep features from a given frame. The LSTM then fuses a sequence of the CNN's outputs for capturing long-range dependencies among the frames, and produces its own deep feature representing the input sequence. Specifically, we use the (variational) autoencoder LSTM <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b15">16]</ref> as a suitable deep architecture for unsupervised learning of video features. Given a distance between the deep representations of the video and selected key frames, our goal is to optimize the frame selector such that this distance is minimized over training examples.</p><p>Recent work, however, demonstrates that specifying a suitable distance of deep features is difficult <ref type="bibr" target="#b18">[19]</ref>. Hence, we resort to the generative adversarial framework <ref type="bibr" target="#b7">[8]</ref>, which extends the aforementioned video summarization network with an additional discriminator network. As shown in <ref type="figure">Fig. 1b</ref>, the decoder part of the summarizer is used to reconstruct a video from the sequence of selected key frames. Then, we use a discriminator, which is another LSTM, to distinguish between the original video and its reconstruction from the summarizer. The auto-encoder LSTM and the frame selector are jointly trained so as to maximally confuse the discriminator LSTM -i.e., they are cast in a role of the discriminator's adversary -such that the discriminator has a high error rate in recognizing between the original and reconstructed videos. When this recognition error becomes maximum, we deem that the frame selector is learned to produce optimal video summarizations.</p><p>As we will show in this paper, our approach allows for an effective regularization of generative-adversarial learning in terms of: (i) limiting the total number of key frames that can be selected; or (ii) maximizing visual diversity among the selected key frames. For a fair comparison with related approaches to fully supervised video summarization -a different setting from ours that provides access to ground-truth key frame annotations in training -we also show how to effectively incorporate the available supervision as an additional type of regularization in learning.</p><p>Evaluation on four benchmark datasets, consisting of videos showing diverse events in first-and third-person views, demonstrates our competitive performance in comparison to fully supervised state-of-the-art approaches.</p><p>Our contributions include:</p><p>1. A new approach to unsupervised video summarization that combines variational auto-encoders and generative-adversarial training of deep architectures.</p><p>2. First specification of generative-adversarial training on high resolution video sequences.</p><p>In the following, Sec. 2 reviews prior work, Sec. 3 briefly introduces the generative adversarial network (GAN) and the variational autoencoder (VAE) models, Sec. 4 specifies main components of our approach, Sec. 5 formulates our end-to-end training, Sec. 6 describes variants of our approach differing in types of regularization we use in learning, and finally Sec. 7 presents our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews related: (i) problem formulations of video summarization; (ii) approaches to supervised and unsupervised video summarization; (iii) deep learning approaches; and (iv) work using the generative adversarial framework in learning.</p><p>Various Problem Formulations. Video summarization is a long-standing problem, with various formulations considered in the literature. For example, the video synopsis <ref type="bibr" target="#b26">[28]</ref> tracks moving objects, and then packs the identified video tubes into a smaller space-time volume. Also, montages <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b35">37]</ref> merge and overlaps key frames into a single summary image. Both of these problem formulations, however, do not require that the video summary preserves the information about a temporal layout of motions in the video. Previous work has also studied hyperlapses where the camera viewpoint is being changed during the timelapse for speeding-up or slowing-down certain parts of the input video <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b11">12]</ref>. Our problem statement is closest to storyboards, representing a subset of representative video frames <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. However, except for <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b39">41]</ref>, existing approaches to generating storyboards do not take advantage of deep learning.</p><p>Supervised vs. Unsupervised Summarization. Supervised methods assume access to human annotations of key frames in training videos, and seek to optimize their frame selectors so as to minimize loss with respect to this ground truth <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b40">42]</ref>. However, for a wide range of domains, it may be impossible to provide reliable and a sufficiently large amount of human annotations (e.g., military, nursing homes). These domains have been addressed with unsupervised methods, which typically use heuristic criteria for ranking and selecting key frames <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b32">34]</ref>. There have been attempts to use transfer learning for domains without supervision <ref type="bibr" target="#b41">[43]</ref>, but the surprisingly better performance of the transfer learning setting compared to the canonical setting, reported in <ref type="bibr" target="#b41">[43]</ref>, suggests a high correlation of the domains for three training dataset and one test dataset , which is hard to ensure in real-world settings.</p><p>Deep Architectures for Video Summarization. In <ref type="bibr" target="#b41">[43]</ref>, two LSTMs are used -one along the time sequence and the other in reverse from the video's end -to select key video frames, and trained by minimizing the cross-entropy loss on annotated ground-truth key frames with an additional objective based on determinantal point process (DPP) to ensure diversity of the selected frames. Our main differences are that we do not consider the key frame annotations, and train our LSTMs using the unsupervised generativeadversarial learning. In <ref type="bibr" target="#b39">[41]</ref>, recurrent auto-encoders are learned to represent annotated temporal intervals in training videos, called highlights. In contrast, we do not require human annotations of highlights in training, and we do not perform temporal video segmentation (highlight vs non-highlight), but key frame selection.</p><p>Generative Adversarial Networks (GANs) have been used for image-understanding problems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b29">31]</ref>, and frame prediction/generation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b4">5]</ref>. But we are not aware of their previous use for video summarization. In <ref type="bibr" target="#b18">[19]</ref>, the discriminator output of a GAN is used to provide a learning signal for the variational auto-encoder (VAE). We extend this approach in three critical ways: (1) We specify a new variational auto-encoder LSTM, whereas their autoencoder is not a recurrent neural network, and thus cannot be used for videos; (2) Our generative-adversarial learning additionally takes into account the frame selector -a component not considered in <ref type="bibr" target="#b18">[19]</ref>; and (3) We formulate regularization of generative-adversarial learning that is suitable for video summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Review of VAE and GAN</head><p>Variational Autoencoder (VAE) <ref type="bibr" target="#b15">[16]</ref> is a directed graphical model which defines a posterior distribution over the observed data, given an unobserved latent variable. Let e ∼ p e (e) be a prior over the unobserved latent variable, and x be the observed data. One can interpret e as the encoding of x and define q(e|x) as the probability of observing e given x. It is typical to set p e (e) as the standard normal distribution. Similarly, p(x|e) identifies the conditional generative distribution for x. Learning is done by minimizing the negative log-likelihood of the data distribution:</p><formula xml:id="formula_0">− log p(x|e)p(e) q(e|x) = − log(p(x|e)) Lreconst + D KL (q(e|x) p(e) Lprior .</formula><p>(1) For efficient learning, Kingma et al. <ref type="bibr" target="#b15">[16]</ref> propose a reparameterization of the variational lower bound suitable for stochastic gradient descent.</p><p>Generative Adversarial Network (GAN) <ref type="bibr" target="#b7">[8]</ref> is a neural network that consists of two competing subnetworks: i) a 'generator' network (G) which generates data mimicking an unknown distribution and ii) a 'discriminator' network (D) which discriminates between the generated samples and the ones from true observations. The goal is to find a generator which fits the true data distribution while maximizing the probability of the discriminator making a mistake.</p><p>Let x be the true data sample, e ∼ p e (e) be the prior input noise, andx = G(e) be the generated sample. Learning is formulated as the following minimax optimization:</p><formula xml:id="formula_1">min G max D E x [log D(x)] + E e [log(1 − D(x))] LGAN ,<label>(2)</label></formula><p>where D is trained to maximize the probability of correct <ref type="figure">Figure 2</ref>: Main components of our approach: The selector LSTM (sLSTM) selects a subset of frames from the input sequence x. The encoder LSTM (eLSTM) encodes the selected frames to a fixed-length feature e, which is then forwarded to the decoder LSTM (dLSTM) for reconstructing a videox. The discriminator LSTM (cLSTM) classifiesx as 'original' or 'summary' class. dLSTM and cLSTM form the generative adversarial network (GAN).</p><p>sample classification (true vs generated) and G is simultaneously trained to minimize log(1 − D(x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main Components of Our Approach</head><p>Our approach consists of the summarizer and discriminator recurrent networks, as illustrated in <ref type="figure">Figure 2</ref>. Given CNN's deep features for every frame of the input video, x = {x t : t = 1, . . . , M }, the summarizer uses a selector LSTM (sLSTM) to select a subset of these frames, and then an encoder LSTM (eLSTM) to encode the sequence of selected frames to a deep feature, e. Specifically, for every frame x t , sLSTM outputs normalized importance scores s = {s t : s t ∈ [0, 1], t = 1, . . . , M } for selecting the frame. The input sequence of frame features x is weighted with these importance scores, and then forwarded to eLSTM. Note that in the special case of discretized scores, s t ∈ {0, 1}, eLSTM receives only a subset of frames for which s t = 1. The last component of the summarizer is a decoder LSTM (dLSTM), which takes e as input, and reconstructs a sequence of features corresponding to the input video,x = {x 1 ,x 2 , ...,x M }.</p><p>The discriminator is aimed at distinguishing between x andx as belonging to two distinct classes: 'original' and 'summary'. This classifier can be viewed as estimating a distance between x andx, and assigning distinct class la- <ref type="figure">Figure 3</ref>: The four loss functions used in our training. L GAN is the augmented GAN loss and L reconst is the reconstruction loss for the recurrent encoder-decoder. In training, we use an additional frame selector s p , governed by a prior distribution (e.g., uniform), which produces the encoded representation e p , and the reconstructed feature sequencex p . The adversarial training of cLSTM is regularized such that it is highly accurate on recognizingx p as 'summary', but that it confusesx as 'original'. bels to x andx if their distance is sufficiently large. In this sense, the discriminator serves to estimate a representation error between the original video and our video summarization. While one way to implement the discriminator could be an energy-based encoder-decoder <ref type="bibr" target="#b43">[45]</ref>, in our experiments a binary sequence classifier have shown better performance. Hence, we specify the discriminator as a classifier LSTM (cLSTM) with a binary-classification output.</p><p>Analogous to the generative adversarial networks presented in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>, we have that dLSTM and cLSTM form the generative adversarial network (GAN). The summarizer and discriminator networks are trained adversarially until the discriminator is not able to discriminate between the reconstructed videos from summaries and the original videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training of sLSTM, eLSTM, and dLSTM</head><p>This section specifies our learning of: (i) Summarizer parameters, {θ s , θ e , θ d }, characterizing sLSTM, eLSTM, and dLSTM; and (ii) GAN parameters, {θ d , θ c }, defining dLSTM and cLSTM. Note that θ d are shared parameters between the summarizer and GAN.</p><p>As illustrated in <ref type="figure">Fig. 3</ref>, our training is defined by four loss functions: 1) Loss of GAN, L GAN , 2) Reconstruction loss for the recurrent encoder-decoder, L reconst , 3) Prior loss, L prior , and 4) Regularization loss, L sparsity . The key idea behind our generative-adversarial training is to introduce an additional frame selector s p , governed by a prior distribution (e.g., uniform distribution), s p ∼ p(s p ). Sampling the input video frames with s p gives a subset which is passed to eLSTM, producing the encoded representation e p . Given e p , dLSTM reconstructs a video sequencex p . We usex p to regularize learning of the discriminator, such that cLSTM is highly accurate on recognizingx p as the 'summary' class, but that it confusesx as 'original' class. Recall that the L prior is imposed by the prior distribution over e as in <ref type="bibr" target="#b0">(1)</ref>.</p><p>Similar to the training of GAN models in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>, we formulate an adversarial learning algorithm that iteratively optimizes the following three objectives:</p><p>1. For learning {θ s , θ e }, minimize (L reconst +L prior +L sparsity ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For learning</head><formula xml:id="formula_2">θ d , minimize (L reconst +L GAN ). 3. For learning θ c , maximize L GAN .</formula><p>In the following, we define L reconst and L GAN , while the specification of L sparsity is deferred to Sec. 6. <ref type="bibr" target="#b18">[19]</ref>. Hence, instead, we define L reconst based on the hidden representation in cLSTM -specifically, the output of the last hidden layer of cLSTM, φ(x), for input x. Note that while x is a sequence of features, φ(x) represents a compact feature vector, capturing long-range dependencies in the input sequence. Therefore, it seems more appropriate to use φ(x), rather than x, for specifying L reconst .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction loss</head><note type="other">L reconst : The standard practice in learning encoder-decoder networks is to use the Euclidean distance between the input and decoded output, x −x 2 , for estimating the reconstruction error. However, recent findings demonstrate shortcomings of this practice</note><p>Specifically, we formulate L reconst as an expectation of a log-likelihood log p(φ(x)|e), given that x has been passed through the frame selector s and eLSTM, resulting in e:</p><formula xml:id="formula_3">L reconst = E[− log p(φ(x)|e)],<label>(3)</label></formula><p>where expectation E is approximated as the empirical mean of training examples. In this paper, we consider</p><formula xml:id="formula_4">p(φ(x)|e)) ∝ exp(− φ(x) − φ(x) 2 )</formula><p>, while other nonGaussian likelihoods are also possible.</p><p>Loss of GAN, L GAN : Following <ref type="bibr" target="#b18">[19]</ref>, our goal is to train the discriminator such that cLSTM classifies reconstructed feature sequencesx as 'summary' and original feature sequences x as 'original'. In order to regularize this training, we additionally enforce that cLSTM learns to classify randomly generated summariesx p as 'summary', wherex p is reconstructed from a subset of video frames randomly selected by sampling from a given prior distribution. In this paper, for this prior, we consider the uniform distribution. This gives:</p><formula xml:id="formula_5">L GAN = log(cLSTM(x)) + log(1 − cLSTM(x)) + log(1 − cLSTM(x p )),<label>(4)</label></formula><p>where cLSTM(·) denotes the binary soft-max output of cLSTM.</p><p>Given the above definitions of L reconst and L GAN , as well as L sparsity explained in Sec. 6, we update the parameters θ s , θ e , θ d and θ c using the Stochastic Gradient Variational Bayes estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>, adapted for recurrent networks <ref type="bibr" target="#b2">[3]</ref>. Algorithm 1 summarizes all steps of our training. Note that Algorithm 1 uses capital letters to denote a mini-batch of the corresponding variables with small-letter notation in the previous text. X ← mini-batch from CNN feature sequences <ref type="bibr">6:</ref> S ← sLSTM(X) % select frames <ref type="bibr">7:</ref> E = eLSTM(X, S) % encoding 8:X = dLSTM(E) % reconstruction <ref type="bibr">9:</ref> S p ← draw samples form the uniform distribution <ref type="bibr">10:</ref> E p = eLSTM(X, S p ) % encoding 11:</p><formula xml:id="formula_6">X p = dLSTM(E Sp ) % reconstruction 12:</formula><p>% Updates using Stochastic Gradient:</p><formula xml:id="formula_7">13: {θ s , θ e } + ← −▽(L reconst + L prior + L sparsity )</formula><p>14: </p><formula xml:id="formula_8">{θ d } + ← −▽(L reconst + L GAN ) 15: {θ c } + ← +▽(L GAN ) % maximization</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Variants of our Approach</head><p>This section explains our regularization of learning. We use the following three types of regularization, which define the corresponding variants of our approach.</p><p>Summary-Length Regularization penalizes having a large number of key frames selected in the summary as:</p><formula xml:id="formula_9">L sparsity = 1 M M t=1 s t − σ 2<label>(5)</label></formula><p>where M is the total number of video frames, and σ is an input hyper-parameter representing a percentage of frames that we expect to be selected in the summary. When our approach uses L sparsity , we call it SUM-GAN. Diversity Regularization enforces selection of frames with high visual diversity, in order to mitigate redundancy in the summary. In this paper, we use two standard definitions for diversity regularization -namely, (i) Determinantal Point Process (DPP) <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">43]</ref>; and (ii) Repelling regularizer (REP) <ref type="bibr" target="#b43">[45]</ref>.</p><p>Following <ref type="bibr" target="#b41">[43]</ref>, our DPP based regularization is defined as:</p><formula xml:id="formula_10">L dpp sparsity = − log(P (s))<label>(6)</label></formula><p>where P (s) is a probability that DPP assigns to the selection indicator s.</p><formula xml:id="formula_11">We compute P (s; L) = det(L(s)) det(L+I)</formula><p>, where L is an M × M similarity matrix between every two hidden states in eLSTM, I is an identity matrix and L(s) is a smaller square matrix, cut down from L given s. Let e t be the hidden state of eLSTM at time t. For time steps t and t ′ the pairwise similarity values are defined as</p><formula xml:id="formula_12">L t,t ′ = s t s t ′ e t e t ′ .</formula><p>When our approach uses L dpp sparsity , we call it SUM-GAN dpp .</p><p>For repelling regularization, we define</p><formula xml:id="formula_13">L rep sparsity = 1 M (M − 1) t t ′ =t e ⊤ t e t ′ e t e t ′ (7)</formula><p>and call this variant of our approach as SUM-GAN rep .</p><p>Keyframe Regularization is specified for the supervised setting where ground-truth annotations of key frames are provided in training. This regularization enables a fair comparison of our approach with recently proposed supervised methods. Note that we here consider importance scores as 2D softmax outputs {s t }, rather than scalar values as introduced in Sec. 4. We define the sparsity loss as the cross-entropy loss:</p><formula xml:id="formula_14">L sup sparsity = 1 M t cross-entropy(s t ,ŝ t ).<label>(8)</label></formula><p>We call this variant of our approach as SUM-GAN sup .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>Datasets. We evaluate our approach on four datasets: SumMe <ref type="bibr" target="#b9">[10]</ref>, TVSum <ref type="bibr" target="#b32">[34]</ref>, Open Video Project (OVP) <ref type="bibr">[24,</ref><ref type="bibr" target="#b1">2]</ref>, and Youtube <ref type="bibr" target="#b1">[2]</ref>. 1) SumMe consists of 25 user videos. The videos capture multiple events such as cooking and sports. The video contents are diverse and include both first-person and third-person camera. The video lengths vary from 1.5 to 6.5 minutes. The dataset provides framelevel importance scores. 2) TVSum contains 50 videos from YouTube. The videos are selected from 10 categories in the TRECVid Multimedia Event Detection (MED) (5 videos per category). The video lengths vary from 1 to 5 minutes. Similar to SumMe, the video contents are diverse and include both ego-centric and third-person camera. 3) For OVP, we evaluate on the same 50 videos used in <ref type="bibr" target="#b1">[2]</ref>. The videos are from various genres (e.g. documentary, educational) and their lengths vary from 1 to 4 minutes. 4) The YouTube dataset includes 50 videos collected from websites. The duration of the videos are from 1 to 10 minutes and the content include cartoons, news and sports.</p><p>Evaluation Setup. For a fair comparison with the state of the art, the keyshot-based metric proposed in <ref type="bibr" target="#b41">[43]</ref> is used for evaluation. Let A be the generated keyshots and B the user-annotated keyshots. The precision and recall are defined based on the amount of temporal overlap between A and B as follows: </p><p>Finally, the harmonic mean F-score is used as the evaluation metric. We follow the steps in <ref type="bibr" target="#b41">[43]</ref> to convert framelevel scores to key frames and key shot summaries, and vice versa in all datasets. To generate key shots for datasets which only provide key frame scores, the videos are initially temporally segmented into disjoint intervals using KTS <ref type="bibr" target="#b24">[26]</ref>. The resulting intervals are ranked based on their importance score where the importance score of an interval is equal to the average score of the frames in that interval. A subset of intervals are selected from the ranked intervals as keyshots such that the total duration of the generated key shots are less than 15% of the duration of the original video.</p><p>For datasets with multiple human annotations (in the form of key shots or key frames), we follow the standard approach described in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b41">43]</ref> to create a single groundtruth set for evaluation. While evaluating our SUM-GAN sup model, we used the same train, test and validation split as in <ref type="bibr" target="#b41">[43]</ref>. For fair comparison, we run it for five different random splits and report the average performance.</p><p>Implementation Details: For fair comparison with <ref type="bibr" target="#b41">[43]</ref>, we choose to use the output of pool5 layer of the GoogLeNet network <ref type="bibr" target="#b36">[38]</ref> (1024-dimensions), trained on ImageNet <ref type="bibr" target="#b30">[32]</ref>, for the feature descriptor of each video frame. We use a two-layer LSTM with 1024 hidden units at each layer for discriminator LSTM (cLSTM). We use two two-layer LSTMs with 2048 hidden units at each layer for eLSTM and dLSTM respectively. It is shown in <ref type="bibr" target="#b33">[35]</ref> that a decoder LSTM which attempts to reconstruct the reverse sequence is easier to train. Similarly, our dLSTM reconstruct the feature sequence in the reverse order. Note that while presenting x andx as the cLSTM input, both sequences should have similar ordering in time.</p><p>We initialize the parameters of eLSTM and dLSTM, with the parameters of a pre-trained recurrent autoencoder model trained on feature sequences from original videos. We find out that this helps to improve the overall accuracy and also results in faster convergence.</p><p>The sLSTM network is a two-layer bidirectional LSTM with 1024 hidden units. The output is a 2-dimensional softmax layer in the case of SUM-GAN sup . We train our framework with Adam optimizer using the default parameters.</p><p>Baselines: It is important to point out that considering the generative structure of our approach and the definition of the update rules in Alg. 1, it is not possible to entirely replace subnetworks of our model baselines. Instead, in addition to different variations of our approach defined in sec. 6, we also evaluate the following baselines:  <ref type="table">Table 1</ref> summarizes the accuracy of different variations of our approach. As is expected, the model with additional frame-level supervision, SUM-GAN sup , outperforms the unsupervised variants by (2-5%).</p><formula xml:id="formula_16">L GAN = log(cLSTM(x)) + log(1 − cLSTM(x)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Quantitative Results</head><p>One interesting observation is that although explicit regularization of the model with 'diversity regularizers' (SUM-GAN dpp and SUM-GAN rep ) performs slightly better than the variant of our model with 'length regularizer' (SUM-GAN) the difference is not statistically significant. Furthermore, in the case of SumMe, SUM-GAN performs better than SUM-GAN rep . This is particularly important because it verifies our main hypothesis that a good summary should include a subset of frames which provide similar content representation as of the original frame sequences. This suggests that if we constrain the summary to be shorter in length, implicitly the frames will be diverse. We also observe that SUM-GAN dpp performs better than SUM-GAN rep in all four datasets. We believe that this is mainly because of the fact that unlike the repelling regularizer, DPP is non-linear and can reinforce stronger regularization. Comparing the accuracy of SUM-GAN w/o-GAN with SUM-GAN shows that training with the combined losses from the VAE and GAN improves the accuracy.</p><p>We are particularly interested in comparing our performance in contrast with prior unsupervised and supervised methods. This comparison is presented in table 2. As shown, our unsupervised SUM-GAN dpp model outperforms all unsupervised approaches in all datasets. For SumMe, our approach is almost 5% better than the state-of-the-art unsupervised approaches. More importantly, the accuracy   Comparing with the state-of-the-art supervised approaches, our supervised variant, SUM-GAN sup , outperforms in all datasets except OVP. Even in the case of OVP, we are statistically close to the best reported accuracy with 0.4% margin. We hypothesize that the accuracy boost is mainly because of the additional learning signal from the cLSTM. Note that the discriminator observes a longer sequence and classifies based on a learned semantic representation of the feature sequence. This enables the discriminator to provide a more informative signal regarding the importance of the frames for content similarity.</p><p>Zhang et al. <ref type="bibr" target="#b41">[43]</ref> augment the SumMe and TVSum datasets with OVP and YouTube datasets and improve the accuracy on SumMe and TVSum. <ref type="table" target="#tab_5">Table 3</ref> shows the accuracy results in comparison with results reported in <ref type="bibr" target="#b41">[43]</ref> when training dataset is augmented. Except for SUM-GAN sup , which we use 80% of the target dataset in training, for the unsupervised variants of our approach we use all four datasets in training. The most important observation is that one of our unsupervised variations, SUM-GAN dpp , outperforms the state of the art in SumMe. This shows that if trained with more unsupervised video data, our model   is able to learn summaries which are competitive with the models trained using key frame annotations. Finally, we evaluate the performance of our approach for different percentages of σ values for our SUM-GAN model. <ref type="figure" target="#fig_1">Fig. 4</ref> shows the resulting F-score values for different σ's on four different datasets. While the performance is consistent for 0.3 ≤ σ ≤ 0.5, it drops rapidly as σ → 1 or σ → 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Comparison with Shallow Features</head><p>We verified the generalizability of our video summarization approach to non-deep features by evaluating our model The colored segments are the selected subset of frames using the specified method.</p><p>with the shallow features employed in <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43]</ref>. <ref type="table" target="#tab_7">Table 4</ref> shows the performance of our model compared to the stateof-the-art models which use shallow features. Besides the reported results in <ref type="bibr" target="#b40">[42]</ref> for TvSum, where the shallow features outperform the deep features, our model consistently performs better the state of the art. Unlike <ref type="bibr" target="#b40">[42]</ref>, our model grounded on deep features still performs better than the same model grounded on shallow features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Qualitative Results</head><p>To better illustrate the temporal selection pattern of different variations of our approach, we demonstrate the selected frames on an example video in <ref type="figure" target="#fig_2">Fig. 5</ref>. The blue background shows the frame-level importance scores. The colored regions are the selected subsets for different methods. The visualized key frames for different variants supports the result presented in <ref type="table">Table 1</ref>. Despite small variations, all four approaches cover the temporal regions with high frame-level score. Most of the failure cases occurred in videos which consist of frames with very slow motions and no scene-change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We propose a generative architecture based on variational recurrent auto-encoders and generative adversarial networks for unsupervised video summarization to select a subset of key frames. The main hypothesis is that the learned representation of the summary video and the original video should be similar. The summarizer aims to summarize the video such that the discriminator is fooled and the discriminator aims to recognize the summary videos from original videos. The entire model is trained in an adversarial manner where the GAN's discriminator is used to learn a discrete similarity measure for training the recurrent encoder/decoder and the frame selector LSTMs. Variations of our approach are defined using different regularizations. Evaluation on benchmark datasets show that all unsupervised variations of our approach outperform the state of the art in video summarization by 2-5% and provides a comparable accuracy to the state-of-the-art supervised approaches. We also verified that the supervised variation of our approach outperforms the state of the art by 1-4%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>overlap between A and B duration of A recall = duration of overlap between A and B duration of B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F-score results for different values of σ on SumMe, TvSum, OpenVideo, and YouTube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example summaries from a sample video in TvSum [34]. The blue bars show the annotation importance scores. The colored segments are the selected subset of frames using the specified method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Algorithm 1 Training SUM/GAN model 1: Input: Training video sequences 2: Output: Learned parameters {θ s , θ e , θ d , θ c }. 3: Initialize all parameters {θ s , θ e , θ d , θ c } 4: for max number of iterations do</figDesc><table>5: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>update 16: end for</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our proposed video summarization approach compared to state of the art. The reported results from the state of the art are from published results. Note that [42, 7] use only 39 sequences of non-cartoon videos.</figDesc><table>Method 
SumMe TVSum 
[42] 
40.9 
-
[43] 
42.9 
59.6 

SUM-GAN 

41.7 
58.9 

SUM-GANrep 

42.5 
59.3 

SUM-GANdpp 

43.4 
59.5 

SUM-GANsup 

43.6 
61.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different variations of our gen- erative video summarization with the state of the art for SumMe and TVSum datasets when the training data is aug- mented with videos from OVP and YouTube datasets. For [43], results w/o domain adaptation are reported of SUM-GAN dpp is comparably close to the supervised methods in TVSum, OVP and YouTube datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different variations of our gen- erative video summarization with the state of the art for SumMe and TVSum datasets when using shallow features.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by DARPA XAI and NSF RI1302700.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video Summaries through MosaicBased Shot and Scene Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="388" to="402" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method. Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E F</forename><surname>De Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P B</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Da Luz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Albuquerque Araújo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fabius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6581</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Variational recurrent auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stimo: Still and moving video storyboard for the web scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Furini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Geraci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montangero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pellegrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="69" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09444</idno>
		<title level="m">Contextual rnn-gans for abstract reasoning diagram generation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2069" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Creating summaries from user videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Creating summaries from user videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="505" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3090" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Egosampling: Wide view hyperlapse from single and multiple egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<idno>abs/1604.07741</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time hyperlapse creation via optimal frame selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<idno>63:1-63:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Largescale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2698" to="2705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stochastic gradient vb and the variational auto-encoder. Talk Slides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">First-person hyperlapse videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="78" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<title level="m">Autoencoding beyond pixels using a learned similarity metric</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-video summarization based on video-mmr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIAMIS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Keyframe-based video summarization using delaunay clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mundur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yesha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="232" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Egosampling: Fast-forward and stereo for egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Webcam synopsis: Peeking around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonchronological video synopsis and indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1971" to="1984" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making a long video short: Dynamic video synopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="435" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tvsum: Summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5179" to="5187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1502.04681</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Statistics</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/yt/press/statistics.html.Accessed" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Salient Montages from Unconstrained Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="472" to="488" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02612</idno>
		<title level="m">Generating videos with scene dynamics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Space-time video montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Quan Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of video highlights via robust recurrent auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4633" to="4641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Summary transfer: Exemplar-based subset selection for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quasi real-time summarization for consumer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2513" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
