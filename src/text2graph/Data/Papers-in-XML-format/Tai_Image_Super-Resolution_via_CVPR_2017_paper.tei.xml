<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-Resolution via Deep Recursive Residual Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Super-Resolution via Deep Recursive Residual Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single Image Super-Resolution (SISR) is a classic computer vision problem, which aims to recover a highresolution (HR) image from a low-resolution (LR) image. Since SISR restores the high-frequency information, it is widely used in applications such as medical imaging <ref type="bibr" target="#b25">[26]</ref>, satellite imaging <ref type="bibr" target="#b28">[29]</ref>, security and surveillance <ref type="bibr" target="#b36">[37]</ref>, where high-frequency details are greatly desired.</p><p>In recent years, due to the powerful learning ability, Deep Learning (DL) models, especially Convolutional Neural Networks (CNN), are widely used to address the ill- * This work was conducted when the first author was a visiting scholar at Michigan State University  <ref type="figure">Figure 1</ref>. PSNR of recent CNN models for scale factor ×3 on Set5 <ref type="bibr" target="#b0">[1]</ref>. Red points are our models. △, ✩, and • are models with less than 5 layers, 20 layers, and more than 30 layers, respectively. DRRN B1U9 means there is 1 recursive block, in which 9 residual units are stacked. With the same depth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. After increasing the depth without adding any parameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and RED30 <ref type="bibr" target="#b16">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</p><p>posed inverse problem of Super Resolution (SR), and have demonstrated superiority over reconstruction-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref> or other learning paradigms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) <ref type="bibr" target="#b1">[2]</ref> predicts the nonlinear LR-HR mapping via a fully convolutional network, and significantly outperforms classical non-DL methods. However, SRCNN does not consider any self similarity property. To address this issue, the Deep Joint Super Resolution (DJSR) jointly utilizes both the wealth of external examples and the power of self examples unique to the input. Inspired by the learning iterative shrinkage and thresholding algorithm <ref type="bibr" target="#b4">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type="bibr" target="#b31">[32]</ref> is trained end-to-end to fully exploit the natural sparsity of images. Shi et al. <ref type="bibr" target="#b24">[25]</ref> observe that the prior models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref> increase LR image's resolution via bicubic interpolation be-fore CNN learning, which increases the computational cost. The Efficient Sub-Pixel Convolutional neural Network (ES-PCN) reduces the computational and memory complexity, by increasing the resolution from LR to HR only at the end of the network. One commonality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN <ref type="bibr" target="#b1">[2]</ref> uses 3 convolutional layers. Their deeper structures with 4 or 5 layers do not achieve better performance, which was attributed to the difficulty of training deeper networks and led to the observation that "the deeper the better" might not be the case in SR. Inspired by the success of very deep networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> on ImageNet <ref type="bibr" target="#b20">[21]</ref>, Kim et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> propose two very deep convolutional networks for SR, both stacking 20 convolutional layers, from the viewpoints of training efficiency and storage, respectively. On the one hand, to accelerate the convergence speed of very deep networks, the VDSR <ref type="bibr" target="#b12">[13]</ref> is trained with a very high learning rate (10 −1 , instead of 10 −4 in SRCNN) and the authors further use residual learning and adjustable gradient clipping to solve gradient explosion problem. On the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) <ref type="bibr" target="#b13">[14]</ref> introduces a very deep recursive layer via a chain structure with up to 16 recursions. To mitigate the difficulty of training DRCN, the authors use recursive-supervision and skip-connection, and adopt an ensemble strategy to further improve the performance. Very recently, Mao et al. <ref type="bibr" target="#b16">[17]</ref> propose a 30-layer convolutional auto-encoder network named RED30 for image restoration, which uses symmetric skip connections to help training. All of the three models learn the residual image between the input Interpolated LR (ILR) image and the ground truth HR image in the residual branch. The residual image is then added to the ILR image from the identity branch to estimate the HR image. The three models outperform the previous DL and non-DL methods by a large margin, which demonstrates "the deeper the better" is still true in SR.</p><p>Despite achieving excellent performance, the very deep networks require enormous parameters. Compared to the compact models, large models demand more storage space and are less applicable to mobile systems <ref type="bibr" target="#b5">[6]</ref>. To address this issue, we propose a novel Deep Recursive Residual Network (DRRN) to effectively build a very deep network structure, which achieves better performance, but with 2×, 6×, and 14× fewer parameters than VDSR, DRCN, and RED30, respectively. In a nutshell, DRRN advances the SR performance with a deeper yet concise network. Specifically, DRRN has two major algorithmic novelties: (1) Both global and local residual learning are introduced in DRRN. In VDSR and DRCN, the residual image is estimated from the input and output of the networks, termed as Global Residual Learning (GRL). Since the SR output is vastly similar to the input, GRL is effective in easing the difficulty of training deep networks. Therefore, we also adopt GRL in our identity branch. Further, very deep networks could suffer from the performance degradation problem, as observed in visual recognition <ref type="bibr" target="#b7">[8]</ref> and image restoration <ref type="bibr" target="#b16">[17]</ref>. The reason may be a significant amount of image details are lost after so many layers. To address this issue, we introduce an enhanced residual unit structure, termed as multi-path mode Local Residual Learning (LRL), where the identity branch not only carries rich image details to late layers, but also helps gradient flow. GRL and LRL mainly differ in that LRL is performed in every few stacked layers, while GRL is performed between the input and output images, i.e., DRRN has many LRLs and only 1 GRL. (2) Recursive learning of residual units is proposed in DRRN to keep our model compact. In DRCN <ref type="bibr" target="#b13">[14]</ref>, a deep recursive layer (up to 16 convolutional recursions) is learned and the weights are shared in the 16 convolutional recursions. Our DRRN has two major differences compared to DRCN: (a) Unlike DRCN that shares weights among convolutional layers, DRRN has a recursive block consisting of several residual units, and the weight set is shared among these residual units. (b) To address the vanishing/exploding gradients problem of very deep models, DRCN supervises every recursion so that the supervision on early recursions help backpropagation. DRRN is relieved from this burden by designing a recursive block with a multi-path structure. Our model can be easily trained even with 52 convolutional layers. Last but not least, through recursive learning, DRRN can improve accuracy by increasing depth without adding any weight parameters.</p><p>To illustrate the effectiveness of the two strategies used in DRRN, <ref type="figure">Fig. 1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> versus the number of parameters, denoted as k. Compared to the prior CNN models, DRRN achieves the best performance with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Since Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref> and DRCN <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates these models via simplified network structures with only 6 convolutional layers, where the activation functions, batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>, are omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ResNet</head><p>The main idea of ResNet <ref type="bibr" target="#b7">[8]</ref> is to use a residual learning framework to ease the training of very deep networks. Instead of hoping every few stacked layers directly fit the desired underlying mapping, the authors explicitly let these layers fit a residual mapping, which is assumed to be easier for optimization. Denoting the input as x and the underlying mapping as H(x), the residual mapping is defined as Simplified structures of (a) ResNet <ref type="bibr" target="#b7">[8]</ref>. The green dashed box means a residual unit. (b) VDSR <ref type="bibr" target="#b12">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type="bibr" target="#b13">[14]</ref>. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light green color) share the same weights. (d) DRRN. The red dashed box refers to a recursive block consisting of two residual units. In the recursive block, the corresponding convolutional layers in the residual units (with light green or light red color) share the same weights. In all four cases, the outputs with light blue color are supervised, and is the element-wise addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Key Strategies Mathematical Formulation ResNet</head><p>Chain mode local residual learning</p><formula xml:id="formula_0">y = f Rec (U U (U U−1 (...(U 1 (f 1 (x)))...))) VDSR Global residual learning y = f Rec (f d−1 (f d−2 (...(f 1 (x))...))) + x DRCN</formula><p>Global residual learning + recursive learning (single weight layer) + multi-CNN ensemble</p><formula xml:id="formula_1">y = Σ T t=1 wt · (f Rec (f (t) 2 (f 1 (x))) + x) DRRN Multi-path mode local residual learning y = f Rec (R B (R B−1 (...(R 1 (x))</formula><p>...))) + x + global residual learning + recursive learning (multiple weight layers in the residual unit) <ref type="table">Table 1</ref>. Strategies used in ResNet <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units in ResNet, convolutional layers in VDSR, recursions in DRCN, and recursive blocks in DRRN, respectively. x and y are input and output of networks. f denotes function of convolutional layer. U denotes function of residual unit structure and R denotes function of our recursive block.</p><p>F(x) := H(x) − x, and a residual unit structure is thus:</p><formula xml:id="formula_2">x = U (x) = σ(F(x, W ) + h(x)),<label>(1)</label></formula><p>wherex is the output of the residual unit, h(x) is an identity mapping <ref type="bibr" target="#b7">[8]</ref> : h(x) = x, W is a set of weights (the biases are omitted to simplify notations), function σ denotes ReLU, F(x, W ) is the residual mapping to be learned, and U denotes the function of the residual unit structure. For a basic residual unit that stacks two convolutional layers,</p><formula xml:id="formula_3">F(x, W ) = W 2 σ(W 1 x)</formula><p>. By stacking such structures to construct a very deep 152-layer network, ResNet won the first place in the ILSVRC 2015 classification competition.</p><p>Since the residual learning in ResNet is adopted in every few stacked layers, this strategy is a form of local residual learning, where residual units are stacked in the chain mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">VDSR</head><p>Differing from ResNet that uses residual learning in every few stacked layers, VDSR <ref type="bibr" target="#b12">[13]</ref> introduces GRL, i.e., residual learning between the input ILR image and the output HR image. There are three notes for VDSR: (1) Unlike SRCNN <ref type="bibr" target="#b1">[2]</ref> that only uses 3 layers, VDSR stacks 20 weight layers (3 × 3 for each layer) in the residual branch, which leads to a much larger receptive field (41 × 41 vs. 13 × 13). (2) GRL and adjustable gradient clipping enable VDSR to converge very fast (∼4 hours on GPU Titan Z). (3) By adopting scale augmentation, a single network of VDSR is robust to images with different scales. Later, we will show that VDSR actually is a special case of DRRN, when there's no residual unit in our recursive block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">DRCN</head><p>DRCN <ref type="bibr" target="#b13">[14]</ref> is motivated by the observation that adding more weight layers introduces more parameters, where the model is likely to overfit and also becomes disk hungry. To address these issues, the authors introduce a recursive layer into the network, so that the model parameters do not increase while more recursions are performed in the recursive layer. DRCN consists of three parts: embedding net, inference net and reconstruction net, which are illustrated as the first, middle 4, and last convolutional layer(s) in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>, respectively. The embedding net f 1 (x) represents a given  <ref type="bibr" target="#b13">[14]</ref>) in a recursive layer, with shared weights among these recursions. Finally, the reconstruction net f Rec (H T ), where H T is the output of the inference net, generates the intermediate HR image. However, since training such a deep network is difficult, the authors further propose two mitigations, recursive-supervision and skip-connection. Specifically, for the t-th intermediate recursion in the recursive layer, the output after reconstruction net is formulated as</p><formula xml:id="formula_4">y t = f Rec (f (t) 2 (f 1 (x))) + x,<label>(2)</label></formula><p>where x is skip-connection and basically a GRL. Each intermediate prediction y t is learned with supervision. Finally, an ensemble strategy is adopted and the output is the weighted average of all predictions y = Σ T t=1 w t · y t , with weights w t learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Recursive Residual Network</head><p>In this section, we present the technical parts of our proposed DRRN. Specifically, we adopt global residual learning in the identity branch and introduce recursive learning into the residual branch by constructing the recursive block structure, in which several residual units are stacked. Noted that in ResNet <ref type="bibr" target="#b7">[8]</ref>, different residual units use different inputs for the identity branch (green dashed boxes in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). However, in our recursive block, a multi-path structure is used and all the residual units share the same input for the identity branch (green dashed boxes in <ref type="figure" target="#fig_1">Fig. 2(d)</ref>), which further facilitates the learning <ref type="bibr" target="#b15">[16]</ref>. We highlight the differences of the network structures between DRRN and the related models in Tab. 1. Now, we will gradually present more details of our model, from the residual unit to the recursive block and finally the whole network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual Unit</head><p>In ResNet <ref type="bibr" target="#b7">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the activation functions (BN <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>) are performed after the weight layers. In contrast to such a "post-activation" structure, He et al. <ref type="bibr" target="#b8">[9]</ref> propose a "preactivation" structure, which performs the activation before the weight layers. They claim that the pre-activation version is much easier to train and generates better performance than the post-activation version. Specifically, the residual unit with pre-activation structure is formulated as</p><formula xml:id="formula_5">H u = F(H u−1 , W u ) + H u−1 ,<label>(3)</label></formula><p>where u = 1, 2, ..., U, U is the number of residual units in a recursive block, H u−1 and H u are the input and output of the u-th residual unit, and F denotes the residual function.</p><p>Instead of directly using the above residual unit, we modify Eq. 3 so that the inputs to the identity branch and the residual branch are different. As described in the beginning of Sec. 3, the inputs to all of the identity branches of the residual units in one recursive block are kept the same, i.e., H 0 in <ref type="figure" target="#fig_2">Fig. 3</ref>. As a result, there are multiple paths between the input and output of our recursive block, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The residual paths help to learn highly complex features and the identity paths help gradient backpropagation during training. Compared to the chain mode, this multipath mode facilitates the learning and is less prone to overfitting <ref type="bibr" target="#b15">[16]</ref>. Therefore, we formulate our residual unit as</p><formula xml:id="formula_6">H u = G(H u−1 ) = F(H u−1 , W ) + H 0 ,<label>(4)</label></formula><p>where G denotes the function of our residual unit, H 0 is the result of the first convolutional layer in the recursive block. Since the residual unit is recursively learned, the weight set W is shared among the residual units within a recursive block, but different across different recursive blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recursive Block</head><p>We now introduce the details of our recursive block. First, we illustrate the structure of our recursive blocks in <ref type="figure" target="#fig_3">Fig. 4</ref>. Motivated by <ref type="bibr" target="#b15">[16]</ref>, we introduce a convolutional layer at the beginning of the recursive block, and then several residual units mentioned in Sec. 3.1 are stacked. We According to Eq. 4, the result of u-th residual unit is</p><formula xml:id="formula_7">H u b = G(H u−1 ) = F(H u−1 b , W b ) + H 0 b .<label>(5)</label></formula><p>Thus, the output of the b-th recursive block x b is</p><formula xml:id="formula_8">x b = H U b = G (U) (f b (x b−1 )) = G(G(...(G(f b (x b−1 )))...)),<label>(6)</label></formula><p>where U-fold operations of G b are performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Structure</head><p>Finally, we simply stack several recursive blocks, followed by a convolutional layer reconstructing the residual between the LR and HR images. The residual image is then added to the global identity mapping from the input LR image. The entire network structure of DRRN is illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>. Actually, VDSR <ref type="bibr" target="#b12">[13]</ref> can be viewed as a special case of DRRN, i.e., when U = 0, DRRN becomes VDSR. DRRN has two key parameters: the recursive block number B and the residual unit number U in each recursive block. Given different B and U, we can learn DRRN with different depths -the number of convolutional layers. Specifically, the depth of DRRN d is calculated as:</p><formula xml:id="formula_9">d = (1 + 2 × U) × B + 1.<label>(7)</label></formula><p>Denoting x and y to be the input and output of DRRN, R to be the function of the b-th recursive block, we have</p><formula xml:id="formula_10">x b = R b (x b−1 ) = G (U) (f b (x b−1 )).<label>(8)</label></formula><p>When b = 1, we define x 0 = x. Then, DRRN can be formulated as</p><formula xml:id="formula_11">y = D(x) = f Rec (R B (R B−1 (...(R 1 (x))...))) + x,<label>(9)</label></formula><p>where f Rec is a function for the last convolutional layer in DRRN to reconstruct the residual. Tab. 1 lists the mathematical formulations of ResNet, VDSR, DRCN and DRRN. Given a training set {x</p><formula xml:id="formula_12">(i) ,x (i) } N i=1</formula><p>, where N is the number of training patches andx (i) is the ground truth HR patch of the LR patch x (i) , the loss function of DRRN is</p><formula xml:id="formula_13">L(Θ) = 1 2N N i=1 x (i) − D(x (i) ) 2 ,<label>(10)</label></formula><p>where Θ denotes the parameter set. The objective function is optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type="bibr" target="#b14">[15]</ref>. We implement DRRN via Caffe <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>By following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, we use a training dataset of 291 images, where 91 images are from Yang et al. <ref type="bibr" target="#b34">[35]</ref> and other 200 images are from Berkeley Segmentation Dataset <ref type="bibr" target="#b17">[18]</ref>. For testing, we utilize four widely used benchmark datasets, Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b35">[36]</ref>, BSD100 <ref type="bibr" target="#b17">[18]</ref> and Urban100 <ref type="bibr" target="#b9">[10]</ref>, which have 5, 14, 100 and 100 images respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Data augmentation is performed on the 291-image training dataset. Inspired by <ref type="bibr" target="#b29">[30]</ref>, the flipped and rotated versions of the training images are considered. Specifically, we rotate the original images by 90</p><p>• , 180</p><p>• , 270</p><p>• and flip them horizontally. After that, for each original image, we have 7 additional augmented versions. Besides, inspired by VDSR <ref type="bibr" target="#b12">[13]</ref>, we also use scale augmentation to train our model, and images with different scales (×2, ×3 and ×4) are all included in the training set. Therefore, for all different scales, we only need to train a single model.</p><p>Training images are split into 31 × 31 patches, with the stride of 21, by considering both the training time and storage complexities. We set the mini-batch size of SGD to 128, momentum parameter to 0.9, and weight decay to 10 −4 . Every weight layer has 128 filters of the size 3 × 3.</p><p>For weight initialization, we use the same method as He et al. <ref type="bibr" target="#b6">[7]</ref>, which is shown to be suitable for networks utilizing ReLU. The initial learning rate is set to 0.1 and then decreased to half every 10 epochs. Since a large learning rate is used in our work, we adopt the adjustable gradient clipping <ref type="bibr" target="#b12">[13]</ref> to boost the convergence rate while suppressing exploding gradients. Specifically, the gradients are clipped to [−   <ref type="table">Table 2</ref>. Benchmark results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100 and Urban100. Red color indicates the best performance of our methods and blue color indicates the best performance of previous methods. depths, and see how the two parameters affect the performance. In <ref type="figure" target="#fig_6">Fig. 6</ref>, we build a grid of B and U, and sample several points in the grid with the depth ranging from 8 to 52 layers. The parameter number keeps the same when more residual units are used in one recursive block, and linearly increases when more recursive blocks are stacked. First, to clearly show how a single parameter affects DRRN, we fix one parameter to 3 and change the other from 1 to 4. <ref type="figure" target="#fig_6">Fig. 6</ref> shows that increasing B or U results in deeper models and achieves better performance, which indicates deeper is still better. Despite different structures, these models are comparable as long as their depths are similar, e.g., B2U3 (d = 15, k = 784K) and B3U2 (d = 16, k = 1, 182K) achieve 33.76 and 33.77 dB, respectively.</p><p>The structures mentioned above all use the recursive learning strategy. Next, we test three very different structures to demonstrate the effectiveness of such a strategy. Specifically, we fix one parameter to 1 and change the other to construct networks with d = 52. This results in two extreme structures: B1U25 (k = 297K) and B17U1 (k = 7, 375K). For B1U25, only one recursive block is used, in which 25 residual units are recursively learned. For B17U1, 17 recursive blocks are stacked, with no recursive learning. We also construct a normal structure B3U8 (d = 52, k = 1, 182K). <ref type="figure" target="#fig_6">Fig. 6</ref> shows that despite different structures, the three networks achieve comparable performance (B17U1 34.03 dB, B3U8 34.04 dB and B1U25 34.03 dB) and outperform the pervious shallow networks. Thanks to the recursive learning strategy, B1U25 can achieve state-ofthe-art results using far fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Art Models</head><p>We now provide quantitative and qualitative comparisons. Considering both the performance and number of parameters, we choose DRRN B1U25 (d = 52, k = 297K) as our best model. For fair comparison, we also construct a DRRN B1U9 (d = 20, k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and non-DL <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> methods in recent years are used for benchmark. Experimental setting is kept the same as those previous methods. Specifically, we first apply bicubic interpolation to the color components of an image and all models are applied to its luminance component only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, we crop pixels near image boundary before evaluation, although this is unnecessary for DRRN.</p><p>Tab. 2 summarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The two DRRN models outperforms all existing methods in all datasets and scale factors, in both PSNR and Structural SIMilarity (SSIM) <ref type="bibr" target="#b0">1</ref> . Especially on the recent difficult Urban100 dataset <ref type="bibr" target="#b9">[10]</ref>, DRRN significantly advances the state of the art, with the improvement margin of 0.47, 0.38, and 0.26 dB on scale factor ×2, ×3 and ×4 respectively.</p><p>Further, we also use another metric: Information Fidelity Criterion (IFC) <ref type="bibr" target="#b23">[24]</ref> for comparison, which claims to have the highest correlation with perceptual scores for SR evaluation <ref type="bibr" target="#b33">[34]</ref>. The results are presented in <ref type="table">Tab</ref>  the results of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> are cited from <ref type="bibr" target="#b19">[20]</ref> 2 , while the results of VDSR come from our re-implementation. Similar to DRRN, the VDSR re-implementation also uses BN and ReLU as the activation functions, unlike the original VDSR <ref type="bibr" target="#b12">[13]</ref>   <ref type="figure">Figure 8</ref>. PSNR for scale factor ×3 on Set5 using VDSR (blue), DRRN NS (green), DRRN C (cyan) and DRRN (red).  <ref type="table">Table 4</ref>. Average PSNR when different DRRN components are turned on or off, for scale factor ×3 on dataset Set5.</p><formula xml:id="formula_14">Methods VDSR DRRN NS C DRRN NS DRRN C DRRN Loc. Res. L × √ √ √ √ Recu. L × × × √ √ Multi-path × × √ × √ PSNR</formula><p>Qualitative comparisons among SRCNN <ref type="bibr" target="#b1">[2]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref> and DRRN are illustrated in <ref type="figure" target="#fig_7">Fig. 7</ref>. For SRCNN and SelfEx, we use their public codes. For VDSR, we use our re-implementation. As we can see, our method produces relatively sharper edges with respect to patterns, while other methods may give blurry results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussions</head><p>Since global residual learning has been well discussed in <ref type="bibr" target="#b12">[13]</ref>, in this section, we mainly focus on local residual learning (LRL), recursive learning and multi-path structure.</p><p>Local Residual Learning To demonstrate the effectiveness of LRL, DRRN is compared with VDSR <ref type="bibr" target="#b12">[13]</ref>, which has no LRL. For fair comparison, the depth and number of parameters are kept the same for both methods. Specifically, we evaluate three depths: 16(B3U2), 22(B3U3), and 28(B3U4) convolutional layers. Each convolutional layer has 128 filters with the size 3 × 3. To keep the parameter number the same, in this test we do not share the weight set of the residual units in one recursive block, and denote this DRRN structure as DRRN NS. <ref type="figure">Fig. 8</ref> shows the PSNR of both methods in different depths. We see that the LRL strategy consistently improves VDSR at all depths. Recursive Learning To contrast our recursive learning strategy, the three DRRN NS versions are compared with the three weight-shared versions <ref type="figure">(Fig. 8)</ref>. Storage is an important factor to consider when building a deep model. The recursive learning strategy can reduce the storage demand and keep a concise model while increasing its depth. Interestingly the weight-shared DRRN versions achieve comparable or even better performance than DRRN NS versions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shallow</head><p>Deep Very deep <ref type="figure">Figure 9</ref>. Comparing deep and shallow models proposed in recent three years that report PSNR for scale factor ×3 on Set5 and Set14.</p><p>while using only a small fraction of parameters, which indicates when limited training set (e.g., 291 images) is used, the recursive learning is indeed effective under the same structure, and less prone to overfitting <ref type="bibr" target="#b15">[16]</ref>. Multi-Path Structure To demonstrate the effectiveness of multi-path structure, we compare DRRN with the chain structure, denoted as DRRN C. As shown in <ref type="figure">Fig. 8</ref>, with the same depth and parameter number, the multi-path structures achieve higher PSNR than the corresponding chain structures in all three cases. Further, Tab. 4 presents the comprehensive study on performance gains using network B3U4 as the example. It shows how different technical parts improve the performance compared to the baseline VDSR. Deep vs. Shallow Finally, we give a comparison of the deep and shallow SISR models, which are published in recent three years (2014 to 2016) that report PSNR for scale factor ×3 on datasets Set5 and Set14. Shallow (non-DL) models include A+ <ref type="bibr" target="#b30">[31]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, RFL <ref type="bibr" target="#b22">[23]</ref>, NBSRF <ref type="bibr" target="#b21">[22]</ref>, PSyCo <ref type="bibr" target="#b19">[20]</ref> and IA <ref type="bibr" target="#b29">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type="bibr" target="#b1">[2]</ref>, DJSR <ref type="bibr" target="#b32">[33]</ref>, CSCN <ref type="bibr" target="#b31">[32]</ref>, ESPCN <ref type="bibr" target="#b24">[25]</ref> and FSRCNN <ref type="bibr" target="#b2">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref>, RED <ref type="bibr" target="#b16">[17]</ref> and DRRN with d = 20 and 52. <ref type="figure">Fig. 9</ref> shows that 1) very deep models significantly outperform the shallow models; 2) DRRN B1U9 (d = 20, k = 297K) already outperforms the state of the arts with the same depth but fewer parameters; 3) a deeper DRRN B1U25 (d = 52, k = 297K) further improves the performance without adding any parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose Deep Recursive Residual Network (DRRN) for single image super-resolution. In DRRN, an enhanced residual unit structure is recursively learned in a recursive block, and we stack several recursive blocks to learn the residual image between the HR and LR images. The residual image is then added to the input LR image from a global identity branch to estimate the HR image.</p><p>Extensive benchmark experiments and analysis show that DRRN is a deep, concise, and superior model for SISR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. It was supported by the National Sci- ence Fund of China under Grant Nos. 91420201, 61472187, 61502235, 61233011, 61373063 and 61602244, the 973 Program No.2014CB349303, Program for Changjiang Scholars and Innovative Research Team in Uni- versity, and partially sponsored by CCF-Tencent Open Research Fund.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Simplified structures of (a) ResNet [8]. The green dashed box means a residual unit. (b) VDSR [13]. The purple line refers to a global identity mapping. (c) DRCN [14]. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light green color) share the same weights. (d) DRRN. The red dashed box refers to a recursive block consisting of two residual units. In the recursive block, the corresponding convolutional layers in the residual units (with light green or light red color) share the same weights. In all four cases, the outputs with light blue color are supervised, and is the element-wise addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A close look at the u-th residual unit in DRRN. The black dashed box represents the residual function F , which consists of two "conv" layers and each stacked by BN-ReLU-weight layers. image x as feature maps H 0 . The inference net f 2 (H 0 ) stacks T recursions (T = 16 in [14]) in a recursive layer, with shared weights among these recursions. Finally, the reconstruction net f Rec (H T ), where H T is the output of the inference net, generates the intermediate HR image. However, since training such a deep network is difficult, the authors further propose two mitigations, recursive-supervision and skip-connection. Specifically, for the t-th intermediate recursion in the recursive layer, the output after reconstruction net is formulated as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Structures of our recursive blocks. U means number of residual units in the recursive block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. An example network structure of DRRN with B = 6 and U = 3. Here, "RB" layer refers to a recursive block. denote B as the number of recursive blocks, x b−1 and x b (b = 1, 2, ..., B) as the input and output of the b-th recursive block, and H 0 b = f b (x b−1 ) as the result after passing x b−1 through the first convolutional layer, whose function is f b . According to Eq. 4, the result of u-th residual unit is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, where γ is the current learning rate and θ = 0.01 is the gradient clipping parameter. Training a DRRN of d = 20 roughly takes 4 days with 2 Titan X GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. PSNR of various DRRNs at B and U combinations. The color of the point indicates the PSNR that corresponds to the bar on the right and 4 depth contours (d = 50, 30, 20, 10) are also plotted. The tests are conducted for scale factor ×3 on Set5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Qualitative comparison. (1) The first row shows image "img059" (Urban100 with scale factor ×3). DRRN recovers sharp lines, while others all give blurry results. (2) The second row shows image "253027" (BSD100 with scale factor ×3). DRRN accurately recovers the pattern. (3) The last row shows image "ppt3" (Set14 with scale factor ×4). Texts in DRRN are sharp, while others are blurry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>4.3. Study of B and U In this subsection, we explore various combinations of B and U to construct different DRRN structures with different</figDesc><table>Dataset 

Scale Bicubic 
SRCNN [2] 
SelfEx [10] 
RFL [23] 
VDSR [13] 
DRCN [14] 
DRRN B1U9 DRRN B1U25 

Set5 

×2 
33.66/0.9299 36.66/0.9542 36.49/0.9537 36.54/0.9537 37.53/0.9587 37.63/0.9588 37.66/0.9589 37.74/0.9591 
×3 
30.39/0.8682 32.75/0.9090 32.58/0.9093 32.43/0.9057 33.66/0.9213 33.82/0.9226 33.93/0.9234 34.03/0.9244 
×4 
28.42/0.8104 30.48/0.8628 30.31/0.8619 30.14/0.8548 31.35/0.8838 31.53/0.8854 31.58/0.8864 31.68/0.8888 

Set14 

×2 
30.24/0.8688 32.45/0.9067 32.22/0.9034 32.26/0.9040 33.03/0.9124 33.04/0.9118 33.19/0.9133 33.23/0.9136 
×3 
27.55/0.7742 29.30/0.8215 29.16/0.8196 29.05/0.8164 29.77/0.8314 29.76/0.8311 29.94/0.8339 29.96/0.8349 
×4 
26.00/0.7027 27.50/0.7513 27.40/0.7518 27.24/0.7451 28.01/0.7674 28.02/0.7670 28.18/0.7701 28.21/0.7720 

BSD100 

×2 
29.56/0.8431 31.36/0.8879 31.18/0.8855 31.16/0.8840 31.90/0.8960 31.85/0.8942 32.01/0.8969 32.05/0.8973 
×3 
27.21/0.7385 28.41/0.7863 28.29/0.7840 28.22/0.7806 28.82/0.7976 28.80/0.7963 28.91/0.7992 28.95/0.8004 
×4 
25.96/0.6675 26.90/0.7101 26.84/0.7106 26.75/0.7054 27.29/0.7251 27.23/0.7233 27.35/0.7262 27.38/0.7284 

Urban100 

×2 
26.88/0.8403 29.50/0.8946 29.54/0.8967 29.11/0.8904 30.76/0.9140 30.75/0.9133 31.02/0.9164 31.23/0.9188 
×3 
24.46/0.7349 26.24/0.7989 26.44/0.8088 25.86/0.7900 27.14/0.8279 27.15/0.8276 27.38/0.8331 27.53/0.8378 
×4 
23.14/0.6577 24.52/0.7221 24.79/0.7374 24.19/0.7096 25.18/0.7524 25.14/0.7510 25.35/0.7576 25.44/0.7638 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>PSNR/SSIM/IFC) (21.10/0.7046/3.134) (21.77/0.7540/3.761) (21.94/0.7608/3.669) (22.58/0.7942/4.341) (22.74/0.7999/4.365) (23.37/0.8158/4.713)</figDesc><table>. 3. Note that Dataset 

Scale Bicubic SRCNN [2] SelfEx [10] RFL [23] PSyCo [20] 
VDSR [13] 
DRRN B1U9 
DRRN B1U25 

Set5 

×2 
6.083 
8.036 
7.811 
8.556 
8.642 
8.569 
8.583 
8.671 
×3 
3.580 
4.658 
4.748 
4.926 
5.083 
5.221 
5.241 
5.397 
×4 
2.329 
2.991 
3.166 
3.191 
3.379 
3.547 
3.581 
3.703 

Set14 

×2 
6.105 
7.784 
7.591 
8.175 
8.280 
8.178 
8.181 
8.320 
×3 
3.473 
4.338 
4.371 
4.531 
4.660 
4.730 
4.732 
4.878 
×4 
2.237 
2.751 
2.893 
2.919 
3.055 
3.133 
3.147 
3.252 

Urban100 

×2 
6.245 
7.989 
7.937 
8.450 
8.589 
8.645 
8.653 
8.917 
×3 
3.620 
4.584 
4.843 
4.801 
5.031 
5.194 
5.259 
5.456 
×4 
2.361 
2.963 
3.314 
3.110 
3.351 
3.496 
3.536 
3.676 

Table 3. Benchmark results. Average IFCs for the scale factor ×2, ×3 and ×4 on datasets Set5, Set14 and Urban100. Red color indicates 
the best performance of our methods and blue color indicates the best performance of previous methods. 

Ground Truth 
Bicubic 
SRCNN 
SelfEx 
VDSR 
DRRN_B1U9 DRRN_B1U25 

((PSNR/SSIM/IFC) (22.55/0.7073/3.591) (23.51/0.7608/4.344) (23.42/0.7587/4.281) (23.99/0.7728/4.716) (24.23/0.7781/4.734) (24.41/0.7805/4.914) 

(PSNR/SSIM/IFC) (21.98/0.8126/1.920) (24.80/0.8928/2.666) (24.85/0.9076/2.941) (25.85/0.9289/3.406) (26.33/0.9365/3.557) (26.48/0.9415/3.822) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>that does not use BN. These results are</figDesc><table>22 layers 

28 layers 
16 layers 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">With two convolutional layers in the residual branch, DRRN achieves state-of-the-art performance. More complex designs have the potential to improve performance but are not the focus of this work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since PSyCo [20] does not present complete PSNR/SSIM performance on the four benchmarks, we do not include it in Tab. 2. faithful since our VDSR re-implementation achieves similar benchmark performance as [13] reported in Tab. 2. Since only Set5, Set14 and Urban100 are used in [20], we omit BSD100 in this test. It is clear that DRRN still outperforms all existing methods in all datasets and scale factors. Regarding the speed, our 20-layer B1U9 network takes 0.25 second to process a 288 × 288 image on a Titan X GPU.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027v2</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PSyCo: Manifold span reduction for super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">ImageNet large scale visual recognition challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Naive bayes superresolution forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cardiac image super-resolution with global correspondence using multi-atlas patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marvao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Subpixel mapping of rural land cover objects from fine spatial resolution satellite sensor imagery using super-resolution pixel-swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="491" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-tuned deep super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single-image superresolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On single image scaleup using sparse-representations. Curves and Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
