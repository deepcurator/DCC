<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Image Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
							<email>ningxu2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Beckman Institute for Advanced Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
							<email>bprice@adobe.com</email>
							<affiliation key="aff2">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
							<email>scohen@adobe.com</email>
							<affiliation key="aff2">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Beckman Institute for Advanced Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Image Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Matting, the problem of accurate foreground estimation in images and videos, has significant practical importance. It is a key technology in image editing and film production and effective natural image matting methods can greatly improve current professional workflows. It necessitates methods that handle real world images in unconstrained scenes.</p><p>Unfortunately, current matting approaches do not generalize well to typical everyday scenes. This is partially due to the difficulty of the problem: as formulated the matting problem is underconstrained with 7 unknown values per pixel but only 3 known values:</p><formula xml:id="formula_0">I i = α i F i + (1 − α i )B i α i ∈ [0, 1].<label>(1)</label></formula><p>where the RGB color at pixel i, I i , is known and the foreground color F i , background color B i and matte estimation α i are unknown. However, current approaches are further limited in their approach.</p><p>The first limitation is due to current methods being designed to solve the matting equation (Eq. 1). This equation formulates the matting problem as a linear combination of two colors, and consequently most current algorithms approach this largely as a color problem. The standard approaches include sampling foreground and background colors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, propagating the alpha values according to the matting equation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22]</ref>, or a hybrid of the two <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>. Such approaches rely largely on color as the distinguishing feature (often along with the spatial position of the pixels), making them incredibly sensitive to situations where the foreground and background color distributions overlap, which unfortunately for these methods is the common case for natural images, often leading to lowfrequency "smearing" or high-frequency "chunky" artifacts depending on the method (see <ref type="figure">Fig 1 top row)</ref>. Even the recently proposed deep learning methods are highly reliant on color-dependent propagation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>A second limitation is due to the focus on a very small dataset. Generating ground truth for matting is very difficult, and the alphamatting.com dataset <ref type="bibr" target="#b24">[25]</ref> made a significant contribution to matting research by providing groundtruth data. Unfortunately, it contains only 27 training images and 8 test images, most of which are objects in front of an image on a monitor. Due to its size and constraints of the dataset (e.g. indoor lab scenes, indoor lighting, no humans or animals), it is by its nature biased, and methods are incentivized to fit to this data for publication purposes. As is the case with all datasets, especially small ones, at some point methods will overfit to the dataset and no longer generalize to real scenes. A recent video matting dataset is available <ref type="bibr" target="#b9">[10]</ref> with 3 training videos and 10 test videos, 5 of which were extracted from green screen footage and the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Trimap</head><p>Closed-form Ours <ref type="figure">Figure 1</ref>. The comparison between our method and the Closedform matting <ref type="bibr" target="#b21">[22]</ref>. The first image is from the Alpha Matting benchmark and the second image is from our 1000 testing images.</p><p>rest using a similar method to <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this work, we present an approach aimed to overcome these limitations. Our method uses deep learning to directly compute the alpha matte given an input image and trimap. Instead of relying primarily on color information, our network can learn the natural structure that is present in alpha mattes. For example, hair and fur (which usually require matting) possess strong structural and textural patterns. Other cases requiring matting (e.g. edges of objects, regions of optical or motion blur, or semi-transparent regions) almost always have a common structure or alpha profile that can be expected. While low-level features will not capture this structure, deep networks are ideal for representing it. Our two-stage network includes an encoder-decoder stage followed by a small residual network for refinement and includes a novel composition loss in addition to a loss on the alpha. We are the first to demonstrate the ability to learn an alpha matte end-to-end given an image and trimap.</p><p>To train a model that will excel in natural images of unconstrained scenes, we need a much larger dataset than currently available. Obtaining a ground truth dataset using the method of <ref type="bibr" target="#b24">[25]</ref> would be very costly and cannot handle scenes with any degree of motion (and consequently cannot capture humans or animals). Instead, inspired by other synthetic datasets that have proven sufficient to train models for use in real images (e.g. <ref type="bibr" target="#b3">[4]</ref>), we create a large-scale matting dataset using composition. Images with objects on simple backgrounds were carefully extracted and were composited onto new background images to create a dataset with 49300 training images and 1000 test images.</p><p>We perform extensive evaluation to prove the effectiveness on our method. Not only does our method achieve first place on the alphamatting.com challenge, but we also greatly outperform prior methods on our synthetic test set. We show our learned model generalizes to natural images with a user study comparing many prior methods on 31 natural images featuring humans, animals, and other objects in varying scenes and under different lighting conditions. This study shows a strong preference for our results, but also shows that some methods which perform well on the alphamatting.com dataset actually perform worse compared to other methods when judged by humans, suggesting that methods are being to overfit on the alphamatting.com test set. Finally, we also show that we are more robust to trimap placement than other methods. In fact, we can produce great results even when there is no known foreground and/or background in the trimap while most methods cannot return any result (see <ref type="figure">Fig 1 bottom row )</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Current matting methods rely primarily on color to determine the alpha matte, along with positional or other lowlevel features. They do so through sampling, propagation, or a combination of the two.</p><p>In sampling-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>, the known foreground and background regions are sampled to find candidate colors for a given pixel's foreground and background, then a metric is used to determine the best foreground/background combination. Different sampling methods are used, including sampling along the boundary nearest the given pixel <ref type="bibr" target="#b31">[32]</ref>, sampling based on ray casting <ref type="bibr" target="#b12">[13]</ref>, searching the entire boundary <ref type="bibr" target="#b15">[16]</ref>, or sampling from color clusters <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref>. The metric to decide among the sampled candidate nearly always includes a matting equation reconstruction error, potentially with terms measuring the distance of samples from the given pixel <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16]</ref> or the similarity of the foreground and background samples <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28]</ref>, and formulations include sparse coding <ref type="bibr" target="#b11">[12]</ref> and KL-divergence approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. Higher-order features like texture <ref type="bibr" target="#b26">[27]</ref> have been used rarely and have limited effectiveness.</p><p>In propagation methods, Eq. 1 is reformulated such that it allows propagation of the alpha values from the known foreground and background regions into the unknown region. A popular approach is Closed-form Matting <ref type="bibr" target="#b21">[22]</ref> which is often used as a post-process after sampling <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref>. It derives a cost function from local smoothness assumption on foreground and background colors and finds the globally optimal alpha matte by solving a sparse linear system of equations. Other propagation methods include random walks <ref type="bibr" target="#b13">[14]</ref>, solving Poisson equations <ref type="bibr" target="#b30">[31]</ref>, and nonlocal propagation methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Recently, several deep learning works have been proposed for image matting. However, they do not directly learn an alpha matte given an image and trimap. Shen et al. <ref type="bibr" target="#b28">[29]</ref> use deep learning for creating a trimap of a person in a portrait image and use <ref type="bibr" target="#b21">[22]</ref> for matting through which matting errors are backpropagated to the network. Cho et al. <ref type="bibr" target="#b7">[8]</ref> take the matting results of <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b4">[5]</ref> and normalized RGB colors as inputs and learn an end-to-end deep network to predict a new alpha matte. Although both our algorithm and the two works leverage deep learning, our algorithm is quite different from theirs. Our algorithm directly learns the alpha matte given an image and trimap while the other two works rely on existing algorithms to compute the actual matting, making their methods vulnerable to the same problems as previous matting methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">New matting dataset</head><p>The matting benchmark on alphamatting.com <ref type="bibr" target="#b24">[25]</ref> has been tremendously successful in accelerating the pace of research in matting. However, due to the carefully controlled setting required to obtain ground truth images, the dataset consists of only 27 training images and 8 testing images. Not only is this not enough images to train a neural network, but it is severely limited in its diversity, restricted to small-scale lab scenes with static objects.</p><p>To train our matting network, we create a larger dataset by compositing objects from real images onto new backgrounds. We find images on simple or plain backgrounds ( <ref type="figure" target="#fig_0">Fig. 2a)</ref>, including the 27 training images from <ref type="bibr" target="#b24">[25]</ref> and every fifth frame from the videos from <ref type="bibr" target="#b25">[26]</ref>. Using Photoshop, we carefully manually create an alpha matte ( <ref type="figure" target="#fig_0">Fig. 2b</ref>) and pure foreground colors <ref type="figure" target="#fig_0">(Fig. 2c</ref>). Because these objects have simple backgrounds we can pull accurate mattes for them. We then treat these as ground truth and for each alpha matte and foreground image, we randomly sample N background images in MS COCO <ref type="bibr" target="#b22">[23]</ref> and Pascal VOC <ref type="bibr" target="#b10">[11]</ref>, and composite the object onto those background images.</p><p>We create both a training and a testing dataset in the above way. Our training dataset has 493 unique foreground objects and 49,300 images (N = 100) while our testing dataset has 50 unique objects and 1000 images (N = 20). The trimap for each image is randomly dilated from its ground truth alpha matte. In comparison to previous matting datasets, our new dataset has several advantages. 1) It has many more unique objects and covers various matting cases such as hair, fur, semi-transparency, etc. 2) Many composited images have similar foreground and background colors and complex background textures, making our dataset more challenging and practical.</p><p>An early concern is whether this process would create a bias due to the composited nature of the images, such that a network would learn to key on differences in the foreground and background lighting, noise levels, etc. However, we found experimentally that we achieved far superior results on natural images compared to prior methods (see Sec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our method</head><p>We address the image matting problem using deep learning. Given our new dataset, we train a neural network to fully utilize the data. The network consists of two stages <ref type="figure" target="#fig_1">(Fig. 3)</ref>. The first stage is a deep convolutional encoderdecoder network which takes an image patch and a trimap as input and is penalized by the alpha prediction loss and a novel compositional loss. The second stage is a small fully convolutional network which refines the alpha prediction from the first network with more accurate alpha values and sharper edges. We will describe our algorithm with more details in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Matting encoder-decoder stage</head><p>The first stage of our network is a deep encoder-decoder network (see <ref type="figure" target="#fig_1">Fig. 3</ref>), which has achieved successes in many other computer vision tasks such as image segmentation <ref type="bibr" target="#b1">[2]</ref>, boundary prediction <ref type="bibr" target="#b32">[33]</ref> and hole filling <ref type="bibr" target="#b23">[24]</ref>.</p><p>Network structure: The input to the network is an image patch and the corresponding trimap which are concatenated along the channel dimension, resulting in a 4-channel input. The whole network consists of an encoder network and a decoder network. The input to the encoder network is transformed into downsampled feature maps by subsequent convolutional layers and max pooling layers. The decoder network in turn uses subsequent unpooling layers which reverse the max pooling operation and convolutional layers to upsample the feature maps and have the desired output, the alpha matte in our case. Specifically, our encoder network has 14 convolutional layers and 5 max-pooling layers. For the decoder network, we use a smaller structure than the encoder network to reduce the number of parameters and speed up the training process. Specifically, our decoder network has 6 convolutional layers, 5 unpooling layers followed by a final alpha prediction layer.</p><p>Losses: Our network leverages two losses. The first loss is called the alpha-prediction loss, which is the absolute difference between the ground truth alpha values and the predicted alpha values at each pixel. However, due to the non-differentiable property of absolute values, we use the following loss function to approximate it.</p><formula xml:id="formula_1">L i α = (α i p − α i g ) 2 + ǫ 2 , α i p , α i g ∈ [0, 1].<label>(2)</label></formula><p>where α i p is the output of the prediction layer at pixel i thresholded between 0 and 1. α i g is the ground truth alpha value at pixel i. ǫ is a small value which is equal to 10 −6 in our experiments. The derivative</p><formula xml:id="formula_2">∂L i α ∂α i p is straightforward. ∂L i α ∂α i p = α i p − α i g (α i p − α i g ) 2 + ǫ 2 .<label>(3)</label></formula><p>The second loss is called the compositional loss, which is the absolute difference between the ground truth RGB colors and the predicted RGB colors composited by the ground truth foreground, the ground truth background and the predicted alpha mattes. Similarly, we approximate it by using the following loss function.</p><formula xml:id="formula_3">L i c = (c i p − c i g ) 2 + ǫ 2 .<label>(4)</label></formula><p>where c denotes the RGB channel, p denotes the image composited by the predicted alpha, and g denotes the image composited by the ground truth alphas. The compositional loss constrains the network to follow the compositional operation, leading to more accurate alpha predictions. The overall loss is the weighted summation of the two individual losses, i.e., L overall = w l ·L α +(1−w l )·L c , where w l is set to 0.5 in our experiment. In addition, since only the alpha values inside the unknown regions of trimaps need to be inferred, we therefore set additional weights on the two types of losses according to the pixel locations, which can help our network pay more attention on the important areas. Specifically, w i = 1 if pixel i is inside the unknown region of the trimap while w i = 0 otherwise. Implementation: Although our training dataset has 49,300 images, there are only 493 unique objects. To avoid overfitting as well as to leverage the training data more effectively, we use several training strategies. First, we randomly crop 320×320 (image, trimap) pairs centered on pixels in the unknown regions. This increases our sampling space. Second, we also crop training pairs with different sizes (e.g. 480×480, 640×640) and resize them to 320×320. This makes our method more robust to scales and helps the network better learn context and semantics. Third, flipping is performed randomly on each training pair. Fourth, the trimaps are randomly dilated from their ground truth alpha mattes, helping our model to be more robust to the trimap placement. Finally, the training inputs are recreated randomly after each training epoch.</p><p>The encoder portion of the network is initialized with the first 14 convolutional layers of VGG-16 <ref type="bibr" target="#b29">[30]</ref> (the 14th layer is the fully connected layer "fc6" which can be transformed to a convolutional layer). Since the network has 4-channel input, we initialize the one extra channel of the first-layer convolutional filters with zeros. All the decoder parameters are initialized with Xavier random variables.</p><p>When testing, the image and corresponding trimap are concatenated as the input. A forward pass of the network is performed to output the alpha matte prediction. When a GPU memory is insufficient for large images, CPU testing can be performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Matting refinement stage</head><p>Although the alpha predictions from the first part of our network are already much better than existing matting algorithms, because of the encoder-decoder structure, the results are sometimes overly smooth. Therefore, we extend our network to further refine the results from the first part. This extended network usually predicts more accurate alpha mattes and sharper edges.</p><p>Network structure: The input to the second stage of our network is the concatenation of an image patch and its alpha prediction from the first stage (scaled between 0 and 255), resulting in a 4-channel input. The output is the corresponding ground truth alpha matte. The network is a fully convolutional network which includes 4 convolutional layers. Each of the first 3 convolutional layers is followed by a non-linear "ReLU" layer. There are no downsampling layers since we want to keep very subtle structures missed in the first stage. In addition, we use a "skip-model" structure where the 4-th channel of the input data is first scaled between 0 and 1 and then is added to the output of the network. The detailed configuration is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The effect of our refinement stage is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. Note that it does not make large-scale changes to the alpha matte, but rather just refines and sharpens the alpha values.</p><p>Implementation: During training, we first update the encoder-decoder part without the refinement part. After the encoder-decoder part is converged, we fix its parameters and then update the refinement part. Only the alpha prediction loss (Eqn. 2) is used due to its simple structure. We also use all the training strategies of the 1st stage except the 4th one. After the refinement part is also converged, finally we fine-tune the the whole network together. We use Adam <ref type="bibr" target="#b19">[20]</ref> to update both parts. A small learning rate 10 −5 is set constantly during the training process.</p><p>During testing, given an image and a trimap, our algorithm first uses the matting encoder-decoder stage to get an initial alpha matte prediction. Then the image and the alpha prediction are concatenated as the input to the refinement stage to produce the final alpha matte prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>In this section we evaluate our method on 3 datasets. 1) We evaluate on the alphamatting.com dataset <ref type="bibr" target="#b24">[25]</ref>, which is the existing benchmark for image matting methods. It includes 8 testing images, each has 3 different trimaps, namely, "small", "large" and "user". 2) Due to the limited size and range of objects in the alphamatting.com dataset, we propose the Composition-1k test set. Our compositionbased dataset includes 1000 images and 50 unique foregrounds. This dataset has a wider range of object types and background scenes. 3) To measure our performance on natural images, we also collect a third dataset including 31 natural images. The natural images cover a wide range of common matting foregrounds such as person, animals, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The alphamatting.com dataset</head><p>Our approach achieves the top results compared to all the other methods on the alphamatting.com benchmark. Specifically, our method ranks the 1st place in terms of the SAD metric. Our method also has the smallest SAD errors for 5 images with all the 3 trimaps <ref type="figure" target="#fig_3">(Fig. 5)</ref>. In addition, our method ranks the 2nd place in terms of both the MSE and Gradient metrics. Overall, our method is one of the best performers on this dataset.</p><p>A key reason for our success is our network's ability to learn structure and semantics, which is important for the accurate estimation of alpha matte when the background scene is complex or the background and foreground colors are similar. For example, in <ref type="figure">Fig 6</ref> the "Troll" example has very similar colors of the hair and the bridge while the "Doll" example has strong textured background. The best results of previous methods (from column 3 to column 6) all have very obvious mistakes in those hard regions. In contrast, our method directly learns object structure and image context. As a result, our method not only avoids the similar mistakes made by previous methods but also predicts more details. It is worth noting that although DCNN matting <ref type="bibr" target="#b7">[8]</ref> is also a deep-learning based method, it learns the non-linear combination of previous matting methods within small local patches. Therefore the method cannot really understand semantics and thus has the same limitations as previous non-deep-learning-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The Composition-1k testing dataset</head><p>We further evaluate 7 top performing prior methods and each component of our approach on the Composition-1k testing dataset. For all prior methods, the authors' provided codes are used. The different variants of our approach include: the matting encoder-decoder network 1) with only the alpha prediction loss, 2) with both the alpha prediction loss and the compositional loss, the matting encoder-   <ref type="figure">Figure 6</ref>. The alpha matte predictions of the test images "Troll" with trimap "user" and "Doll" with trimap "small". The first column shows the test images. For each test image, the 1st ranked result to the 5th ranked result under the SAD metric are displayed from column two to column six in decreasing orders. In both examples, our method achieves the best results. decoder network 3) post-processed by the Guided filter <ref type="bibr" target="#b16">[17]</ref> and 4) post-processed by the matting refinement network. The quantitative results under the SAD, MSE, Gradient and Connectivity errors proposed by <ref type="bibr" target="#b24">[25]</ref> are displayed in <ref type="table" target="#tab_0">Table 1</ref>. Clearly all variants of our approach have much better results than the other methods. The main reason is still the capability of our deep model understanding the complex context of images while the other methods cannot. By comparing the variants of our approach, we can also validate the effectiveness of each component of our approach: 1) the compositional loss helps our model learn the compositional operation, and thus leads to better results, 2) the results of our matting encoder-decoder network can be improved by combining with previous edge-preserving filters (e.g. Guided filter <ref type="bibr" target="#b16">[17]</ref>) as well as our matting refinement network. But the latter one has more obvious improvement both visually and quantitatively since it is directly trained with the outputs of our encoder-decoder network.</p><p>We test the sensitivity of our method to trimap placement in <ref type="figure" target="#fig_4">Fig. 7</ref>. We evaluate over a subset of our dataset that includes one randomly-chosen image for each unique object for a total of 50 images. To form the trimap, we dilate the ground truth alpha for each image by d pixels for increasing values of d. The SAD errors at a particular parameter d are averaged over all images. The results of all the methods at parameters d ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> are shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. Clearly our method has a low and stable error rate with the increasing values of d whiles the error rate of the other approaches increases rapidly. Our good performance derives from both our training strategies as well as a good understanding of image context.</p><p>Some visual examples are shown in <ref type="figure" target="#fig_5">Fig. 8</ref> to demonstrate the good performance of our approach on different matting cases such as hair, holes and semi-transparency. Moreover, our approach can also handle objects with no pure foreground pixels, as shown in the last example in <ref type="figure" target="#fig_5">Fig. 8</ref>. Since previous sampling-based and propagation-based methods must leverage known foreground and background pixels, they cannot handle this case, while our approach can learn the appearance of fine details directly from data.  other methods on real images, we conduct a user study on the real image dataset. These images consist of images pulled from the internet as well as images provided by the ICCV 2013 tutorial on image matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The real image dataset</head><p>Because our subjects may not be acquainted with alpha mattes, we instead evaluate the results of compositions. For each method, the computed alpha matte is used to blend the test image onto a black background and onto a white background. For the user test, we present the image and the two composition results of two randomly selected approaches to Image Trimap Shared <ref type="bibr" target="#b12">[13]</ref> Learning <ref type="bibr" target="#b33">[34]</ref> Comprehensive <ref type="bibr" target="#b27">[28]</ref> Global <ref type="bibr" target="#b15">[16]</ref> Closed-form <ref type="bibr" target="#b21">[22]</ref> KNN <ref type="bibr" target="#b4">[5]</ref> DCNN <ref type="bibr" target="#b7">[8]</ref> Ours-refined Image Trimap Shared <ref type="bibr" target="#b12">[13]</ref> Learning <ref type="bibr" target="#b33">[34]</ref> Comprehensive <ref type="bibr" target="#b27">[28]</ref> Global <ref type="bibr" target="#b15">[16]</ref> Closed-form <ref type="bibr" target="#b21">[22]</ref> KNN <ref type="bibr" target="#b4">[5]</ref> DCNN <ref type="bibr" target="#b7">[8]</ref> Ours-refined <ref type="figure">Figure 9</ref>. Example results from our real image dataset. an user and ask which results are more accurate and realistic especially in the regions of fine details (e.g. hair, edges of object, and semi-transparent areas). To avoid evaluation bias, we conduct the user study on the Amazon Mechanical Turk. As a result, there are total 392 users participating the user study and each method pair on one image is evaluated by 5 to 6 unique users. The pairwise comparison results are displayed in Tbl. 2, where each column presents the preference of one approach over the other methods. For example, users preferred our result 83.7% of the time over <ref type="bibr" target="#b12">[13]</ref>. Notably almost 4 out of 5 users prefer our method over the prior methods, which well demonstrates that our method indeed produces better visual results. See <ref type="figure">Fig. 9</ref> for some visual results.</p><p>It is also worth noting that the ranking of other methods differs in this test compared to the other two experiments. For example, Closed-Form Matting <ref type="bibr" target="#b21">[22]</ref> is the lowest ranked method on alphamatting.com of the methods we compare here, yet to users it is preferable to all other methods except our own and <ref type="bibr" target="#b27">[28]</ref>. On the other hand, while DCNN <ref type="bibr" target="#b7">[8]</ref> is the prior state-of-the-art method on alphamatting.com, is only preferred over two methods on the real images. It is unclear whether this is due to methods overfitting the alphamatting.com dataset or whether the standard error metrics fail to accurately measure human perceptual judgment of alpha matting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In order to generalize to natural images, matting algorithms must move beyond using color as a primary cue and leverage more structural and semantic features. In this work, we show that a neural network is capable of capturing such high-order features and applying them to compute improved matting results. Our experiments show that our method does not only outperform prior methods on the standard dataset, but that it generalizes to real images significantly better as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Dataset creation. (a) An input image with a simple background is matted manually. The (b) computed alpha matte and (c) computed foreground colors are used as ground truth to composite the object onto (d-f) various background images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Our network consists of two stages, an encoder-decoder stage (Sec. 4.1) and a refinement stage (Sec. 4.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The effect of our matting refinement network. (a) The input images. (b) The results of our matting encoder-decoder stage. (c) The results of our matting refinement stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. SAD results on the alphamatting.com dataset. The top 5 methods are shown. Our method is emphasized by a red rectangle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The SAD error at different levels of trimap dilation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The visual comparison results on the Composition-1k testing dataset. "Ours-raw' denotes the results of our matting encoderdecoder stage while "Ours-refined" denotes the results of our matting refinement stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>The quantitative results on the Composition-1k testing dataset. The variants of our approaches are emphasized in italic. The best results are emphasized in bold.</figDesc><table>Methods 
SAD MSE Gradient Connectivity 
Shared Matting [13] 
128.9 0.091 126.5 
135.3 
Learning Based Matting [34] 113.9 0.048 
91.6 
122.2 
Comprehensive Sampling [28] 143.8 0.071 102.2 
142.7 
Global Matting [16] 
133.6 0.068 
97.6 
133.3 
Closed-Form Matting [22] 
168.1 0.091 126.9 
167.9 
KNN Matting [5] 
175.4 0.103 124.1 
176.4 
DCNN Matting [8] 
161.4 0.087 115.1 
161.9 
Encoder-Decoder network 
(single alpha prediction loss) 
59.6 0.019 
40.5 
59.3 

Encoder-Decoder network 
54.6 0.017 
36.7 
55.3 
Encoder-Decoder network 
+ Guided filter[17] 
52.2 0.016 
30.0 
52.6 

Encoder-Decoder network 
+ Refinement network 
50.4 0.014 
31.0 
50.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Matting methods should generalize well to real-world images. To validate the performance of our approach and</figDesc><table>Image 

Trimap 
Shared [13] 
Learning [34] Comprehensive[28] 
Global [16] 

Closed-form [22] 
KNN [5] 
DCNN [8] 
Ours-raw 
Ours-refined 
GT 

Image 
Trimap 
Shared [13] 
Learning [34] Comprehensive[28] 
Global [16] 

Closed-form [22] 
KNN [5] 
DCNN [8] 
Ours-raw 
Ours-refined 
GT 

Image 
Trimap 
Shared [13] 
Learning [34] Comprehensive[28] 
Global [16] 

Closed-form [22] 
KNN [5] 
DCNN [8] 
Ours-raw 
Ours-refined 
GT 

Image 
Trimap 
Shared [13] 
Learning [34] Comprehensive[28] 
Global [16] 

Closed-form [22] 
KNN [5] 
DCNN [8] 
Ours-raw 
Ours-refined 
GT 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>The user study on the real image dataset. The preferred method in each pairwise comparison is emphasized in bold.Comprehensive [28] 21.5 39.8 -25.8 43.3 20.4 29.2 78.8 Global [16] 20.4 45.4 74.2 -53.3 30.0 42.0 84.2 Closed-Form [22] 30.3 46.6 56.7 46.7 -25.0 38.1 80.4</figDesc><table>Methods 
[13] [34] [28] [16] [22] [5] [8] Ours 
Shared [13] 
-60.0 78.5 79.6 69.7 40.6 57.8 83.7 
Learning [34] 
40.0 -60.2 54.6 53.4 27.3 35.1 83.6 
KNN [5] 
59.4 72.7 79.6 70.0 75.0 -73.3 97.0 
DCNN [8] 
42.2 64.9 70.8 58.0 61.9 26.7 -
83.7 
Ours 
16.3 16.4 21.2 15.8 19.6 3.0 16.3 
-

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matting with sequential pair selection using graph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Kabbany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Symposium on Vision, Modeling, and Visualization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Method for removing from an image the background surrounding a selected object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dadourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vlahos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Knn matting. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A propagation matting method based on the local sampling and knn classification with adaptive feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Design and Computer Graphics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image matting with local and nonlocal smooth priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<meeting>the 2001 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">264</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptually motivated benchmark for video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vatolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A cluster sampling method for image matting via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shared sampling for realtime alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VIIP</title>
		<meeting>VIIP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative transductive learning for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4282" to="4286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse coding for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Varnousfaderani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image matting with kl-divergence based sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlocal matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporally coherent and spatially accurate video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics</title>
		<meeting>Eurographics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weighted color and texture sample selection for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving image matting using comprehensive sampling sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Poisson matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2002" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning based digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
