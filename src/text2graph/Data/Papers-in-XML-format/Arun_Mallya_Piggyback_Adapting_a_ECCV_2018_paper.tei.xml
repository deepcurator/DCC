<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
							<email>amallya2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Davis</surname></persName>
							<email>ddavis14@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
							<email>slazebni@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Incremental Learning, Binary Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that "piggyback" on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-toend differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Our performance is agnostic to task ordering and we do not suffer from catastrophic forgetting or competition between tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The most popular method used in prior work for training a deep network for a new task or dataset is fine-tuning an established pre-trained model, such as the VGG-16 <ref type="bibr" target="#b0">[1]</ref> trained on ImageNet classification <ref type="bibr" target="#b1">[2]</ref>. A major drawback of finetuning is the phenomenon of "catastrophic forgetting" <ref type="bibr" target="#b2">[3]</ref>, by which performance on the old task degrades significantly as the new task is learned, necessitating one to store specialized models for each task or dataset. For achieving progress towards continual learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, we need better methods for augmenting capabilities of an existing network while avoiding catastrophic forgetting and requiring as few additional parameters as possible.</p><p>Prior methods for avoiding catastrophic forgetting, such as Learning without Forgetting (LwF) <ref type="bibr" target="#b5">[6]</ref> and Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b3">[4]</ref>, maintain performance on older tasks through proxy losses and regularization terms while modifying network weights. Another recent work, PackNet <ref type="bibr" target="#b6">[7]</ref>, adopts a different route of iteratively pruning unimportant weights and fine-tuning them for learning new tasks. As a result of pruning and weight modifications, a binary parameter usage mask is produced by PackNet. We question whether the weights of a network have to be changed at all to learn a new task, or whether we can get competitive with the best methods <ref type="bibr" target="#b15">[16]</ref> on the Visual Decathlon challenge <ref type="bibr" target="#b16">[17]</ref> while using the least amount of additional parameters. Finally, we show that our method can be used to train a fully convolutional network for semantic segmentation starting from a classification backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While multiple prior works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> have explored multi-task training, wherein data of all tasks is available at the time of training, we consider the setting in which new tasks are available sequentially, a more realistic and challenging scenario. Prior work under this setting is based on Learning without Forgetting (LwF) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref> and Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>. LwF uses initial network responses on new data as regularization targets during new task training, while EWC imposes a smooth penalty on changing weights deemed to be important to prior tasks. An issue with these methods is that it is not possible to determine the change in performance on prior tasks beforehand since all weights of the network are allowed to be modified to varying degrees. PackNet <ref type="bibr" target="#b6">[7]</ref> avoids this issue by identifying weights important for prior tasks through network pruning, and keeping the important weights fixed after training for a particular task. Additional information is stored per weight parameter of the network to indicate which tasks it is used by. However, for each of these methods, performance begins to drop as many tasks are added to the network. In the case of LwF, a large domain shift for a new task causes significant drop in prior task performance <ref type="bibr" target="#b5">[6]</ref>. For PackNet, performance on a task drops as it is added later to the network due to the lack of available free parameters, and the total number of tasks that can be added is ultimately limited due to the fixed size of the network <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our proposed method does not change weights of the initial backbone network and learns a different mask per task. As a result, it is agnostic to task ordering and the addition of a task does not affect performance on any other task. Further, an unlimited number of tasks can piggyback onto a backbone network by learning a new mask. The parameter usage masks in PackNet were obtained as a by-product of network pruning <ref type="bibr" target="#b22">[23]</ref>, but we learn appropriate masks based on the task at hand. This idea of masking is related to PathNet <ref type="bibr" target="#b23">[24]</ref>, which learns selective routing through neurons using evolutionary strategies. We achieve similar behavior through an end-to-end differentiable method, which is less computationally demanding. The learning of separate masks per task decouples the learning of multiple tasks, freeing us from having to choose hyperparameters such as batch mixing ratios <ref type="bibr" target="#b19">[20]</ref>, pruning ratios <ref type="bibr" target="#b6">[7]</ref>, and cost weighting <ref type="bibr" target="#b5">[6]</ref>.</p><p>Similar to our proposed method, another set of methods adds new tasks by learning additional task-specific parameters. For a new task, Progressive Neural Networks <ref type="bibr" target="#b24">[25]</ref> duplicates the base architecture while adding lateral connections to layers of the existing network. The newly added parameters are optimized for the new task, while keeping old weights fixed. This method incurs a large overhead as the network is replicated for the number of tasks added. The method of Residual Adapters <ref type="bibr" target="#b16">[17]</ref> develops on the observation that linearly parameterizing a convolutional filter bank of a network is the same as adding an additional per-task convolutional layer to the network. The most recent Deep Adaptation Networks (DAN) <ref type="bibr" target="#b15">[16]</ref> allows for learning new filters that are linear combinations of existing filters. Similar to these methods, we enable the learning of new pertask filters. However, these new filters are constrained to be masked versions of existing filters. Our learned binary masks incur an overhead of 1 bit per network parameter, smaller than all of the prior work. Further, we do not find it necessary to learn task-specific layer biases and batch normalization parameters.</p><p>Our method for training binary masks is based on the technique introduced by Courbariaux et al. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> for the training of a neural network with binaryvalued weights from scratch. The authors maintain a set of real-valued weights that are passed through a binarizer function during the forward pass. Gradients are computed with respect to the binarized weights during the backward pass through the application of the chain rule, and the real-valued weights are updated using the gradients computed for the binarized versions. In <ref type="bibr" target="#b25">[26]</ref>, the authors argue that even though the gradients computed in this manner are noisy, they effectively serve as a regularizer and quantization errors cancel out over multiple iterations. Subsequent work including <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> has extended this idea to ternaryvalued weights. Unlike these works, we do not train a quantized network from scratch but instead learn quantized masks that are applied to fixed, real-valued filter weights. Work on sparsifying dense neural networks, specifically <ref type="bibr" target="#b29">[30]</ref>, has used the idea of masked weight matrices. However, only their weight matrix was trainable and their mask values were a fixed function of the magnitude of the weight matrix and not explicitly trainable. In contrast, we treat the weight matrix of the backbone network as a fixed constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The key idea behind our method is to learn to selectively mask the fixed weights of a base network, so as to improve performance on a new task. We achieve this by maintaining a set of real-valued weights that are passed through a deterministic thresholding function to obtain binary masks, that are then applied to existing weights. By updating the real-valued weights through backpropagation, we hope to learn binary masks appropriate for the task at hand. This process is illustrated in <ref type="figure">Figure 1</ref>. By learning different binary-valued {0, 1} masks per task, which are element-wise applied to network parameters, we can re-use the same underlying base network for multiple tasks, with minimal overhead. Even though we do not modify the weights of the network, a large number of different filters can be obtained through masking. In practice, we begin with a network such as the VGG-16 or ResNet-50 pre-trained on the ImageNet classification task as our base network, referred to as the backbone network, and associate a real-valued mask variable with each weight parameter of all the convolutional and fully-connected layers. By combining techniques used in network binariza-tion <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> and pruning <ref type="bibr" target="#b29">[30]</ref>, we train these mask variables to learn the task at hand in an end-to-end fashion, as described in detail below. The choice of the initialization of the backbone network is crucial for obtaining good performance, and is further analyzed in Section 5.1.</p><p>For simplicity, we describe the mask learning procedure using the example of a fully-connected layer, but this idea can easily be extended to a convolutional layer as well. Consider a simple fully-connected layer in a neural network. Let the input and output vectors be denoted by</p><formula xml:id="formula_0">x = (x 1 , x 2 , · · · , x m )</formula><p>T of size m × 1, and y = (y 1 , y 2 , · · · , y n )</p><p>T of size n × 1, respectively. Let the weight matrix of the layer be W = [w] ji of size n × m. The input-output relationship is then given by y = Wx, or y j = m i=1 w ji · x i . The bias term is ignored for ease of notation. Let δv denote the partial derivative of the error function E with respect to the variable v. The backpropagation equation for the weights W of this fully-connected layer is given by</p><formula xml:id="formula_1">δw ji ∂E ∂w ji = ∂E ∂y j · ∂y j ∂w ji (1) = δy j · x i (2) ∴ δW ∂E ∂w ji = δy · x T ,<label>(3)</label></formula><p>where δy = (δy 1 , δy 2 , · · · , δy n ) T is of size n × 1. Our modified fully-connected layer associates a matrix of real-valued mask weights m r = [m r ] ji with every weight matrix W, of the same size as W (n×m), as indicated by the rightmost filter in <ref type="figure">Figure 1</ref>. We obtain thresholded mask matrices m = [m] ji by passing the real-valued mask weight matrices m r through a hard binary thresholding function given by</p><formula xml:id="formula_2">m ji = 1, if m r ji ≥ τ 0, otherwise ,<label>(4)</label></formula><p>where τ is a selected threshold. The binary-valued matrix m activates or switches off contents of W depending on whether a particular value m ji is 0 or 1. The layer's input-output relationship is given by the equation</p><formula xml:id="formula_3">y = (W ⊙ m) x, or y j = m i=1 w ji ·m ji ·x i ,</formula><p>where ⊙ indicates elementwise multiplication or masking. As mentioned previously, we set the weights W of our modified layer to those from the same architecture pre-trained on a task such as ImageNet classification. We treat the weights W as fixed constants throughout, while only training the real-valued mask weights m r . The backpropagation equation for the thresholded mask weights m of this fully-connected layer is given by</p><formula xml:id="formula_4">δm ji ∂E ∂m ji = ∂E ∂y j · ∂y j ∂m ji (5) = δy j · w ji · x i (6) ∴ δm ∂E ∂m ji = (δy · x T ) ⊙ W.<label>(7)</label></formula><p>Even though the hard thresholding function is non-differentiable, the gradients of the thresholded mask values m serve as a noisy estimator of the gradients of the real-valued mask weights m r , and can even serve as a regularizer, as shown in prior work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. We thus update the real-valued mask weights m r using gradients computed for m, the thresholded mask values. After adding a new final classification layer for the new task, the entire system can be trained in an end-to-end differentiable manner. In our experiments, we did not train per-task biases as prior work <ref type="bibr" target="#b6">[7]</ref> showed that this does not have any significant impact on performance. We also did not train per-task batch-normalization parameters for simplicity. Section 5.3 analyzes the benefit of training per-task batchnorm parameters, especially for tasks with large domain shifts.</p><p>After training a mask for a given task, we no longer require the real-valued mask weights. They are discarded, and only the thresholded masks associated with the backbone network layers are stored. A typical neural network parameter is represented using a 32-bit float value (including in our PyTorch implementation). A binary mask only requires 1 extra bit per parameter, leading to an approximate per-task overhead of 1/32 or 3.12% of the backbone network size.</p><p>Practical optimization details. From Eq. 7, we observe that |δm|, |δm r | ∝ |W|. The magnitude of pre-trained weights varies across layers of a network, and as a result, the mask gradients would also have different magnitudes at different layers. This relationship requires us to be careful about the manner in which we initialize and train mask weights m r . There are two possible approaches: 1) Initialize m r with values proportional to the weight matrix W of the corresponding layer. In this case, the ratio |δm r |/|m r | will be similar across layers, and a constant learning rate can be used for all layers.</p><p>2) Initialize m r with a constant value, such as 0.01, for all layers. This would require a separate learning rate per layer, due to the scaling of the mask gradient by the layer weight magnitude. While using SGD, scaling gradients obtained at each layer by a factor of 1/avg(|W|), while using a constant learning rate, has the same effect as layer-dependent learning rates. Alternatively, one could use adaptive optimizers such as Adam, which would learn appropriate scaling factors.</p><p>The second initialization approach combined with the Adam optimizer produced the best results, with a consistent gain in accuracy by ∼ 2% compared to the alternatives. We initialized the real-valued weights with a value of 1e-2 with a binarizer threshold (τ , in Equation 4) of 5e-3 in all our experiments. Randomly initializing the real-valued mask weights such that the thresholded binary masks had an equal number of 0s and 1s did not give very good performance. Ensuring that all thresholded mask values were 1 provides the same network initialization as that of the baseline methods.</p><p>We also tried learning ternary masks {−1, 0, 1} by using a modified version of Equation 4 with two cut-off thresholds, but did not achieve results that were significantly different from those obtained with binary masks. As a result, we only focus on results obtained with binary masks in the rest of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We consider a wide variety of datasets, statistics of which are summarized in <ref type="table" target="#tab_1">Table 1</ref>, to evaluate our proposed method. Similar to PackNet <ref type="bibr" target="#b6">[7]</ref>, we evaluate our method on two large-scale datasets, the ImageNet object classification dataset <ref type="bibr" target="#b1">[2]</ref> and the Places365 scene classification dataset <ref type="bibr" target="#b30">[31]</ref>, each of which has over a million images, as well as the CUBS <ref type="bibr" target="#b7">[8]</ref>, Stanford Cars <ref type="bibr" target="#b8">[9]</ref>, and Flowers <ref type="bibr" target="#b9">[10]</ref> fine-grained classification datasets. Further, we include two more datasets with significant domain shifts from the natural images of ImageNet, the WikiArt Artists classification dataset, created from the WikiArt dataset <ref type="bibr" target="#b10">[11]</ref>, and the Sketch classifcation dataset <ref type="bibr" target="#b11">[12]</ref>. The former includes a wide genre of painting styles, as shown in <ref type="figure" target="#fig_2">Figure 2a</ref>, while the latter includes black-and-white sketches drawn by humans, as shown in <ref type="figure" target="#fig_2">Figure 2b</ref>. For all these datasets, we use networks with an input image size of 224 × 224 px.     <ref type="table">Table 2</ref> reports the errors obtained on fine-grained classification tasks by learning binary-valued piggyback masks for a VGG-16 network pre-trained on ImageNet classification. The first baseline considered is Classifier Only, which only trains a linear classifier using fc7 features extracted from the pre-trained VGG-16 network. This is a commonly used method that has low overhead as all layers except for the last classification layer are re-used amongst tasks. The second and more powerful baseline is Individual Networks, which finetunes a separate network per task. We also compare our method to the recently introduced PackNet <ref type="bibr" target="#b6">[7]</ref> method, which adds multiple tasks to a network through iterative pruning and re-training. We train all methods for 30 epochs. We train the piggyback and classifier only, using the Adam optimizer with an initial learning rate of 1e-4, which is decayed by a factor of 10 after 15 epochs. We found SGDm with an initial learning rate of 1e-3 to work better for the individual VGG network baseline. For PackNet, we used a 50% pruned initial network trained with SGDm with an initial learning rate of 1e-3 using the same decay scheme as before. We prune the network by 75% and re-train for 15 epochs with a learning rate of 1e-4 after each new task is added. All errors are averaged over 3 independent runs.  <ref type="table">Table 2</ref>: Errors obtained by starting from an ImageNet-trained VGG-16 network and then using various methods to learn new fine-grained classification tasks. PackNet performance is sensitive to order of task addition, while the rest, including our proposed method, are agnostic. ↓ and ↑ indicate that tasks were added in the CUBS → Sketch, and Sketch → CUBS order, resp. Values in parentheses are top-5 errors, rest are top-1 errors.</p><p>As seen in <ref type="table">Table 2</ref>, training individual networks per task clearly provides a huge benefit over the classifier only baseline for all tasks. PackNet significantly improves over the classifier only baseline, but begins to suffer when more than 3 tasks are added to a single network. As PackNet is sensitive to the ordering of tasks, we try two settings -adding tasks in order from CUBS to Sketch (top to bottom in <ref type="table">Table 2</ref>), and the reverse. The order of new task addition has a large impact on the performance of PackNet, with errors increasing by 4-7% as the addition of a task is delayed from first to last (fifth). The error on ImageNet is also higher in the case of PackNet, due to initial network pruning. By training binary piggyback masks, we are able to obtain errors slightly lower than the individual network case. We believe that this is due to the regularization effect caused by the constrained filter modification allowed by our method. Due to the learning of independent masks per task, the obtained performance is agnostic to the ordering of new tasks, albeit at a slightly higher storage overhead as compared to PackNet. The number of weights switched off varies per layer and by dataset depending on its similarity to the ImageNet dataset. This effect is further examined in Section 5.2.</p><p>While the results above were obtained by adding multiple smaller fine-grained classification tasks to a network, the next set of results in <ref type="table" target="#tab_4">Table 3</ref> examines the effect of adding a large-scale dataset, the Places365 scene classification task with 1.8M images, to a network. Here, instead of the Classifier Only baseline, we compare against the Jointly Trained Network of <ref type="bibr" target="#b30">[31]</ref>, in which a single network is simultaneously trained for both tasks. Both PackNet and Piggyback were trained for 20 epochs on Places365. Once again, we are able to achieve close to best-case performance on the Places365 task, obtaining top-1 errors within 0.36% of the individual network, even though the baselines were trained for 60-90 epochs <ref type="bibr" target="#b30">[31]</ref>. The performance is comparable to PackNet, and for the case of adding just one task, both incur a similar overhead.   The previous results were obtained using the large VGG-16 network, and it is not immediately obvious whether the piggyback method would work for much deeper networks that have batch normalization layers. Masking out filter weights can change the average magnitude of activations, requiring changes to batchnorm parameters. We present results obtained with a VGG-16 network with batch normalization layers, the ResNet-50, and DenseNet-121 networks in <ref type="table" target="#tab_5">Table 4</ref>. We observe that the method can be applied without any changes to these network architectures with batchnorm, residual, and skip connections. In the presented results, we do not learn task-specific batchnorm parameters. We however notice that the deeper a network gets, the larger the gap between the performance of piggyback and individual networks. For the VGG-16 architecture, piggyback can often do as well as or better than individual models, but for the ResNet and DenseNet architectures, the gap is ∼2%. In Section 5.3 we show that learning task-specific batchnorm parameters in the case of datasets that exhibit a large domain shift, such as WikiArt, for which the performance gap is 4-5% (as seen in <ref type="table" target="#tab_5">Table 4</ref>), helps further close the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Does Initialization Matter?</head><p>Here, we analyze the importance of the initialization of the backbone network. It is well known that training a large network such as the VGG-16 from scratch on a small dataset such as CUBS, or Flowers leads to poor performance, and the most popular approach is to fine-tune a network pre-trained on the ImageNet classification task. It is not obvious whether initialization is just as important for the piggyback method. <ref type="table" target="#tab_7">Table 5</ref> presents the errors obtained by training piggyback masks for tasks using the ResNet-50 as the backbone network, but with different initializations. We consider 3 different initializations: 1) a network trained on the ImageNet classification task, the popular initialization for fine-tuning, 2) a network trained from scratch on the Places365 scene classification task, a dataset larger than ImageNet (1.8 M v/s 1.3 M images), but with fewer classes (365 v/s 1000), and lastly 3) a randomly initialized network.</p><p>We observe in <ref type="table" target="#tab_7">Table 5</ref> that initialization does indeed matter, with the ImageNetinitialized network outperforming both the Places365 and randomly initialized network on all tasks. In fact, by training a piggyback mask for the Places365 dataset on an ImageNet-initialized backbone network, we obtain an accuracy very similar to a network trained from scratch on the Places365 dataset. The ImageNet dataset is very diverse, with classes ranging from animals, to plants, cars and other inanimate objects, whereas the Places365 dataset is solely devoted to the classification of scenes such as beaches, bedrooms, restaurants, etc. As a result, the features of the ImageNet-trained network serve as a very general and flexible initialization A very interesting observation is that even a randomly initialized network obtains non-trivial accuracies on all datasets. This indicates the learning a mask is indeed a powerful technique of utilizing fixed filters and weights for adapting a network to a new task.   <ref type="table">Table 6</ref>: Percentage of zeroed out weights after training a binary mask for the respective network architectures and datasets. <ref type="table">Table 6</ref> reports the total sparsity, or the number of mask values set to 0 in a binary piggyback mask learned for the corresponding choice of dataset and network architecture. This measures the amount of change that is required to be made to the backbone network, or the deviation from the ImageNet pretrained initialization, in order to obtain good performance on a given dataset. We note that the amount of sparsity obtained on fine-grained datasets seems to be proportional to the errors obtained by the Classifier Only method on the respective datasets. The easiest Flowers dataset requires the least number of changes, or a sparsity of 4.51%, while the harder WikiArt dataset leads to a 34.14% sparsity for a VGG-16 network mask. Across network architectures, we observe a similar pattern of sparsity based on the difficulty of the tasks. The sparsity obtained is also a function of the magnitude of the real-valued mask initialization and threshold used for the binarization (See Equation 4), with a higher threshold leading to higher sparsity. The numbers in <ref type="table">Table 6</ref> were obtained using our default settings of a binarizer threshold of 5e-3 and a uniform real-valued mask initialization of 1e-2. We observe that a Places365-initialized network requires more changes as compared to an ImageNet-initialized network (refer to the ResNet-50 column of <ref type="table">Table 6</ref>). This once again indicates that features learned on ImageNet are more diverse and serve as better initialization than those learned on Places365. <ref type="figure" target="#fig_3">Figure 3</ref> shows the sparsity obtained per layer of the ImageNet pre-trained VGG-16 network, for three datasets considered. While the total amount of sparsity obtained per dataset is different, we observe a consistent pattern of sparsity across the layers. In general, the number of changes increases with depth of the network layer. For datasets similar to ImageNet, such as CUBS, and Flowers, we observe that the low-level features (conv1-conv3) are mostly re-used without any major changes. WikiArt, which has a significant domain shift from ImageNet, requires some changes in the low-level features. All tasks seem to require changes to the mid-level (conv4-conv5) and high-level features (fc6-fc7) in order to learn new task-specific features. Similar behavior was also observed for the deeper ResNet and DenseNet networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learned sparsity and its distribution across network layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Handling large input domain shifts</head><p>In <ref type="table" target="#tab_5">Table 4</ref>, we observe that WikiArt, which has a large domain shift from the ImageNet dataset on which the backbone network was trained on, has a larger gap in performance (4-5%) between the piggyback and individual network methods, especially for the deeper ResNet and DenseNet networks. Those numbers are duplicated in the Piggyback -Fixed BN and Individual Network columns of <ref type="table" target="#tab_8">Table 7</ref>. We suspect that keeping batchnorm parameters fixed while training the piggyback masks might be a reason for the gap in performance, as the domain shift is likely to cause a larger discrepancy between the ideal batchnorm parameter values and those inherited from ImageNet, the effect of which is cascaded through the large number of layers. We performed these experiments again, but while updating batchnorm parameters, and report the results in the Piggyback -Trained BN column of   date. For the Sketch dataset, training separate batchnorm parameters leads to a small decrease in error. Task-specific batchnorm parameters thus help improve performance, while causing a small increase of ∼1 MB in the storage overhead for both networks considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results on Visual Decathlon &amp; Semantic Segmentation</head><p>We also evaluate our proposed method on the newly introduced Visual Decathlon challenge <ref type="bibr" target="#b16">[17]</ref> consisting of 10 classification tasks. While the images of this task are of a lower resolution (72 × 72 px), they contain a wide variety of tasks such as pedestrian, digit, aircraft, and action classification, making it perfect for testing the generalization abilities of our method. Evaluation on this challenge reports per-task accuracies, and assigns a cumulative score with a maximum value of 10,000 (1,000 per task) based on the per-task accuracies. The goal is to learn models for maximizing the total score over the 10 tasks while using the least number of parameters. Complete details about the challenge settings, evaluation, and datasets used can be found at http://www.robots.ox.ac.uk/~vgg/decathlon/. <ref type="table" target="#tab_10">Table 8</ref> reports the results obtained on the online test set of the challenge. Consistent with prior work <ref type="bibr" target="#b16">[17]</ref>, we use a Wide Residual Network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> with a depth of 28, widening factor of 4, and a stride of 2 in the first convolutional layer of each block. We use the 64 × 64 px ImageNet-trained network of <ref type="bibr" target="#b15">[16]</ref> as our backbone network, and train piggyback masks for the remaining 9 datasets. We train for a total of 60 epochs per dataset, with learning rate decay by a factor of 10 after 45 epochs. Adam with a base learning rate of 1e-4 was used for updating the real-valued piggyback masks. Data augmentation by random cropping, horizontal flipping, and resizing the entire image was chosen based on cross-validation. As observed in <ref type="table" target="#tab_10">Table 8</ref>, our method obtains performance competitive with the state-of-the-art, while using the least amount of additional parameters. Assuming that the base network uses 32-bit parameters, it accounts for a parameter cost of 32n bits, where n is the number of parameters. A binary mask per dataset requires n bits, leading to a total cost of approximately (32n + 9n) = 41n bits, or a parameter ratio of (41/32) = 1.28, as reported. The results presented in Section 4 only required a single fully connected layer to be added on top of the backbone network. Our method can also be extended to cases where more than one layers are added and trained from scratch on top of a backbone network, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. We tested our method on the task of pixelwise segmentation using the basic Fully Convolutional Network architecture <ref type="bibr" target="#b31">[32]</ref> which has fully connected layer followed by a deconvolutional layer of stride 32. We trained our networks on the 21-class PASCAL 2011 + SBD dataset, using the official splits provided by <ref type="bibr" target="#b32">[33]</ref> for 15 epochs. Using the VGG-16 finetuned network, we obtain a mean IOU of 61.08</p><p>1 . Using the piggyback method, we obtain a competitive mean IOU of 61.41. Instead of replicating the whole VGG-16 network of ∼500 MB, we only need an overhead of 17 MB for masking the backbone network and 7.5 MB for the newly added layers. These results show that piggyback does not face any issues due to mixed training schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a novel method for utilizing the fixed weights of a network for obtaining good performance on a new task, empirically showing that the proposed method works for multiple datasets and network architectures. We hope that the piggyback method will be useful in practical scenarios where new skills need to be learned on a deployed device without having to modify existing weights or download a new large network. The re-usability of the backbone network and learned masks should help simplify and scale the learning of a new task across large numbers of potential users and devices. One drawback of our current method is that there is no scope for added tasks to benefit from each other. Apart from addressing this issue, another interesting area for future work is the extension to tasks such as object detection that require specialized layers, and expanding existing layers with more capacity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Datasets unlike ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Percentage of weights masked out per ImageNet pre-trained VGG-16 layer. Datasets similar to ImageNet share a lot of the lower layers, and require fewer changes. The number of masked out weights increases with depth of layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Mixed training of layers using finetuning from scratch and piggyback masking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Summary of datasets used.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Adding a large-scale dataset to an ImageNet-trained VGG-16 network. Values in parentheses are top-5 errors, rest are top-1 errors. * indicates models downloaded from https://github.com/CSAILVision/places365, trained by [31].</figDesc><table>Dataset 
Classifier PackNet [7] Piggyback Individual 
Only 
↓ 
↑ 
(ours) 
Networks 
VGG-16 BN 

ImageNet 
26.63 
27.18 
26.63 
26.63 
(8.49) 
(8.69) 
(8.49) 
(8.49) 
CUBS 
33.88 
20.21 23.82 
18.37 
19.57 
Stanford Cars 
51.62 
14.05 17.60 
9.87 
9.41 
Flowers 
19.38 
7.82 
7.85 
4.84 
4.55 
WikiArt 
48.05 
30.21 29.59 
27.50 
26.68 
Sketch 
59.96 
25.47 23.53 
21.41 
21.92 
# Models (Size) 1 (537 MB) 1 (587 MB) 1 (621 MB) 6 (3,222 MB) 
ResNet-50 

ImageNet 
23.84 
24.29 
23.84 
23.84 
(7.13) 
(7.18) 
(7.13) 
(7.13) 
CUBS 
29.97 
19.59 28.62 
18.41 
17.17 
Stanford Cars 
47.20 
13.89 19.99 
10.38 
8.17 
Flowers 
14.01 
6.96 
9.45 
5.23 
3.44 
WikiArt 
44.40 
30.60 29.69 
28.67 
24.40 
Sketch 
49.14 
23.83 21.30 
20.09 
19.22 
# Models (Size) 1 (94 MB) 
1 (103 MB) 1 (109 MB) 6 (564 MB) 
DenseNet-121 

ImageNet 
25.56 
25.60 
25.56 
25.56 
(8.02) 
(7.89) 
(8.02) 
(8.02) 
CUBS 
26.55 
19.26 30.36 
19.50 
18.08 
Stanford Cars 
43.19 
15.35 22.09 
10.87 
8.64 
Flowers 
16.56 
8.94 
8.46 
5.31 
3.49 
WikiArt 
45.08 
33.66 30.81 
29.56 
23.59 
Sketch 
46.88 
25.35 21.08 
20.30 
19.48 
# Models (Size) 1 (28 MB) 
1 (31 MB) 
1 (33 MB) 
6 (168 MB) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Results on other network architectures. Values in parentheses are top-5 errors, rest are top-1 errors. ↑ and ↓ indicate order of task addition for PackNet.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Errors obtained by piggyback masks for the ResNet-50 backbone net-
work with different initializations. Errors in parentheses are top-5 errors, the 
rest are top-1 errors. 

Dataset 
VGG-16 
VGG-16 
ResNet-50 
Dense-
BN 
ImNet-init. Places-init. Net-121 
CUBS 
14.09% 
13.24% 
12.21% 
15.22% 
12.01% 
Stanford Cars 
17.03% 
16.70% 
15.65% 
17.72% 
15.80% 
Flowers 
4.51% 
4.52% 
4.48% 
6.45% 
5.28% 
WikiArt 
34.14% 
33.01% 
30.47% 
30.04% 
29.11% 
Sketch 
27.23% 
26.05% 
23.04% 
24.23% 
22.24% 
ImageNet 
-
-
-
37.59% 
-
Places365 
43.47% 
-
37.99% 
-
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 .</head><label>7</label><figDesc>The top-1 error on WikiArt reduces from 28.67% to 25.92% for the ResNet-50 network, and from 29.56% to 25.90% for the DenseNet-121 network if the batchnorm parameters are allowed to up-</figDesc><table>Dataset 
Piggyback (ours) 
Individual 
Fixed BN Trained BN Network 
ResNet-50 
WikiArt 
28.67 
25.92 
24.40 
Sketch 
20.09 
19.82 
19.22 

DenseNet-121 
WikiArt 
29.56 
25.90 
23.59 
Sketch 
20.30 
20.12 
19.48 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Effect of task-specific batch normalization layers on the top-1 error.#par ImNet. Airc. C100 DPed DTD GTSR Flwr Oglt SVHN UCF Mean Score</figDesc><table>Method 
Scratch [17] 
10 
59.87 
57.1 75.73 
91.2 
37.77 
96.55 
56.3 88.74 96.63 
43.27 70.32 
1625 
Feature [17] 
1 
59.67 
23.31 63.11 80.33 45.37 
68.16 73.69 58.79 43.54 
26.8 
54.28 
544 
Finetune [17] 
10 
59.87 
60.34 82.12 92.82 55.53 
97.53 81.41 87.69 96.55 
51.2 
76.51 
2500 
Res. Adapt. [17] 
2 
59.67 
56.68 81.2 
93.88 50.85 
97.05 66.24 89.62 96.13 
47.45 73.88 
2118 
Res. Adapt. (J) [17] 
2 
59.23 
63.73 81.31 
93.3 
57.02 
97.47 83.43 89.82 96.17 
50.28 77.17 
2643 
DAN [16] 
2.17 
57.74 
64.12 80.07 
91.3 
56.54 
98.46 86.05 89.67 96.77 
49.38 77.01 
2851 
Piggyback (Ours) 
1.28 
57.69 
65.29 79.87 96.99 57.45 
97.27 79.09 87.63 97.24 
47.48 76.60 
2838 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Top-1 accuracies obtained on the Visual Decathlon online test set.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is lower than the 63.6 mIOU obtained by [32] owing to differences in the Caffe and PyTorch VGG-16 initializations, as documented at https://goo.gl/quvmm2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<title level="m">Overcoming catastrophic forgetting in neural networks</title>
		<imprint>
			<publisher>PNAS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Encoder based lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05769</idno>
		<title level="m">PackNet: Adding multiple tasks to a single network by iterative pruning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCVGIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale classification of fine-art paintings: Learning the right metric on the right feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICDMW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<title level="m">How do humans sketch objects? In: SIGGRAPH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. In: BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04228</idno>
		<title level="m">Incremental learning through deep adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Incremental learning of object detectors without catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<title level="m">PathNet: Evolution channels gradient descent in super neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<title level="m">Binaryconnect: Training deep neural networks with binary weights during propagations. In: NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04711</idno>
		<title level="m">Ternary weight networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Trained ternary quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">BerekeleyVision: Segmentation data splits</title>
		<ptr target="https://github.com/shelhamer/fcn.berkeleyvision.org/tree/master/data/pascalAccessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
