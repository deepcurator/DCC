<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curriculum Dropout</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Analysis &amp; Computer Vision (PAVIS)</orgName>
								<orgName type="institution">Istituto Italiano di Tecnologia -Genova</orgName>
								<address>
									<postCode>16163</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Cavazza</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Analysis &amp; Computer Vision (PAVIS)</orgName>
								<orgName type="institution">Istituto Italiano di Tecnologia -Genova</orgName>
								<address>
									<postCode>16163</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN)</orgName>
								<orgName type="institution">Università degli Studi di Genova -Genova</orgName>
								<address>
									<postCode>16145</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Analysis &amp; Computer Vision (PAVIS)</orgName>
								<orgName type="institution">Istituto Italiano di Tecnologia -Genova</orgName>
								<address>
									<postCode>16163</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN)</orgName>
								<orgName type="institution">Università degli Studi di Genova -Genova</orgName>
								<address>
									<postCode>16145</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
							<email>rvidal@cis.jhu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedial Engineering</orgName>
								<orgName type="institution">Johns Hopkins University -Baltimore</orgName>
								<address>
									<postCode>21218</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
							<email>vittorio.murino@iit.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Analysis &amp; Computer Vision (PAVIS)</orgName>
								<orgName type="institution">Istituto Italiano di Tecnologia -Genova</orgName>
								<address>
									<postCode>16163</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Università di Verona -Verona</orgName>
								<address>
									<postCode>37134</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Curriculum Dropout</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since <ref type="bibr" target="#b16">[17]</ref>, deep neural networks have become ubiquitous in most computer vision applications. The reason is generally ascribed to the powerful hierarchical feature representations directly learnt from data, which usually outperform classical hand-crafted feature descriptors.</p><p>As a drawback, deep neural networks are difficult to train <ref type="figure">Figure 1</ref>. From left to right, during training (red arrows), our curriculum dropout gradually increases the amount of Bernoulli multiplicative noise, generating multiple partitions (orange boxes) within the dataset (yellow frame) and the feature representation layers (not shown here). Differently, the original dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> (blue arrow) mainly focuses on the hardest partition only, complicating the learning from the beginning and potentially damaging the network classification performance.</p><p>because non-convex optimization and intensive computations for learning the network parameters. Relying on availability of both massive data and hardware resources, the aforementioned training challenges can be empirically tackled and deep architectures can be effectively trained in an end-to-end fashion, exploiting parallel GPU computation. However, overfitting remains an issue. Indeed, such a gigantic number of parameters is likely to produce weights that are so specialized to the training examples that the network's generalization capability may be extremely poor.</p><p>The seminal work of <ref type="bibr" target="#b12">[13]</ref> argues that overfitting occurs as the result of excessive co-adaptation of feature detectors which manage to perfectly explain the training data. This leads to overcomplicated models which unsatisfactory fit unseen testing data points. To address this issue, the Dropout algorithm was proposed and investigated in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> and is nowadays extensively used in training neural networks. The method consists in randomly suppressing neurons during training according to the values r sampled from a Bernoulli distribution. More specifically, if r = 1 that unit is kept unchanged, while if r=0 the unit is suppressed. The effect of suppressing a neuron is that the value of its output is set to zero during the forward pass of training, and its weights are not updated during the backward pass. One one forward-backward pass is completed, a new sample of r is drawn from each neuron, and another forward-backward pass is done and so on till convergence. At testing time, no neuron is suppressed and all activations are modulated by the mean value of the Bernoulli distribution. The resulting model is in fact often interpreted as an average of multiple models, and it is argued that this improves its generalization ability <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Leveraging on the Dropout idea, many works have proposed variations of the original strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref>. However, it is still unclear which variation improves the most with respect to the original dropout formulation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. In many works (such as <ref type="bibr" target="#b21">[22]</ref>) there is no real theoretical justification of the proposed approach other than favorable empirical results. Therefore, providing a sound justification still remains an open challenge. In addition, the lack of publicly available implementations (e.g., <ref type="bibr" target="#b19">[20]</ref>) make fair comparisons problematic.</p><p>The point of departure of our work is the intuition that the excessive co-adaptation of feature detectors, which leads to overfitting, are very unlikely to occur in the early epochs of training. Thus, Dropout seems unnecessary at the beginning of training. Inspired by these considerations, in this work we propose to dynamically increase the number of units that are suppressed as a function of the number of gradient updates. Specifically, we introduce a generalization of the dropout scheme consisting of a temporal scheduling -a curriculum -for the expected number of suppressed units. By adapting in time the parameter of the Bernoulli distribution used for sampling, we smoothly increase the suppression rate as training evolves, thereby improving the generalization of the model. In summary, the main contributions of this paper are the following.</p><p>1. We address the problem of overfitting in deep neural networks by proposing a novel regularization strategy called Curriculum Dropout that dynamically increases the expected number of suppressed units in order to improve the generalization ability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We draw connections between the original dropout framework <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> with regularization theory <ref type="bibr" target="#b7">[8]</ref> and curriculum learning <ref type="bibr" target="#b1">[2]</ref>. This provides an improved justification of (Curriculum) Dropout training, relating it to existing machine learning methods.</p><p>3. We complement our foundational analysis with a broad experimental validation, where we compare our Curriculum Dropout versus the original one <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> and anti-Curriculum <ref type="bibr" target="#b21">[22]</ref> paradigms, for (convolutional) neural network-based image classification. We evaluate the performance on standard datasets (MNIST <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>, SVHN <ref type="bibr" target="#b20">[21]</ref>, CIFAR-10/100 <ref type="bibr" target="#b15">[16]</ref>, Caltech-101/256 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>). As the results certify, the proposed method generally achieves a superior classification performance.</p><p>The remaining of paper is outlined as follows. Relevant related works are summarized in §2 and Curriculum Dropout is presented in §3 and §4, providing foundational interpretations. The experimental evaluation is carried out in §5. Conclusions and future work are presented in §6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>As previously mentioned, dropout is introduced by Hinton et al. <ref type="bibr" target="#b12">[13]</ref> and Sivrastava et al. <ref type="bibr" target="#b24">[25]</ref>. Therein, the method is detailed and evaluated with different types of deep learning models (Multi-Layer Perceptrons, Convolutional Neural Networks, Restricted Boltzmann Machines) and datasets, confirming the effectiveness of this approach against overfitting. Since then, many works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22]</ref> have investigated the topic.</p><p>Wan et al. <ref type="bibr" target="#b28">[29]</ref> propose Drop-Connect, a more general version of Dropout. Instead of directly setting units to zero, only some of the network connections are suppressed. This generalization is proven to be better in performance but slower to train with respect to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. Li et al. <ref type="bibr" target="#b19">[20]</ref> introduce data-dependent and Evolutional-dropout for shallow and deep learning, respectively. These versions are based on sampling neurons form a multinomial distribution with different probabilities for different units. Results show faster training and sometimes better accuracies. Wang et al. <ref type="bibr" target="#b29">[30]</ref> accelerate dropout. In their method, hidden units are dropped out using approximated sampling from a Gaussian distribution. Results show that <ref type="bibr" target="#b29">[30]</ref> leads to fast convergence without deteriorating the accuracy. Bayer et al. <ref type="bibr" target="#b0">[1]</ref> carry out a fine analysis, showing that dropout can be proficiently applied to Recurrent Neural Networks. <ref type="bibr">Wu and Gu [31]</ref> analyze the effect of dropout on the convolutional layers of a CNN: they define a probabilistic weighted pooling, which effectively acts as a regularizer. Zhai and Zhang <ref type="bibr" target="#b32">[33]</ref> investigate the idea of dropout once applied to matrix factorization. Ba and Frey <ref type="bibr" target="#b13">[14]</ref> introduce a binary belief network which is overlaid on a neural network to selectively suppress hidden units. The two networks are jointly trained, making the overall process more computationally expensive. Wager et al. <ref type="bibr" target="#b27">[28]</ref> apply Dropout on generalized linear models and approximately prove the equivalence between data-dependent L 2 regularization and dropout training with AdaGrad optimizer. Rennie et al. <ref type="bibr" target="#b21">[22]</ref> propose to adjust the dropout rate, linearly decreasing the unit suppression rate during training, until the network experiences no dropout.</p><p>While some of the aforementioned methods can be applied in tandem, there is still a lack of understanding about which one is superior -this is also due to the lack of pub-licly released code (as happens in <ref type="bibr" target="#b19">[20]</ref>). In this respect, <ref type="bibr" target="#b21">[22]</ref> is the most similar to our work. A few papers do not go beyond a bare experimental evaluation of the proposed dropout variation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>, omitting to justify the soundness of their approach. Conversely, while some works are much more formal than ours <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, all of them rely on approximations to carry out their analysis which is biased towards shallow models (logistic <ref type="bibr" target="#b27">[28]</ref> or linear regression <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28]</ref> and matrix factorization <ref type="bibr" target="#b32">[33]</ref>). Differently, in our paper, in addition to its experimental effectiveness, we provide several natural justifications to corroborate the proposed dropout generalization for deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Time Scheduling for the Dropout Rate</head><p>Deep Neural Networks display co-adaptations between units in terms of concurrent activations of highly organized clusters of neurons. During training, the latter specialize themselves in detecting certain details of the image to be classified, as shown by Zeiler and Fergus <ref type="bibr" target="#b31">[32]</ref>. They visualize the high sensitivity of certain filters in different layers in detecting dogs, people's faces, wheels and more general ordered geometrical patterns [32, <ref type="figure" target="#fig_0">Fig. 2]</ref>. Moreover, such co-adaptations are highly generalizable across different datasets as proved by Torralba's work <ref type="bibr" target="#b33">[34]</ref>. Indeed, the filter responses provided in the AlexNet within conv1, pool2/5 and fc7 layers are very similar [34, <ref type="figure">Fig. 5</ref>], despite the images used for the training are very different: objects from ImageNet versus scenes from Places datasets.</p><p>These arguments support the existence of some positive co-adaptations between neurons in the network. Nevertheless, as soon as the training keeps going, some coadaptations can also be negative if excessively specific of the training images exploited for updating the gradients. Consequently, exaggerated co-adaptations between neurons weaken the network generalization capability, ultimately resulting in overfitting. To prevent it, Dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> precisely contrasts those negative co-adaptations.</p><p>The latter can be removed by randomly suppressing neurons of the architecture, restoring an improved situation where the neurons are more "independent". This empirically reflects into a better generalization capability <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Network training is a dynamic process. Despite the previous interpretation is totally sound, the original Dropout algorithm cannot precisely accommodate for it. Indeed, the suppression of a neuron in a given layer is modeled by a Bernoulli(θ) random variable 1 , 0 &lt; θ ≤ 1. Employing such distribution is very natural, since it statistically models binary activation/inhibition processes. In spite of that, it seems suboptimal that θ should be fixed during the whole <ref type="bibr" target="#b0">1</ref> To avoid confusion in our notation, please note that θ is the equivalent of p in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, i.e the probability of retaining a neuron. training stage. With this operative choice, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> is actually treating the negative co-adaptations phenomena as uniformly distributed during the whole training time. Differently, our intuition is that, at the beginning of the training, if any co-adaptation between units is displayed, this should be preserved as positively representing the selforganization of the network parameters towards their optimal configuration.</p><p>We can understand this by considering the random initialization of the network's weights. They are statistically independent and actually not co-adapted at all. Also, it is quite unnatural for a neural network with random weights to overfit the data. On the other hand, the risk of overdone co-adaptations increases as the training proceeds since the loss minimization can achieve a small objective value by overcomplicating the hierarchical representation learnt from data. This implies that overfitting caused by excessive co-adaptations appears only after a while.</p><p>Since a fixed parameter θ is not able to handle increasing levels of negative co-adaptations, in this work, we tackle this issue by proposing a temporal dependent θ(t) parameter. Here, t denotes the training time, measured in gradient updates t ∈ {0, 1, 2, . . . }. Since θ(t) models the probability for a given neuron to be retained, D · θ(t) will count the average number of units which remain active over the total number D in a given layer. Intuitively, such quantity must be higher for the first gradient updates, then starting decreasing as soon as the training gears. In the late stages of training, such decrease should be stopped. We thus constrain θ(t) to be θ(t) ≥ θ for any t, where θ is a limit value, to be taken as 0.5 ≤ θ ≤ 0.9 as prescribed by the original dropout scheme <ref type="bibr">[25, §A.4]</ref> (the higher the layer hierarchy, the lower the retain probability).</p><p>Inspired by the previous considerations, we propose the following definition for a curriculum function θ(t) aimed at improving dropout training (as it will become clear in section 4, from now on we will often use the terms curriculum and scheduling interchangeably).</p><formula xml:id="formula_0">Definition 1. Any function t → θ(t) such that θ(0) = 1</formula><p>and lim t→∞ θ(t) ց θ is said to be a curriculum function to generalize the original dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> formulation with retain probability θ.</p><p>Starting from the initial condition θ(0) = 1 where no unit suppression is performed, dropout is gradually introduced in a way that θ(t) ≥ θ for any t. Eventually (i.e. when t is big enough), the convergence θ(t) → θ models the fact that we retrieve the original formulation of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> as a particular case of our curriculum.</p><p>Among the functions as in Def. 1, in our work we fix</p><formula xml:id="formula_1">θ curriculum (t) = (1 − θ) exp(−γt) + θ, γ &gt; 0 (1)</formula><p>By considering <ref type="figure" target="#fig_0">Figure 2</ref>, we can provide intuitive and straightforward motivations regarding our choice. The blue curves in <ref type="figure" target="#fig_0">Fig. 2</ref> are polynomials of increasing degree δ = {1, . . . , 10} (left to right). Despite fulfilling the initial constraint θ(0) = 1, they have to be manually thresholded to impose θ(t) → θ when t → ∞. This introduces two more (undesired) parameters (δ and the threshold) with respect to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, where the only quantity to be selected is θ.</p><p>The very same argument discourages the replacement of the variable t by t α in (1), (green curves in <ref type="figure" target="#fig_0">Fig. 2</ref>, α = {2, . . . , 10}, left to right). Moreover, by evaluating the area under the curve, we can intuitively measure how aggressively the green curves behave while delaying the dropping out scheme they eventually converge to (as θ(t) → θ). Precisely, that convergence is faster while moving to the green curves more on the left, being the fastest one achieved by our scheduling function (1) (red curve, <ref type="figure" target="#fig_0">Fig. 2)</ref>.</p><p>One could still argue that the parameter γ &gt; 0 is annoying since it requires cross validation. This is not necessary: in fact, γ can actually be fixed according to the following heuristics. Despite Def. 1 considers the limit of θ(t) for t → ∞, such condition has to be operatively replaced by t ≈ T , being T the total number of gradient updates needed for optimization. It is thus totally reasonable to assume that the order of magnitude of T is a priori known and fixed to be some power of 10 such as 10 4 , 10 5 . Therefore, for a curriculum function as in Def. 1, we are interested in furthermore imposing θ(t) ≈ θ when t ≈ T . Actually, a rule of thumb such as γ = 10/T (2) implies |θ curriculum (T ) − θ| &lt; 10 −4 and was used for all the experiments 2 in §5. Additionally, from <ref type="figure" target="#fig_0">Figure 2</ref>, we can grab some intuitions about the fact that the asymptotic convergence to θ is indeed realized for a quite consistent part of the training and well before t ≈ T . This means that during a big portion of the training, we are actually dropping out neurons as prescribed in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, addressing the overfitting issue. In addition to these arguments, we will provide <ref type="bibr" target="#b1">2</ref> Check the Supplementary Material where we proved that our approach is extremely robust with respect to different γ values.</p><p>complementary insights on our scheduled implementation for dropout training.</p><p>Smarter initialization for the network weights. The problem of optimizing deep neural networks is non-convex due to the non-linearities (ReLUs) and pooling steps. In spite of that, a few theoretical papers have investigated this issue under a sound mathematical perspective. For instance, under mild assumptions, Haeffele and Vidal <ref type="bibr" target="#b10">[11]</ref> derive sufficient conditions to ensure that a local minimum is also a global one to guarantee that the former can be found when starting from any initialization. The same theory presented in <ref type="bibr" target="#b10">[11]</ref> cannot be straightforwardly applied to the dropout case due to the pure deterministic framework of the theoretical analysis that is carried out. Therefore, it is still an open question whether all initializations are equivalent for the sake of a dropout training and, if not, which ones are preferable. Far from providing any theoretical insight in this flavor, we posit that Curriculum Dropout can be interpreted as a smarter initialization. Indeed, we implement a soft transition between a classical dropout-free training of a network versus the dropout one <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. Under this perspective, our curriculum seems equivalent to performing dropout training of a network whose weights have already been slightly optimized, evidently resulting in a better initialization for them.</p><p>As a naive approach, one can think to perform regular training for a certain amount of gradient updates and then apply dropout during the remaining ones. We call that Switch-Curriculum. This actually induces a discontinuity in the objective value which can damage the performance with respect to the smooth transition performed by our curriculum (1) -check <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>Curriculum Dropout as adaptive regularization. Several connections <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> have been established between Dropout and model training with noise addition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>. The common trend discovered is that when an unregularized loss function is optimized to fit artificially corrupted data, this is actually equivalent to minimize the same loss augmented by a data dependent penalizing term. In both <ref type="bibr">[28,</ref>   <ref type="bibr" target="#b2">3</ref> by θ(1 − θ). When θ = θ, the impact of the regularization is just fixed, therefore rising potential over-and under-fitting issues <ref type="bibr" target="#b7">[8]</ref>. But, for θ = θ curriculum (t), when t is small, the regularizer is set to zero (θ curriculum (0) = 1) and we do not perform any regularization at all. Indeed, the latter is simply not necessary: the network weights still have values which are close to their random and statistically independent initialization. Hence, overfitting is unlikely to occur at early training steps. Differently, we should expect it to occur as soon as training proceeds: by using (1), the regularizer is now weighted by</p><formula xml:id="formula_2">θ curriculum (t)(1 − θ curriculum (t)),<label>(3)</label></formula><p>which is an increasing function of t. Therefore, the more the gradient updates t, the heavier the effect of the regularization. This is the reason why overfitting is better tackled by the proposed curriculum. Despite the overall idea of an adaptive selection of parameters is not novel for either regularization theory <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6]</ref> or tuning of network hyper-parameters (e.g. learning rate, <ref type="bibr" target="#b4">[5]</ref>), to the best of our knowledge, this is the first time that this concept of timeadaptive regularization is applied to deep neural networks.</p><p>Compendium. Let us conclude with some general comments. We posit that there is no overfitting at the beginning of the network training. Therefore, differently from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, we allow for a scheduled retain probability θ(t) which gradually drops neurons out. Among other plausible curriculum functions as in Def. 1, the proposed choice (1) introduces no additional parameter to be tuned and implicitly provides a smarter weight initialization for dropout training. The superiority of (1) also relates to i) the smoothly increasingly amount of units suppressed and ii) the soft adaptive regularization performed to contrast overfitting.</p><p>Throughout these interpretations, we can retrieve a common idea of smoothly changing difficulty of the training which is applied to the network. This fact can be better understood by finding the connections with Curriculum Learning <ref type="bibr" target="#b1">[2]</ref>, as we explain in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Curriculum Learning, Curriculum Dropout</head><p>For the sake of clarity, let us remind the concept of curriculum learning <ref type="bibr" target="#b1">[2]</ref>. Within a classical machine learning algorithm, all training examples are presented to the model in an unordered manner, frequently applying a random shuffling. Actually, this is very different from what happens for the human training process, that is education. Indeed, the latter is highly structured so that the level of difficulty of the concepts to learn is proportional to the age of the people, managing easier knowledge when babies and harder when adults. This "start small" paradigm will likely guide the learning process <ref type="bibr" target="#b1">[2]</ref>.</p><p>Following the same intuition, <ref type="bibr" target="#b1">[2]</ref> proposes to subdivide the training examples based on their difficulty. Then, the learning is configured so that easier examples come first, eventually complicating them and processing the hardest ones at the end of the training. This concept is formalized by introducing a learning time λ ∈ [0, 1], so that training begins at λ = 0 and ends at λ = 1. At time λ, Q λ (z) denotes the distribution which a training example z is drawn from. The notion of curriculum learning is formalized requiring that Q λ ensures a sampling of examples z which are easier than the ones sampled from Q λ+ε , ε &gt; 0. Mathematically, this is formalized by assuming</p><formula xml:id="formula_3">Q λ (z) ∝ W λ (z)P (z).<label>(4)</label></formula><p>In <ref type="formula" target="#formula_3">(4)</ref>, P (z) is the target training distribution, accounting for all examples, both easy and hard ones. The sampling from P is corrected by the factor 0 ≤ W λ (z) ≤ 1 for any λ and z. The interpretation for W λ (z) is the measure of the difficulty of the training example z. The maximal complexity for a training example is fixed to 1 and reached at the end of the training, i.e.</p><formula xml:id="formula_4">W 1 (z) = 1, i.e. Q 1 (z) = P (z). The relationship W λ (z) ≤ W λ+ε (z)<label>(5)</label></formula><p>represents the increased complexity of training examples from instant λ to λ + ε. Moreover, the weights W λ (z) must be chosen in such a way that</p><formula xml:id="formula_5">H(Q λ ) &lt; H(Q λ+ε ),<label>(6)</label></formula><p>where Shannon's entropy H(Q λ ) models the fact that the quantity of information exploited by the model during training increases with respect to λ. In order to prove that our scheduled dropout fulfills this definition, for simplicity, we will consider it as applied to the input layer only. This is not restrictive since the same considerations apply to any intermediate layer, by considering that each layer trains the feature representation used as input by the subsequent one.</p><p>As the images exploited for training, consider the partitions in the dataset including all the (original) clean data and all the possible ways of corrupting them through the Bernoulli multiplicative noise (see <ref type="figure">Fig. 1</ref>). Let π denote the probability of sampling an uncorrupted d-dimensional image within an image dataset (nothing more than a uniform distribution over the available training examples). Let us fix the gradient update t. The case of sampling a dropped-out z is equivalent to sampling the corresponding uncorrupted image z 0 from π and then overlapping it with a binary mask b (of size d), where each entry of b is zero with probability 1 − θ(t). By mapping b to the number i of its zeros,</p><formula xml:id="formula_6">P[z] = P[z 0 , i] = d i (1 − θ(t)) i θ(t) d−i · π(z 0 ). (7) Indeed, (1 − θ(t)) i θ(t) d−i</formula><p>is the probability of sampling one binary mask b with i zeros and d i accounts for all the possible combinations. Re-parameterizing the training time t = λT , we get</p><formula xml:id="formula_7">Q λ (z) = d i (1 − θ(λT )) i θ(λT ) d−i · π(z 0 ). (8)</formula><p>By defining P (z) = Q 1 (z) and</p><formula xml:id="formula_8">W λ (z) = 1 P (z) d i (1 − θ(λT )) i θ(λT ) d−i · π(z 0 ),<label>(9)</label></formula><p>we can easily prove (refer to the Supplementary Material for the complete proof) that the definition in <ref type="bibr" target="#b1">[2]</ref> is fulfilled by the choice (8) for curriculum learning distribution Q λ (z).</p><p>To conclude, we give an additional interpretation to Curriculum Dropout. At λ = 0, θ(0) = 1 and no entry of z 0 is set to zero. This clearly corresponds to the easiest available example, since the learning starts at t = 0 by considering all possible available visual information. When θ start decreasing to θ(λT ) ≈ 0.99, only 1% of z 0 is suppressed (on average) and still almost all the information of the original dataset Z 0 is available for training the network. But, as λ grows, θ(λT ) decreases and a bigger number of entries are set to zero. This complicates the task, requiring an improved effort from the model to capitalize from the reduced uncorrupted information which is available at that stage of the training process.</p><p>After all, this connection between Dropout and Curriculum Learning was possible thanks to our generalization through Def. 1. Consequently, the original Dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> can be interpreted as considering the single specific value λ such that θ(λT ) = θ, being θ the constant retain probability on <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. This means that, as previously found for the adaptive regularization (see §3), the level of difficulty W λ (z) of the training examples z is fixed in the original Dropout. This encounters the concrete risk of either oversimplifying or overcomplicating the learning, with detrimental effects on the model's generalization capability. Hence, the proposed method allows to setup a progressive curriculum Q λ (z), complicating the examples z in a smooth and adaptive manner, as opposed to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, where such complication is fixed to equal the maximal one from the very beginning <ref type="figure">(Fig. 1)</ref>.</p><p>To conclude, let us note that the aforementioned work <ref type="bibr" target="#b21">[22]</ref> proposes a linear increase of the retain probability. According to equations <ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref> this implements what <ref type="bibr" target="#b1">[2]</ref> calls an anti-curriculum: this is shown to perform slightly better or worse than the no-curriculum strategy <ref type="bibr" target="#b1">[2]</ref> and always worse than any curriculum implementation. Our experiments confirm this finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this Section, we applied Curriculum Dropout to neural networks for image classification problems on different datasets, using Convolutional Neural Network (CNN) architectures and Multi-Layer Perceptrons (MLPs) <ref type="bibr" target="#b3">4</ref> . In particular, we used two different CNN architectures: LeNet <ref type="bibr" target="#b17">[18]</ref> and a deeper one (conv-maxpool-conv-maxpool-convmaxpool-fc-fc-softmax), further called CNN-1 and CNN-2, respectively. In the following, we detail the datasets used and the network architectures adopted in each case.</p><p>MNIST <ref type="bibr" target="#b18">[19]</ref> -A dataset of grayscale images of handwritten digits (from 0 to 9), of resolution 28 × 28. Training and test sets contain 60.000 and 10.000 images, respectively. For this dataset, we used a three-layer MLP, with 2.000 units in each hidden layer, and CNN-1.</p><p>Double MNIST -This is a static version of <ref type="bibr" target="#b25">[26]</ref>, generated by superimposing two random images of two digits (either distinct or equal), in order to generate 64 × 64 images. The total amount of images are 70.000, with 55 total classes (10 unique digits classes + 10 2 = 45 unsorted couples of digits) . Training and test sets contain 60.000 and 10.000 images, respectively. Training set's images were generated using MNIST training images, and test set's images were generated using MNIST test images. We used CNN-2.</p><p>SVHN <ref type="bibr" target="#b20">[21]</ref> -Real world RGB images of street view house numbering. We used the cropped 32 × 32 images representing a single digit (from 0 to 9). We exploited a subset of the dataset, consisting in 6.000 images for training and 1.000 images for testing, randomly selected. We used CNN-2 also in this case.</p><p>CIFAR-10 and CIFAR-100 [16] -These datasets collect 32 × 32 tiny RGB natural images, reporting 6000 and 600 elements per each of the 10 or 100 classes, respectively. In both datasets, training and test sets contain 50.000 and 10.000 images, respectively. We used CNN-1 for both datasets.</p><p>Caltech-101 [9] -300 × 200 resolution RGB images of 101 classes. For each of them, a variable size of instances is available: from 30 to 800. To have a balanced dataset, we used 20 and 10 images per class for training and testing, respectively. Images were reshaped to 128×128 pixels. We used CNN-2 again here.</p><p>Caltech-256 [10] -31000 RGB images for 256 total classes. For each class, we used 50 and 20 images for training and testing, respectively. Images were reshaped to 128 × 128 pixels. We used CNN-2.</p><p>For training CNN-1, CNN-2 and MLP, we exploited a cross-entropy cost function with Adam optimizer <ref type="bibr" target="#b14">[15]</ref> and a momentum term of 0.95, as suggested in <ref type="bibr" target="#b24">[25]</ref>. We used mini-batches of 128 images and fixed the learning rate to be 10 −4 . Please refer to the Supplementary Material for additional details regarding the architectures and the hyperparameters.</p><p>We applied curriculum dropout using the function (1) where γ is picked using the heuristics (2) and θ is fixed as follows. For both CNN-1 and CNN-2, the retain probability for the input layer was set to θ input = 0.9, selecting θ conv = 0.75 and θ fc = 0.5 for convolutional and fully connected layers, respectively. For the MLP, θ input = 0.8 and MNIST <ref type="bibr" target="#b18">[19]</ref>    Before reporting our results, let us emphasize that our aim is to improve the standard dropout framework <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, not to compete for the state-of-the art performance in image classification tasks. For this reason, we did not use engineering tricks such as data augmentation or any particular pre-processing, and neither we tried more complex (or deeper) network architectures.</p><p>In <ref type="figure" target="#fig_1">Fig. 3</ref>, we qualitatively compared Curriculum Dropout (green) versus the original Dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>  no Dropout, training of a network (black). Since CNN-1, CNN-2 and MLP are trained from scratch, in order to ensure a more robust experimental evaluation, we have repeated the weight optimization 10 times for all the cases. Hence, in <ref type="figure" target="#fig_1">Fig. 3</ref>, we report the mean accuracy value curves, representing with shadows the standard deviation errors.</p><p>Additionally, we report in <ref type="table">Table 1</ref> the percentage accuracy improvements of Dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, anti-Curriculum Dropout <ref type="bibr" target="#b21">[22]</ref> and Curriculum Dropout (proposed) versus a baseline network where no neuron is suppressed. To do that, we selected the average of the 10 highest mean accuracies obtained by each paradigm during each trial; then we averaged them over the 10 runs. We accommodated the metric of <ref type="bibr" target="#b26">[27]</ref> to measure the boost in accuracy over <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. Also, we reproduced for two datasets the cases of fixed layer size n or fixed nθ as in <ref type="bibr">[25, §7.3]</ref>. Here the network layers' size n is preliminary increased by a factor 1/θ, since on average a fraction θ of the units is dropped out. However, we notice that those bigger architectures tend to overfit the data. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results obtained on Double MNIST dataset by scheduling the dropout with a step function, i.e. no suppression is performed until a certain switch-epoch is reached ( §3). Precisely, we switched at 10-20-50 epochs. This curriculum is similar to the one induced by the polynomial functions of <ref type="figure" target="#fig_0">Figure 2</ref>: in fact, both curves have a similar shape and share the drawback of a threshold to be introduced. Yet, Switch-Curriculum shows an additional shortcoming: as highlighted by the spikes of both training and test accuracies, the sudden change in the network connections, induced by the sharp shift in the retain probabilities, makes the network lose some of the concepts learned up to that moment. While early switches are able to recover quickly to good performances, late ones are deleterious. Moreover, we were not able to find any heuristic rule for the switch-epoch, which would then be a parameter to be validated. This makes Switch-Curriculum a less powerful option compared to a smoothly-scheduled curriculum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Switch-Curriculum.</head><p>Discussion. The proposed Curriculum Dropout, implemented through the scheduling function (1), improves the generalization performance of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> in almost all cases. As the only exception, in MNIST <ref type="bibr" target="#b18">[19]</ref> with MLP, the scheduling is just equivalent to the original dropout framework <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. Our guess is that the simpler the learning task, the less effective Curriculum Learning. After all, for a task which is relatively easy itself, there is less need for "starting easy". This is in any case done at no additional cost nor training time requirements.</p><p>As expected, anti-Curriculum was improved by a more significant gap by our scheduling. Also, sometimes, an anti-Curriculum strategy even performs worse than a nonregularized network (e.g., Caltech 256 <ref type="bibr" target="#b9">[10]</ref>). This is coherent with the findings of <ref type="bibr" target="#b1">[2]</ref> and with our discussion in §4 concerning Annealed Dropout <ref type="bibr" target="#b21">[22]</ref>, of which antiCurriculum represents a generalization. In addition, while neither regular nor Curriculum Dropout ever need early stopping, anti-Curriculum often does. part of this research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Curriculum functions. Eq. (1) (red), polynomial (blue) and exponential (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Curriculum Dropout (green) compared with regular Dropout [13, 25] (blue), anti-Curriculum (red) and a regular training of a network with no units suppression (black). For all cases, we plot mean test accuracy (averaged over 10 different re-trainings) as a function of gradient updates. Shadows represent standard deviation errors. Best viewed in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Switch-Curriculum. We compare the Curriculum (green) and the regular Dropout (blue) with three cases where we switch from regular to dropout training i) at the beginning (pink) ii) in the middle (violet), iii) almost at the end (purple) of the learning. From left to right, curriculum functions, cross-entropy loss and test accuracy curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 .</head><label>2</label><figDesc>] for linear/logistic regression and [25, §9.1] for least squares, it is proved that Dropout induces a regularizer which is scaled</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>(MLP) MNIST [19] (CNN-1)</figDesc><table>Double MNIST [26] n fixed 
Double MNIST [26] nθ fixed 

SVHN [21] n fixed 
SVHN [21] nθ fixed 

CIFAR-10 [16] 
CIFAR-100 [16] 

Caltech-101 [9] 
Caltech-256 [10] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>(blue), anti-Curriculum Dropout (red) and an unregularized, i.e.CIFAR-10 [16] CNN-1 n 10 73.06 +0.22 -0.68 +0.62 (182%) CIFAR-100 [16] CNN-1 n 100 39.70 +1.01 +0.01 +1.66 (64.4%) Caltech-101 [9] CNN-2 n 101 28.56 +4.21 +1.57 +4.72 (12.1%) Caltech-256 [10] CNN-2 n 256 14.39 +2.36 -0.22 +3.23(36.9%) Table 1. Comparison of the proposed scheduling versus [13, 25] in terms of percentage accuracy improvement.</figDesc><table>Dataset 

Architecture 
Configuration 

(n or nθ 

fixed) 

Classes 
Unregularized 

network 
Dropout [13, 25] 
Anti-Curriculum 

Curriculum 
Dropout 
(percent boost [27] 
over Dropout [13, 

25]) 

MNIST [19] 
MLP 
n 10 
98.67 +0.38 +0.04 +0.36 (-5.3%) 
CNN-1 n 
99.25 +0.15 -0.05 +0.18 (20.0%) 

Double MNIST 
CNN-2 n 55 92.48 
+1.42 +0.73 +2.35 (65.5%) 
CNN-2 nθ 
+0.87 +0.53 +1.11 (27.6%) 

SVHN [21] 
CNN-2 n 10 84.63 
+2.35 +1.17 +2.65 (12.8%) 
CNN-2 nθ 
+1.59 +1.51 +2.06 (29.6%) 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Please, check the Supplementary Material where we extended such result for a deep neural network, also allowing for a time-dependent θ(t).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code available at https://github.com/pmorerio/ curriculum-dropout.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this paper we have propose a scheduling for dropout training applied to deep neural networks. By softly increasing the amount of units to be suppressed layerwise, we achieve an adaptive regularization and provide a better smooth initialization for weight optimization. This allows us to implement a mathematically sound curriculum <ref type="bibr" target="#b1">[2]</ref> and justifies the proposed generalization of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Through a broad experimental evaluation on 7 image classification tasks, the proposed Curriculum Dropout have proved to be more effective than both the original Dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> and the Annealed <ref type="bibr" target="#b21">[22]</ref>, the latter being an example of anti-Curriculum <ref type="bibr" target="#b1">[2]</ref> and therefore achieving an inferior performance to our more disciplined approach in ease dropout training. Globally, we always outperform the original Dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> using various architectures, and we improve the idea of <ref type="bibr" target="#b21">[22]</ref> by margin.</p><p>We have tested Curriculum Dropout on image classification tasks only. However, our guess is that, as standard Dropout, our method is very general and thus applicable to different domains. As a future work, we will apply our scheduling to other computer vision tasks, also extending it for the case of inter-neural connection inhibitions <ref type="bibr" target="#b28">[29]</ref> and Recurrent Neural Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of one Tesla K40 GPU used for</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On fast dropout and its applicability to recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR:1311.0701</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to Tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A robust adaptive stochastic gradient method for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Caglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR:1703.00788</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Active Regression with Adaptive Huber loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<idno>CoRR:1606.01568</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive regularization of weight vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regularization networks and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computational Mathematics</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>7694</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Global optimality in tensor factorization, deep learning, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno>CoRR:1506.07540</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pruning from adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1222" to="1231" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>CoRR:1207.0580</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive dropout for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="541" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">22782324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved dropout for shallow and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Annealed dropout training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings onf the IEEE Workshop on SLT</title>
		<meeting>onf the IEEE Workshop on SLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adding noise to the input of a model trained with a regularized objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno>CoRR:1104.3250</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Robust subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<idno>CoRR:1301.2603</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Video Representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>CoRR:1502.04681</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 2013</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards dropout training for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dropout training of matrix factorization and autoencoders for link prediction in sparse graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR:1512.04483</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno>NIPS. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
