<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<addrLine>Hous-ton</addrLine>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<addrLine>Hous-ton</addrLine>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<addrLine>Hous-ton</addrLine>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 35 th International Conference on Machine Learning</title>
						<meeting>the 35 th International Conference on Machine Learning <address><addrLine>Stockholm, Sweden, PMLR 80</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2018">2018</date>
						</imprint>
					</monogr>
					<note>The code is available at https://github. com/Sandbox3aster/Deep-K-Means Correspondence to: Zhangyang Wang &lt;at-laswang@tamu.edu&gt;, Yingyan Lin &lt;yingyan.lin@rice.edu&gt;.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, the high energy consumption of convolutions limits its deployment on mobile devices. To this end, we proposed a simple yet effective scheme for compressing convolutions though applying k-means clustering on the weights, compression is achieved through weightsharing, by only recording K cluster centers and weight assignment indexes. We then introduced a novel spectrally relaxed k-means regularization, which tends to make hard assignments of convolutional layer weights to K learned cluster centers during re-training. We additionally propose an improved set of metrics to estimate energy consumption of CNN hardware implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual hardware measurements. We finally evaluated Deep k-Means across several CNN models in terms of both compression ratio and energy consumption reduction, observing promising results without incurring accuracy loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, the high energy consumption of convolutions limits its deployment on mobile devices. To this end, we proposed a simple yet effective scheme for compressing convolutions though applying k-means clustering on the weights, compression is achieved through weightsharing, by only recording K cluster centers and weight assignment indexes. We then introduced a novel spectrally relaxed k-means regularization, which tends to make hard assignments of convolutional layer weights to K learned cluster centers during re-training. We additionally propose an improved set of metrics to estimate energy consumption of CNN hardware implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual hardware measurements. We finally evaluated Deep k-Means across several CNN models in terms of both compression ratio and energy consumption reduction, observing promising results without incurring accuracy loss. The code is available at https://github. com/Sandbox3aster/Deep-K-Means</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have gained considerable interest due to their record-breaking performance in many recognition tasks <ref type="bibr" target="#b23">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b11">Girshick et al., 2013;</ref><ref type="bibr" target="#b38">Taigman et al., 2014)</ref>. In parallel, there has been a tremendously growing need to bring CNNs into resourceconstrained mobile devices in line with the recent surge of edge computing in which raw data are processed locally in edge devices using their embedded machine learning algorithms <ref type="bibr" target="#b34">(Shi et al., 2016)</ref>  <ref type="bibr" target="#b29">(Lin et al., 2017)</ref>. The advantage lies in that local processing avoids transferring data back and forth between data centers and edge devices, thus reducing communication cost, latency, and enhancing privacy. However, deploying CNNs into resource-constrained platforms is a non-trivial task. Devices at the edge, such as smart phones and wearables, have limited energy, computation and storage resources since they are battery-powered and have a small form factor. In contrast, powerful CNNs require a large number of weights that corresponds to considerable storage and memory bandwidth. For example, the amount of weights in state-of-the-art CNNs AlexNet and VGG-16 are over 200MB and 500MB, respectively . Further, CNN-based applications can drain a battery very quickly if executed frequently. For example, smartphones nowadays cannot even run classification using AlexNet in real-time for more than one hour <ref type="bibr" target="#b41">(Yang et al., 2017)</ref>.</p><p>To close the gap between the constrained resources of edge devices and the growing complexity of CNNs, compression techniques have been widely investigated to reduce the precision of weights and the number of operations during or after CNN training in order to shrink their large implementation cost while maintaining the desired inference performance. Various CNN compression techniques have been proposed, such as weight compression      and decomposition <ref type="bibr" target="#b3">(Changpinyo et al., 2017)</ref>  <ref type="bibr" target="#b19">(Howard et al., 2017)</ref>  , and compact architectures <ref type="bibr">(Iandola et al., 2016)</ref>  <ref type="bibr" target="#b27">(Lin et al., 2014)</ref>. However, there are two major shortcomings in existing CNN compression techniques.</p><p>• A myriad of CNN compression techniques focus on the fully-connected layers of CNNs which convention-ally have dominant parameters. However, recent successful CNNs tend to shift more parameters towards convolutional layers and have only one or even no fully-connected layers. For example, 85% of the parameters lie in the convolutional layers of GoogleNet <ref type="bibr">(Chen et al., 2016a)</ref>. Despite the growing trend toward CNN models using more convolutional layers and fewer fully-connected layers, only few compression techniques are dedicated for convolutional layers.</p><p>• The majority of CNN compression techniques, including most of the very few that focus on compressing convolutional layers, are designed to merely reduce the CNN model size or the amount of computation, which does not necessarily lead to reduced energy consumption. In fact, the recent work <ref type="bibr" target="#b41">(Yang et al., 2017)</ref> argues that the number of weights, multiply-and-accumulate (MAC) operations, and speedup ratio are often not good approximations for energy consumption, which also heavily depends on memory data movement and more. For example, the authors show an interesting result that although <ref type="bibr">SqueezeNet (Iandola et al., 2016)</ref> has 51.8× fewer weights than AlexNet, it consumes 33% more energy due to its larger amount of computation and data movement. A compression technique that aims to reduce both model size and energy-aware complexity is hence highly desired for enabling extensive resource-constrained CNN applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contribution</head><p>In this paper, we propose Deep k-Means, a compression pipeline that is well suited for trimming down the complexity of convolutional layers that dominate both the model size as well as energy consumption of recently developed state-of-the-art CNNs. Deep k-Means consists of two steps. First, a novel spectrally relaxed k-means regularization is developed to enforce highly clustered weight structures during re-training. After that, compression is performed via weight-sharing, by only recording cluster centers and weight assignment indexes. We evaluate the performance of Deep k-Means in comparison with several state-of-the-art compression techniques focused on compressing convolutional layers. The results show that Deep k-Means consistently achieves higher accuracy at the same compression ratio (CR) as its competitors. Furthermore, Deep k-Means is also evaluated in terms of energy-aware metrics developed by us, and its compressed models show favorable energy efficiency as well. Our main contributions are summarized as follows:</p><p>• We introduce a novel spectrally relaxed k-means regularization that automatically learns hard(er) assignments of convolutional layer weights during re-training, to favor the subsequent compression via k-means weight-sharing. Our regularization approach is effective, efficient, simple to implement and use, and easily scalable to large CNN models.</p><p>• Inspired by a recently developed dataflow called "rowstationary", that minimizes data movement energy consumption on CNN hardware implementation <ref type="bibr" target="#b6">(Chen et al., 2016b)</ref>, we reformulate the weights into row vectors for weigh-sharing clustering. Such a formulation has the potential to result in CNN models that are in favor of energy-efficient hardware implementation.</p><p>• In order to bridge the gap between algorithm and hardware design of CNNs, we propose an improved set of energy-aware metrics based on . Our energy consumption estimation results are verified to be consistent with those from the tool in <ref type="bibr" target="#b41">(Yang et al., 2017)</ref>, which was extrapolated from actual hardware measurements. We expect our metrics to broadly benefit future research in energy-aware CNN design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related Work</head><p>Parameter pruning and sharing has been used both to reduce network complexity and to avoid over-fitting. Early pruning approaches include Biased Weight Decay <ref type="bibr" target="#b16">(Hanson &amp; Pratt, 1989)</ref>, Optimal Brain Damage <ref type="bibr" target="#b8">(Cun et al., 1990)</ref>, and Optimal Brain Surgeon <ref type="bibr" target="#b17">(Hassibi &amp; Stork, 1993)</ref>. Recent works <ref type="bibr" target="#b35">(Srinivas &amp; Babu, 2015)</ref> made use of the redundancy among neurons. The Deep Compression method introduced in ) employed a three stage pipeline to prune the redundant connections, quantize the weights via scalar weight sharing, and then encode the quantized weights using Huffman coding. An effective soft weight-sharing method described in <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref> showed competitive CRs on state-of-the-art CNNs, e.g., Wide ResNet.</p><p>With fully-connected layers traditionally considered as the memory bottleneck, numerous works focused on compressing these layers. For example, <ref type="bibr" target="#b13">(Gong et al., 2014b)</ref> proposed applying k-means clustering to the densely-connected layers and showed a good balance between model size and accuracy. <ref type="bibr" target="#b4">(Chen et al., 2015)</ref> proposed HashedNet that used a low-cost hash function to group weights into hash buckets for parameter sharing. On the other hand, a few recent works embraced the trend towards more convolutional layers in CNNs and attempted to compress convolutional layers. For example, <ref type="bibr">(Chen et al., 2016a)</ref> proposed an architecture called FreshNets to compress filters of convolutional layers in the frequency domain. The recent work (Abbasi-Asl &amp; Yu, 2017) iteratively pruned filters based on the classification accuracy reduction index, and achieved substantially higher classification accuracy compared to other structural compression schemes, e.g., <ref type="bibr" target="#b18">(He et al., 2014;</ref><ref type="bibr" target="#b34">Li et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Approach</head><p>2.1. Parameter Sharing via Row-wise k-Means Assuming a convolutional layer ∈ R s×s×c×m , where s denotes the filter size, c the input channel number, and m the output channel number. Following the convention in CNNs, we reshape it as a matrix W ∈ R s×N , where N = s × c × m, each column vector ∈ R s being a row from an original convolutional filter. Following the product quantization approach for fully-connected layers in <ref type="bibr" target="#b12">(Gong et al., 2014a)</ref>, we treat all columns of W as N samples, and apply k-means to assign them with K clusters. When K N , we need only to store the cluster indexes and codebooks after k-means. We define the cluster rate for each layer as K N here. For compressing multiple convolutional layers, we adopt a "uniform parameter sharing" scheme for simplicity, i.e., each convolutional layer chooses its K value such that all layers have the same cluster rate, expect for the first layer whose cluster rate is often set higher.</p><p>We notice other alternatives to enforce structured parameter sharing among convolutional layers using k-means, e.g., reshaping each convolutional filter as vectors R s 2 and then clustering over c × m samples, or converting each output channel into R s×s×c and clustering over resulting m samples. In practice, we find their performance to be close (with k chosen in different proper ways). One major motivation for choosing the row-wise k-means is that it could lead to higher data reuse opportunity and thus result in more energy-efficient hardware implementations, according to the row-stationary dataflow recently proposed in <ref type="bibr" target="#b6">(Chen et al., 2016b)</ref>, which has shown to be superior in terms of energy efficiency compared to other dataflows. Another motivation arises from reducing the complexity (see Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">k-Means Regularized Re-Training</head><p>Simply pruning or sharing weights in CNNs will usually hurt the inference accuracy. Re-training has often been exploited to enforce the favorable structures in the pruned/shared weights and compensate for the accuracy loss . In order to be compatible with k-means parameter sharing, we would favor a re-training scheme that "naturally" encourages the weights to be concentrated tightly around, or exactly at, a number of cluster components which are optimized for high predictive accuracy. The goal is fulfilled by introducing a novel spectrally relaxed k-Means regularization below.</p><p>The original sum-of-squares function of k-Means usually employs a Lloyd-type algorithm to solve. The spectral relaxation technique of k-Means was introduced in <ref type="bibr" target="#b43">(Zha et al., 2002)</ref>, by first equivalently re-formulating sum-of-squares into a trace form with special constraints. Specifically, to cluster N samples of R s , represented as W ∈ R s×N , into K clusters, the spectral relaxation converts the k-means objective into the following problem:</p><formula xml:id="formula_0">min W ;F ∈F T r(W T W ) − T r(F T W T W F ),<label>(1)</label></formula><p>where T r denotes the matrix trace. F ∈ R N ×k is the normalized cluster index matrix, and F denotes its special structure requirement: F ij = 1/ √ n j if column i belongs to the cluster j and there is a total of n j samples in the cluster j; and F ij = 0 otherwise, i = 1, ..., N , j = 1, ..., K, and</p><formula xml:id="formula_1">K j=1 n j = N .</formula><p>The original spectral relaxation <ref type="bibr" target="#b43">(Zha et al., 2002)</ref> considers W as given; thus (1) is reduced to:</p><formula xml:id="formula_2">max F ∈F T r(F T W T W F )<label>(2)</label></formula><p>The authors of <ref type="bibr" target="#b43">(Zha et al., 2002)</ref> then proposed ignoring the special structure of F and let it be an arbitrary orthogonal matrix. <ref type="formula" target="#formula_2">(2)</ref> is thus relaxed to a trace maximization problem over a Stiefel manifold:</p><formula xml:id="formula_3">max F T r(F T W T W F ), s.t. F T F = I<label>(3)</label></formula><p>It results in a closed-form solution of F , by composing the first k singular vectors of W , according to the well-known Ky Fan theorem.</p><p>As a critical difference with <ref type="bibr" target="#b43">(Zha et al., 2002)</ref>, here our goal is not to cluster a static W . Rather, we would like to encourage W to stay "suited" for k-means during the dynamic re-training, without incurring a significant increase in complexity. We are thus motivated to utilize (1) as a regularization term on learning W , rather than a stand-alone objective. We discuss just one convolutional layer W for simplicity: assume that the original CNN training minimizes the energy function E(W ), w.r.t. W . The retraining minimizes the regularized objective below (λ is a scalar):</p><formula xml:id="formula_4">min W,F E(W ) + λ 2 [T r(W T W ) − T r(F T W T W F )], s.t. F T F = I<label>(4)</label></formula><p>Note that F is treated as an auxiliary variable to promote a clustered structure in W . Solving (4) could be iterated between the updates of W and F . Updating W can follow the standard stochastic gradient descent (SGD), with the gradient given as:</p><formula xml:id="formula_5">∇E(W ) + λW (I − F F T )</formula><p>. F is updated using the same closed-form solution to (3), by computing the k-truncated singular value decomposition (SVD) of W .</p><p>By the interaction between F and W during re-training, the regularization keeps W in a highly clustered state, in addition to optimizing it for inference accuracy. Although F has been relaxed from the "hard" normalized cluster index matrix to an arbitrary orthogonal one, we observe in practice that it still tends to enforce weights close to those taking around K unique vector values, i.e., encouraging "approximately hard" (or "harder" than soft weight sharing) K-cluster assignments during re-training.</p><p>In Deep k-Means, starting from an uncompressed pretrained model as initialization, we will re-train it with adding this novel data-dependent weight regularizer (4) to each convolutional layer, while other training protocols remain unchanged. The re-training typically converges into a much smaller number of epochs than in the original training. After that, we apply row-wise k-means on the learned W for the final parameter-sharing step.</p><p>Complexity Analysis For each convolutional layer W , the extra complexity incurred by applying the spectrally relaxed k-means regularization term includes two parts: <ref type="formula" target="#formula_0">(1)</ref> updating W : the only extra burden is to compute</p><formula xml:id="formula_6">λW (I − F F T ), which takes O(sKN ) or O(s 2 cmK) (computing W F F T ); (2) updating F via SVD, which costs O(s 2 N ) or O(s 3 cm):</formula><p>that also serves another motivation to create W with lower row dimensions (e.g., s rather than s 2 or s 2 c), since it will reduce the SVD complexity of W . Considering that s is usually small, the total extra complexity O((s 2 K + s 3 )cm) is quite affordable, enabling our methods to scale well for modern CNNs. In practice, we also implement the F update in a very "lazy" way so that SVD will merely be computed once for every five epochs, for further accelerating the re-training, with only marginal impacts on the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Comparison with Existing Work</head><p>Directly enforcing a k-means friendly weight structure is not straightforward. <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref> presents an elegant and inspiring Bayesian regularization form of "soft cluster assignment". During re-training, the authors fit a Gaussian mixture model (GMM) prior model over the weights, to encourage the distribution of weights to be close to K clusters. After re-training, each weight was quantized to the mean of the GMM component that takes most responsibility, for parameter sharing. Their pipeline is the closet peer work to ours, with the major difference being that we pursue harder cluster assignment during re-training. As is well known, GMM is reduced to k-means when the mixture variance gets close to zero. Therefore, the retraining process in <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref> could also be viewed as a "softened" version of k-means. However, the differences between the two methods manifest in multiple folds:</p><p>• First, our "harder" cluster assignment is directly derived from the original k-means objective (1). We expect it to be better aligned with the k-means parameter sharing stage. Our experimental observations show that this leads to more skewed weight distributions, and achieves better results than <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref>.</p><p>• Second, compared to the Bayesian form in <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref>, our regularization adds very little extra complexity to the standard SGD. The implementation only calls for minor changes (a new regularizer term); and thanks to its low complexity, it is ready to be applied to larger-scale CNNs.</p><p>• Third, <ref type="bibr" target="#b39">(Ullrich et al., 2017</ref>) discussed their high sensitivity to the choices of learning rates for mixture parameters (e.g., means, log-variances): a higher learning rate may cause model collapse and a lower one results in slow convergence. In contrast, Deep k-Means has merely one hyper-parameter λ. We find Deep k-Means insensitive to λ (λ between 10 −4 and 10 −3 is found to work almost equally well). Deep k-Means needs no special learning rate scheduling. It is also free of postprocessing, e.g., removing redundant components as <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref> needed to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Energy-Aware Metrics for CNN Energy Consumption Estimation</head><p>While CR or reduction in the number of operations are widely adopted by existing CNN compression techniques as generic performance metrics, these metrics are not necessarily tied to improved energy efficiency as pinpointed by <ref type="bibr" target="#b41">(Yang et al., 2017)</ref> according to their energy estimation tool extrapolated from actual hardware measurements. Therefore, it is important to evaluate compression techniques using a set of energy-aware metrics other than CR.</p><p>However, it is non-trivial to estimate the energy consumption of CNNs because a significant portion of energy consumption in CNNs is consumed by data movement, which mainly depends on the employed memory hierarchy and dataflow when implementing CNNs and is thus difficult to be estimated directly from the model. An energy estimation tool extrapolated from actual hardware measurements was proposed by <ref type="bibr" target="#b41">(Yang et al., 2017)</ref> to bridge the gap between algorithm and hardware design. Unfortunately, their tool currently only supports AlexNet and GoogLeNet_v1.</p><p>We hereby propose the following energy-aware metrics:</p><p>• Computational cost measures the computational resources needed to generate a single decision and is defined in terms of the number of 1 bit full adders (FAs), which is a canonical building block of arithmetic units. Specifically, assuming that the arithmetic operations are executed using the commonly employed ripple carry adder and BaughWooley multiplier architectures, the number of FAs needed to compute a Ddimensional dot product between the activations and weights is <ref type="bibr" target="#b28">(Lin et al., 2016)</ref>:</p><formula xml:id="formula_7">DB w B x + (D − 1)(B x + B w + log 2 (D) − 1)<label>(5)</label></formula><p>where B w and B x denote the fixed-point precision assigned to the weights and activations, respectively.</p><p>• Weight representational cost measures the storage complexity and data movement costs corresponding to the weights and is defined as the product between the total number of bits needed to represent all weight parameters and the total number of times that the weights are used to compute convolutions:</p><formula xml:id="formula_8">N w |W| B w<label>(6)</label></formula><p>where N w and W denote the total number of times that the weights are used to compute convolutions and the index sets of all weights in the network, respectively.</p><p>• Activation representational cost is similar to the weight representational cost above and is defined as:</p><formula xml:id="formula_9">N x |X | B x<label>(7)</label></formula><p>N x and X denote the total number of times that the activations are used to compute convolutions and the index sets of all activations in the network, respectively.</p><p>The concepts of computational and representational costs were first proposed in  to describe network complexity. To better reflect the CNN energy cost, we modify the definition of representational cost in  to include the number of times that weights or activations are loaded for computing convolutions, in order to capture the associated data movement cost. Specifically, if a certain weight filter is removed due to compression, then the corresponding activation would be loaded less frequently, thus leading to reduced data movement costs. This can be reflected by the reduction of N x in our modified definition but not the originally defined representational cost in . Also, we separate the representational costs for the weights and activations to evaluate the impact of compression in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate Deep k-Means in terms of CR and energyaware metrics respectively, with the resulting accuracy loss ∆ after compression, using two sets of experiments. The default λ is 10 −4 . Unless otherwise specified, we will focus on compressing convolutional layers only.</p><p>For the first set of experiments on CR, we first create a simple baseline CNN for simulation experiments w.r.t. varying CR. The CR definition follows . We then compare Deep k-Means against four latest and competitive comparison baselines for compressing convolutional layers: Ultimate Tensorization <ref type="bibr" target="#b10">(Garipov et al., 2016)</ref>, FreshNet <ref type="bibr">(Chen et al., 2016a)</ref>, Greedy Filter Pruning (Abbasi-Asl &amp; Yu, 2017), and Soft Weight-Sharing <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref>. The first three have been optimized towards compressing CNNs dominated by the convolutions and reported results on their self-designed models. <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref> outperformed strong baselines such as <ref type="bibr" target="#b14">(Han et al., 2015)</ref> on the standard MNIST benchmark; the authors then reported compression results on the state-of-the-art Wide ResNet model <ref type="bibr" target="#b42">(Zagoruyko &amp; Komodakis, 2016</ref>) that mainly consist of convolutions. We also compare Deep k-Means with the baseline of Deep k-Means without re-training (Deep k-Means WR), i.e., directly performing row-wise k-means on original weights.</p><p>For the second set of experiments, we first validate the estimated energy consumption using our metrics to match the actual hardware-based extrapolation <ref type="bibr" target="#b41">(Yang et al., 2017)</ref>, and then evaluate Deep k-Means against the aforementioned baselines from an energy consumption perspective. While it is overall challenging to estimate energy consumption accurately due to the multitude of factors involved, our proposed metrics are simple, effective (i.e., showing a good match with the results extrapolated by actual hardware measurement), and thus can help CNN model designers understand various design trade-offs. We also provide insights regarding the impact of different types (i.e., parameter-sharing or pruning) of compression techniques on the computational and representational costs in Eqs. <ref type="formula" target="#formula_7">(5)</ref>, <ref type="formula" target="#formula_8">(6) and (7)</ref>.  <ref type="bibr" target="#b10">(Garipov et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison on Compression Ratio</head><p>. <ref type="bibr" target="#b10">(Garipov et al., 2016)</ref>  We evaluate Deep k-Means on the TT-conv-CNN model at CR = 2 and 4. <ref type="table">Table 1</ref> compares them with the compression results in <ref type="bibr" target="#b10">(Garipov et al., 2016)</ref>. Deep k-means incurs minimal accuracy loss even at CR = 4. More surprisingly, it even slightly increases the accuracy after compression at CR = 2. It concurs with the previous observations by <ref type="bibr" target="#b39">(Ullrich et al., 2017;</ref><ref type="bibr" target="#b7">Cheng et al., 2017)</ref>: removing parameter redundancy improves CNN generalization on some small networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">COMPARISON WITH FRESHNET</head><p>The Frequency-Sensitive Hashed Nets (FreshNets) was proposed in <ref type="bibr">(Chen et al., 2016a)</ref> to exploit inherent redundancy in convolutional layers. The authors observed that convolutional weights to be typically smooth and low-frequency. They were thus motivated to first convert filter weights to the frequency domain, after which they group frequency parameters into hash buckets to achieve parameter sharing. The authors evaluated their method on their self-designed CNN (referred to as FreshNet-CNN hereinafter) consisting of five convolutional layers and one fully-connected layer. They reported the uncompressed FreshNet-CNN to obtain the top-1 accuracy of 85.09% on CIFAR-10. <ref type="table">Table 2</ref> reports the compression results of Deep k-Means and Deep k-Means WR on FreshNet-CNN at CR = 16. We also include two original baselines in <ref type="bibr">(Chen et al., 2016a)</ref>: low-rank decomposition (LRD) <ref type="bibr" target="#b9">(Denil et al., 2013)</ref> and HashedNet <ref type="bibr" target="#b4">(Chen et al., 2015)</ref>. In this example, even the accuracy of Deep k-Means WR is very competitive. After retraining, Deep k-Means shows a sharp further improvement.  <ref type="table">Table 2</ref>. Compressing FreshNet-CNN in <ref type="bibr">(Chen et al., 2016a)</ref>.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">COMPARISON WITH GREEDY FILTER PRUNING</head><p>The recent work (Abbasi-Asl &amp; Yu, 2017) introduced a greedy structural compression scheme that prunes redundant convolutional filters in a trained CNN, based on a classification accuracy reduction (CAR) algorithm. The authors reported promising results on LeNet-5, AlexNet and ResNet-50, and their evaluations adopted a unique layerwise compression fashion: taking LeNet-5 for example, each time the authors pruned filters in one convolutional layer (first or second) while leaving other layers untouched, and then reported the overall accuracy.</p><p>We compare Deep k-Means with CAR (with re-training, the best performer in (Abbasi-Asl &amp; Yu, 2017)) using LeNet-5 on MNIST, and follow their layer-wise compression setting. Thus, unlike our other experiments, we report the accuracy w.r.t. "layer-wise" CR, i.e., measuring how many times the current layer is compressed, rather than the overall CR that measures the entire model. As <ref type="figure" target="#fig_1">Figure 1</ref> shows, both Deep k-Means and CAR produce similar results at small layer-wise CRs; CAR is more competitive at small CRs for Conv2. However, Deep k-Means is clearly superior at high layer-wise CRs for both layers.   . <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref> reported the compression performance of soft weight-sharing on the state-of-the-art Wide ResNet model <ref type="bibr" target="#b42">(Zagoruyko &amp; Komodakis, 2016)</ref>, a convolutiondominant CNN with 2.7M parameters, at one single CR = 45 using CIFAR-10 (the uncompressed baseline top-1 error is 6.48%). Thanks to the light computational burden of Deep k-Means, we are able to evaluate various CRs. Note that at the same CR, soft weight-sharing and Deep k-Means will lead to identical layer-wise dimensions and the same number of unique weights in each layer. Thus, their performance difference can only arise from the effects of their different regularization ways during re-training.</p><p>Promoting Sparsity in Re-Training. During the review stage, one anonymous reviewer commented that <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref> tried to explicitly enforce weight values to a cluster centered at zero, while the above default routine of Deep k-Means had not such constraint. Such a sparsity-promotion operation may marginally decrease compression performance as it restricts the flexibility of setting centroids, but can gain more in both speedup and energy savings <ref type="bibr" target="#b30">(Parashar et al., 2017)</ref>. To ensure a fair comparison with <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref>, we implement a similar sparsitypromoting feature for Deep k-Means, in this specific experiment only. Without referring to sophisticated options such as semi-supervised clustering <ref type="bibr" target="#b1">(Basu et al., 2002)</ref>, we follow a simple heuristic which incurs almost no extra complexity: at each time of "lazy update" for layer W ∈ R s×N , we first rank all N columns of W in terms of their 2 norms. We then assign the pN (0 &lt; p &lt;1) smallest-norm columns to one cluster with a fixed center at zero, before solving (4). At the parameter-sharing step, we similarly threshold pN smallest-norm columns in W to be all-zero, and then perform (k − 1)-clustering for remaining columns. The group of layer-wise p that we used for all 16 layers is: <ref type="bibr">[0, 0.3, 0.4, 0.5, 0.4, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6, 0.9, 0.5, 0.75, 0.9]</ref>. <ref type="table" target="#tab_3">Table 3</ref> demonstrates the superiority of Deep k-Means (with the above-described sparsity promotion) over <ref type="bibr" target="#b39">(Ullrich et al., 2017)</ref>, by comparing their top-1 accuracy drops: 1.63% versus 2.02 %, at CR = 45. We further display the results at CR = 47 and 50, with a smooth accuracy decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">EVALUATION WITH GOOGLENET ON IMAGENET</head><p>We finally evaluate Deep k-Means on the GoogleNet <ref type="bibr" target="#b36">(Szegedy et al., 2015)</ref> trained with the ImageNet ILSVRC12 dataset <ref type="bibr">(Russakovsky et al., 2015)</ref>. We use single center crop during testing, and evaluate the performance based on the top-1 and top-5 accuracy drops on the validation set, compared to the uncompressed baseline whose top-1 accuracy is 69.76% and top-5 89.63%. We include two comparison methods: one-shot network compression <ref type="bibr" target="#b21">(Kim et al., 2015)</ref>, and low-rank regularization <ref type="bibr" target="#b37">(Tai et al., 2015)</ref>. According to <ref type="table">Table 4</ref>, Deep k-Means proves to scale well on large models/datasets, and achieves significantly better results over the two baselines: its compression at CR ≤ 3 is almost lossless, with top-5 errors again observed to slightly increase after compression. The GoogleNet compression performance is found to deteriorate quickly when CR &gt; 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison on Energy-Aware Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">ENERGY-AWARE METRICS VERIFICATION</head><p>We first evaluate our energy-aware metrics by comparing its estimated energy consumption with that of the tool in <ref type="bibr" target="#b41">(Yang et al., 2017)</ref>. Note that the unit of energy: 1) in <ref type="bibr" target="#b41">(Yang et al., 2017)</ref> is normalized in terms of number of MAC operations</p><formula xml:id="formula_10">Model ∆ † % ∆ ‡ % CR</formula><p>One-shot <ref type="bibr" target="#b21">(Kim et al., 2015)</ref> N/A -0.24 1.28 Low-rank <ref type="bibr" target="#b37">(Tai et al., 2015)</ref>  while the computational cost in Eq. <ref type="formula" target="#formula_7">(5)</ref> is normalized in terms of number of FAs; and 2) for the representational cost in Eqs. <ref type="formula" target="#formula_8">(6)</ref> and <ref type="formula" target="#formula_9">(7)</ref> is different from that of the computational cost in Eq. (5). Therefore, we first normalize the representational cost in terms of the computational cost assuming that a global on-chip buffer is employed, implying that the representational cost of a MAC is about 6 times that of performing a MAC computation <ref type="bibr" target="#b6">(Chen et al., 2016b)</ref>. This normalized representational cost is then added to the computational cost to obtain our total energy. Lastly, we normalize this total energy in terms of the number of MACs to be the same as that of <ref type="bibr" target="#b6">(Chen et al., 2016b)</ref>.</p><p>We calculate the coefficient of determination (R 2 ), between the estimated energy consumptions using our proposed metrics, and using the tool in <ref type="bibr" target="#b41">(Yang et al., 2017)</ref>, of the same compressed models. We use Deep k-means to compress both AlexNet and GoogLeNet_v1 1 , which are the only two CNN models currently supported by <ref type="bibr" target="#b41">(Yang et al., 2017)</ref>. The energy consumptions are estimated as we choose the cluster rate to vary between: (AlexNet) <ref type="bibr">[0.5, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.01]</ref>, and (GoogLeNet_v1) <ref type="bibr">[0.33, 0.18, 0.05, 0.012]</ref>, respectively, to ensure negligible accuracy loss. We have R 2 to be 0.9931 for AlexNet, and 0.9675 for GoogLeNet_v1, suggesting the estimated energy consumptions using our proposed metrics to be strongly linearly correlated with the results extrapolated from actual hardware measurements <ref type="bibr" target="#b41">(Yang et al., 2017</ref>). Yet different from their tool, our metrics are generally applicable to any CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">COMPARISON WITH GREEDY FILTER PRUNING</head><p>An ideal CNN model to be deployed on resourceconstrained platforms should simultaneously possess compact model size and low energy cost. In general, the result-ing computational/representational cost reduction via compression depends on how the network is trimmed, i.e., which parts of the network is compressed. Conceptually, we point out that different compression schemes (e.g., parametersharing versus pruning) will affect the analysis of the computational and representational costs defined in Eqs. <ref type="formula" target="#formula_7">(5)</ref>, <ref type="formula" target="#formula_8">(6)</ref> and <ref type="formula" target="#formula_9">(7)</ref>. First, the weight representational cost is directly proportional to CR in both parameter-sharing (e.g. Deep k-Means and soft weight-sharing) and pruning (e.g. CAR) cases, because they both in effect can reduce the term |W| in (6). Second, the activation representational cost is proportional to CR for the case of weight pruning, but is independent of CR if the compression is done by weight sharing. This is because weight pruning results in skipping of the corresponding computations and thus can reduce the number of times that the corresponding activations are used (i.e., W in (7)), whereas there is no computation or connection skipping in the case of weight-sharing. Third, the computational cost is again proportional to CR for weight pruning; yet it would become input-dependent when it comes to weight sharing. Specifically, only when all the weights corresponding to the same input/activation are shared, the computational cost reduction ratio becomes equal to CR.  Deep k-Means has constantly obtained the best CR performance among the aforementioned baselines. To provide a concrete example, we choose the CAR (with retraining) baseline in <ref type="bibr" target="#b0">(Abbasi-Asl &amp; Yu, 2017)</ref>, which produces slightly inferior but still competitive CR results, and discuss its potential energy efficiency improvement compared with Deep k-Means. The same layer-wise compression setting in Section 4.1.3 is adopted, for the first two convolutional layers of LeNet-5. A similar analysis could be done for other methods too.</p><p>Deep k-Means compresses the network via weight sharing, whereas CAR relies on weight pruning. The accuracy versus CR comparison (i.e., <ref type="figure" target="#fig_1">Figure 1</ref>) in Section 4.1.3 shows that Deep k-Means is clearly superior to CAR at high layer-wise CRs, for either of the two convolutional layers. The potential energy consumption comparison between Deep k-Means and CAR in terms of the three metrics are as follows 2 .</p><p>First, Deep k-Means will achieve higher weight representational cost reduction since it mostly offers higher CR with the same or even better accuracy, in particular at high CRs. <ref type="figure" target="#fig_3">Figure 2</ref> (a) and (b) compares the accuracy versus weight representational cost reduction ratio of Deep k-Means and CAR for compressing the first and second convolutional layers in LeNet 5, respectively 3 . We observe that Deep k-Means achieves about 10% and 17.2% higher weight representational cost reduction, respectively, compared to CAR when compressing the first and second layers, without incurring accuracy loss. Second, CAR always outperforms Deep k-Means in terms of activation representational cost, because it removes filters and thus reduces the numbers of feature maps. In theory, CAR can achieve up to (layer-wise) "CR times" better activation representational cost reduction ratio than Deep k-Means. Third, the achievable computational cost reduction by Deep k-Means is either smaller or equal to that of CAR, depending on the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussions</head><p>This paper proposes Deep k-Means, a retraining-thenparameter-sharing pipeline for compressing convolutional layers in deep CNNs. A novel spectrally relaxed k-means regularization is derived to make hard assignments of convolutional layer weights to learned cluster centers during re-training. Deep k-Means demonstrates clear superiority over several recently-proposed competitive methods, in terms of both compression ratio and energy efficiency. Our future work will exploit more adaptive cluster rates for different layers instead of the current uniform scheme. Based on our proposed metrics, we also aim to incorporate more energy-aware regularizations into Deep k-Means for direct minimization of energy consumptions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>proposed a tensor factorization based method specifically for compressing convolutional layers. The authors proposed a Tensor Train (TT) Decomposi- tion approach for convolutional kernel, denoted as TT-conv (naive). It could be further enhanced by introducing a new type of TT-conv layer, denoted as TT-conv. The authors evaluated TT-conv (naive) and TT-conv on a self-designed architecture, called TT-conv-CNN, consisting of six con- volutional layers and one fully-connected layer. TT-conv- CNN is dominated by the convolutions (occupying 99.54% parameters of the network), and the authors reported the un- compressed model's top-1 accuracy of 90.7% on CIFAR-10 (Krizhevsky &amp; Hinton, 2009).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Compressing LeNet following the layer-wise setting in (Abbasi-Asl &amp; Yu, 2017): (a) The overall classification accuracy of LeNet when only the first convolutional layer (Conv1) is compressed, w.r.t. layer-wise CR; (b) The overall classification accuracy of LeNet when only the second convolutional layer (Conv2) is compressed, w.r.t. layer-wise CR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison between Deep k-Means and CAR, in terms of the ratio between weight representational cost reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Compressing TT-conv-CNN in</figDesc><table>4.1.1. COMPARISON WITH ULTIMATE TENSORIZATION 

Model 
∆ (%) CR 

TT-conv (naive) -2.4 
2.02 
TT-conv (naive) -3.1 
2.90 
TT-conv 
-0.8 
2.02 
TT-conv 
-1.5 
2.53 
TT-conv 
-1.4 
3.23 
TT-conv 
-2.0 
4.02 
Deep k-Means 
+0.05 2 
Deep k-Means 
-0.04 
4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Compressing</figDesc><table>Wide ResNet in comparison to soft weight-
sharing (Ullrich et al., 2017). 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For both networks, we employ our proposed methods in convolutional layers only. For AlexNet, we only quantize weight and activation to 8 bit and to 16 bit in fully-connected layers, respectively. For GooLeNet_v1, we use the one with global average pooling, which has no fully-connected layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We are unable to directly verify the total energy consumption of CAR using our metrics due to the lack of their model parameters or pre-trained model. 3 We did not consider the cost of weight assignment indexes as it is negligible due to achievable high CR.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank all anonymous reviewers for their tremendously useful comments to help improve our work. We acknowledge the inspiring discussions with Dr. Yang Zhang at IBM Watson and Dr. Edwin Park at Qualcomm Technologies, Inc. We acknowledge the Texas A&amp;M and Rice High Performance Research Computing for providing a part of the computing resources used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Abbasi-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07356</idno>
		<title level="m">Structural compression of convolutional neural networks based on greedy filter pruning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering by seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sugato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 19th International Conference on Machine Learning (ICML-2002</title>
		<meeting>19th International Conference on Machine Learning (ICML-2002</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparsification and separation of deep learning layers for constrained resource inference on wearables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SenSys</title>
		<meeting>SenSys</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soravit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06257</idno>
		<title level="m">The power of sparsity in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1504.04788</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compressing convolutional neural networks in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="1475" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09282</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems 2. chapter Optimal Brain Damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
		<idno>1-55860-100-7</idno>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ultimate tensorization: compressing convolutional and fc layers alike</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03214</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<idno>abs/1412.6115</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<idno>abs/1506.02626</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing biases for minimal network construction with back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lorien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Touretzky, D. S.</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Hanson, S. J., Cowan, J. D., and Giles, C. L.</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reshaping deep neural network for fast decoding by node-pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="245" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menglong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weijun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Squeezenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions Iandola</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Deok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eunhyeok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sungjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taelim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06530</idno>
		<title level="m">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepx: A software accelerator for low-power deep learning inference on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sourav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Forlivesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorena</forename><surname>Qendro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPSN</title>
		<meeting>IPSN</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Igor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variation-tolerant architectures for convolutional neural networks in the near threshold voltage regime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Workshop on Signal Processing Systems (SiPS)</title>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predictivenet: An energy-efficient convolutional neural network via zero prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjune</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCAS</title>
		<meeting>ISCAS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minsoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rangharajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brucek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analytical guarantees on numerical precision of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charbel</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjune</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, Doina and Teh, Yee Whye</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Edge computing: Vision and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="637" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Data-free parameter pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<idno>abs/1507.06149</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaogang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06067</idno>
		<title level="m">Convolutional neural networks with low-rank regularization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc&amp;apos;aurelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Soft weight-sharing for neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Meeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04008</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identify your font from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Jianchao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hailin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepfont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Designing energy-efficient convolutional neural networks using energy-aware pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu-Hsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spectral relaxation for k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hongyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaofeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
