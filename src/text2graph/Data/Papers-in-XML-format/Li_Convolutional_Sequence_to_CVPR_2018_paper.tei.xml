<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Sequence to Sequence Model for Human Dynamics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Gim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Sequence to Sequence Model for Human Dynamics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding human motion is extremely important for various applications in computer vision and robotics, particularly for applications that require interaction with humans. For example, an unmanned vehicle must have the ability to predict human motion in order to avoid potential collision in a crowded street. Besides, applications such as sports analysis and medical diagnosis may also benefit from human motion modeling.</p><p>The biomechanical dynamics of human motion is extremely complicated. Although several analytic models have been proposed, they are limited to few simple actions such as standing and walking <ref type="bibr" target="#b17">[18]</ref>. For more complicated actions, data-driven methods are required to attain acceptable accuracy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. In this paper, we focus on the human motion prediction task, using learning based methods from motion capture data. Recently, along with the success of deep learning in various areas of computer vision and machine learning, deep recurrent neural network based models have been introduced in human motion prediction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>. In earlier works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, it is often observed that there is a significant discontinuity between the first predicted frame of motion and the last frame of the true observed motion. Martinez et al. <ref type="bibr" target="#b13">[14]</ref> solved the problem by adding a residual unit in the recurrent network. However, their residual unit based model often converges to an undesired mean pose in the long-term predictions, i.e., the predictor gives static predictions similar to the mean of the ground truth of future sequences (see <ref type="figure" target="#fig_0">Figure 1</ref>). We believe that the mean pose problem is caused by the fact that it is difficult for recurrent models to learn to keep track of long-term information; the mean pose becomes a good prediction of the future pose when the model loses track of information from the distant past. For a chainstructured RNN model, it takes n steps for two elements that are n time steps apart to interact with each other; this may make it difficult for an RNN to learn a structure that is able to exploit long-term correlations <ref type="bibr" target="#b5">[6]</ref>.</p><p>The current state-of-the-art for human motion modeling <ref type="bibr" target="#b13">[14]</ref> is based on the sequence-to-sequence model, which is first proposed for machine translation <ref type="bibr" target="#b15">[16]</ref>. The sequenceto-sequence model consists of an encoder and a decoder, in which the encoder maps a given seed sequence to a hidden variable, and the decoder maps the hidden variable to the target sequence. A major difference between human motion prediction and other sequence-to-sequence tasks is that human motion is a highly constrained system by environment properties, human body properties and Newton's Laws. As a result, the encoder needs to learn these constraints from a relative long seed sequence. However, RNN may not be able to learn these constraints accurately, and the accumulation of these errors in decoder may result in larger error in long-term prediction.</p><p>Furthermore, the human body is often not static stable during motion, and our central neural system must make multiple parts of our body coordinate with each other to stabilize the motion under gravity and other loads <ref type="bibr" target="#b19">[20]</ref>. Thus joints from different limbs have both temporal and spatial correlations. A typical example is human walking. During walking, most people tend to move left arm forward while moving right leg forward. However, RNN based methods have difficulties learning to capture this kind of spatial correlations well, and thus may generate some unrealistic predictions. Despite these limitations, the RNN based method <ref type="bibr" target="#b13">[14]</ref> is considered to be the current state-of-the-art as its performance is superior to other human motion prediction methods in terms of accuracy.</p><p>In this paper, we build a convolutional sequence-tosequence model for the human motion prediction problem. Unlike previous chain-structured RNN models, the hierarchical structure of convolutional neural networks allows it to naturally model and learn both spatial dependencies as well as long-term temporal dependencies <ref type="bibr" target="#b5">[6]</ref>. We evaluate the proposed method on the Human3.6M and the CMU Motion Capture datasets. Experimental results show that our method can better avoid the long-term mean pose problem, and give more realistic predictions. The quantitative results also show that our algorithm outperforms state-ofthe-art methods in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The main task of this work is human motion prediction via convolutional models, while previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref> mainly focus on RNN based models. We briefly review the literature as follows.</p><p>Modeling of human motion Data driven methods in human motion modeling face a series of difficulties including high-dimensionality, complicated non-linear dynamics and the uncertainty of human movement. Previously, Hidden Markov Model <ref type="bibr" target="#b1">[2]</ref>, linear dynamics system <ref type="bibr" target="#b14">[15]</ref>, Gaussian Process latent variable models <ref type="bibr" target="#b18">[19]</ref> etc. have been applied to model human motion. Due to limited computational resource, all these models have some trade-off between model capacity and inference complexity. Conditional Restricted Boltzmann Machine (CRBM) based method has also been applied to human motion modeling <ref type="bibr" target="#b16">[17]</ref>. However, CRBM requires a more complicated training process and it also requires sampling for approximate inference.</p><p>RNN based human motion prediction Due to the success of recurrent models in sequence-to-sequence learning, a series of recurrent neural network based methods are proposed for the human motion prediction task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>. Most of these works have a recurrent network based encoder-decoder structure, where the encoder accepts a given motion frames or sequence and propagates an encoded hidden variable to the decoder, which then generates the future motion frame or series. The main differences in these works lie in their different encoder and decoder structures. For example, in the Encoder-Recurrent-Decoder (ERD) model <ref type="bibr" target="#b4">[5]</ref>, additional non-recurrent spatial encoder and decoder are are added to its recurrent part, which captures the temporal dependencies <ref type="bibr" target="#b15">[16]</ref>. In Structural-RNN <ref type="bibr" target="#b9">[10]</ref>, several RNNs are stacked together according to a hand-crafted spatial-temporal graph. Martinez et al. <ref type="bibr" target="#b13">[14]</ref> proposed a residual based model, which predicts the gradient of human motion rather than human motion directly, and used a standard sequence-to -sequence learning model with (GRU) <ref type="bibr" target="#b3">[4]</ref> cell as the encoder and decoder. In these RNN based models, fully-connected layers are used to learn a representation of human action, and the recurrent middle layers are applied to model the temporal dynamics. In contrast to previous RNN based models, we use a convolutional model to learn the spatial and the temporal dynamics at the same time and we show that this outperforms the state-of-the-art methods in human motion prediction.</p><p>Convolutional sequence-to-sequence model The task of a sequence-to-sequence model is to generate a target sequence from a given seed sequence. Most sequence-tosequence models consist of two parts, an encoder which encodes the seed sequence into a hidden variable and a decoder which generates the target sequence from the hidden variable. Although RNNs seem to be a natural choice for sequential data, convolutional models have also been adopted to sequence-to-sequence tasks such as machine translation. Kalchbrenner and Blunsom <ref type="bibr" target="#b10">[11]</ref> proposed the Recurrent Continuous Translation Model (RCTM) which uses a convolutional model as encoder to generate hidden variables and a RNN as decoder to generate target sequences, while later methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref> are fully convolutional sequenceto-sequence model. However, unlike machine translation, where only temporal correlations exist, there exist complicated spatial-temporal dynamics in human motion. Thus we design a convolutional sequence-to-sequence model that is suitable for complicated spatial-temporal dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>We adapt a multi-layer convolutional architecture, which has the advantage of expressing input sequences hierarchically. In particular, when we apply convolution to the input skeleton sequences, lower layers will capture dependencies between nearby frames and higher layers will capture dependencies between distant frames. Unlike the chainstructured RNN, the hierarchical structure of a multi-layer convolutional architecture is designed to capture long-term dependencies. <ref type="figure" target="#fig_1">Figure 2</ref> shows an illustration of the architecture of our network, where the convolutional encoding module (CEM) plays the central role. We use the CEM module as long and short-term encoder. The long-term encoder is used to memorize a given motion sequence as the long-term hidden variable z e l , and the short-term encoder is used to map a shorter sequence to the short-term hidden variable z e s . Finally the hidden variables z e l and z e s are concatenated together and propagated to the decoder to give a prediction of next frame. The short-term encoder and decoder are applied recursively to produce the whole predicted sequence. We use convolutional layers with stride two in the CEM, thus two elements with distance n are able to interact with each other in O(log n) operations. Furthermore, we use a rectangle convolution kernel to get a larger perception range in the spatial domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutional sequence-to-sequence model</head><p>Similar to previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>, we also use an encoder-decoder model as a predictor to generate future motion sequences. However unlike previous works, we adapt a convolutional model for this sequence-to-sequence modeling task. Specifically, both the encoder and the decoder consist of similar convolutional structure, which computes a hidden variable based on a fixed number of inputs. There have been several convolutional sequence-tosequence models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref> that have been shown to give better performance than RNN based models in machine translation. However, these models mainly use convolution in the temporal domain to capture correlations, while in human motion there are also complicated spatial correlations between different body parts.</p><p>We first formalize the human motion prediction problem before giving more details of our convolutional model. Assume that we are given a series of seed human motion poses</p><formula xml:id="formula_0">X 1:t = [x 1 , x 2 , . . . , x t ], where each x i ∈ R</formula><p>L is a parameterization of human pose. The goal of human motion prediction is to generate a target predictionX (t+1):(t+T ) for the next T frame poses.</p><p>We aim to capture the long-term information, such as categories of actions, human body properties (e.g. step length, step pace etc.), environmental constraints etc. from the seed human motion poses. To this end, a convolutional long-term encoder is used in our model. It maps the whole sequence X 1:t = [x 1 , x 2 , . . . , x t ] to a hidden variable</p><formula xml:id="formula_1">z e l = h e l (X 1:t | w e l ),<label>(1)</label></formula><p>where w e l is the parameter of the long-term encoder h e l . Our decoder has an encoder-decoder structure, which consists of a short-term encoder and a spatial decoder. The short-term encoder</p><formula xml:id="formula_2">z e s = h e s (X t−C+1:t | w e s ),<label>(2)</label></formula><p>where w e s is the parameter. It maps a shorter sequence X t−C+1:t , which consists of C neighboring frames of the current frame, to a hidden variable. Note that our short-term encoder is a sliding window of size C, it only encodes the most recent C frames. Finally, the long-term and short-term hidden variables z e l and z e e are concatenated together as a input of the spatial decoder, which predicts the next posex t+1 asx</p><formula xml:id="formula_3">t+1 = h d ([z e l , z e s ]| w d ),<label>(3)</label></formula><p>where w d is the parameter of the spatial decoder h d . To predict a sequence, the short-term encoder will slide one frame forward once a new frame is generated, thus the short-term encoder and decoder are applied recursively as</p><formula xml:id="formula_4">z e s (k) = h e s (X t−C+k:t+k−1 | w e s ), x t+k = h d ([z e l , z e s (k)]| w d ),<label>(4)</label></formula><formula xml:id="formula_5">whereX t−C+k:t+k−1 = [x t−C+k , x t−C+k , . . . , x t , x t+1 , . . . ,x t+k−1 ].<label>(5)</label></formula><p>In our model, the long-term encoder and short-term encoder have the similar structure, i.e. the CEM, which includes 3 convolutional layers and 1 fully connected layers. The number of output channels for each convolutional layer is 64, 128 and 128, and the output number of the fully connected layer is 512. As discussed earlier, the CEM needs to capture long-term correlations in order to improve the prediction accuracy. Thus the stride of every convolutional layer is set to 2. With such convolutional layers, two elements of distance n are able to interact with each other with a path length O(log(n)), while O(n) steps are required in a conventional RNN. Furthermore, the perception range of the CEM in the spatial domain should be large enough to capture the spatial correlations of joints from different limbs. Hence, we use a rectangle 2 × 7 convolutional kernel (2 along temporal domain, and 7 along spatial domain) to enlarge the perception domain in the spatial domain. We use a simple two layer fully-connected neural network for the spatial decoder. The first layer maps a 1024 dimensional hidden variable to 512 dimensions and uses a leaky ReLU as the activation function. The second layer maps the hidden variable to one frame of human poses, and does not include an activation function. We also use a residual link in our network as suggested by previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. This means that out decoder actually predicts the residual value rather than directly generates the next frame. Consequently, the output of our network consists of two parts:</p><formula xml:id="formula_6">x t+k = h d ([z e l , z e s (k)]| w d ) +x t+k−1 .<label>(6)</label></formula><p>h d and w d denote the decoder and its parameters.</p><p>Comparison to RNN In recurrent neural network based models (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>), the encoded hidden variable often serves as the initial state of the decoding RNN. Thus during the long propagation path in RNN, the encoded information may vanish. However, our proposed model does not have this problem because the encoded hidden variable z e l is always maintained. In recurrent neural networks, the model captures short-term dynamical information through variation of hidden states. In our model, the short-term dynamical information is captured by the short-term encoder from a short sequence. By using such a structure, our model is able to capture long-term invariant information and short-term dynamical information, and thus resulting in better performance in both long-term and short-term predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization</head><p>During training, we use the mean squared error of the predicted poses as the loss function:</p><formula xml:id="formula_7">ℓ model (X (t+1):(t+T ) , X (t+1):(t+T ) ) = 1 T T t ′ =1 x t+t ′ − x t+t ′ 2 2 .<label>(7)</label></formula><p>Three different types of regularizing technique are used to prevent overfitting -dropout, ℓ 2 regularizer and adversarial regularizer. We added a dropout layer between the last convolutional and first fully-connected layers in our CEM module. In our decoder, we added a dropout layer between the two fully-connected layer. The dropout probability in both dropout layers is set to 0.5. Motivated by the generative adversarial network (GAN), we apply an adversarial regularizer for the proposed model, which mainly improves the qualitative performance. We train an additional discriminator to classify the generated and real sequences as follows</p><formula xml:id="formula_8">min w D − X 1:t+T log D(X 1:t+T | w D ) (8) − [X1:t,X t+1:t+T ] log(1 − D([X 1:t ,X t+1:t+T ]| w D )).</formula><p>The discriminator D is then used to encourage the generation of realistic sequences. Finally, the objective becomes</p><formula xml:id="formula_9">min w e l ,w e s ,w d X 1:t+T ℓ model (X (t+1):(t+T ) , X (t+1):(t+T ) ) + λ 2 [ w e l 2 2 + w e s 2 2 + w d 2 2 ] (9) − λ adv log(D([X 1:t ,X t+1:t+T ]| w D )),</formula><p>where the weights λ 2 and λ adv are set to 0.001 and 0.01, respectively. In the optimization procedure, we used stochastic gradient descent based optimizer to run iteratively optimizing over <ref type="bibr" target="#b8">(9)</ref> and <ref type="bibr" target="#b7">(8)</ref>.</p><p>Remarks There are multiple choices ofX t−C+k:t+k−1 in (5), which may have different effects on the training results. In previous works, the corresponding part is often set to ground truth, or ground truth with noise <ref type="bibr" target="#b4">[5]</ref>. Besides settingX t−C+k:t+k−1 as (5), it can also be set tō</p><formula xml:id="formula_10">X t−C+k:t+k−1 = [x t−C+k , x t−C+k+1 , . . . , x t ,<label>(10)</label></formula><formula xml:id="formula_11">ηx t+1 + (1 − η) x t+1 , . . . , ηx t+k−1 + (1 − η) x t+k−1 ],</formula><p>where η ∈ [0, 1] is a manually specified parameter.</p><p>Note that the window size of the short-term encoder C may also affect the results. The model may not capture enough short-term information when C is too small. On the other hand, it may be a waste of computation when C is too large since we already have the long-term encoder. Hence, the value of C should be a trade-off between accuracy and computation. The effect of different window sizes are explored in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we apply the proposed convolutional model on several human motion prediction tasks. The proposed method is compared with several recent and state-ofthe-art matching algorithms:</p><p>• The Encoder-Recurrent-Decoder (ERD) method <ref type="bibr" target="#b4">[5]</ref>;</p><p>• An three layer LSTM with linear encoder and decoder (LSTM-3LR) <ref type="bibr" target="#b4">[5]</ref>; • Stuctural Recurrent Neural Networks (SRNN) <ref type="bibr" target="#b9">[10]</ref>;</p><p>• Residual Recurrent Neural Networks (RRNN) <ref type="bibr" target="#b13">[14]</ref>;</p><p>• An three layer LSTM with an denoising auto encoder (LSTM-AE) <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our model is implemented in tensorflow, and we used the ADAM <ref type="bibr" target="#b12">[13]</ref> optimizer to optimize over our model. The batch size is set to 64 and the learning rate is 0.0002. For more optimizing details, please refer to Section 3.2. Following the setting of previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, the length of seed pose sequence is set to 50, and the length of target sequence is set to 25. We trained RRNN <ref type="bibr" target="#b13">[14]</ref> model based on the public available implementation 2 . We quote the results from <ref type="bibr" target="#b13">[14]</ref> for ERD, LSTM-3LR, SRNN, and <ref type="bibr" target="#b6">[7]</ref> for LSTM-AE.</p><p>Action specific model v.s.general model ERD <ref type="bibr" target="#b4">[5]</ref>, LSTM-3LR <ref type="bibr" target="#b4">[5]</ref> and SRNN <ref type="bibr" target="#b9">[10]</ref> are action specific models, where they train a specific model for each action. On the other hand, the RRNN model <ref type="bibr" target="#b13">[14]</ref> considers the more challenging task of training a general model for multiple actions. In our experiments, we also train a single model for multiple actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Preprosessing</head><p>In the experiments, we consider two datasets: the Human 3.6M dataset <ref type="bibr" target="#b8">[9]</ref> and the CMU Motion Capture dataset <ref type="bibr" target="#b2">3</ref> . The Human 3.6M dataset is currently the largest available video pose dataset, which provides accurate 3D body joint locations recorded by a Vicon motion capture system. It is regarded as one of the most challenging datasets because of the large pose variations performed by different actors. There are 15 activity scenarios in total. Each action scenario includes 12 trials lasting between 3000 to 5000 frames. The 12 trials are categorized as 6 subjects, where each subject includes 2 trials. Each 3D pose consists of 32 joints plus a root orientation and displacement represented as an exponential map.</p><p>During the experiments, each pose would subtract to the mean pose over all trials and gets divided by the standard deviation. We eliminate the joint angle dimensions with constant standard deviation, which corresponds to joints with less than three degrees of freedom. Furthermore, the global rotation and translation are set to zero since our models are not trained with this information. Finally, the dimension of the input vector is set to 54. Similar to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>, we treat the two sequences in subject 5 as the test set and all others as the training set. For evaluation, we calculate the Euclidean error in terms of Euler angle. Specifically, we measure the Euclidean distance between our predictions and the ground truth in terms of Euler angle for each action, followed by calculating the mean value over all sequences which are randomly selected from the test set.</p><p>We also apply our model to the CMU Motion Capture dataset in order to test its generalization ability. There are five main categories in the dataset -"human interaction", "interaction with environment", "locomotion", "physical activities &amp; sports" and "situations &amp; scenarios". We choose some of the actions for our experiments based on some criteria. Firstly, we do not use data from the "human interaction" category since multiple subjects motion prediction is out of the scope of this paper. Secondly, action categories which include less than six trials are excluded on the consideration that we need enough data for each action to train our model. Lastly, some action categories in the dataset are actually combinations of other actions, e.g. actions in the subcategory "playground" consist of jump, climb and other actions which already exist in the dataset. We do not chose these action categories to avoid repetition. Finally, eight actions are selected for our experiments -running, walking and jumping from category "locomotion", basketball and soccer from category "physical activities &amp; sports", wash windows from category "common behaviours and expressions", traffic direction and basketball signals from category "communication gestures and  <ref type="bibr" target="#b9">[10]</ref> 0.81 0. <ref type="figure" target="#fig_0">94 1.16 1.30 N/A 0.97 1.14 1.35 1.46 N/A 1.45 1.68 1.94 2.08 N/A 1.22 1.49 1.83 1.93 N</ref>  <ref type="table">Table 2</ref>. Motion prediction error in terms of Euler angle error for the rest actions in the Human3.6M dataset for short-term of 80, 160, 320, 400, and long-term of 1000ms (best result in bold).</p><p>Directions Greeting Phoning Posing ms 80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 RRNN <ref type="bibr" target="#b13">[14]</ref>  signals". We pre-process the data and evaluate the results in the same way as we did on the Human 3.6M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Human3.6M and CMU Datasets</head><p>We first report our results on all actions in the Human 3.6M dataset for both short-term prediction of 80 ms, 160 ms, 320 ms, 400 ms and long-term prediction of 1000 ms. Among the 15 actions in the dataset, the four actions "walking", "eating", "smoking" and "discussion" are commonly used in comparison of action specific human motion prediction methods. Thus we compare the accuracy of our method against four action specific methods ERD <ref type="bibr" target="#b4">[5]</ref>, LSTM-3LR <ref type="bibr" target="#b4">[5]</ref>, Structural RNN (SRNN) <ref type="bibr" target="#b9">[10]</ref> and LSTM-AE <ref type="bibr" target="#b6">[7]</ref>, as well as one general motion prediction method RRNN <ref type="bibr" target="#b13">[14]</ref> in <ref type="table" target="#tab_0">Table 1</ref>. From the results, we can see that our method outperforms the others in most cases. We also provide the qualitative comparison results with the state-of-the-art RRNN method in <ref type="figure">Figure 3</ref>. Both RRNN and our model achieve good result on "walking" because of its periodic property, which makes the action easier to model. But for other aperiodic classes like "eating", "smoking" and "discussion", RRNN quickly converges to a mean pose -the predicted figure could not put its hands down in "eating" and raise its hand up in "discussion".</p><p>Rather than maintaining a gesture in which one leg should be put on the other one in the action "smoking", RRNN generates an implausible motion in real life that would cause the subject to go off balance. This further shows that it is very important to take the correlation between different body parts into consideration so that the predicted pose is more realistic. In comparison, our model predicts plausible motions for both "eating" and "smoking". Furthermore, it is observed in the highly aperiodic action "discussion" that our model can still predict the correct motion trend, i.e. raising the hands while talking, even though this motion is not exactly the same as the ground truth. We compare our algorithm with the general human prediction model RRNN <ref type="bibr" target="#b13">[14]</ref> for the other 11 actions. The quantitative comparison results are provided in <ref type="table">Table 2</ref>, which suggest that our algorithm outperforms RRNN in most cases. Additionally, our method outperforms RRNN on the average in both long and short-term predictions. The out-performance of our method becomes more significant for longer term predictions.</p><p>We only consider the more challenging task of training a general motion prediction model for all actions using the CMU Motion Caption dataset. Hence, we only show comparison results with the state-of-the-art RRNN method. For a fair comparison, both our model and RRNN are trained using the same settings on the Human3.6M dataset. The testing error of each action is given in <ref type="table" target="#tab_1">Table 3</ref> and the average testing error is given in <ref type="table" target="#tab_2">Table 4</ref>. In the quantitative comparison, our method outperforms the RRNN method in several challenging actions such as jumping and running. The qualitative comparisons of running and jumping are also shown in <ref type="figure" target="#fig_6">Figure 5</ref>. In the qualitative comparisons, we can see that <ref type="figure">Figure 3</ref>. Qualitative results on for long-term prediction based on the Human3.6M dataset. Starting from the left top clockwisely the four actions are "walking", "sitting", "smoking" and "discussion". For each action, the top, middle and bottom sequences correspond to RRNN, our model and ground truth respectively. The first four frames are the last four frames of conditional seed frames and the next ones are predicted frames. The RRNN converges to mean pose for eating and discussion, and generates a prediction which is not realistic for smoking. Our model suffers less from the mean pose problem and predicts more realistic future.  RRNN converges to mean pose for both running and jumping. On the other hand, our prediction for running is very close to the realistic one. For jumping, our model also predicts the correct motion trend, i.e. squatting followed by jumping, and the main error comes from the duration of squatting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>The role of long-term encoder The long-term encoder in our model is used to capture long-term dependencies. We verify its effectiveness by removing it from our model. The results in <ref type="table" target="#tab_3">Table 5</ref> suggest that the average error gets larger without the long-term encoder, especially for long-term prediction of 1000 ms.</p><p>Rectangular kernel over spatial axis We use a rectangular kernel over the spatial axis (2 × 7 kernel) in our CEM in order to better capture the dependencies between different body parts. We also verify the effectiveness by comparing it with square kernel (4 × 4 kernel) and rectangular kernel over the temporal axis (7 × 2 kernel). The results in <ref type="table" target="#tab_4">Table 6</ref> indicate that the 2 × 7 kernel is the best choice.</p><p>Adversarial regularizer We use an adversarial regularizer in our model to help generate more plausible motion. In order to explore the role of the adversarial regularizer, we compare the performance of our model with and without the regularizer. The result in <ref type="figure" target="#fig_5">Figure 4</ref> suggests that the adversarial regularizer helps to improve the performance of our model even though marginally. Moreover, since the adversarial regularizer is only used during training, it does not add complexity to our model during inference.</p><p>Different window size C In our decoder, different window sizes C result in different perception range. Intuitively, enlarging the window size may enlarge the perception range and results in better performance, but also requires more Running Soccer Walking Washwindow ms 80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 RRNN <ref type="bibr" target="#b13">[14]</ref>    computation resources. We thus train three different models with window size C = 5 10 and 20. In the right plot of <ref type="figure" target="#fig_5">Figure 4</ref>, we show average testing error over all 15 actions. The result suggests that there is not much improvement when the window size is larger than 10. Consequently, we set C = 20 for our model in view of the trade-off between accuracy and computational accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed a convolutional sequence-tosequence model for human motion prediction. We adopted two types of convolutional encoders in our model, namely the long-term encoder and short-term encoder, so that both distant and nearby temporal motion information can be used for future prediction. We demonstrated that our model performs better than existing state-of-the-art RNN based models, especially for long-term prediction tasks. Moreover, we show that our model can generate better predictions for complex actions due to the use of hierarchical convolutional structure for modeling complicated spatial-temporal correlations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Frames on the left are the observations fed into our network. The middle part is the short-term prediction results for RNN (on the top) and our model (in the bottom). The right part is the long-term prediction results, in which RNN converge to a mean pose, while our model can predict future frames which are similar to the ground truth (in the middle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An illustration of our network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Left: Prediction error vs time for the model. For the model with LSTM as encoder (LSTM ENC), the error accumulates much faster than the original model, which means that our model performs better especially for long-term prediction. Middle: Comparison of testing error with different window length C for training, namely 5, 10, 20. The long-term error decreases when window size increases from 5 to 10, while the improvement is not obvious when further increase from 10 to 20. Right : Compare the testing error of our model with and without adversarial regularizer, it shows that adversarial regularizer helps to train a model with better performance in long-term.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative result on the CMU Motion Capture dataset. Top: "Running" action; Bottom: "Jumping" action. For each action, the top, middle are the results of RRNN and our model, and the ground truth is given in the bottom. The first four frames are the last four frames of the conditional seed frames and the next are the predicted ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Motion prediction error in terms of Euler angle error for walking, eating, smoking and discussion in the Human3.6M dataset for short-term of 80, 160, 320, 400, and long-term of 1000ms (best result in bold).</figDesc><table>Walking 
Eating 
Smoking 
Discussion 
ms 
80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 

ERD[5] 

0.93 1.18 1.59 1.78 N/A 1.27 1.45 1.66 1.80 N/A 1.66 1.95 2.35 2.42 N/A 2.27 2.47 2.68 2.76 N/A 
LSTM-3LR[5] 0.77 1.00 1.29 1.47 N/A 0.89 1.09 1.35 1.46 N/A 1.45 1.68 1.94 2.08 N/A 1.88 2.12 2.25 2.23 N/A 

SRNN </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Motion prediction error in terms of Euler angle error for eight actions in the CMU Motion capture dataset for short-term of 80, 160, 320, 400, and long-term of 1000ms (best results in bold).RRNN [14] 0.50 0.80 1.27 1.45 1.78 0.41 0.76 1.32 1.54 2.15 0.33 0.59 0.93 1.10 2.05 0.56 0.88 1.77 2.02 2.40 Ours 0.37 0.62 1.07 1.18 1.95 0.32 0.59 1.04 1.24 1.96 0.25 0.56 0.89 1.00 2.04 0.39 0.60 1.36 1.56 2.01</figDesc><table>Basketball 
Basketball Signal 
Directing Traffic 
Jumping 
ms 
80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Average testing error on the CMU Motion Capture dataset in terms of Euler angle error.</figDesc><table>ms 
80 
160 
320 
400 1000 

RRNN [14] 

0.38 0.62 1.02 1.17 1.67 

Ours 

0.31 0.52 0.86 0.99 1.55 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Our model w/wo long-term Encoder on Human3.6M.</figDesc><table>ms 
80 
160 
320 
400 1000 

With long-term Encoder 

0.38 0.68 1.01 1.13 1.77 

Without long-term Encoder 

0.41 0.72 1.05 1.17 1.88 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 6 .</head><label>6</label><figDesc>Comparison of different kernels on Human3.6M.</figDesc><table>ms 
80 
160 
320 
400 1000 

4 × 4 kernel 

0.41 0.72 1.05 1.16 1.80 

7 × 2 kernel 

0.40 0.71 1.05 1.17 1.79 

2 × 7 kernel 

0.38 0.68 1.01 1.13 1.77 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/una-dinosauria/humanmotion-prediction</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Available at http://mocap.cs.cmu.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the Singapore MOE Tier 1 grant R-252-000-637-112 and the National University of Singapore AcRF grant R-252-000-639-114.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quasirecurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Style machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning human motion models for long-term predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02827</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">413</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning switching linear models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="981" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human body dynamics: classical mechanics and human movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tözeren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gaussian process dynamical models for human motion. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human balance and posture control during standing and walking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gait &amp; posture</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="214" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
