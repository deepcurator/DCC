<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multiple Tasks with Multilinear Relationship Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cncaozhangjie14@gmail.compsyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multiple Tasks with Multilinear Relationship Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negativetransfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised learning machines trained with limited labeled samples are prone to overfitting, while manual labeling of sufficient training data for new domains is often prohibitive. Thus it is imperative to design versatile algorithms for reducing the labeling consumption, typically by leveraging off-theshelf labeled data from relevant tasks. Multi-task learning is based on the idea that the performance of one task can be improved using related tasks as inductive bias <ref type="bibr" target="#b3">[4]</ref>. Knowing the task relationship should enable the transfer of shared knowledge from relevant tasks such that only task-specific features need to be learned. This fundamental idea of task relatedness has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref>, and multi-task relationship learning that models inherent task relationship <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Learning inherent task relatedness is a hard problem, since the training data of different tasks may be sampled from different distributions and fitted by different models. Without prior knowledge on the task relatedness, the distribution shift may pose a major difficulty in transferring knowledge across different tasks. Unfortunately, if cross-task knowledge transfer is impossible, then we will overfit each task due to limited amount of labeled data. One way to circumvent this dilemma is to use an external data source, e.g. ImageNet, to learn transferable features through which the shift in the inductive biases can be reduced such that different tasks can be correlated more effectively. This idea has motivated some latest deep learning methods for learning multiple tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref>, which learn a shared representation in feature layers and multiple independent classifiers in classifier layer.</p><p>However, these deep multi-task learning methods do not explicitly model the task relationships. This may result in under-transfer in the classifier layer as knowledge can not be transferred across different classifiers. Recent research also reveals that deep features eventually transition from general to specific along the network, and feature transferability drops significantly in higher layers with increasing task dissimilarity <ref type="bibr" target="#b27">[28]</ref>, hence the sharing of all feature layers may be risky to negativetransfer. Therefore, it remains an open problem how to exploit the task relationship across different deep networks while improving the feature transferability in task-specific layers of the deep networks. This paper presents Multilinear Relationship Network (MRN) for multi-task learning, which discovers the task relationships based on multiple task-specific layers of deep convolutional neural networks. Since the parameters of deep networks are natively tensors, the tensor normal distribution <ref type="bibr" target="#b20">[21]</ref> is explored for multi-task learning, which is imposed as the prior distribution over network parameters of all task-specific layers to learn find-grained multilinear relationships of tasks, classes and features. By jointly learning transferable features and multilinear relationships, MRN is able to circumvent the dilemma of negative-transfer in feature layers and under-transfer in classifier layer. Experiments show that MRN learns fine-grained relationships and yields state-of-the-art results on standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-task learning is a learning paradigm that learns multiple tasks jointly by exploiting the shared structures to improve generalization performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> and mitigate manual labeling consumption. There are generally two categories of approaches: (1) multi-task feature learning, which learns a shared feature representation such that the distribution shift across different tasks can be reduced <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref>; (2) multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> or task covariance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8]</ref>. While these methods have achieved improved performance, they may be restricted by their shallow learning paradigm that cannot embody task relationships by suppressing the task-specific variations in transferable features.</p><p>Deep networks learn abstract representations that disentangle and hide explanatory factors of variation behind data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. Deep representations manifest invariant factors underlying different populations and are transferable across similar tasks <ref type="bibr" target="#b27">[28]</ref>. Thus deep networks have been successfully explored for domain adaptation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> and multi-task learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>, where significant performance gains have been witnessed. Most multi-task deep learning methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref> learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships. However, this may result in under-transfer in the classifier layer as knowledge cannot be adaptively propagated across different classifiers, while the sharing of all feature layers may still be vulnerable to negative-transfer in the feature layers, as the higher layers of deep networks are tailored to fit task-specific structures and may not be safely transferable <ref type="bibr" target="#b27">[28]</ref>. This paper presents a multilinear relationship network based on novel tensor normal priors to learn transferable features and task relationships that mitigate both under-transfer and negative-transfer. Our work contrasts from prior relationship learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> and multi-task deep learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref> methods in two key aspects. (1) Tensor normal prior: our work is the first to explore tensor normal distribution as priors of network parameters in different layers to learn multilinear task relationships in deep networks. Since the network parameters of multiple tasks natively stack into high-order tensors, previous matrix normal distribution <ref type="bibr" target="#b12">[13]</ref> cannot be used as priors of network parameters to learn task relationships. (2) Deep task relationship: we define the tensor normal prior on multiple task-specific layers, while previous deep learning methods do not learn the task relationships. To our knowledge, multi-task deep learning by tensor factorization <ref type="bibr" target="#b26">[27]</ref> is the first work that tackles multi-task deep learning by tensor factorization, which learns shared feature subspace from multilayer parameter tensors; in contrast, our work learns multilinear task relationships from multiplayer parameter tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tensor Normal Distribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probability Density Function</head><p>Tensor normal distribution is a natural extension of multivariate normal distribution and matrix-variate normal distribution <ref type="bibr" target="#b12">[13]</ref> to tensor-variate distributions. The multivariate normal distribution is order-1 tensor normal distribution, and matrix-variate normal distribution is order-2 tensor normal distribution. Before defining tensor normal distribution, we first introduce the notations and operations of order-K tensor. An order-K tensor is an element of the tensor product of K vector spaces, each of which has its own coordinate system. A vector x ∈ R d1 is an order-1 tensor with dimension d 1 . A matrix X ∈ R d1×d2 is an order-2 tensor with dimensions</p><formula xml:id="formula_0">(d 1 , d 2 ). A order-K tensor X ∈ R d1×...×d K with dimensions (d 1 , . . . , d K ) has elements {x i1...i K : i k = 1, . . . , d k }.</formula><p>The vectorization of X is unfolding the tensor into a vector, denoted by vec(X ). The matricization of X is a generalization of vectorization, reordering the elements of X into a matrix. In this paper, to simply the notations and describe the tensor relationships, we use the mode-k matricization and denote by X (k) the mode-k matrix of tensor X , where row i of X (k) contains all elements of X having the k-th index equal to i.</p><p>Consider an order-K tensor X ∈ R d1×...×d K . Since we can vectorize X to a ( K k=1 d k ) × 1 vector, the normal distribution on a tensor X can be considered as a multivariate normal distribution on vector vec(X ) of dimension K k=1 d k . However, such an ordinary multivariate normal distribution ignores the special structure of X as a d 1 × . . . × d K tensor, and as a result, the covariance characterizing the correlations across elements of X is of size (</p><formula xml:id="formula_1">K k=1 d k ) × ( K k=1 d k )</formula><p>, which is often prohibitively large for modeling and estimation. To exploit the structure of X , tensor normal distributions assume that the (</p><formula xml:id="formula_2">K k=1 d k ) × ( K k=1 d k ) covariance matrix Σ 1:</formula><p>K can be decomposed into the Kronecker product Σ 1:K = Σ 1 ⊗ . . . ⊗ Σ K , and elements of X (in vectorization) follow the normal distribution,</p><formula xml:id="formula_3">vec (X ) ∼ N (vec (M) , Σ 1 ⊗ . . . ⊗ Σ K ) ,<label>(1)</label></formula><p>where ⊗ is the Kronecker product, Σ k ∈ R d k ×d k is a positive definite matrix indicating the covariance between the d k rows of the mode-k matricization</p><formula xml:id="formula_4">X (k) of dimension d k × ( k =k d k )</formula><p>, and M is a mean tensor containing the expectation of each element of X . Due to the decomposition of covariance as the Kronecker product, the tensor normal distribution of an order-K tensor X , parameterized by mean tensor M and covariance matrices Σ 1 , . . . , Σ K , can define probability density function as <ref type="bibr" target="#b20">[21]</ref> p (x) = (2π)</p><formula xml:id="formula_5">−d/2 K k=1 |Σ k | −d/(2d k ) × exp − 1 2 (x − µ) T Σ −1 1:K (x − µ) ,<label>(2)</label></formula><p>where |·| is the determinant of a square matrix, and</p><formula xml:id="formula_6">x = vec (X ) , µ = vec (M) , Σ 1:K = Σ 1 ⊗. . .⊗ Σ K , d = K k=1 d k .</formula><p>The tensor normal distribution corresponds to the multivariate normal distribution with Kronecker decomposable covariance structure. X following tensor normal distribution, i.e. vec (X ) following the normal distribution with Kronecker decomposable covariance, is denoted by</p><formula xml:id="formula_7">X ∼ T N d1×...×d K (M, Σ 1 , . . . , Σ K ) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Maximum Likelihood Estimation</head><p>Consider a set of n samples</p><formula xml:id="formula_8">{X i } n i=1</formula><p>where each X i is an order-3 tensor generated by a tensor normal distribution as in Equation <ref type="bibr" target="#b1">(2)</ref>. The maximum likelihood estimation (MLE) of the mean tensor M is</p><formula xml:id="formula_9">M = 1 n n i=1 X i .<label>(4)</label></formula><p>The MLE of covariance matrices Σ 1 , . . . , Σ 3 are computed by iteratively updating these equations:</p><formula xml:id="formula_10">Σ1 = 1 nd2d3 n i=1 (Xi − M) (1) Σ3 ⊗ Σ2 −1 (Xi − M) T (1) , Σ2 = 1 nd1d3 n i=1 (Xi − M) (2) Σ3 ⊗ Σ1 −1 (Xi − M) T (2) , Σ3 = 1 nd1d2 n i=1 (Xi − M) (3) Σ2 ⊗ Σ1 −1 (Xi − M) T (3) . (5)</formula><p>This flip-flop algorithm <ref type="bibr" target="#b20">[21]</ref> is efficient to solve by simple matrix manipulations and convergence is guaranteed. Covariance matrices Σ 1 , . . . , Σ 3 are not identifiable and the solutions to maximizing density function <ref type="bibr" target="#b1">(2)</ref> are not unique, while only the Kronecker product Σ 1 ⊗. . .⊗Σ K (1) is identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multilinear Relationship Networks</head><p>This work models multiple tasks by jointly learning transferable representations and task relationships. Given T tasks with training data {X t , Y t }   <ref type="formula" target="#formula_3">(1)</ref> convolutional layers conv1-conv5 and fully-connected layer f c6 learn transferable features, so their parameters are shared across tasks; (2) fully-connected layers f c7-f c8 fit task-specific structures, so their parameters are modeled by tensor normal priors for learning multilinear relationships of features, classes and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>We start with deep convolutional neural networks (CNNs) <ref type="bibr" target="#b15">[16]</ref>, a family of models to learn transferable features that are well adaptive to multiple tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>. The main challenge is that in multitask learning, each task is provided with a limited amount of labeled data, which is insufficient to build reliable classifiers without overfitting. In this sense, it is vital to model the task relationships through which each pair of tasks can help with each other to enable knowledge transfer if they are related, and can remain independent to mitigate negative transfer if they are unrelated. With this idea, we design a Multilinear Relationship Network (MRN) that exploits both feature transferability and task relationship to establish effective and robust multi-task learning. <ref type="figure" target="#fig_1">Figure 1</ref> shows the architecture of the proposed MRN model based on AlexNet <ref type="bibr" target="#b15">[16]</ref>, while other deep networks are also applicable.</p><p>We build the proposed MRN model upon AlexNet <ref type="bibr" target="#b15">[16]</ref>, which is comprised of convolutional layers (conv1-conv5) and fully-connected layers (f c6-f c8). The -th f c layer learns a nonlinear mapping h</p><formula xml:id="formula_11">t, n = a W t, h t, −1 n + b t,</formula><p>for task t, where h t, n is the hidden representation of each point x t n , W t, and b t, are the weight and bias parameters, and a is the activation function, taken as ReLU a (x) = max(0, x) for hidden layers or softmax units a (x) = e x / |x| j=1 e xj for the output layer. Denote by y = f t (x) the CNN classifier of t-th task, and the empirical error of CNN on</p><formula xml:id="formula_12">{X t , Y t } is min ft Nt n=1 J f t x t n , y t n ,<label>(6)</label></formula><p>where J is the cross-entropy loss function, and f t (x t n ) is the conditional probability that CNN assigns x t n to label y t n . We will not describe how to compute the convolutional layers since these layers can learn transferable features in general <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18]</ref>, and we will simply share the network parameters of these layers across different tasks, without explicitly modeling the relationships of features and tasks in these layers. To benefit from pre-training and fine-tuning as most deep learning work, we copy these layers from a model pre-trained on ImageNet 2012 <ref type="bibr" target="#b27">[28]</ref>, and fine-tune all conv1-conv5 layers.</p><p>As revealed by the recent literature findings <ref type="bibr" target="#b27">[28]</ref>, the deep features in standard CNNs must eventually transition from general to specific along the network, and the feature transferability decreases while the task discrepancy increases, making the features in higher layers f c7-f c8 unsafely transferable across different tasks. In other words, the f c layers are tailored to their original task at the expense of degraded performance on the target task, which may deteriorate multi-task learning based on deep neural networks. Most previous methods generally assume that the multiple tasks can be well correlated given the shared representation learned by the feature layers conv1-f c7 of deep networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref>. However, it may be vulnerable if different tasks are not well correlated under deep features, which is common as higher layers are not safely transferable and tasks may be dissimilar. Moreover, existing multi-task learning methods are natively designed for binary classification tasks, which are not good choices as deep networks mainly adopt multi-class softmax regression. It remains an open problem to explore the task relationships of multi-class classification for multi-task learning.</p><p>In this work, we jointly learn transferable features and multilinear relationships of features and tasks for multiple task-specific layers L in a Bayesian framework. Based on the transferability of deep networks discussed above, the task-specific layers L are set to {f c7, f c8}.</p><formula xml:id="formula_13">Denote by X = {X t } T t=1 , Y = {Y t } T t=1</formula><p>the complete training data of T tasks, and by W t, ∈ R D 1 ×D 2 the network parameters of the t-th task in the -th layer, where D 1 and D 2 are the rows and columns of matrix W t, . In order to capture the task relationship in the network parameters of all T tasks, we construct the -th layer parameter tensor as</p><formula xml:id="formula_14">W = W 1, ; . . . ; W T, ∈ R D 1 ×D 2 ×T . Denote by W = W :</formula><p>∈ L the set of parameter tensors of all the task-specific layers L = {f c7, f c8}. The Maximum a Posteriori (MAP) estimation of network parameters W given training data {X , Y} for learning multiple tasks is</p><formula xml:id="formula_15">p ( W| X , Y) ∝ p (W) · p (Y |X , W ) = ∈L p W · T t=1 Nt n=1 p y t n x t n , W ,<label>(7)</label></formula><p>where we assume that for prior p (W), the parameter tensor of each layer W is independent on the parameter tensors of the other layers W = , which is a common assumption made by most feedforward neural network methods <ref type="bibr" target="#b2">[3]</ref>. Finally, we assume when the network parameter is sampled from the prior, all tasks are independent. These independence assumptions lead to the factorization of the posteriori in Equation <ref type="formula" target="#formula_15">(7)</ref>, which make the final MAP estimation in deep networks easy to solve.</p><p>The maximum likelihood estimation (MLE) part p (Y |X , W ) in Equation <ref type="formula" target="#formula_15">(7)</ref> is modeled by deep CNN in Equation <ref type="formula" target="#formula_12">(6)</ref>, which can learn transferable features in lower layers for multi-task learning. We opt to share the network parameters of all these layers (conv1-f c6). This parameter sharing strategy is a relaxation of existing deep multi-task learning methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref>, which share all the feature layers except for the classifier layer. We do not share task-specific layers (the last feature layer f c7 and classifier layer f c8), with the expectation to potentially mitigate negative-transfer <ref type="bibr" target="#b27">[28]</ref>.</p><p>The prior part p (W) in Equation <ref type="formula" target="#formula_15">(7)</ref> is the key to enabling multi-task deep learning since this prior part should be able to model the multilinear relationship across parameter tensors. This paper, for the first time, defines the prior for the -th layer parameter tensor by tensor normal distribution <ref type="bibr" target="#b20">[21]</ref> as</p><formula xml:id="formula_16">p W = T N D 1 ×D 2 ×T O, Σ 1 , Σ 2 , Σ 3 ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_17">Σ 1 ∈ R D 1 ×D 1 , Σ 2 ∈ R D 2 ×D 2</formula><p>, and Σ 3 ∈ R T ×T are the mode-1, mode-2, and mode-3 covariance matrices, respectively. Specifically, in the tensor normal prior, the row covariance matrix Σ 1 models the relationships between features (feature covariance), the column covariance matrix Σ 2 models the relationships between classes (class covariance), and the mode-3 covariance matrix Σ 3 models the relationships between tasks in the -th layer network parameters {W 1, , . . . , W T, }. A common strategy used by previous methods is to use identity covariance for feature covariance <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref> and class covariance <ref type="bibr" target="#b1">[2]</ref>, which implicitly assumes independent features and classes and cannot capture the dependencies between them. This work learns all feature covariance, class covariance, task covariance and all network parameters from data to build robust multilinear task relationships.</p><p>We integrate the CNN error functional (6) and tensor normal prior (8) into MAP estimation (7) and taking negative logarithm, which leads to the MAP estimation of the network parameters W, a regularized optimization problem for Multilinear Relationship Network (MRN) formally writing as</p><formula xml:id="formula_18">min f t | T t=1 ,Σ k | K k=1 T t=1 N t n=1 J ft x t n , y t n + 1 2 ∈L vec(W ) T (Σ 1:K ) −1 vec(W ) − K k=1 D D k ln |Σ k | ,<label>(9)</label></formula><p>where D = K k=1 D k and K = 3 is the number of modes in parameter tensor W, which could be K = 4 for the convolutional layers (width, height, number of feature maps, and number of tasks); Σ 1:3 = Σ 1 ⊗ Σ 2 ⊗ Σ 3 is the Kronecker product of the feature covariance Σ 1 , class covariance Σ 2 , and task covariance Σ 3 . Moreover, we can assume shared task relationship across different layers as Σ 3 = Σ 3 , which enhances connection between task relationships on features f c7 and classifiers f c8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm</head><p>The optimization problem (9) is jointly non-convex with respect to the parameter tensors W as well as feature covariance Σ 1 , class covariance Σ 2 , and task covariance Σ 3 . Thus, we alternatively optimize one set of variables with the others fixed. We first update W t, , the parameter of task-t in layer-. When training deep CNN by back-propagation, we only require the gradient of the objective function (denoted by O) in Equation <ref type="formula" target="#formula_3">(10)</ref>  </p><formula xml:id="formula_19">) ∂W t, = ∂J (f t (x t n ) , y t n ) ∂W t, + (Σ 1:3 ) −1 vec W ··t ,<label>(10)</label></formula><p>where [(Σ 1:3 ) −1 vec W ] ··t is the (:, :, t) slice of a tensor folded from elements (Σ 1:3 ) −1 vec(W ) that are corresponding to parameter matrix W t, . Since training a deep CNN requires a large amount of labeled data, which is prohibitive for many multi-task learning problems, we fine-tune from an AlexNet model pre-trained on ImageNet as in <ref type="bibr" target="#b27">[28]</ref>. In each epoch, after updating W, we can update the feature covariance Σ 1 , class covariance Σ 2 , and task covariance Σ 3 by the flip-flop algorithm as</p><formula xml:id="formula_20">Σ 1 = 1 D 2 T (W ) (1) Σ 3 ⊗ Σ 2 −1 (W ) T (1) + I D 1 , Σ 2 = 1 D 1 T (W ) (2) Σ 3 ⊗ Σ 1 −1 (W ) T (2) + I D 2 , Σ 3 = 1 D 1 D 2 (W ) (3) Σ 2 ⊗ Σ 1 −1 (W ) T (3) + IT .<label>(11)</label></formula><p>where the last term of each update equation is a small penalty traded off by for numerical stability.</p><p>However, the above updating equations <ref type="formula" target="#formula_3">(11)</ref> are computationally prohibitive, due to the dimension explosion of the Kronecker product, e.g.</p><formula xml:id="formula_21">Σ 2 ⊗ Σ 1 is of dimension D 1 D 2 × D 1 D 2 .</formula><p>To speed up computation, we will use the following rules of Kronecker product: (A ⊗ B)</p><formula xml:id="formula_22">−1 = A −1 ⊗ B −1 and B T ⊗ A vec (X) = vec (AXB).</formula><p>Taking the computation of Σ 3 ∈ R T ×T as an example, we have</p><formula xml:id="formula_23">(Σ 3 ) ij = 1 D 1 D 2 (W ) (3),i· Σ 2 ⊗ Σ 1 −1 (W ) T (3),j· + I ij = 1 D 1 D 2 (W ) (3),i· vec (Σ 1 ) −1 W ··j (Σ 2 ) −1 + I ij ,<label>(12)</label></formula><p>where (W ) (3),i· denotes the i-th row of the mode-3 matricization of tensor W , and W ··j denotes the (:, :, j) slice of tensor W . We can derive that updating Σ 3 has a computational complexity of</p><formula xml:id="formula_24">O T 2 D 1 D 2 D 1 + D 2</formula><p>, similarly for Σ 1 and Σ 2 . The total computational complexity of updating covariance matrices</p><formula xml:id="formula_25">Σ k | 3 k=1 will be O D 1 D 2 T D 1 D 2 + D 1 T + D 2 T ,</formula><p>which is still expensive. A key to computation speedup is that the covariance matrices Σ k | 3 k=1 should be low-rank, since the features and tasks are enforced to be correlated for multi-task learning. Thus, the inverses of</p><formula xml:id="formula_26">Σ k | 3 k=1</formula><p>do not exist in general and we have to compute the generalized inverses using eigendecomposition. We perform eigendecomposition for each Σ k and maintain all eigenvectors with eigenvalues greater than zero. The rank r of the eigen-reconstructed covariance matrices should be r ≤ min <ref type="figure" target="#fig_1">(D 1 , D 2 , T )</ref>. Thus, the total computational complexity for</p><formula xml:id="formula_27">Σ k | 3 k=1 is reduced to O rD 1 D 2 T D 1 + D 2 + T .</formula><p>It is straight-forward to see the computational complexity of updating the parameter tensor W is the cost of back-propagation in standard CNNs plus the cost for computing the gradient of regularization term by Equation <ref type="formula" target="#formula_3">(10)</ref>, which is O rD 1</p><formula xml:id="formula_28">D 2 T D 1 + D 2 + T given generalized inverses (Σ k ) −1 | 3 k=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>The proposed Multilinear Relationship Network (MRN) is very flexible and can be easily configured to deal with different network architectures and multi-task learning scenarios. For example, replacing the network backbone from AlexNet to VGGnet <ref type="bibr" target="#b23">[24]</ref> boils down to configuring task-specific layers L = {f c7, f c8}, where f c7 is the last feature layer while f c8 is the classifier layer in the VGGnet. The architecture of MRN in <ref type="figure" target="#fig_1">Figure 1</ref> can readily cope with homogeneous multi-task learning where all tasks share the same output space. It can cope with heterogeneous multi-task learning where different tasks have different output spaces by setting L = {f c7}, by only considering feature layers.</p><p>The multilinear relationship learning in Equation <ref type="formula" target="#formula_18">(9)</ref> is a general framework that readily subsumes many classical multi-task learning methods as special cases. Many regularized multi-task algorithms can be classified into two main categories: learning with feature covariances <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref> and learning with task relations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref>. Learning with feature covariances can be viewed as a representative formulation in feature-based methods while learning with task relations is for parameter-based methods <ref type="bibr" target="#b29">[30]</ref>. More specifically, previous multi-task feature learning methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> can be viewed as a special case of Equation <ref type="formula" target="#formula_18">(9)</ref> by setting all covariance matrices but the feature covariance to identity matrix, i.e. Σ k = I| K k=2 ; and previous multi-task relationship learning methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref> can be viewed as a special case of Equation <ref type="formula" target="#formula_18">(9)</ref> by setting all covariance matrices but the task covariance to identity matrix, i.e. Σ k = I| K−1 k=1 . The proposed MRN is more general in the architecture perspective in dealing with parameter tensors in multiple layers of deep neural networks.</p><p>It is noteworthy to highlight a concurrent work on multi-task deep learning using tensor decomposition <ref type="bibr" target="#b26">[27]</ref>, which is feature-based method that explicitly learns the low-rank shared parameter subspace. The proposed multilinear relationship across parameter tensors can be viewed as a strong alternative to the tensor decomposition, with the advantage to explicitly model the positive and negative relations across features and tasks. As a defense of <ref type="bibr" target="#b26">[27]</ref>, the tensor decomposition can extract finer-grained feature relations (what to share and how much to share) than the proposed multilinear relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare MRN with state-of-the-art multi-task and deep learning methods to verify the efficacy of learning transferable features and multilinear task relationships. Codes and datasets will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Office-Caltech <ref type="bibr" target="#b11">[12]</ref> This dataset is the standard benchmark for multi-task learning and transfer learning. The Office part consists of 4,652 images in 31 categories collected from three distinct domains (tasks): Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which are images taken by Web camera and digital SLR camera under different environmental variations. This dataset is organized by selecting the 10 common categories shared by the Office dataset and the Caltech-256 (C) dataset <ref type="bibr" target="#b11">[12]</ref>, hence it yields four multi-class learning tasks. Office-Home 1 <ref type="bibr" target="#b25">[26]</ref> This dataset is to evaluate transfer learning algorithms using deep learning. It consists of images from 4 different domains: Artistic images (A), Clip Art (C), Product images (P) and Real-World images (R). For each domain, the dataset contains images of 65 object categories collected in office and home settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageCLEF-DA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>This dataset is the benchmark for ImageCLEF domain adaptation challenge, organized by selecting the 12 common categories shared by the following four public datasets (tasks): Caltech-256 (C), ImageNet ILSVRC 2012 (I), Pascal VOC 2012 (P), and Bing (B). All three datasets are evaluated using DeCAF 7 <ref type="bibr" target="#b8">[9]</ref> features for shallow methods and original images for deep methods.</p><p>We compare MRN with standard and state-of-the-art methods: Single-Task Learning (STL), MultiTask Feature Learning (MTFL) <ref type="bibr" target="#b1">[2]</ref>, Multi-Task Relationship Learning (MTRL) <ref type="bibr" target="#b30">[31]</ref>, Robust MultiTask Learning (RMTL) <ref type="bibr" target="#b4">[5]</ref>, and Deep Multi-Task Learning with Tensor Factorization (DMTL-TF) <ref type="bibr" target="#b26">[27]</ref>. STL performs per-task classification in separate deep networks without knowledge transfer. MTFL extracts the low-rank shared feature representations by learning feature covariance. RMTL extends MTFL to further capture the task relationships using a low-rank structure and identify outlier tasks using a group-sparse structure. MTRL captures the task relationships using task covariance of a matrix normal distribution. DMTL-TF tackles multi-task deep learning by tensor factorization, which learns shared feature subspace instead of multilinear task relationship in multilayer parameter tensors.</p><p>To go deep into the efficacy of jointly learning transferable features and multilinear task relationships, we evaluate two MRN variants: (1) MRN 8 , MRN using only one network layer f c8 for multilinear relationship learning; (2) MRN t , MRN using only task covariance Σ 3 for single-relationship learning. The proposed MRN model can natively deal with multi-class problems using the parameter tensors. However, most shallow multi-task learning methods such as MTFL, RMTL and MTRL are formulated  only for binary-class problems, due to the difficulty in dealing with order-3 parameter tensors for multi-class problems. We adopt one-vs-rest strategy to enable them working on multi-class datasets.</p><p>We follow the standard evaluation protocol <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5]</ref> for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as training set and use the rest of the samples as test set. We compare the average classification accuracy for all tasks based on five random experiments, where standard errors are generally less than ±0.5%, which are not significant and thus are not reported for space limitation. We conduct model selection for all methods using five-fold cross-validation on the training set. For deep learning methods, we adopt AlexNet <ref type="bibr" target="#b15">[16]</ref> and VGGnet <ref type="bibr" target="#b23">[24]</ref>, fix convolutional layers conv1-conv5, fine-tune fully-connected layers f c6-f c7, and train classifier layer f c8 via back-propagation. As the classifier layer is trained from scratch, we set its learning rate to be 10 times that of the other layers. We use mini-batch stochastic gradient descent (SGD) with 0.9 momentum and learning rate decaying strategy, and select learning rate between 10 −5 and 10 −2 by stepsize 10 1 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>The multi-task classification results on the Office-Caltech, Office-Home and ImageCLEF-DA datasets based on 5%, 10%, and 20% sampled training data are shown in <ref type="table" target="#tab_1">Tables 1, 2</ref> and 3, respectively. We observe that the proposed MRN model significantly outperforms the comparison methods on most multi-task problems. The substantial accuracy improvement validates that our multilinear relationship networks through multilayer and multilinear relationship learning is able to learn both transferable features and adaptive task relationships, which enables effective and robust multi-task deep learning.</p><p>We can make the following observations from the results.</p><p>(1) Shallow multi-task learning methods MTFL, RMTL, and MTRL outperform single-task deep learning method STL in most cases, which confirms the efficacy of learning multiple tasks by exploiting shared structures. Among the shallow multi-task methods, MTRL gives the best accuracies, showing that exploiting task relationship may be more effective than extracting shared feature subspace for multi-task learning. It is worth noting that, although STL cannot learn from knowledge transfer, it can be fine-tuned on each task to improve performance, and thus when the number of training samples are large enough and when different tasks are dissimilar enough (e.g. Office-Home dataset), STL may outperform shallow multi-task learning methods, as evidenced by the results in <ref type="table" target="#tab_2">Table 2</ref>. (2) Deep multi-task learning method DMTL-TF outperforms shallow multi-task learning methods with deep features as input, which confirms the importance of learning deep transferable features to enable knowledge transfer across tasks. However, DMTL-TF only learns the shared feature subspace based on tensor factorization of the network parameters, while the task relationships in multiple network layers are not captured. This may result in negative-transfer in the feature layers <ref type="bibr" target="#b27">[28]</ref> and under-transfer in the classifier layers. Negative-transfer can be witnessed by comparing multi-task methods with single-task methods: if multi-task learning methods yield lower accuracy in some of the tasks, then negative-transfer arises. We go deeper into MRN by reporting the results of the two MRN variants: MRN 8 and MRN t , all significantly outperform the comparison methods but generally underperform MRN (full), which verify our motivation that jointly learning transferable features and multilinear task relationships can bridge multiple tasks more effectively.</p><p>(1) The disadvantage of MRN 8 is that it does not learn the task relationship in the lower layers f c7, which are not safely transferable and may result in negative transfer <ref type="bibr" target="#b27">[28]</ref>. <ref type="formula" target="#formula_5">(2)</ref> The shortcoming of MRN t is that it does not learn the multilinear relationship of features, classes and tasks, hence the learned relationships may only capture the task covariance without capturing the feature covariance and class covariance, which may lose some intrinsic relations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualization Analysis</head><p>We show that MRN can learn more reasonable task relationships with deep features than MTRL with shallow features, by visualizing the Hinton diagrams of task covariances learned by MTRL and MRN (Σ f c8</p><p>3 ) in <ref type="figure" target="#fig_3">Figures 3(a) and 3(b)</ref>, respectively. Prior knowledge on task similarity in the Office-Caltech dataset <ref type="bibr" target="#b11">[12]</ref> describes that tasks A, W and D are more similar with each other while they are relatively dissimilar to task C. MRN successfully captures this prior task relationship and enhances the task correlation across dissimilar tasks, which enables stronger transferability for multi-task learning. Furthermore, all tasks are positively correlated (green color) in MRN, implying that all tasks can better reinforce each other. However, some of the tasks (D and C) are still negatively correlated (red color) in MTRL, implying these tasks should be drawn far apart and cannot improve with each other.</p><p>We illustrate the feature transferability by visualizing in <ref type="figure" target="#fig_3">Figures 3(c) and 3(d)</ref> the t-SNE embeddings <ref type="bibr" target="#b17">[18]</ref> of the images in the Office-Caltech dataset with DMTL-TF features and MRN features, respectively. Compared with DMTL-TF features, the data points with MRN features are discriminated better across different categories, i.e., each category has small intra-class variance and large inter-class margin; the data points are also aligned better across different tasks, i.e. the embeddings of different tasks overlap well, implying that different tasks reinforce each other effectively. This verifies that with multilinear relationship learning, MRN can learn more transferable features for multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented multilinear relationship networks (MRN) that integrate deep neural networks with tensor normal priors over the network parameters of all task-specific layers, which model the task relatedness through the covariance structures over tasks, classes and features to enable transfer across related tasks. An effective learning algorithm was devised to jointly learn transferable features and multilinear relationships. Experiments testify that MRN yields superior results on standard datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>T t=1 , where X t = {x t 1 , . . . , x t Nt } and Y t = {y t 1 , . . . , y t Nt } are the N t training examples and associated labels of the t-th task, respectively drawn from D- dimensional feature space and C-cardinality label space, i.e. each training example x t n ∈ R D and y t n ∈ {1, . . . , C}. Our goal is to build a deep network for multiple tasks y t n = f t (x t n ) which learns transferable features and adaptive task relationships to bridge different tasks effectively and robustly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multilinear relationship network (MRN) for multi-task learning: (1) convolutional layers conv1-conv5 and fully-connected layer f c6 learn transferable features, so their parameters are shared across tasks; (2) fully-connected layers f c7-f c8 fit task-specific structures, so their parameters are modeled by tensor normal priors for learning multilinear relationships of features, classes and tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of the Office-Home dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hinton diagram of task relationships (a)(b) and t-SNE embedding of deep features (c)(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>w.r.t. W t, on each data point (x</figDesc><table>t 

n , y 

t 

n ), which can be computed as 
∂O (x 

t 

n , y 

t 

n </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy on Office-Caltech with standard evaluation protocol (AlexNet).</figDesc><table>Method 
5% 
10% 
20% 
A 
W 
D 
C 
Avg 
A 
W 
D 
C 
Avg 
A 
W 
D 
C 
Avg 

STL (AlexNet) 
88.9 73.0 80.4 88.7 82.8 92.2 80.9 88.2 88.9 87.6 91.3 83.3 93.7 94.9 90.8 
MTFL [2] 
90.0 78.9 90.2 86.9 86.5 92.4 85.3 89.5 89.2 89.1 93.5 89.0 95.2 92.6 92.6 
RMTL [6] 
91.3 82.3 88.8 89.1 87.9 92.6 85.2 93.3 87.2 89.6 94.3 87.0 96.7 93.4 92.4 
MTRL [31] 
86.4 83.0 95.1 89.1 88.4 91.1 87.1 97.0 87.6 90.7 90.0 88.8 99.2 94.3 93.1 
DMTL-TF [27] 91.2 88.3 92.5 85.6 89.4 92.2 91.9 97.4 86.8 92.0 92.6 97.6 94.5 88.4 93.3 
MRN8 
91.7 96.4 96.9 86.5 92.9 92.7 97.1 97.3 86.6 93.4 93.2 96.9 99.4 82.8 94.4 
MRNt 
91.1 96.3 97.4 86.1 92.7 92.5 97.7 96.6 86.7 93.4 91.9 96.6 95.9 90.0 93.6 
MRN (full) 
92.5 97.5 97.9 87.5 93.8 93.6 98.6 98.6 87.3 94.5 94.4 98.3 99.9 89.1 95.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy on Office-Home with standard evaluation protocol (VGGnet).</figDesc><table>Method 
5% 
10% 
20% 
A 
C 
P 
R 
Avg 
A 
C 
P 
R 
Avg 
A 
C 
P 
R 
Avg 

STL (VGGnet) 
35.8 31.2 67.8 62.5 49.3 51.0 40.7 75.0 68.8 58.9 56.1 54.6 80.4 71.8 65.7 
MTFL [2] 
40.1 30.4 61.5 59.5 47.9 50.3 35.0 66.3 65.0 54.2 55.2 38.8 69.1 70.0 58.3 
RMTL [6] 
42.3 32.8 62.3 60.6 49.5 49.7 34.6 65.9 64.6 53.7 55.2 39.2 69.6 70.5 58.6 
MTRL [31] 
42.7 33.3 62.9 61.3 50.1 51.6 36.3 67.7 66.3 55.5 55.8 39.9 70.2 71.2 59.3 
DMTL-TF [27] 49.2 34.5 67.1 62.9 53.4 57.2 42.3 73.6 69.9 60.8 58.3 56.1 79.3 72.1 66.5 
MRN8 
52.7 34.7 70.1 67.6 56.3 59.1 42.7 75.1 72.8 62.4 58.4 55.6 80.4 72.4 66.7 
MRNt 
52.0 34.0 69.9 66.8 55.7 58.6 42.6 74.9 72.4 62.1 57.7 54.8 80.2 71.6 66.1 
MRN (full) 
53.3 36.4 70.5 67.7 57.0 59.9 42.7 76.3 73.0 63.0 58.5 55.6 80.7 72.8 66.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy on ImageCLEF-DA with standard evaluation protocol (AlexNet).DMTL-TF [27] 87.9 70.0 58.1 34.1 62.5 89.1 82.1 58.7 48.0 69.5 91.7 80.0 63.2 54.1 72.2</figDesc><table>Method 
5% 
10% 
20% 
C 
I 
P 
B 
Avg 
C 
I 
P 
B 
Avg 
C 
I 
P 
B 
Avg 

STL (AlexNet) 
77.4 60.3 48.0 45.0 57.7 78.9 70.5 48.1 41.8 59.8 83.3 74.9 49.2 47.1 63.6 
MTFL [2] 
79.9 68.6 43.4 41.5 58.3 82.9 71.4 56.7 41.7 63.2 83.1 72.2 54.5 52.5 65.6 
RMTL [6] 
81.1 71.3 52.4 40.9 61.4 81.5 71.7 55.6 45.3 63.5 83.3 73.3 53.7 49.2 64.9 
MTRL [31] 
80.8 68.4 51.9 42.9 61.0 83.1 72.7 54.5 45.5 63.9 83.7 75.5 57.5 49.4 66.5 
MRN8 
87.0 74.4 61.8 47.6 67.7 89.1 82.2 64.4 49.3 71.2 91.1 84.1 65.7 54.1 73.7 
MRNt 
88.5 73.5 63.3 51.1 69.1 88.0 83.1 67.4 54.8 73.3 91.1 83.5 65.7 55.7 74.0 
MRN (full) 
89.6 76.9 65.4 49.4 70.3 88.1 84.6 68.7 55.6 74.3 92.8 83.3 67.4 57.8 75.3 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://hemanthdv.org/OfficeHome-Dataset 2 http://imageclef.org/2014/adaptation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key R&amp;D Program of China (2016YFB1000701), National Natural Science Foundation of China (61772299, 61325008, 61502265, 61672313) and TNList Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convex multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="243" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convex formulation for learning a shared predictive structure from multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1025" to="1038" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating low-rank and group-sparse structures for robust multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task recurrent neural network for immediacy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convex learning of multiple tasks and their structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ciliberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Matrix variate distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Nagar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Chapman &amp; Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustered multi-task learning: A convex formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning with whom to share in multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning task grouping and overlap in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The benefit of multitask representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2853" to="2884" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The multilinear normal distribution: Introduction and some basic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ohlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Von Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="37" to="47" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multisource deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilinear multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multiple tasks with a sparse matrix-normal penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<title level="m">A survey on multi-task learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
