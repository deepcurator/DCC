<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generate To Adapt: Aligning Domains using Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generate To Adapt: Aligning Domains using Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The development of powerful learning algorithms such as Convolutional Neural Networks (CNNs) has provided an effective pipeline for solving many classification problems <ref type="bibr" target="#b29">[30]</ref>. The abundance of labeled data has resulted in remarkable improvements for tasks such as the Imagenet challenge: beginning with the CNN framework of AlexNet <ref type="bibr" target="#b11">[12]</ref> and more recently ResNets <ref type="bibr" target="#b8">[9]</ref> and its variants. Another example is the steady improvements in performance on the LFW dataset <ref type="bibr" target="#b28">[29]</ref>. The common theme across all these approaches is the dependence on large amounts of labeled data. While labeled data is available and getting labeled data has been easier over the years, the lack of uniformity of label distributions across different domains results in suboptimal performance of even the most powerful CNN-based algorithms on realistic unseen test data. For example, labeled synthetic data is available in plenty but al- * First two authors contributed equally gorithms trained only on synthetic data perform poorly on real data. This is of vital importance in cases where labeled real data is unavailable. The use of such unlabeled target data to mitigate the shift between source and target distributions is the most useful direction among domain adaptation approaches. Hence this paper focuses on the topic of unsupervised domain adaptation. In this work, we learn an embedding that is robust to the shift between source and target distributions. We achieve this by using unsupervised data sampled from the target distribution to guide the supervised learning procedure that uses data sampled from the source distribution. We propose an adversarial image generation approach to directly learn the shared feature embedding using labeled data from source and unlabeled data from the target. It should be noted that while there have been a few approaches that use an adversarial framework for solving the domain adaptation problem, the novelty of the proposed approach is in using a joint generative discriminative method: the embeddings are learned using a combination of classification loss and an image generation procedure that is modeled using a variant of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure">Figure 1</ref> illustrates the pipeline of the proposed approach. During training, the source images are passed through the feature extraction network (encoder) to obtain an embedding which is then used by the label prediction network (classifier) for predicting the source label and also used by the generator to generate a realistic source image. The realistic nature of the images from the generator (G) is controlled by the discriminator (D). The encoder is updated based on the discriminative gradients from the classifier and generative gradients from the adversarial framework. Given unlabeled target images, the encoder is updated using only gradients from the adversarial part, since the labels are unavailable. Thus, the encoder learns to discriminate better even in the target domain using the knowledge imparted by the generator-discriminator pair. By using the discriminator as a multi-class classifier, we ensure that the gradient signals backpropagated by the discriminator for the unlabeled target images belong to the feature space of the respective classes. By sampling from the distribution of the generator  <ref type="figure">Figure 1</ref>: Illustration of the proposed approach. In the training phase, our pipeline consists of two parallel streams -(1) Stream 1: classification branch where F-C networks are updated using supervised classification loss and (2) Stream 2: adversarial branch which is a Auxiliary Classifier GAN (ACGAN) framework (G-D pair). F-G-D networks are updated so that both source and target embeddings produce source-like images. Note: The auxiliary classifier in ACGAN uses only the source domain labels, and is needed to ensure that class-consistent images are generated (e.g) embedding of digit 3 generates an image that looks like 3. In the test phase, we remove Stream 2, and classification is performed using the F-C pair after training, we show that the network has indeed learned to bring the source and target distributions closer.</p><p>The main contribution of this work is to provide an adversarial image generation approach for unsupervised domain adaptation that directly learns a joint feature space in which the distance between source and target distributions is minimized. Different from contemporary approaches that achieve a similar objective by using a GAN as a data augmenter, our approach achieves superior results even in cases where a stand along image generation process is bound to fail (such as in the OFFICE dataset). This is done by utilizing the GAN framework to address the domain shift directly in the feature space learnt by the encoder. Our experiments show that the proposed approach yields superior results compared to similar approaches which update the embedding based on auto-encoders <ref type="bibr" target="#b4">[5]</ref> or disentangling the domain information from the embedding by learning a separate domain classifier <ref type="bibr" target="#b3">[4]</ref>. This paper is organized as follows: We begin in Section 2 by describing existing approaches for the unsupervised domain adaptation problem. In Section 3, we describe in detail the formulation of our approach and the iterative training procedure. The experimental setups and the results are discussed in Section 4 using both quantitative and qualitative experiments, followed by discussion and conclusion in Section 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain adaptation is an actively researched topic in many areas of Artificial Intelligence including Machine Learning, Natural Language Processing and Computer Vision. In this section, we describe techniques related to visual domain adaptation. Earlier approaches to domain adaptation focused on building feature representations that are invariant across domains. This was accomplished either by feature reweighting and selection mechanisms <ref type="bibr" target="#b9">[10]</ref> [2], or by learning an explicit feature transformation that aligns source distribution to the target distribution ( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b5">[6]</ref>). The ability to deep neural networks to learn powerful representations [ <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>] has been harnessed to perform unsupervised domain adaptation in recent works <ref type="bibr">[[4]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b31">[32]</ref>]. The underlying idea behind such methods is to minimize a suitable loss function that captures domain discrepancy, in addition to the task being solved.</p><p>Deep learning methods for visual domain adaptation can be broadly grouped into few major categories. One line of work uses Maximum Mean Discrepancy(MMD) as a metric to measure the shift across domains. Deep Domain Confusion (DDC) <ref type="bibr" target="#b32">[33]</ref> jointly minimizes the classification loss and MMD loss of the last fully connected layer. Deep Adap-tation Networks (DAN) <ref type="bibr" target="#b15">[16]</ref> extends this idea by embedding all task specific layers in a reproducing kernel Hilbert space and minimizing the MMD in the projected space. In addition to MMD, Residual Transfer Networks (RTN) <ref type="bibr" target="#b17">[18]</ref> uses a gated residual layer for classifier adaptation. Joint Adaptation Networks <ref type="bibr" target="#b18">[19]</ref> learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a Joint Maximum Mean Discrepancy (JMMD) criterion.</p><p>Another class of methods uses adversarial losses to perform domain adaptation. Revgrad <ref type="bibr" target="#b3">[4]</ref> employs a domain classification network which aims to discriminate the source and the target embeddings. The goal of the feature extraction network is to produce embeddings that maximize the domain classifier loss, while at the same time minimizing the label prediction loss. This is accomplished by negating the gradients coming from the domain classification network. Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b31">[32]</ref> on the other hand learns separate feature extraction networks for source and target, and trains the target CNN so that a domain classifier cannot distinguish the embeddings produced by the source or target CNNs.</p><p>While methods discussed above apply adversarial losses in the embedding space, there has been a lot of interest recently to perform adaptation by applying adversarial losses in the pixel space. Such approaches primarily use generative models such as GANs to perform cross-domain image mapping. <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b0">[1]</ref> use adversarial networks to map source images to target and perform adaptation in the transferred space. Coupled GAN (CoGAN) <ref type="bibr" target="#b14">[15]</ref> on the other hand trains a coupled generative model that learns the joint data distribution across the two domains. A domain invariant classifier is learnt by sharing weights with the discriminator of the CoGAN network.</p><p>Comparison to recent GAN-based DA approaches: While previous approaches such as <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b0">[1]</ref> use GANs as a data augmentation step, we use a GAN to obtain rich gradient information that makes the learned embeddings domain adaptive. Unlike the previous methods, our approach does not completely rely on a successful image generation process. As a result, our method works well in cases where image generation is hard (eg. in the OFFICE dataset where the number of samples per class is limited). We observed that in such cases, even though the generator network we use performs a mere style transfer, yet this is sufficient for providing good gradient information for successfully aligning the domains, as demonstrated by our superior performance on the OFFICE dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Problem Description: In this section, we provide a formal treatment of the proposed approach and discuss in detail our iterative optimization procedure. Let</p><formula xml:id="formula_0">X = {x i } N i=1</formula><p>be an input space of images and Y = {y i } N i=1 be the label space. We assume there exists a source distribution, S(x, y) and target distribution T (x, y) over the samples in X. In unsupervised domain adaptation, we have access to the source distribution using labeled data from X and the target distribution via unlabeled data sampled from X. Operationally, the problem of unsupervised domain adaptation can be stated as learning a predictor that is optimal in the joint distribution space by using labeled source data and unlabeled target data sampled from X. We consider problems where the data from X takes discrete labels from the set L = {1, 2, 3, ...N c }, where N c is the total number of classes. Our objective is to learn an embedding map</p><formula xml:id="formula_1">F : X → R d and a prediction function C : R d → L.</formula><p>In this work, both F and C are modeled as deep neural networks. The predictor has access to the labels only for the data sampled from the source distribution and not from the target distribution. By extracting information from the target data during training, F implicitly learns the domain shift between S and T . In the rest of this section, we use the terms source (target) distribution and source (target) domain interchangeably.</p><p>Several approaches including learning entropy-based metrics <ref type="bibr" target="#b17">[18]</ref>, learning a domain classifier based on a embedding network <ref type="bibr" target="#b3">[4]</ref> or denoising autoencoders <ref type="bibr" target="#b4">[5]</ref> have been used to transfer information between source and target distributions. In this work, we propose a GAN-based approach to bridge the gap between source and target domains. We accomplish this by using both generative and a discriminative processes thus ensuring a rich information transfer to the learnt embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of GANs:</head><p>In a traditional GAN, two competing mappings are learned: the discriminator D and the generator G, both of which are modeled as deep neural networks. G and D play a minmax game where D tries to classify the generated samples as fake and G tries to fool D by producing examples that are as realistic as possible. More formally, to train a GAN, the following optimization problem is solved in an iterative manner:</p><formula xml:id="formula_2">min G max D E x∼p data (log(D(x)) + E z∼pnoise log(1 − D(G(z))) (1)</formula><p>D(x) represents the probability that x came from the real data distribution rather than the distribution modeled by the generator G. As an extension to traditional GANs, conditional GANs <ref type="bibr" target="#b19">[20]</ref> enable conditioning the generator and discriminator mappings on additional data such as a class label or an embedding. They have been shown to generate images of digits and faces conditioned on the class label or the embedding respectively <ref type="bibr" target="#b30">[31]</ref>. Training a conditional GAN involves optimizing the following minimax objective:</p><formula xml:id="formula_3">min G max D E x∼p data (log(D(x|y)) + E {z∼pnoise} log(1 − D(G(z|y))) (2)</formula><p>Proposed Approach: In this work, we employ a variant of the conditional GAN called Auxiliary Classifier GAN (AC-GAN) <ref type="bibr" target="#b21">[22]</ref> where the discriminator is modeled as a multi-class classifier instead of providing conditioning information at the input. We modify the AC-GAN set up for the domain adaptation problem as follows:</p><p>(a) Given a real image x as input to F , the input to the generator network G is x g = [F (x), z, l], which is a concatenated version of the encoder embedding F (x), a random noise vector z ∈ R d sampled from N (0, 1) and a one hot encoding of the class label, l ∈ {0, 1} (Nc+1) with N c real classes and {N c + 1} being the fake class. For all target samples, since the class labels are unknown, l is set as the one hot encoding of the fake class {N c + 1}.</p><p>(b) We employ a classifier network C that takes as input the embedding generated by F and predicts a multiclass distribution C(x) i.e. the class probability distribution of the input x, which is modeled as a (N c )-way classifier.</p><p>(c) The discriminator mapping D takes the real image x or the generated image G(x g ) as input and outputs two distributions: (1) D data (x): the probability of the input being real, which is modeled as a binary classifier. (2) D cls (x): the class probability distribution of the input x, which is modeled as a (N c )-way classifier. To clarify notation, we use D cls (x) y to imply the probability assigned by the classifier mapping D cls for input x to class y. It should be noted that, for target data, since class labels are unknown, only D data is used to backpropagate the gradients. Now, we describe our optimization procedure in detail. To jointly learn the embedding and the generatordiscriminator pair, we optimize the D, G, F and C networks in an alternating manner:</p><p>1. Given source images as input, D outputs two distributions D data and D cls . D data is optimized by minimizing a binary cross entropy loss L data,src and D cls is optimized by minimizing the cross entropy loss L cls,src between the source labels and the model predictive distribution D cls (x). In the case of source inputs, the gradients are generated using the following loss functions:</p><formula xml:id="formula_4">L data,src + L cls,src = E x∼S max D log(D data (x)) + log(1 − D data (G(x g ))) + log(D cls (x) y )<label>(6)</label></formula><p>2. Using the gradients from D, G is updated using a combination of adversarial loss and classification loss to produce realistic class consistent source images.</p><formula xml:id="formula_5">L G = min G E x∼S − log(D cls (G(x g )) y ) + log(1 − D data (G(x g ))),<label>(7)</label></formula><p>3. F and C are updated based on the source images and source labels in a traditional supervised manner. F is also updated using the adversarial gradients from D so that the feature learning and image generation processes co-occur smoothly.</p><formula xml:id="formula_6">L C = min C min F E x∼S − log(C(F (x)) y ), L cls,src = min F E x∼S − α log(D cls (G(x g )) y ))<label>(8)</label></formula><p>4. In the final step, the real target images are presented as input to F . The target embeddings output by F along with the random noise vector z and the fake label encoding l are input to G. The generated target images G(x g ) are then given as input to D. As described above, D outputs two distributions but the loss function is evaluated only for D data since in the unsupervised case considered here, target labels are not provided during training. Hence, D is updated to classify the generated target images as fake as follows:</p><formula xml:id="formula_7">L adv,tgt = max D E x∼T log(1 − D data (G(x g ))) (9)</formula><p>In order to transfer the knowledge of target distribution to the embedding, F is updated using the gradients from D data that corresponds to the generated target images being classified as real:</p><formula xml:id="formula_8">L F adv = min F E x∼T β log(1 − D data (G(x g )))<label>(10)</label></formula><p>The proposed iterative optimization procedure is summarized as a pseudocode in Algorithm 1. α and β are the coefficients that trade off between the classification loss and the source and target adversarial losses. Based on our experiments, we find that our approach is not overly sensitive to the cost coefficients α and β. However, the value of the parameter is dependent on the application and size of the dataset. Such specifications are mentioned in the supplementary material.</p><p>Use of unlabeled target data: The main strength of our approach is how the target images are used to update the embedding. Given a batch of target images as input, we update the embedding F by using the following binary loss term: min Sample k images with labels from source domain S:</p><formula xml:id="formula_9">F β log(1 − D data (G(x g ))<label>(11)</label></formula><formula xml:id="formula_10">{s i , y i } k i=1 4:</formula><p>Let f i = F (s i ) be the embeddings computed for the source images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Sample k images from target domain T :</p><formula xml:id="formula_11">{t i } k i=1 6:</formula><p>Let h i = F (t i ) be the embeddings computed for the target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Sample k random noise samples {z i } k i=1 ∼ N (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Let f gi and h gi be the concatenated inputs to the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Update discriminator using the following objectives:</p><formula xml:id="formula_12">L D = L data,src + L cls,src + L adv,tgt (3) • L data,src = max D 1 k k i=1 log(D data (s i )) + log(1 − D data (G(f gi ))) • L cls,src = max D 1 k k i=1 log(D cls (s i ) yi ) • L adv,tgt = max D 1 k k i=1 log(1 − D data (G(h gi ))) 10:</formula><p>Update the generator, only for source data, through the discriminator gradients computed using real labels.</p><formula xml:id="formula_13">L G = min G 1 k k i=1 − log(D cls (G(f gi )) yi ) + log(1 − D data (G(f gi )))<label>(4)</label></formula><p>11:</p><p>Update the embedding F using a linear combination of the adversarial loss and classification loss. Update the classifier C for the source data using a cross entropy loss function.</p><formula xml:id="formula_14">L F = L C + α L cls,src + β L F adv (5) • L C = min C min F 1 k k i=1 − log(C(f i ) yi ) • L cls,src = min F 1 k k i=1 − log(D cls (G(f gi )) yi ) • L F adv = min F 1 k k i=1 log(1 − D data (G(h gi ))) 12:</formula><p>end for where x g is the concatenated input to G as described earlier and β is the weight coefficient for the target adversarial loss. The use of target data is intended to bring the source and target distributions closer in the feature space learned by F . To achieve this, we update the F network to produce class consistent embeddings for both source and target data. Performing this update for source data is straightforward since the source labels are available during training. Since labels are unavailable for target data, we use the generative ability of the G-D pair for obtaining the required gradients.</p><p>Given source inputs, G is updated to fool D using gradients from Eq. (7) which provide the conditioning required for G to produce class consistent fake images. Given target inputs, the update in Eq. (11) encourages F to produce target embeddings that are aligned with the source distribution. As training progresses, the class conditioning information learned by G during the source update (Eq. <ref type="formula" target="#formula_5">(7)</ref>) was found to be sufficient for it to produce class consistent images for target embeddings as well. This symbiotic relationship between the embedding and the adversarial framework contributes to the success of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>This section reports the experimental validation of our approach. We perform a thorough study by conducting experiments across three adaptation settings: (1) low domain shift and simple data distribution: DIGITS dataset, (2) moderate domain shift and complex data distribution: OFFICE dataset, (3) high domain shift and complex data distribution: Synthetic to real adaptation. By complex data distribution, we denote datasets containing images with high variability and limited number of samples. Our methods performs well in all three regimes, thus demonstrating the versatility of our approach.  <ref type="table">Table 2</ref>: Accuracy (mean ± std%) values on the OFFICE dataset for the standard protocol for unsupervised domain adaptation <ref type="bibr" target="#b5">[6]</ref>. Results are reported as an average over 5 independent runs. The best numbers are indicated in bold and the second best are underlined. − denotes unreported results. A: Amazon, W: Webcam, D: DSLR</p><formula xml:id="formula_15">Method A → W D → W W → D A → D D → A W → A Average</formula><p>ResNet -Source only <ref type="bibr" target="#b8">[9]</ref> 68.4 ± 0.2 96.7 ± 0.1 99.3 ± 0.1 68.9 ± 0.2 62.5 ± 0.3 60.7 ± 0.3 76.1 TCA <ref type="bibr" target="#b22">[23]</ref> 72.7 ± 0.0 96.7 ± 0.0 99.6 ± 0.0 74.1 ± 0.0 61.7 ± 0.0 60.9 ± 0.0 77.6 GFK <ref type="bibr" target="#b5">[6]</ref> 72. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Digit Experiments</head><p>The first set of experiments involve digit classification in three standard DIGITS datasets: MNIST <ref type="bibr" target="#b12">[13]</ref>, USPS <ref type="bibr" target="#b10">[11]</ref> and SVHN <ref type="bibr" target="#b20">[21]</ref>. Each dataset contains digits belonging to 10 classes (0-9). MNIST and USPS are large datasets of handwritten digits captured under constrained conditions. SVHN dataset, on the other hand was obtained by cropping house numbers in Google Street View images and hence captures much more diversity. We test the three common domain adaptation settings: SVHN → MNIST, MNIST → USPS and USPS → MNIST. In each setting, we use the label information only from the source domain, thus following the unsupervised protocol.</p><p>For all digit experiments, following other recent works <ref type="bibr" target="#b3">[4]</ref>[32], we use a modified version of Lenet architecture as our F network. For G and D networks, we use architectures similar to those used in DCGAN <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) MNIST ↔ USPS</head><p>We start with the easy case of adaptation involving MNIST and USPS. The MNIST dataset is split into 60000 training and 10000 test images, while the USPS dataset contains 7291 training and 2007 test images. We run our experiments in two settings: (1) using the entire training set of MNIST and USPS (MNIST ↔USPS (f)), and (2) using the protocol established in <ref type="bibr" target="#b16">[17]</ref>, sampling 2000 images from MNIST and 1800 images from USPS (MNIST ↔USPS (p)). Table. 1 presents the results of the proposed approach in comparison with other contemporary approaches. The reported numbers are averaged over 5 independent runs with different random samplings or initializations. We can observe that our approach achieves the best performance in all cases except in the MNIST → USPS full protocol case where our accuracy is very close to the best performing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) SVHN → MNIST</head><p>Compared to the previous experiment, SVHN → MNIST presents a harder case of domain adaptation owing to larger domain gap. Following other works <ref type="bibr" target="#b3">[4]</ref> [32], we use the entire training set (labeled 73257 SVHN images and unlabeled 60000 MNIST images) to train our model, and evaluate on the training set of the target domain (MNIST dataset). From <ref type="table">Table.</ref> 2, we observe that our method significantly improves the performance of the source-only model from 60.3% to 92.4%, which results in a performance gain of 32.1%. We also outperform other methods by a large margin, obtaining at least 10.4% performance improvement. A visualization of this improvement in performance is done in <ref type="figure" target="#fig_1">figure 2</ref>, where we show a t-SNE plot of the features of the embedding network F for the adapted and non-adapted cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">OFFICE experiments</head><p>The next set of experiments involve the OFFICE dataset, which is a small scale dataset containing images belonging to 31 classes from three domains -Amazon, Webcam and DSLR, each containing 2817, 795 and 498 images respectively. The small dataset size poses a challenge to our approach since we rely on GAN which demands more data for better image generation. Nevertheless, we perform experiments on the OFFICE dataset to demonstrate that though our method does not succeed in generating very realistic images, the approach still results in improved performance by using the generative process to obtain domain invariant feature representations.</p><p>Training deep networks with randomly initialized weights on small datasets give poor performance. So, an effective technique used in practice is to fine-tune networks trained on a related task having large data <ref type="bibr" target="#b33">[34]</ref>. Following this rationale, we initialized the F network using a pretrained ResNet-50 <ref type="bibr" target="#b8">[9]</ref> model trained on Imagenet. For D and G networks, we used architectures similar to the ones used in the Digits experiments. It should be noted that even though the inputs are 224 × 224, the G network is made to generate a downsampled version of size 64 × 64. Standard data augmentation steps involving mean normalization, random cropping and mirroring were performed.</p><p>In all our experiments, we follow the standard unsupervised protocol -using the entire labeled data in the source domain and unlabeled data in the target domain. <ref type="table">Table 2</ref> reports the performance of our method in comparison to other methods. We observe that our method obtains the stateof-the-art performance in all the settings. In particular, we get good performance improvement consistently in all hard transfer cases: A → W , A → D, W → A and D → A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Synthetic to Real experiments</head><p>To test the effectiveness of the proposed approach further, we perform experiments in the hardest case of domain adaptation involving adaptation from synthetic to real datasets. This setting is particularly interesting because of its enormous practical implications. In this experiment, we use CAD synthetic dataset <ref type="bibr" target="#b24">[25]</ref> and a subset of PASCAL VOC dataset <ref type="bibr" target="#b2">[3]</ref> as our source and target sets respectively. The CAD synthetic dataset contains multiple renderings of 3D CAD models of the 20 object categories contained in the PASCAL dataset. To create the datasets, we follow the protocol described in <ref type="bibr" target="#b23">[24]</ref>: The CAD dataset contains six subsets with different configurations (i.e. RR-RR, W-RR, W-UG, RR-UG, RG-UG, RG-RR). Of these, we use images with white background (W-UG subset) as our training set. To generate the target set, we crop 14976 patches from 4952 images of the PASCAL VOC 2007 test set using the object bounding boxes provided. The lack of realistic background and texture in the CAD synthetic dataset increases the disparity from the natural image manifold, thus making domain adaptation extremely challenging.</p><p>Due to the high domain gap, we observed that models trained on the CAD synthetic dataset with randomly initialized weights performed very poorly on the target dataset. So, similar to the previous set of experiments, we initialized the F network with pretrained models. In particular, we removed the last fully connected layer from the VGG16 model trained on Imagenet and used it as our F network. Note that the same F network is used to train all other methods for fair comparison. <ref type="table">Table.</ref> 3 reports the results of the experiments we ran on the Synthetic to real setting. We can observe that our method improves the baseline performance from 38.1% to 50.4% in addition to outperforming all other compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">VISDA challenge</head><p>In this section, we present the results on VISDA dataset <ref type="bibr" target="#b25">[26]</ref> -a large scale testbed for unsupervised domain adaptation algorithms. The task is to train classification models on synthetic dataset generated from the renderings of 3D CAD models and adapt these models to real images <ref type="table">Table 3</ref>: Accuracy (mean ± std%) values over five independent runs on the Synthetic to real setting. The best numbers are indicated in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CAD → PASCAL VGGNet -Source only 38.1 ± 0.4 RevGrad <ref type="bibr" target="#b3">[4]</ref> 48.3 ± 0.7 RTN <ref type="bibr" target="#b17">[18]</ref> 43.2 ± 0.5 JAN <ref type="bibr" target="#b18">[19]</ref> 46.4 ± 0.8 Ours 50.4 ± 0.6 which are drawn from Microsoft COCO <ref type="bibr" target="#b13">[14]</ref>(validation set) and Youtube Bounding Box dataset <ref type="bibr" target="#b27">[28]</ref>(test set). We train our models using the same hyper-parameter settings and data augmentation scheme as the previous experiment. Table. 4 presents the results on the VISDA classification challenge. We find that our method achieves significant performance gains compared to the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In this experiment, we study the effect of each individual component to the overall performance. The embedding network F is updated using a combination of losses from two streams (1) supervised classification stream and (2) adversarial stream, as shown in <ref type="figure">Figure 1</ref>. The adversarial stream consists of the G-D pair, with D containing two components -real/fake classifier which we denote as C 1 , and auxiliary classifier which we denote as C 2 . We report the performance on the following three settings: (1) using only the Stream 1 and only using source data to train -this corresponds to the Source-only setting (2) Using stream 1 + C 1 classifier from stream 2 -this corresponds to the case where source and target embeddings are forced to produce sourcelike images, but class information is not provided to the discriminator and (3) Using stream 1 + stream2 (C 1 + C 2 ) -this is our entire system. For settings <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula">(3)</ref> we utilized labeled source data and unlabeled target data during training. <ref type="table" target="#tab_4">Table 5</ref> presents the results of this experiment. We observe that using only the real/fake classifier C 1 in the discriminator does improve performance, but the auxiliary classifier C 2 is needed to get the full performance benefit. This can be attributed to the mode collapse problem in traditional GANs (we observed that training without C 2 resulted in missing modes and mismatched mappings where embeddings get mapped to images of wrong classes), hence resulting in sub-optimal performance. Use of an auxiliary classifier objective in D stabilizes the GAN training as observed in <ref type="bibr" target="#b21">[22]</ref> and significantly improves the performance of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we addressed the problem of unsupervised visual domain adaptation. We proposed a joint adversarialdiscriminative approach that transfers the information of the target distribution to the learned embedding using a generator-discriminator pair. We demonstrated the superiority of our approach over existing methods that address this problem using experiments on three different tasks, thus making our approach more generally applicable and versatile. Some avenues for future work include using stronger encoder architectures and applications of our approach to more challenging domain adaptation problems such as RGB-D object recognition and medical imaging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>89.5 ± 0.5 97.9 ± 0.3 99.8 ± 0.4 87.7 ± 0.5 72.8 ± 0.3 71.4 ± 0.4 86.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: TSNE visualization of SVHN → MNIST adaptation. In (a), the source data shown in red is classified well into distinct clusters but the target data is clustered poorly. On applying the proposed approach, as shown in (b), both the source and target distributions are brought closer in a class consistent manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (mean ± std%) values for cross-domain recognition tasks over five independent runs on the digits based datasets. The best numbers are indicated in bold and the second best are underlined. − denotes unreported results. MN: MNIST, US: USPS, SV: SVHN. MN→US (p) denotes the MN→US experiment run using the protocol established in [17], while MN→US (f) denotes the experiment run using the entire datasets. (Refer to Digits experiments section for more details)</figDesc><table>Method 
MN → US (p) MN → US (f) US → MN 
SV → MN 

Source only 
75.2 ± 1.6 
79.1 ± 0.9 
57.1 ± 1.7 
60.3 ± 1.5 
RevGrad [4] 
77.1 ± 1.8 
-
73.0 ± 2.0 
73.9 
DRCN [5] 
91.8 ± 0.09 
-
73.7 ± 0.04 82.0 ± 0.16 
CoGAN [15] 
91.2 ± 0.8 
-
89.1 ± 0.8 
-
ADDA [32] 
89.4 ± 0.2 
-
90.1 ± 0.8 
76.0 ± 1.8 
PixelDA [1] 
-
95.9 
-
-
Ours 
92.8 ± 0.9 
95.3 ± 0.7 
90.8 ± 1.3 
92.4 ± 0.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Performance (accuracy) of our approach on VISDA classification dataset</figDesc><table>Model 
Visda-C: Val 

Source-only Adapted Gain 

Resnet-18 35.3 
63.1 
78.7% 
Resnet-50 
40.2 
69.5 
72.8% 
Resnet-152 44.5 
77.1 
73.2% 

Visda-C: Test 

Resnet-152 40.9 
72.3 
76.7% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Ablation study for OFFICE A→W setting</figDesc><table>Setting 
Accuracy(in %) 

Stream 1 -Source only 
68.4 
Stream 1 + Stream 2 (C 1 only) 
80.5 
Stream 1 + Stream 2 (C 1 + C 2 ) 
89.5 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">Training code: https://goo.gl/zUVeqC</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05424</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Neural Information Processing Systems, NIPS&apos;06</title>
		<meeting>the 19th International Conference on Neural Information Processing Systems, NIPS&apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>abs/1602.04433</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Synthetic to real adaptation with deep generative correlation alignment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1701.05524</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Visda: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1710.06924</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large highprecision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1702.05464</idno>
		<imprint>
			<date type="published" when="2003" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3474</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
