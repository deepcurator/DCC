<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaNet: Adaptive Structural Learning of Artificial Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gonzalvo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kuznetsov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">AdaNet: Adaptive Structural Learning of Artificial Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present new algorithms for adaptively learning artificial neural networks. Our algorithms (ADANET) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classification tasks extracted from the CIFAR-10 dataset and on the Criteo dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved by neural networks found by standard approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-layer artificial neural networks form a powerful learning model which has helped achieve a remarkable performance in several applications in recent years. Representing the input through increasingly more abstract layers of feature representation has shown to be very effective in natural language processing, image captioning, speech recognition and several other areas <ref type="bibr" target="#b28">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b44">Sutskever et al., 2014)</ref>. However, despite the compelling arguments for adopting multi-layer neural networks as a general template for tackling learning problems, training these models and designing the right network for a given task has raised several theoretical questions and faced numerous practical challenges.</p><p>A critical step in learning a large multi-layer neural network for a specific task is the choice of its architecture, which includes the number of layers and the number of units within each layer. Standard training methods for neural networks return a model admitting precisely the number 1 Google Research, New York, NY, USA; 2 Courant Institute of Mathematical Sciences, New York, NY, USA. Correspondence to: Vitaly Kuznetsov &lt;vitalyk@google.com&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, <ref type="bibr">Australia, PMLR 70, 2017</ref><ref type="bibr">. Copyright 2017</ref> by the author(s). of layers and units specified since there needs to be at least one path through the network for the hypothesis to be nontrivial. Single weights can be pruned <ref type="bibr" target="#b16">(Han et al., 2015)</ref> via a technique originally termed Optimal Brain Damage <ref type="bibr" target="#b31">(LeCun et al., 1990)</ref>, but the global architecture remains unchanged. Thus, this imposes a stringent lower bound on the complexity of the model, which may not match that of the learning task considered: complex networks trained with insufficient data may be prone to overfitting and, in reverse, simpler architectures may not suffice to achieve an adequate performance.</p><p>This places a considerable burden on the user who is left with the requirement to specify an architecture with the right complexity, which is often a difficult task even with a significant level of experience and domain knowledge. As a result, the choice of the network is typically left to a hyperparameter search using a validation set. This search space can quickly become exorbitantly large <ref type="bibr" target="#b45">(Szegedy et al., 2015;</ref><ref type="bibr" target="#b18">He et al., 2015)</ref> and large-scale hyperparameter tuning to find an effective network architecture often wasteful of data, time, and resources (e.g. grid search, random search <ref type="bibr" target="#b6">(Bergstra et al., 2011)</ref>). This paper seeks precisely to address some of these issues. We present a theoretical analysis of the problem of learning simultaneously both the network architecture and its parameters. To the best of our knowledge, our results are the first generalization bounds for the problem of structural learning of neural networks. These general guarantees can guide the design of a variety of different algorithms for learning in this setting. We describe in detail two such algorithms (ADANET algorithms) that directly benefit from our theory.</p><p>Rather than enforcing a pre-specified architecture and thus a fixed network complexity, our ADANET algorithms adaptively learn the appropriate network architecture for a learning task. Starting from a simple linear model, our algorithms incrementally augment the network with more units and additional layers, as needed. The choice of the additional subnetworks depends on their complexity and is directly guided by our learning guarantees. Remarkably, the optimization problems for both of our algorithms turn out to be strongly convex and thus guaranteed to admit a unique global solution. <ref type="figure">Figure 1</ref>. An example of a general network architecture: the output layer (in green) is connected to all of the hidden units as well as some input units. Some hidden units (in red and yellow) are connected not only to the units in the layer directly below, but also to units at other levels.</p><p>The paper is organized as follows. In Appendix A, we give a detailed discussion of previous work related to this topic. Section 2 describes the general network architecture and therefore the hypothesis set that we consider. Section 3 provides a formal description of our learning scenario. In Section 4, we prove strong generalization guarantees for learning in this setting, which help guide the design of the algorithm described in Section 5, as well as a variant described in Appendix C. We report the results of our experiments with ADANET in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Network architecture</head><p>In this section, we describe the general network architecture we consider for feedforward neural networks, thereby also defining our hypothesis set. To simplify the presentation, we restrict our attention to the case of binary classification. However, all our results can be straightforwardly extended to multi-class classification, including the network architecture, by augmenting the number of output units, and, our generalization guarantees, by using existing multi-class counterparts of the binary classification ensemble margin bounds we use.</p><p>A common model for feedforward neural networks is the multi-layer architecture where units in each layer are only connected to those in the layer below. We will consider more general architectures where a unit can be connected to units in any of the layers below, as illustrated by <ref type="figure">Figure 1</ref>. In particular, the output unit in our network architectures can be connected to any other unit. These more general architectures include as special cases standard multilayer networks (by zeroing out appropriate connections) as well as somewhat more exotic ones <ref type="bibr" target="#b18">(He et al., 2015;</ref><ref type="bibr" target="#b19">Huang et al., 2016)</ref>. In fact, our definition covers any architecture that can be represented as a directed acyclic graph (DAG).</p><p>More formally, the artificial neural networks we consider are defined as follows. Let l denote the number of intermediate layers in the network and n k the maximum number of units in layer k ∈ [l]. Each unit j ∈ [n k ] in layer k represents a function denoted by h k,j (before composition with an activation function). Let X denote the input space and for any x ∈ X, let Ψ(x) ∈ R n0 denote the corresponding feature vector. Then, the family of functions defined by the first layer functions h 1,j , j ∈ [n 1 ], is the following:</p><formula xml:id="formula_0">H 1 = x → u · Ψ(x) : u ∈ R n0 , u p ≤ Λ 1,0 ,<label>(1)</label></formula><p>where p ≥ 1 defines an l p -norm and Λ 1,0 ≥ 0 is a hyperparameter on the weights connecting layer 0 and layer 1. The family of functions h k,j , j ∈ [n k ], in a higher layer k &gt; 1 is then defined as follows:</p><formula xml:id="formula_1">H k = x → k−1 s=1 u s · (ϕ s • h s )(x) : u s ∈ R ns , u s p ≤ Λ k,s , h k,s ∈ H s ,<label>(2)</label></formula><p>where, for each unit function h k,s , u s in (2) denotes the vector of weights for connections from that unit to a lower layer s &lt; k. The Λ k,s s are non-negative hyperparameters and ϕ s • h s abusively denotes a coordinate-wise composition:</p><formula xml:id="formula_2">ϕ s • h s = (ϕ s • h s,1 , . . . , ϕ s • h s,ns ).</formula><p>The ϕ s s are assumed to be 1-Lipschitz activation functions. In particular, they can be chosen to be the Rectified Linear Unit function (ReLU function) x → max{0, x}, or the sigmoid function x → For the networks we consider, the output unit can be connected to all intermediate units, which therefore defines a function f as follows:</p><formula xml:id="formula_3">f = l k=1 n k j=1 w k,j h k,j = l k=1 w k · h k ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">h k = [h k,1 , . . . , h k,n k ] ∈ H n k</formula><p>k and w k ∈ R n k is the vector of connection weights to units of layer k. Observe that, for u s = 0 for s &lt; k − 1 and w k = 0 for k &lt; l, our architectures coincides with standard multi-layer feedforward ones.</p><p>We will denote by F the family of functions f defined by (3) with the absolute value of the weights summing to one:</p><formula xml:id="formula_5">F = l k=1 w k · h k : h k ∈ H n k k , l k=1 w k 1 = 1 .</formula><p>Let H k denote the union of H k and its reflection, H k = H k ∪ (−H k ), and let H denote the union of the families For any k ∈ [l], we will also consider the family H * k derived from H k by setting Λ k,s = 0 for s &lt; k − 1, which corresponds to units connected only to the layer below. We similarly define H * </p><formula xml:id="formula_6">H k : H = l k=1 H k . Then,</formula><formula xml:id="formula_7">k = H * k ∪ (−H * k ) and H * = ∪ l k=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning problem</head><p>We consider the standard supervised learning scenario and assume that training and test points are drawn i.i.d. according to some distribution D over X × {−1, +1} and denote by S = ((x 1 , y 1 ), . . . , (x m , y m )) a training sample of size m drawn according to D m .</p><p>For a function f taking values in R, we denote by</p><formula xml:id="formula_8">R(f ) = E (x,y)∼D [1 yf (x)≤0</formula><p>] its generalization error and, for any ρ &gt; 0, by R S,ρ (f ) its empirical margin error on the sample</p><formula xml:id="formula_9">S: R S,ρ (f ) = 1 m m i=1 1 yif (xi)≤ρ .</formula><p>The learning problem consists of using the training sample S to determine a function f defined by (3) with small generalization error R(f ). For an accurate predictor f , we expect many of the weights to be zero and the corresponding architecture to be quite sparse, with fewer than n k units at layer k and relatively few non-zero connections. In that sense, learning an accurate function f implies also learning the underlying architecture.</p><p>In the next section, we present data-dependent learning bounds for this problem that will help guide the design of our algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Generalization bounds</head><p>Our learning bounds are expressed in terms of the Rademacher complexities of the hypothesis sets H k . The empirical Rademacher complexity of a hypothesis set G for a sample S is denoted by R S (G) and defined as follows:</p><formula xml:id="formula_10">R S (G) = 1 m E σ sup h∈G m i=1 σ i h(x i ) ,</formula><p>where σ = (σ 1 , . . . , σ m ), with σ i s independent uniformly distributed random variables taking values in {−1, +1}. Its Rademacher complexity is defined by R m (G) =</p><formula xml:id="formula_11">E S∼D m [ R S (G)]</formula><p>. These are data-dependent complexity measures that lead to finer learning guarantees <ref type="bibr" target="#b25">(Koltchinskii &amp; Panchenko, 2002;</ref><ref type="bibr" target="#b5">Bartlett &amp; Mendelson, 2002)</ref>.</p><p>As pointed out earlier, the family of functions F is the convex hull of H. Thus, generalization bounds for ensemble methods can be used to analyze learning with F. In particular, we can leverage the recent margin-based learning guarantees of <ref type="bibr" target="#b10">Cortes et al. (2014)</ref>, which are finer than those that can be derived via a standard Rademacher complexity analysis <ref type="bibr" target="#b25">(Koltchinskii &amp; Panchenko, 2002)</ref>, and which admit an explicit dependency on the mixture weights w k defining the ensemble function f . That leads to the following learning guarantee.</p><p>Theorem 1 (Learning bound). Fix ρ &gt; 0. Then, for any δ &gt; 0, with probability at least 1 − δ over the draw of a sample S of size m from D m , the following inequality holds</p><formula xml:id="formula_12">for all f = l k=1 w k · h k ∈ F: R(f ) ≤ R S,ρ (f ) + 4 ρ l k=1 w k 1 R m ( H k ) + 2 ρ log l m + C(ρ, l, m, δ), where C(ρ, l, m, δ) = 4 ρ 2 log( ρ 2 m log l ) log l m + log(2/δ) 2m = O 1 ρ log l m .</formula><p>The proof of this result, as well as that of all other main theorems are given in Appendix B. The bound of the theorem can be generalized to hold uniformly for all ρ ∈ (0, 1], at the price of an additional term of the form log log 2 (2/ρ)/m using standard techniques (Koltchinskii &amp; Panchenko, 2002).</p><p>Our first result gives an upper bound on the Rademacher complexity of H k in terms of the Rademacher complexity of other layer families.</p><p>Lemma 1. For any k &gt; 1, the empirical Rademacher complexity of H k for a sample S of size m can be upperbounded as follows in terms of those of H s s with s &lt; k:</p><formula xml:id="formula_13">R S (H k ) ≤ 2 k−1 s=1 Λ k,s n 1 q s R S (H s ).</formula><p>For the family H * k , which is directly relevant to many of our experiments, the following more explicit upper bound can be derived, using Lemma 1.</p><formula xml:id="formula_14">Lemma 2. Let Λ k = k s=1 2Λ s,s−1 and N k = k s=1 n s−1 .</formula><p>Then, for any k ≥ 1, the empirical Rademacher complexity of H * k for a sample S of size m can be upper bounded as follows:</p><formula xml:id="formula_15">R S (H * k ) ≤ r ∞ Λ k N 1 q k log(2n 0 ) 2m .</formula><p>Note that N k , which is the product of the number of units in layers below k, can be large. This suggests that values of p closer to one, that is larger values of q, could be more helpful to control complexity in such cases. More generally, similar explicit upper bounds can be given for the Rademacher complexities of subfamilies of H k with units connected only to layers</p><formula xml:id="formula_16">k, k − 1, . . . , k − d, with d fixed, d &lt; k.</formula><p>Combining Lemma 2 with Theorem 1 helps derive the following explicit learning guarantee for feedforward neural networks with an output unit connected to all the other units.</p><formula xml:id="formula_17">Corollary 1 (Explicit learning bound). Fix ρ &gt; 0. Let Λ k = k s=1 4Λ s,s−1 and N k = k s=1 n s−1 .</formula><p>Then, for any δ &gt; 0, with probability at least 1 − δ over the draw of a sample S of size m from D m , the following inequality holds</p><formula xml:id="formula_18">for all f = l k=1 w k · h k ∈ F * : R(f ) ≤ R S,ρ (f ) + 2 ρ l k=1 w k 1 r ∞ Λ k N 1 q k 2 log(2n 0 ) m + 2 ρ log l m + C(ρ, l, m, δ), where C(ρ, l, m, δ) = 4 ρ 2 log( ρ 2 m log l ) log l m + log( 2 δ ) 2m = O 1 ρ log l m</formula><p>, and where</p><formula xml:id="formula_19">r ∞ = E S∼D m [r ∞ ].</formula><p>The learning bound of Corollary 1 is a finer guarantee than previous ones by <ref type="bibr" target="#b4">Bartlett (1998)</ref>, <ref type="bibr" target="#b38">Neyshabur et al. (2015)</ref>, or <ref type="bibr" target="#b43">Sun et al. (2016)</ref>. This is because it explicitly differentiates between the weights of different layers while previous bounds treat all weights indiscriminately. This is crucial to the design of algorithmic design since the network complexity no longer needs to grow exponentially as a function of depth. Our bounds are also more general and apply to more other network architectures, such as those introduced in <ref type="bibr" target="#b18">(He et al., 2015;</ref><ref type="bibr" target="#b19">Huang et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Algorithm</head><p>This section describes our algorithm, ADANET, for adaptive learning of neural networks. ADANET adaptively grows the structure of a neural network, balancing model complexity with empirical risk minimization. We also describe in detail in Appendix C another variant of ADANET which admits some favorable properties.</p><p>Let x → Φ(−x) be a non-increasing convex function upper-bounding the zero-one loss, x → 1 x≤0 , such that Φ is differentiable over R and Φ (x) = 0 for all x. This surrogate loss Φ may be, for instance, the exponential function Φ(x) = e x as in AdaBoost <ref type="bibr" target="#b13">(Freund &amp; Schapire, 1997)</ref>, or the logistic function, Φ(x) = log(1 + e x ) as in logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Objective function</head><p>Let {h 1 , . . . , h N } be a subset of H * . In the most general case, N is infinite. However, as discussed later, in practice, the search is limited to a finite set. For any j ∈ [N ], we will denote by r j the Rademacher complexity of the family H kj that contains h j : r j = R m (H kj ).</p><p>ADANET seeks to find a function f = N j=1 w j h j ∈ F * (or neural network) that directly minimizes the datadependent generalization bound of Corollary 1. This leads to the following objective function:</p><formula xml:id="formula_20">F (w) = 1 m m i=1 Φ 1 − y i N j=1 w j h j + N j=1 Γ j |w j |,<label>(4)</label></formula><p>where w ∈ R N and Γ j = λr j + β, with λ ≥ 0 and β ≥ 0 hyperparameters. The objective function (4) is a convex function of w. It is the sum of a convex surrogate of the empirical error and a regularization term, which is a weighted-l 1 penalty containing two sub-terms: a standard norm-1 regularization which admits β as a hyperparameter, and a term that discriminates the functions h j based on their complexity.</p><p>The optimization problem consisting of minimizing the objective function F in (4) is defined over a very large space of base functions h j . ADANET consists of applying coordinate descent to (4). In that sense, our algorithm is similar to the DeepBoost algorithm of <ref type="bibr" target="#b10">Cortes et al. (2014)</ref>. However, unlike DeepBoost, which combines decision trees, ADANET learns a deep neural network, which requires new methods for constructing and searching the space of func-(a) (b) <ref type="figure">Figure 2</ref>. Illustration of the algorithm's incremental construction of a neural network. The input layer is indicated in blue, the output layer in green. Units in the yellow block are added at the first iteration while units in purple are added at the second iteration. Two candidate extensions of the architecture are considered at the the third iteration (shown in red): (a) a two-layer extension; (b) a three-layer extension. Here, a line between two blocks of units indicates that these blocks are fully-connected.</p><p>tions h j . Both of these aspects differ significantly from the decision tree framework. In particular, the search is particularly challenging. In fact, the main difference between the algorithm presented in this section and the variant described in Appendix C is the way new candidates h j are examined at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Description</head><p>We start with an informal description of ADANET. Let B ≥ 1 be a fixed parameter determining the number of units per layer of a candidate subnetwork. The algorithm proceeds in T iterations. Let l t−1 denote the depth of the neural network constructed before the start of the t-th iteration. At iteration t, the algorithm selects one of the following two options:</p><p>1. augmenting the current neural network with a subnetwork with the same depth as that of the current network h ∈ H * B lt−1 , with B units per layer. Each unit in layer k of this subnetwork may have connections to existing units in layer k − 1 of ADANET in addition to connections to units in layer k − 1 of the subnetwork.</p><p>2. augmenting the current neural network with a deeper subnetwork h ∈ H * B lt−1 , with depth l t−1 + 1. The set of connections allowed is defined in the same way as for h.</p><p>The option selected is the one leading to the best reduction of the current value of the objective function, which depends both on the empirical error and the complexity of the subnetwork added, which is penalized differently in these two options. <ref type="figure">Figure 2</ref> illustrates this construction and the two options just described. An important aspect of our algorithm is that the units of a subnetwork learned at a previous iteration (say h 1,1 in <ref type="figure">Figure 2</ref>) can serve as input to a deeper subnetwork added later (for example h 2,2 or h 2,3 in the <ref type="figure">Figure)</ref>. Thus, the deeper subnetworks added later can take advantage of the embeddings that were learned at the previous iterations. The algorithm terminates after T rounds or if the ADANET architecture can no longer be extended to improve the objective (4).</p><p>More formally, ADANET is a boosting-style algorithm that applies (block) coordinate descent to (4). At each iteration of block coordinate descent, descent coordinates h (base learners in the boosting literature) are selected from the space of functions H * . These coordinates correspond to the direction of the largest decrease in (4). Once these coordinates are determined, an optimal step size in each of these directions is chosen, which is accomplished by solving an appropriate convex optimization problem.</p><p>Note that, in general, the search for the optimal descent coordinate in an infinite-dimensional space or even in finite but large sets such as that of all decision trees of some large depth may be intractable, and it is common to resort to a heuristic search (weak learning algorithm) that returns δ-optimal coordinates. For instance, in the case of boosting with trees one often grows trees according to some particular heuristic <ref type="bibr" target="#b13">(Freund &amp; Schapire, 1997)</ref>.</p><p>We denote the ADANET model after t − 1 rounds by f t−1 , which is parameterized by w t−1 . Let h k,t−1 denote the vector of outputs of units in the k-th layer of the ADANET model, l t−1 be the depth of the ADANET architecture, n k,t−1 be the number of units in k-th layer after t − 1 rounds. At round t, we select descent coordinates by considering two candidate subnetworks h ∈ H * lt−1 and h ∈ H * lt−1+1 that are generated by a weak learning algorithm WEAKLEARNER. Some choices for this algorithm in our setting are described below. Once we obtain h and h , we select one of these vectors of units, as well as a vector of weights w ∈ R B , so that the result yields the best improvement in (4). This is equivalent to minimizing the following objective function over w ∈ R B and u ∈ {h, h }:</p><formula xml:id="formula_21">F t (w, u) = 1 m m i=1 Φ 1 − y i f t−1 (x i ) − y i w · u(x i ) + Γ u w 1 ,<label>(5)</label></formula><p>where Γ u = λr u + β and r u is R m H lt−1 if u = h and R m H lt−1+1 otherwise. In other words, if</p><formula xml:id="formula_22">min w F t (w, h) ≤ min w F t (w, h ), then w * = argmin w∈R B F t (w, h), h t = h ADANET(S = ((x i , y i ) m i=1 ) 1 f 0 ← 0 2 for t ← 1 to T do 3 h, h ← WEAKLEARNER S, f t−1 4 w ← MINIMIZE F t (w, h) 5 w ← MINIMIZE F t (w, h ) 6 if F t (w, h ) ≤ F t (w , h ) then 7 h t ← h 8 else h t ← h 9 if F (w t−1 + w * ) &lt; F (w t−1 ) then 10 f t ← f t−1 + w * · h t 11</formula><p>else return f t−1 12 return f T <ref type="figure">Figure 3</ref>. Pseudocode of the ADANET algorithm. On line 3 two candidate subnetworks are generated (e.g. randomly or by solving (6)). On lines 3 and 4, <ref type="formula" target="#formula_21">(5)</ref> is solved for each of these candidates. On lines 5-7 the best subnetwork is selected and on lines 9-11 termination condition is checked.</p><p>and otherwise</p><formula xml:id="formula_23">w * = argmin w∈R B F t (w, h ), h t = h</formula><p>If F (w t−1 + w * ) &lt; F (w t−1 ) then we set f t = f t−1 + w * · h t and otherwise we terminate the algorithm.</p><p>There are many different choices for the WEAKLEARNER algorithm. For instance, one may generate a large number of random networks and select the one that optimizes (5).</p><p>Another option is to directly minimize (5) or its regularized version:</p><formula xml:id="formula_24">F t (w, h) = 1 m m i=1 Φ 1−y i f t−1 (x i )−y i w · h(x i ) + R(w, h),<label>(6)</label></formula><p>over both w and h. Here R(w, h) is a regularization term that, for instance, can be used to enforce that u s p ≤ Λ k,s in (2). Note that, in general, (6) is a non-convex objective. However, we do not rely on finding a global solution to the corresponding optimization problem. In fact, standard guarantees for regularized boosting only require that each h that is added to the model decreases the objective by a constant amount (i.e. it satisfies δ-optimality condition) for a boosting algorithm to converge <ref type="bibr" target="#b39">(Rätsch et al., 2001;</ref><ref type="bibr" target="#b36">Luo &amp; Tseng, 1992)</ref>.</p><p>Furthermore, the algorithm that we present in Appendix C uses a weak-learning algorithm that solves a convex subproblem at each step and that additionally has a closedform solution. This comes at the cost of a more restricted search space for finding a descent coordinate at each step of the algorithm.</p><p>We conclude this section by observing that in our description of ADANET we have fixed B for all iterations and only two candidate subnetworks are considered at each step. Our approach easily extends to an arbitrary number of candidate subnetworks (for instance of different depth l) as well as varying number of units per layer B. Furthermore, selecting an optimal subnetwork among the candidates is easily parallelizable allowing for efficient and effective search for optimal descent directions. We also note that the choice of subnetworks need not be restricted to standard feedforward architectures and more exotic choices can be employed including the ones in <ref type="bibr" target="#b18">(He et al., 2015;</ref><ref type="bibr" target="#b19">Huang et al., 2016)</ref>. In our experiments we will restrict attention to simple feedforward subnetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section we present the results of our experiments with ADANET. Some additional experimental results are given in Appendix D and further implementation details presented in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">CIFAR-10</head><p>In our first set of experiments, we used the CIFAR-10 dataset <ref type="bibr" target="#b27">(Krizhevsky, 2009</ref>). This dataset consists of 60,000 images evenly categorized in 10 different classes. To reduce the problem to binary classification, we considered five pairs of classes: deer-truck, deer-horse, automobile-truck, cat-dog, dog-horse. Raw images have been pre-processed to obtain color histograms and histogram of gradient features. The result is 154 real valued features with ranges in [0, 1].</p><p>We compared ADANET to standard feedforward neural networks (NN) and logistic regression (LR) models. Note that convolutional neural networks are often a more natural choice for image classification problems such as CIFAR-10. However, the goal of our experiments was not to obtain state-of-the-art results for this particular task, but a proof-of-concept showing that our structural learning approach can be very competitive with traditional approaches for finding efficient architectures and training corresponding networks.</p><p>Our ADANET algorithm requires the knowledge of complexities r j , which, in some cases, can be estimated from data. In our experiments, we used the upper bound of Lemma 2. Our algorithm admits a number of hyperparameters: regularization hyperparameters λ, β, number of units B in each layer of new subnetworks that are used to extend the model at each iteration, and a bound Λ k on weights u in each unit. As discussed in Section 5, there are different approaches to finding candidate subnetworks in each iteration. In our experiments, we searched for candidate subnet- works by minimizing (6) with R = 0. This also requires a learning rate hyperparameter η. These hyperparamers have been optimized over the following ranges: λ ∈ {0, 10 −8 , 10 −7 , 10 −6 , 10 −5 , 10 −4 }, B ∈ {100, 150, 250}, η ∈ {10 −4 , 10 −3 , 10 −2 , 10 −1 }. We have used a single Λ k for all k &gt; 1 optimized over {1.0, 1.005, 1.01, 1.1, 1.2}. For simplicity, we chose β = 0.</p><p>Neural network models also admit a learning rate η and a regularization coefficient λ as hyperparameters, as well as the number of hidden layers l and the number of units n in each hidden layer. The range of η was the same as for ADANET and we varied l in {1, 2, 3}, n in {100, 150, 512, 1024, 2048} and λ ∈ {0, 10 −5 , 10 −4 , 10 −3 , 10 −2 , 10 −1 }. Logistic regression only admits as hyperparameters η and λ which were optimized over the same ranges. Note that the total number of hyperparameter settings for ADANET and standard neural networks is exactly the same. Furthermore, the same holds for the number of hyperparameters that determine the resulting architecture of the model: Λ and B for ADANET and l and n for neural network models. Observe that, while a particular setting of l and n determines a fixed architecture, Λ and B parameterize a structural learning procedure that may result in a different architecture depending on the data.</p><p>In addition to the grid search procedure, we have conducted a hyperparameter optimization for neural networks using Gaussian process bandits (NN-GP), which is a sophisticated Bayesian non-parametric method for response-surface modeling in conjunction with a bandit algorithm <ref type="bibr" target="#b42">(Snoek et al., 2012)</ref>. Instead of operating on a pre-specified grid, this allows one to search for hyperparameters in a given range. We used the following ranges: λ ∈ [10 <ref type="bibr">100,</ref><ref type="bibr">2048]</ref>. This algorithm was run for 500 trials, which is more than the number of hyperparameter settings considered by ADANET and NN. Observe that this search procedure can also be applied to our algorithm but we chose not to use it in this set of experiments to further demonstrate competitiveness of our structural learning approach.</p><formula xml:id="formula_25">−5 , 1], η ∈ [10 −5 , 1], l ∈ [1, 3] and n ∈ [</formula><p>In all our experiments, we use ReLu as the activation functions. NN, NN-GP and LR are trained using stochastic gradient method with batch size of 100 and maximum of 10,000 iterations. The same configuration is used for solving (6). We use T = 30 for ADANET in all our experiments although in most cases algorithm terminates after 10 rounds.</p><p>In each of the experiments, we used standard 10-fold crossvalidation for performance evaluation and model selection. In particular, the dataset was randomly partitioned into 10 folds, and each algorithm was run 10 times, with a different assignment of folds to the training set, validation set and test set for each run. Specifically, for each i ∈ {0, . . . , 9}, fold i was used for testing, fold i + 1 (mod 10) was used for validation, and the remaining folds were used for training. For each setting of the parameters, we computed the average validation error across the 10 folds, and selected the parameter setting with maximum average accuracy across validation folds. We report the average accuracy (and standard deviations) of the selected hyperparameter setting across test folds in <ref type="table">Table 1</ref>.</p><p>Our results show that ADANET outperforms other methods on each of the datasets. The average architectures for all label pairs are provided in <ref type="table" target="#tab_1">Table 2</ref>. Note that NN and NN-GP always select a one-layer architecture. The architectures selected by ADANET also typically admit a single layer, with fewer nodes than those selected by NN and NN-GP. However, for the more challenging problem cat-dog, ADANET opts for a more complex model with two layers, which results in a better performance. This further illustrates how our approach helps learn network architectures in an adaptive fashion, based on the complexity of the task.</p><p>As discussed in Section 5, different heuristics can be used to generate candidate subnetworks on each iteration of ADANET. In a second set of experiments, we varied the objective function (6), as well as the domain over which it is optimized. This allowed us to study the sensitivity of ADANET to the choice of a heuristic used to generate candidate subnetworks. In particular, we considered the following variants of ADANET. ADANET.R uses R(w, h) = Γ h w 1 as a regularization term in (6). As the ADANET architecture grows, each new subnetwork is connected to all the previous subnetworks, which significantly increases the number of connections in the network and the overall complexity of the model. ADANET.P and ADANET.D are restricting connections to existing subnetworks in different ways. ADANET.P connects each new subnetwork only to the subnetwork that was added on the previous iteration. ADANET.D uses dropout on the connections to previously added subnetworks. Finally, while ADANET is based on the upper bounds on the Rademacher complexities of Lemma 2, ADANET.SD uses instead standard deviations of the outputs of the last hidden layer on the training data as surrogates for Rademacher complexities. The advantage of using this data-dependent measure of complexity is that it eliminates the hyperparameter Λ, thereby reducing the hyperparameter search space. We report the average accuracies across test folds for the deer-truck pair in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Criteo Click Rate Prediction</head><p>We also compared ADANET to NN on the Criteo Click Rate Prediction dataset (https://www.kaggle.com/c/ criteo-display-ad-challenge). This dataset consists of 7 days of data where each instance is an impression and a binary label (clicked or not clicked). Each impression admits 13 count features and 26 categorical features. Count features have been transformed by taking the natural logarithm. The values of categorical features appearing less than 100 times are replaced by 0. The rest of the values are then converted to integers, which are then used as keys to look up embeddings (that are trained together with each model). If the number of possible values for a feature x is d(x), then the embedding dimension is set to 6d(f ) 1/4 for d(f ) &gt; 25. Otherwise, the embedding dimension is d(f ). Missing feature values are set to 0. We split the labeled set provided in the link above into training, validation and test sets.</p><p>1 Our training set covered the first 5 days of data (32,743,299 instances) and the validation and test sets consisted of 1 day (6,548,659 instances). Gaussian processes bandits were used to find the best hyperparameter settings on validation set both for ADANET and NN. For ADANET we optimized over the following hyperparameter ranges: B ∈ {125, 256, 512}, Λ ∈ [1, 1.5], η ∈ [10 −4 , 10 −1 ], λ ∈ [10 −12 , 10 −4 ]. For NN the ranges were as follows: l ∈ [1, 6], n ∈ {250, 512, 1024, 2048}, η ∈ [10 −5 , 10 −1 ], λ ∈ [10 −6 , 10 −1 ]. We trained NNs for 100,000 iterations using mini-batch stochastic gradient method with batch size of 512. The same configuration was used at each iteration of ADANET to solve (6). The maximum number of hyperparameter trials was 2,000 for both methods. The results are presented in <ref type="table" target="#tab_2">Table 4</ref>. In this experiment, NN chooses an architecture with four hidden layers and 512 units in each hidden layer. Remarkably, ADANET achieves a better accuracy with an architecture consisting of single layer with just 512 nodes. While the difference in performance appears to be small, it is in fact statistically significant in this challenging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented a new framework and algorithms for adaptively learning artificial neural networks. Our algorithm, ADANET, benefits from strong theoretical guarantees. It simultaneously learns a neural network architecture and its parameters, by balancing a trade-off between model complexity and empirical risk minimization. We reported favorable experimental results demonstrating that our algorithm is able to learn network architectures that perform better than those found via a grid search. Our techniques are general and can be applied to other neural network architectures such as CNNs and RNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>F coincides with the convex hull of H: F = conv(H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Table 1 . 10 .</head><label>110</label><figDesc>Experimental results for ADANET, NN, LR and NN-GP for different pairs of labels in CIFAR-Boldfaced± 0.0082 0.8997 ± 0.0066 0.9213 ± 0.0065 0.9220 ± 0.0069 deer-horse 0.8430 ± 0.0076 0.7685 ± 0.0119 0.8055 ± 0.0178 0.8060 ± 0.0181 automobile-truck 0.8461 ± 0.0069 0.7976 ± 0.0076 0.8063 ± 0.0064 0.8056 ± 0.0138 cat-dog 0.6924 ± 0.0129 0.6664 ± 0.0099 0.6595 ± 0.0141 0.6607 ± 0.0097 dog-horse 0.8350 ± 0.0089 0.7968 ± 0.0128 0.8066 ± 0.0087 0.8087 ± 0.0109</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Average number of units in each layer.Table 3. Experimental results for different variants of ADANET, for the deer-truck label pair in CIFAR-10.</figDesc><table>Label pair 
ADANET 
NN NN-GP 

1st layer 2nd layer 

deer-truck 
990 
0 
2048 1050 
deer-horse 
1475 
0 
2048 488 
automobile-truck 2000 
0 
2048 1595 
cat-dog 
1800 
25 
512 155 
dog-horse 
1600 
0 
2048 1273 

Algorithm 
Accuracy (± std. dev.) 

ADANET.SD 
0.9309 ± 0.0069 
ADANET.R 
0.9336 ± 0.0075 
ADANET.P 
0.9321 ± 0.0065 
ADANET.D 
0.9376 ± 0.0080 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Experimental results for Criteo dataset.</figDesc><table>Algorithm Accuracy 

ADANET 
0.7846 
NN 
0.7811 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Observe that the bound of the theorem depends only logarithmically on the depth of the network l. But, perhaps more remarkably, the complexity term of the bound is a w k 1 -weighted average of the complexities of the layer hypothesis sets H k , where the weights are precisely those defining the network, or the function f . This suggests that a function f with a small empirical margin error and a deep architecture benefits nevertheless from a strong generalization guarantee, if it allocates more weights to lower layer units and less to higher ones. Of course, when the weights are sparse, that will imply an architecture with relatively fewer units or connections at higher layers than at lower ones. The bound of the theorem further gives a quantitative guide for apportioning the weights, depending on the Rademacher complexities of the layer hypothesis sets. This data-dependent learning guarantee will serve as a foundation for the design of our structural learning algorithms in Section 5 and Appendix C. However, to fully exploit it, the Rademacher complexity measures need to be made explicit. One advantage of these data-dependent measures is that they can be estimated from data, which can lead to more informative bounds. Alternatively, we can derive useful upper bounds for these measures which can be more conveniently used in our algorithms. The next results in this section provide precisely such upper bounds, thereby leading to a more explicit generalization bound. We will denote by q the conjugate of p, that is 1 p + 1 q = 1, and define r ∞ = max i∈[1,m] Ψ(x i ) ∞ .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The test set available from this link does not include ground truth labels and therefore could be used in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work of M. Mohri and that of S. Yang were partly funded by NSF awards IIS-1117591 and CCF-1535987.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the number of neurons in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Provable bounds for learning some deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05653</idno>
		<title level="m">Why are deep nets reversible: A simple theory, with implications for training</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otkrist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rémi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kégl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balázs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arous</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0233</idno>
		<title level="m">The loss surfaces of multilayer networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the expressive power of deep learning: a tensor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Syed</surname></persName>
		</author>
		<title level="m">Deep boosting. In ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The power of depth for feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03965</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
<note type="report_type">Hypernetworks</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A structure optimisation algorithm for feedforward neural network construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Gui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Fei</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="347" to="357" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01240</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A constructive algorithm for training cooperative neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monirul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="820" to="834" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new adaptive merging and growing algorithm for designing artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monirul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Sattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faijul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="705" to="722" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janzamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Majid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08473</idno>
		<title level="m">Generalization bounds for neural networks through tensor factorization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Empirical margin distributions and bounding the generalization error of combined classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladmir</forename><surname>Koltchinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A structural learning algorithm for multi-layered neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manabu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Kajiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenzo</forename><surname>Akazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1105" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-class deep boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vitaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Syed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constructive algorithms for structure learning in feedforward neural networks for regression problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin-Yau</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dit-Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="630" to="645" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modelling with constructive backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Lehtokangas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="707" to="716" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tuning of the structure and parameters of a neural network using an improved genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Hak-Keung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Sai-Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Asynchronous parallel stochastic gradient for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yijun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2719" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the computational efficiency of training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="855" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the convergence of coordinate descent method for convex differentiable minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Quan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="35" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new strategy for adaptively constructing multilayer feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liying</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pramod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delashmit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Manry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Maldonado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2831" to="2847" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Norm-based capacity control in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>In COLT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the convergence of leveraging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ugur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6615</idno>
		<title level="m">Explorations on high dimensional landscapes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<title level="m">Convolutional neural fabrics. CoRR, abs/1606.02492</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the depth of deep neural networks: A theoretical view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shizhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tie-Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Benefits of depth in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saizheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Yuhuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhouhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03528</idno>
		<title level="m">regularized neural networks are improperly learnable in polynomial time</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
