<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Evaluate Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Evaluate Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, ME-TEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rulebased metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning to automatically generate captions to summarize the content of an image is considered as a crucial task in Computer Vision. The evaluation of image captioning models is generally performed using metrics such as BLEU <ref type="bibr" target="#b26">[27]</ref>, METEOR <ref type="bibr" target="#b19">[20]</ref>, ROUGE <ref type="bibr" target="#b22">[23]</ref> or CIDEr <ref type="bibr" target="#b30">[31]</ref>, all of which mainly measure the word overlap between generated and reference captions. The recently proposed SPICE <ref type="bibr" target="#b2">[3]</ref> measures the similarity of scene graphs constructed from the candidate and reference sentence, and shows better correlation with human judgments.</p><p>These commonly used evaluation metrics face two challenges. Firstly, many metrics fail to correlate well with hu- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN LSTM</head><p>Image representation Caption representation Binary classification <ref type="figure">Figure 1</ref>. An overview of our proposed captioning evaluation metric. From a set of images and corresponding human written and machine generated captions, we train a model to discriminate between human and generated captions. The model comprises three major components: a CNN to compute image representations, an RNN with LSTM cells to encode the caption, and a binary classifier as the critique. After training, the learned critique can be used as a metric to evaluate the quality of candidate captions with respect to the context (i.e., the image and reference human captions).</p><p>man judgments. Metrics based on measuring word overlap between candidate and reference captions find it difficult to capture semantic meaning of a sentence, therefore often lead to bad correlation with human judgments. Secondly, each evaluation metric has its well-known blind spot, and rule-based metrics are often inflexible to be responsive to new pathological cases. For example, SPICE is sensitive to the semantic meaning of a caption but tends to ignore its syntactic quality. Liu et al. <ref type="bibr" target="#b24">[25]</ref> shows that SPICE prefers to give high score to long sentences with repeating clauses. It's not easy to let SPICE take such pathological cases into account. Since it's difficult to completely avoid such blind spots, a good evaluation metric for image captioning should be flexible enough to adapt to pathological cases once identified, while correlating well with human judgments. To address the aforementioned two challenges, we propose a metric that directly discriminates between human and machine generated captions while being able to flexibly adapt to pathological cases of our interests. Since real human judgment is impractical to obtain at scale, our proposed learning based metric is trained to perform like a human critique, as illustrated in <ref type="figure">Fig. 1</ref>. We use a state-ofthe-art CNN architecture to capture high-level image representations, and a RNN with LSTM cells to encode captions. To design the learned critique, we follow insights from the COCO Captioning Challenge in 2015 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, in which a large-scale human judgment experiment was performed. In particular, our critique is a binary classifier that makes a Turing Test type judgment in which it differentiates between human-written and machine-generated captions.</p><p>In order to capture targeted pathological cases, we propose to incorporate these pathological sentences as negative training examples. To systematically create such pathological sentences, we define several transformations to generate unnatural sentences that might get high scores in an evaluation metric. Our proposed data augmentation (Sec. 3.3) scheme uses these transformations to generate large number of negative examples, which guide our metric to explore a variety of possible sentence constructions that are rare to be found in real world data. Further, we propose a systematic approach to measure the robustness of an evaluation metric to a given pathological transformation (Sec. 3.4). Extensive experiments (Sec. 4) verify the effectiveness and robustness of our proposed evaluation metric and demonstrate better correlation with human judgments on COCO and Flickr 8k, compared with commonly-used image captioning metrics.</p><p>Our key contributions can be summarized as follows:</p><p>• We propose a novel learned based captioning evaluation metric that directly captures human judgments while being flexible to targeted pathological cases.</p><p>• We demonstrate key factors for how to successfully train a good captioning evaluation metric.</p><p>• We conduct comprehensive studies that demonstrates the effectiveness of the proposed metric, in particular its correlation to human judgment and robustness toward pathological transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Captioning evaluation. Despite recent interests, image captioning is notoriously difficult to evaluate due to the inherent ambiguity. Human evaluation scores are reliable but costly to obtain. Thus, current image captioning models are usually evaluated with automatic metrics instead of human judgments. Commonly used evaluation metrics BLEU <ref type="bibr" target="#b26">[27]</ref>, METEOR <ref type="bibr" target="#b19">[20]</ref>, ROUGE <ref type="bibr" target="#b22">[23]</ref> and CIDEr <ref type="bibr" target="#b30">[31]</ref> are mostly based on n-gram overlap and tend to be insensitive to semantic information. Anderson et al. recently proposed the SPICE <ref type="bibr" target="#b2">[3]</ref> that is based on scene graph similarity. Although SPICE obtains significantly higher correlation with human judgments, it encounters difficulties with repetitive sentences, as pointed out in <ref type="bibr" target="#b24">[25]</ref>. It is worth noting that all above mentioned metrics rely solely on similarity between candidate and reference captions, without taking the image into consideration. Our proposed metric, on the other hand, takes image feature as input. While all the previous metrics are rule-based, our proposed metric learns to score candidate captions by training to distinguish positive and negative examples. Moreover, our proposed training scheme could flexibly take new pathological cases into account, yet traditional metrics find it hard to adapt.</p><p>Adversarial training and evaluation. Generative Adversarial Networks (GANs) <ref type="bibr" target="#b11">[12]</ref> have been recently applied to generate image captions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30]</ref>. Although GANs could provide discriminators to tell apart human and machine generated captions, they differ from our works as our discriminator focuses on evaluation instead of generation. All existing adversarial evaluation approaches define the generator performance to be inversely proportional to the classification performance of the discriminator, motivated by the intuition that a good generator should produce outputs that are hard for the discriminator to distinguish from real data. The specific configurations differ among the approaches. Im et al. <ref type="bibr" target="#b15">[16]</ref> propose to train a pair of GANs and interchange their opponents during testing. Iowe et al. <ref type="bibr" target="#b25">[26]</ref> attempt to train a single discriminator on a large corpus of dialogue responses generated by different dialogue systems. Other approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref> train one discriminator separately for each model. Different from implicitly generated negative examples by a generator in these work, we incorporate explicitly defined pathological transformations to generate negative examples. Moreover, none of the above literature has verified the effectiveness of their metrics by the correlation with human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discriminative Evaluation</head><p>A caption is considered of high quality if it is judged well by humans. In particular, the quality of a generated caption is measured by how successful it can fool a critique into believing it is written by human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Metric</head><p>The proposed evaluation metric follows the general setup of a Turing Test. First, we train an automatic critique to distinguish generated captions from human-written ones. We then score candidate captions by how successful they are in fooling the critique.</p><p>Formally, given a critique parametrized by Θ, a reference image i, and a generated captionĉ, the score is defined as the probability for the caption of being human-written, as <ref type="figure">Figure 2</ref>. The model architecture of the proposed learned critique with Compact Bilinear Pooling. We use a deep residual network and an LSTM to encode the reference image and human caption into context vector. The identical LSTM is applied to get the encoding of a candidate caption. The context feature and the feature extracted from the candidate caption are combined by compact bilinear pooling. The classifier is supervised to perform a Turing Test by recognizing whether a candidate caption is human written or machine generated.</p><p>assigned by the critique:</p><formula xml:id="formula_0">score Θ (ĉ, i) = P (ĉ is human written | i, Θ)<label>(1)</label></formula><p>The score is conditioned on the reference image, because the task of the evaluation is not simply to decide whether a given sentence is written by a human or machine generated, but also to evaluate whether it accurately captures the image content and focuses on the important aspects of the image. More generally, the reference image represents the context in which the generated caption is evaluated. To provide further information about the relevance and salience of the image content, a reference caption can additionally be supplied to the context. Let C(i) denotes the context of image i, then reference caption c could be included as part of context, i.e., c ∈ C(i). The score with context becomes</p><formula xml:id="formula_1">score Θ (ĉ, i) = P (ĉ is human written | C(i), Θ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Architecture</head><p>The proposed model can be generally described in two parts. In the first part, the context information including the image and reference caption are encoded as feature vectors. These two feature vectors are then concatenated as a single context vector. In the second part, the candidate caption is encoded into a vector, in the same way as the reference caption. We then fed it into a binary classifier, together with the context vector. <ref type="figure">Fig. 2</ref> gives an overview of the model.</p><p>To encode the image i as a feature vector i, we use a ResNet <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> with fixed weights. The reference caption c as well as the candidate captionĉ are encoded as feature vectors c andĉ using an LSTM-based <ref type="bibr" target="#b13">[14]</ref> sentence encoder. To form the input of the LSTM, each word is represented as a d-dimensional word embedding vector x ∈ R d which is initialized from GloVe <ref type="bibr" target="#b27">[28]</ref>. The LSTMs used to encode the two captions share the same weights. The weights of the initial word embedding as well as of the LSTM are updated during training.</p><p>Once the encoded feature vectors are computed, they are combined into a single vector. In our experiments, we use two different ways to combine these features; both methods provide comparable results. The first method simply concatenates the vectors followed by a MLP:</p><formula xml:id="formula_2">v = ReLU(W · concat([i, c,ĉ]) + b)<label>(3)</label></formula><p>where ReLU(x) = max(x, 0). For the second method, we first concatenate the context information as concat([i, c]) and subsequently combine it with the candidate caption using Compact Bilinear Pooling (CBP) <ref type="bibr" target="#b10">[11]</ref>, which has been demonstrated in <ref type="bibr" target="#b9">[10]</ref> to be very effective in combining heterogeneous information of image and text. CBP uses Count Sketch <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> to approximate the outer product between two vectors in a lower dimensional space. This results in a feature vector v that captures 2 nd order feature interactions compactly as represented by:</p><formula xml:id="formula_3">v = Φ concat([i, c]) ⊗ Φ ĉ<label>(4)</label></formula><p>where Φ(·) represents Count Sketch and ⊗ is the circular convolution. In practice, circular convolution is usually calculated in frequency domain via Fast Fourier Transform (FFT) and its inverse (FFT −1 ). The feature combination is followed by a 2-way softmax classifier representing the class probabilities of being human written or machine generated. Finally, the classifier is trained using the cross-entropy loss function H(·, ·):</p><formula xml:id="formula_4">L = 1 N N n=1 H(p n , q n )<label>(5)</label></formula><p>where N is the number of training examples, p is the output of the softmax classifier and q is a one-hot vector indicating the ground truth of whether a candidate caption is indeed human written or machine generated.</p><p>By assigning a loss function that directly captures human judgment, the learned metric is capable of measuring the objective of the image captioning task. During inference, the probability from the softmax classifier of being the human written class is used to score candidate captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data sampling and augmentation</head><p>We would like to use data augmentation to incorporate pathological cases as negative examples during training. We define several transformations of the training data to generate a large amount of pathological sentences. Formally, a transformation T takes an image-caption dataset and generates a new one:</p><formula xml:id="formula_5">T ({(c, i) ∈ D}; γ) = {(c ′ 1 , i ′ 1 ), . . . , (c ′ n , i ′ n )}<label>(6)</label></formula><p>where i, i ′ i are images, c, c ′ i are captions, D is a list of caption-image tuples representing the original dataset, and γ is a hyper-parameter that controls the strength of the transformation. Specifically, we define following three transformations to generate pathological image-captions pairs:</p><p>Random Captions(RC). To ensure our metric pays attention to the image content, we randomly sample human written captions from other images in the training set:</p><formula xml:id="formula_6">T RC (D; γ) = {(c ′ , i)|(c, i), (c ′ , i ′ ) ∈ D, i ′ ∈ N γ (i)} (7)</formula><p>where N γ (i) represents the set of images that are top γ percent nearest neighbors to image i. Word Permutation(WP). To make sure that our metric pays attention to sentence structure, we randomly permute at least 2 words in the reference caption:</p><formula xml:id="formula_7">T W P (D; γ) = {(c ′ , i)|(c, i) ∈ D, c ′ ∈ P γ (c) \ {c}} (8)</formula><p>where P γ (c) represents all sentences generated by permuting γ percent of words in caption c.  <ref type="figure">Figure 4</ref>. Relative word frequency (in log scale) in the captions generated by "NeuralTalk" <ref type="bibr" target="#b17">[18]</ref>, "Show and Tell" <ref type="bibr" target="#b31">[32]</ref>, "Show, Attend and Tell" <ref type="bibr" target="#b32">[33]</ref>, and human captions. Machine generated captions have drastically different word frequency distributions from human written captions, as human captions tend to contain much more infrequent words. As a result, a discriminator could simply detect the rare words and achieve low classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Word (RW).</head><p>To explore rare words we replace from 2 to all words of the reference caption with random words from the vocabulary:</p><formula xml:id="formula_8">T RW (D; γ) = {(c ′ , i)|(c, i) ∈ D, c ′ ∈ W γ (c) \ {c}} (9)</formula><p>where W γ (c) represents all sentences generated by randomly replacing γ percent words from caption c. Note that all the γ's are specifically defined to be a percentage. γ% = 0 denotes the original caption without transformation, while γ% = 1 provides the strongest possible transformations. <ref type="figure" target="#fig_1">Fig. 3</ref> shows example captions before and after these transformations.</p><p>The need for data augmentation can be further illustrated by observing the word frequencies. <ref type="figure">Fig. 4</ref> shows the relative word frequency in the captions generated by three popular captioning models as well as the frequency in human captions. Apparently, a discriminator can easily tell human and generated captions apart by simply looking at what words are used. In fact, a simple critique only trained on human written and machine-generated captions tends to believe that a sequence of random words is written by a human, simply because it contains many rare words. To address this problem, our augmented data also includes captions generated using Monte Carlo Sampling, which contains a much higher variety of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Performance Evaluation</head><p>A learned critique should be capable of correctly distinguishing human written captions from machine generated ones. Therefore, the objective of the critique is to assign scores close to 0 to generated captions and scores close to 1 to human captions. In light of this, we define the performance of a critique as how close it gets to the ideal objectives, which is either the score assigned to a human caption or one minus the score assigned to a generated caption:</p><formula xml:id="formula_9">s(Θ, (ĉ, i)) = 1 − score Θ (ĉ, i), ifĉ is generated score Θ (ĉ, i), otherwise</formula><p>where Θ represents the critique,ĉ is the candidate caption, and i is the imageĉ summarizes. The performance of a model is then defined as the averaged performance on all the image-caption pairs in a test or validation set:</p><formula xml:id="formula_10">s(Θ, D) = 1 |D| (ĉ,i)∈D s(Θ, (ĉ, i))<label>(10)</label></formula><p>where D is the set of all image-caption pairs in a held-out validation or test set. Given a pathological transformation T and γ, we could compute the average score of a metric Θ on the transformed validation set T (D, γ), i.e. s(Θ, T (D, γ)). We define the robustness score with respect to transformation T as the Area-Under-Curve (AUC) of s(Θ, T (D, γ)) by varying all possible γ:</p><formula xml:id="formula_11">R(Θ, T ) = s(Θ, T (D, γ))dγ<label>(11)</label></formula><p>We expect a robust evaluation metric to give low scores to the image-caption pairs generated by the pathological transformations. To compare metrics with different scales, we normalize the scores given by each metric such that the ground truth human caption receives a score of 1. Detailed experiments are presented in Sec. 4.2 and Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Using the Learned Metrics</head><p>To use the learned metrics in practice, one needs to first fix both the model architecture of the discriminator and all the hyper-parameters of the training process. When evaluating a captioning model, we need the generated captions of the model for a set of images (i.e., validation or test set of a image captioning dataset). We then split the results into two folds. The discriminative metric is trained with image-caption pairs in first fold as training data, together with ground truth captions written by human. Then we use the trained metric to score the image-caption pairs on the other fold. Similarly, we score all the image-caption pairs in the first fold using a metric trained from the second fold. Once we get all the image-caption pairs scored in the dataset, the average score will be used as the evaluation of the captioning model. One could reduce the variance of the evaluation score by training the metric multiple times and use the averaged evaluation score across all the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setup</head><p>Data. We use the COCO dataset <ref type="bibr" target="#b23">[24]</ref> to evaluate the performance of our proposed metric. To test the capability (Sec. 4.2) and robustness (Sec. 4.3) of the proposed models, we use the data split from <ref type="bibr" target="#b17">[18]</ref>, which re-splits the original COCO dataset into a training set with 113,287 images, a validation and a test set, each contains 5,000 images. Each image is annotated by roughly 5 human annotators. We use the validation set for parameter tuning. For the system level human correlation study (Sec. 4.5), we use 12 submission entries from the 2015 COCO Captioning Challenge on the COCO validation set <ref type="bibr" target="#b0">1</ref> . The caption level human correlation study (Sec. 4.4) uses human annotations in Flickr 8k dataset <ref type="bibr" target="#b14">[15]</ref>. Flickr 8k collects two sets of human annotations, each on a different set of image caption pairs. Among these image-caption pairs, candidate captions are sampled from human captions in the dataset. In the first set of human annotation (Expert Annotation), human experts are asked to rate the image-caption pairs with scores ranging from 1: The selected caption is unrelated to the image to 4: The selected caption describes the image without any errors. The second set of annotation (Crowd Flower Annotation) is collected by asking human raters to decide whether a caption describes the corresponding image or not.</p><p>Image Captioning Models. We use publicly available implementations of "NeuralTalk" (NT) <ref type="bibr" target="#b17">[18]</ref>, "Show and Tell" (ST) <ref type="bibr" target="#b31">[32]</ref>, "Show, Attend and Tell" (SAT) <ref type="bibr" target="#b32">[33]</ref> as image captioning models to train and evaluate our metric.</p><p>Implementation Details. Our image features are extracted from a Deep Residual Network with 152 layers (ResNet-152) <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet. We follow the preprocessing from <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18</ref>] to prepare vocabulary on COCO dataset. We fix the step size of the LSTM to be 15, padding shorter sentences with a special token while cutting longer ones to 15 words. All words are represented as 300-dimensional vectors initialized from GloVe <ref type="bibr" target="#b27">[28]</ref>. We use a batch size of 100 and sample an equal number of positive and negative examples in each batch. Linear projection is used to reduce the dimension of image feature to match that of caption features. For Compact Bilinear Pooling, we use the feature dimension of 8192 as suggested in <ref type="bibr" target="#b10">[11]</ref>. We use 1 LSTM layer with a hidden dimension of 512 in all experiments unless otherwise stated. All the model are trained using Adam <ref type="bibr" target="#b18">[19]</ref> optimizer for 30 epochs with an initial learning rate of 10 −3 . We decay the learning rate by a factor of 0.9 after every epoch. Our code (in Tensorflow <ref type="bibr" target="#b1">[2]</ref>) is available at: https://github.com/richardaecn/ cvpr18-caption-eval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Capability</head><p>To measure the capability of our metric to differentiate between human and generated captions, we train variants of models using generated captions from ST, SAT and NT, together with human captions from the training set. <ref type="figure" target="#fig_2">Fig. 5(a)</ref> and <ref type="figure" target="#fig_2">Fig. 5(b)</ref> show the average score on the validation set for human and generated captions respectively. The results show that all models give much higher scores to human captions than machine generated captions, indicating that they are able to differentiate human-written captions from the machine-generated ones.</p><p>With respect to the choice of context, we observe that including image features into the context clearly improves performance. Also, adding a reference caption does not lead to a significant improvement over only using image features. This indicates that the image itself provides enough contextual information for the critique to successfully discriminate between human and machine generated captions. The reason that none of the commonly used metrics includes images as context is likely due to the difficulties of capturing image-text similarity. Our metric circumvents this issue by implicitly learning the image-text relationship directly from the data.</p><p>It is worth noting that achieving high model performance in terms of discrimination between human and generated captions does not necessarily imply that the learned metric is good. In fact, we observe that a critique trained without data augmentation can achieve even higher discrimination performance. Such critique, however, also gives high scores to human written captions from other images, indicating that the classification problem is essentially reduced to putting captions into categories of human and non-human written without considering the context image. If trained with the proposed data sampling and augmentation technique, the critique learns to pay attention to image context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness</head><p>To evaluate whether the proposed metric can capture pathological image-caption pairs, we conduct robustness studies as described in Sec. 3.4 on the three pathological transformations defined in Sec. 3.3. The robustness comparisons are illustrated in <ref type="figure">Fig. 6</ref>. In the first row we compare different variants of the proposed metric. The results illustrate that, although achieving high discrimination performance, a metric learned without data sampling or augmentation also gives high scores to human captions from other images (T RC ), with random words (T RW ), or word permutations (T W P ). This indicates that the model tends to focus on an overall human vs. non-human classification without considering contextual information in the image or the syntactic structure of the candidate sentence.</p><p>Further, even with data augmentation, a linear model with concatenated context and candidate caption features gives high scores to human captions from other images, possibly because there is no sufficient interaction between the context and candidate caption features. Non-linear interactions such as Compact Bilinear Pooling or a non-linear classifier with hidden layers solve this limitation. The nonlinear model in <ref type="figure">Fig. 6</ref> refers to a model with concatenated context and candidate features followed by a nonlinear classifier. Compact bilinear pooling (not shown in the figure for clarity of visualization) achieves similar results.</p><p>In the second row of <ref type="figure">Fig. 6</ref> we compare our metric with other commonly used image captioning metrics. The proposed metric outperforms all others with respect to random (T RW ) as well as permuted (T W P ) words and is reasonably robust to human captions from similar images (T RC ). Further, we observe that the recently proposed metrics CIDEr and SPICE perform well for human captions from similar images, but fall behind with respect to sentence structure. This could be caused by their increased focus on informative and scene specific words.  <ref type="figure">Figure 6</ref>. Normalized evaluation score for transformations TRC (human caption from other images), TW P (random permutation of words), and TRW (words replaced by random words) with different amount of transformation (γ%). When γ% = 0%, the original dataset is kept unchanged; when γ% = 100%, maximum amount of transformation is applied to the dataset. The first row shows results of our metrics using either linear or non-linear model trained with or without data augmentation. The second row compares our non-linear model trained with data augmentation to other metrics. The score after each metric shows the robustness score defined in Sec. 3.4, i.e., the Area Under Curve (AUC). The lower the score the more robust the metric is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Caption Level Human Correlation</head><p>We use both the Expert Annotations and the Crowd Flower Annotations from Flickr 8k dataset <ref type="bibr" target="#b14">[15]</ref> to compute caption level correlation with human judgments. We follow the procedure in SPICE paper <ref type="bibr" target="#b2">[3]</ref> to compute the Kendall's τ rank correlation in the Expert Annotations. The τ correlation for the Crowd Flower Annotation is computed between scores generated by the evaluation metric and percentage of raters who think that the caption describes the image with possibly minor mistakes. During training, all negative samples are generated by transformation T RC , i.e., human caption from random image.</p><p>The results in <ref type="table">Table 1</ref> show that our metrics achieve the best caption level correlation in both Expert Annotations and Crowd Flower Annotations. Note that the Crowd Flower Annotations use a binary rating setup, while the setup from the Expert Annotations makes a finer-grained ratings. Despite the fact that our model is trained on a simpler binary objective, it still correlates well with human judgments from the Expert Annotations. Note that we do not use any human annotations during the training, since all of our negative examples could be generated automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">System Level Human Correlation</head><p>We compare our metric with others on the Pearson's ρ correlation between all common metrics and human judgments collected in the 2015 COCO Captioning Challenge <ref type="bibr" target="#b0">[1]</ref>. In particular, we use two human judgment M1: Percentage of captions that are evaluated as better or equal to human caption and M2: Percentage of captions that pass the Turing Test. We don't use M3: correctness, M4: detailness and M5: salience, as they are not used to rank image captioning models, but are intended for an ablation study to understand which aspects make captions good.</p><p>Since we don't have access to the COCO test set annotations, where the human judgments are collected on, we perform our experiments on the COCO validation set. There are 15 teams participated in the 2015 COCO captioning challenge and we use 12 of them that submitted results on the validation set. We assume the human judgment on the validation set is sufficiently similar to the judgment on the test set. We don't use any additional training data besides the submission files on the validation set and the data augmentation described in Sec. 3.3. To get evaluation scores on the whole validation set, we split the set in two halves and,   <ref type="table">Table 2</ref>. Pearson's ρ correlation between human judgments and evaluation metrics. The human correlation of our proposed metric surpasses all other metrics by large margins. Scores reported in SPICE <ref type="bibr" target="#b2">[3]</ref> were calculated on the COCO test set for all 15 teams, whereas ours were from 12 teams on the COCO validation set.</p><p>for each submission, train our critique on each split and get scores (probability of being human written) on the other.</p><p>The results in <ref type="table">Table 2</ref> show that our learned metric surpasses all other metrics including the recently proposed SPICE <ref type="bibr" target="#b2">[3]</ref> by large margins, especially trained with data augmentation. This indicates that aligning the objective with human judgments and using data augmentation yield a better evaluation metric. <ref type="figure">Fig. 7</ref> illustrates our metric compared with human judgment -M1 on COCO validation set. Our metric aligns well with human judgment, especially for top performing methods.  <ref type="figure">Figure 7</ref>. Our metric vs. human judgment on COCO validation set. Our metric is able to reflect most of the rankings from human judgment correctly, especially for top performing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we have proposed a novel learning based evaluation metric for image captioning that is trained to act like a human critique to distinguish between human-written and machine-generated captions while also being flexible to adapt to targeted pathological cases. Further, we have shown how to use data sampling and augmentation to successfully train a metric that behaves robustly against captions generated from pathological transformations. From extensive experimental evaluations, we have demonstrated that the proposed metric is robust and correlates better to human judgments than previous metrics. In conclusion, the proposed metric could be an effective complementary to the existing rule-based metrics, especially when the pathological cases are easy to generate but difficult to capture with traditional hand-crafted metrics.</p><p>In this study, we have not taken different personalities among human annotators into consideration. Different human personalities could give rise to different types of human captions. One direction of future work could aim to capture the heterogeneous nature of human annotated captions and incorporate such information into captioning evaluation. Another direction for future work could be training a caption generator together with the proposed evaluation metric (discriminator) in a generative adversarial setting. Finally, gameability is definitely a concern, not only for our learning based metric, but also for other rule-based metrics. Learning to be more robust to adversarial examples is also a future direction of learning based evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example ground truth captions before and after transformations. Robustness of our learned metrics is evaluated on human captions from similar images (TRC ) as well as with random (TRW ) and permuted (TW P ) words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Top: Average score of human captions from the validation set. Bottom: Average score of generated captions. Color of the bar indicates what context information is used for the critique. The horizontal axis represents three different strategies for the combination of the features from candidate caption with the context as well as the used classifier (Concat + Linear: concatenation and linear classifier; Bilinear + Linear: compact bilinear pooling and linear classifier; Concat + MLP: concatenation and MLP with one hidden layer of size 512).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Ours (no DA) 0.821 (0.000) 0.807 (0.000) Ours 0.939 (0.000) 0.949 (0.000) M1: Percentage of captions that are evaluated as better or equal to human caption. M2: Percentage of captions that pass the Turing Test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Expert Annotations: experts score image-caption pairs from 1 to 4; 1 means caption doesn't describe the image. Crowd Flower: human raters mark 1 if the candidate caption describes the image, and mark 0 if not. Table 1. Caption level Kendall's τ correlation between Flickr 8K [15]'s human annotations and evaluation metrics' scores. Our re- ported scores with * differ from the ones reported in SPICE [3].</figDesc><table>Expert Annotations 

Crowd Flower 
BLEU-1 
0.191* 
0.206 
BLEU-2 
0.212 
0.212 
BLEU-3 
0.209 
0.204 
BLEU-4 
0.206* 
0.202 
METEOR 
0.308* 
0.242 
ROUGE-L 
0.218* 
0.217 
CIDEr 
0.289* 
0.264 
SPICE 
0.456 
0.252 
Ours 
0.466 
0.295 
Inter-human 
0.736 
-
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Among 15 participating teams, 3 didn't provide submissions on validation set. Thus, we use submission entries from the remaining 12 teams.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by a Google Focused Research Award, AWS Cloud Credits for Research and a Facebook equipment donation. We would like to thank the COCO Consortium for agreeing to run our code on entries in the 2015 COCO Captioning Challenge.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://mscoco.org/dataset/#captions-challenge2015" />
		<title level="m">The coco 2015 captioning challenge</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<editor>CoNLL</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICALP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Show, adapt and tell: Adversarial training of crossdomain image captioner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<title level="m">Generating images with recurrent adversarial networks. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial evaluation of dialogue models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Recurrent topic-transition gan for visual paragraph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved image captioning via policy gradient optimization of spider</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards an automatic turing test: Learning to evaluate dialogue responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Angelard-Gontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speaking the same language: Matching machine to human captions by adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
