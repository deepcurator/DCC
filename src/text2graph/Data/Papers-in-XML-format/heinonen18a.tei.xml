<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning unknown ODE models with Gaussian processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Agatay Yıldız</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Mannerström</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Intosalmi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Lähdesmäki</surname></persName>
						</author>
						<title level="a" type="main">Learning unknown ODE models with Gaussian processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model's capabilities to infer dynamics from sparse data and to simulate the system forward into future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dynamical systems modelling is a cornerstone of experimental sciences. In biology, as well as in physics and chemistry, modelers attempt to capture the dynamical behavior of a given system or a phenomenon in order to improve its understanding and make predictions about its future state. Systems of coupled ordinary differential equations (ODEs) are undoubtedly the most widely used models in science. Even simple ODE functions can describe complex dynamical behaviours <ref type="bibr" target="#b18">(Hirsch et al., 2004)</ref>. Typically, the dynamics are firmly grounded in physics with only a few parameters to be estimated from data. However, equally ubiquitous are the cases where the governing dynamics are partially or completely unknown.</p><p>We consider the dynamics of a system governed by multi- variate ordinary differential functions:</p><formula xml:id="formula_0">x(t) = dx(t) dt = f (x(t))<label>(1)</label></formula><p>where x(t) ∈ X = R D is the state vector of a Ddimensional dynamical system at time t, and theẋ(t) ∈ X = R D is the first order time derivative of x(t) that drives the state x(t) forward, and where f : R D → R D is the vector-valued derivative function. The ODE solution is determined by</p><formula xml:id="formula_1">x(t) = x 0 + t 0 f (x(τ ))dτ,<label>(2)</label></formula><p>where we integrate the system state from an initial state x(0) = x 0 for time t forward. We assume that f (·) is completely unknown and we only observe one or several multivariate time series Y = (y 1 , . . . , y N ) T ∈ R N ×D obtained from an additive noisy observation model at observation time points T = (t 1 , . . . , t N ) ∈ R N , y(t) = x(t) + ε t ,</p><p>where ε t ∼ N (0, Ω) follows a stationary zero-mean multivariate Gaussian distribution with diagonal noise variances Ω = diag(ω 2 1 , . . . , ω 2 D ). The observation time points do not need to be equally spaced. Our task is to learn the differential function f (·) given observations Y , with no prior knowledge of the ODE system.</p><p>There is a vast literature on conventional ODEs <ref type="bibr" target="#b4">(Butcher, 2016)</ref> where a parametric form for function f (x; θ, t) is assumed to be known, and its parameters θ are subsequently optimised with least squares or Bayesian approach, where the expensive forward solution x θ (t i ) = ti 0 f (x(τ ); θ, t)dτ is required to evaluate the system responses x θ (t i ) from parameters θ against observations y(t i ). To overcome the computationally intensive forward solution, a family of methods denoted as gradient matching <ref type="bibr" target="#b35">(Varah, 1982;</ref><ref type="bibr" target="#b6">Ellner et al., 2002;</ref><ref type="bibr" target="#b28">Ramsay et al., 2007)</ref> have proposed to replace the forward solution by matching f (y i ) ≈ẏ i to empirical gradientsẏ i of the data instead, which do not require the costly integration step. Recently several authors have proposed embedding a parametric differential function within a Bayesian or Gaussian process (GP) framework <ref type="bibr" target="#b13">(Graepel, 2003;</ref><ref type="bibr">Calderhead et al., 2008;</ref><ref type="bibr" target="#b5">Dondelinger et al., 2013;</ref><ref type="bibr" target="#b39">Wang and Barber, 2014;</ref><ref type="bibr" target="#b23">Macdonald, 2017</ref>) (see <ref type="bibr" target="#b24">Macdonald et al. (2015)</ref> for a review). GPs have been successfully applied to model linear differential equations as they are analytically tractable <ref type="bibr" target="#b10">(Gao et al., 2008;</ref><ref type="bibr" target="#b27">Raissi et al., 2017)</ref>.</p><p>However, conventional ODE modelling can only proceed if a parametric form of the driving function f (·) is known. Recently, initial work to handle unknown or non-parametric ODE models have been proposed, although with various limiting approximations. Early works include spline-based smoothing and additive functions D j f j (x j ) to infer gene regulatory networks <ref type="bibr">(De Hoon et al., 2002;</ref><ref type="bibr" target="#b16">Henderson and Michailidis, 2014)</ref>. <ref type="bibr">Äijö and Lähdesmäki (2009)</ref> proposed estimating the unknown nonlinear function with GPs using either finite time differences, or analytically solving the derivative function as a function of only time,ẋ(t) = f (t) <ref type="bibr">(Äijö et al., 2013)</ref>. In a seminal technical report of Heinonen and d'Alche Buc (2014) a full vector-valued kernel model f (x) was proposed, however using a gradient matching approximation. To our knowledge, there exists no model that can learn non-linear ODE functionsẋ(t) = f (x(t)) over the state x against the true forward solutions x(t i ).</p><p>In this work we propose NPODE 1 : the first ODE model for learning arbitrary, and a priori completely unknown nonparametric, non-linear differential functions f : X →Ẋ from data in a Bayesian way. We do not use gradient matching or other approximative models, but instead propose to directly optimise the exact ODE system with the fully forward simulated responses against data. We parameterise our model as an augmented Gaussian process vector field with inducing points, while we propose sensitivity equations to efficiently compute the gradients of the system. Our model can forecast continuous-time systems arbitrary amounts to future, and we demonstrate the state-of-the-art performance in human motion datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Nonparametric ODE Model</head><p>The differential function f (x) to be learned defines a vector field 2 f , that is, an assignment of a gradient vector f (x) ∈ R D to every state x ∈ R D . We model the vector field as a vector-valued Gaussian process <ref type="bibr" target="#b29">(Rasmussen and Williams, 2006)</ref> </p><formula xml:id="formula_3">f (x) ∼ GP(0, K(x, x )),<label>(4)</label></formula><p>which defines a priori distribution over function values f (x) whose mean and covariances are and where the kernel K(x, x ) ∈ R D×D is matrixvalued. A GP prior defines that for any collection of</p><formula xml:id="formula_4">E[f (x)] = 0 (5) cov[f (x), f (x )] = K(x, x ),<label>(6)</label></formula><formula xml:id="formula_5">states X = (x 1 , . . . , x N ) T ∈ R N ×D , the function val- ues F = (f (x 1 ), . . . , f (x N )) T ∈ R N ×D follow a matrix- valued normal distribution, p(F ) = N (vec(F )|0, K(X, X)),<label>(7)</label></formula><p>where</p><formula xml:id="formula_6">K(X, X) = (K(x i , x j )) N i,j=1 ∈ R N D×N D is a block matrix of matrix-valued kernels K(x i , x j ).</formula><p>The key property of Gaussian processes is that they encode functions where similar states x, x induce similar differentials f (x), f (x ), and where the state similarity is defined by the kernel K(x, x ).</p><p>In standard GP regression we would obtain the posterior of the vector field by conditioning the GP prior with the data <ref type="bibr" target="#b29">(Rasmussen and Williams, 2006)</ref>. In ODE models the conditional f (x)|Y of a vector field is intractable due to the integral mapping (2) between observed states y(t i ) and differentials f (x). Instead, we resort to augmenting the Gaussian process with a set of M inducing points z ∈ X and u ∈Ẋ , such that f (z) = u <ref type="bibr" target="#b26">(Quiñonero-Candela and Rasmussen, 2005)</ref>. We choose to interpolate the differential function between the inducing points as (See <ref type="figure" target="#fig_0">Figure 1</ref>)</p><formula xml:id="formula_7">f (x) K θ (x, Z)K θ (Z, Z) −1 vec(U ),<label>(8)</label></formula><p>which supports the function f (x) with inducing locations</p><formula xml:id="formula_8">Z = (z 1 , . . . , z M ), inducing vectors U = (u 1 , . . . , u M ),</formula><p>and θ are the kernel parameters. The function above corresponds to a vector-valued kernel function <ref type="bibr" target="#b1">(Alvarez et al., 2012)</ref>, or to a multi-task Gaussian process conditional mean without the variance term <ref type="bibr" target="#b29">(Rasmussen and Williams, 2006)</ref>. This definition is then compatible with the deterministic nature of the ODE formalism. Due to universality of several kernels and kernel functions <ref type="bibr" target="#b32">(Shawe-Taylor and Cristianini, 2004)</ref>, we can represent arbitrary vector fields with appropriate inducing point and kernel choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Operator-valued Kernels</head><p>The vector-valued kernel function (8) uses operator-valued kernels, which result in matrix-valued kernels K θ (z, z ) ∈ R D×D for real valued states x, z, while the kernel matrix over data points becomes</p><formula xml:id="formula_9">K θ = (K(z i , z j )) M i,j=1 ∈ R M D×M D (See Alvarez et al. (2012) for a review). Most straightforward operator-valued kernel is the identity de- composable kernel K dec (z, z ) = k(z, z ) · I D , where the scalar Gaussian kernel K θ (z, z ) = σ 2 f exp   − 1 2 D j=1 (z j − z j ) 2 2 j  <label>(9)</label></formula><p>with differential variance σ 2 f and dimension-specific lengthscales = ( 1 , . . . , D ) are expanded into a diagonal matrix of size D × D. We collect the kernel parameters as θ = (σ f , ).</p><p>We note that more complex kernels can also be considered given prior information of the underlying system characteristics. The divergence-free matrix-valued kernel induces vector fields that have zero divergence <ref type="bibr" target="#b36">(Wahlström et al., 2013;</ref><ref type="bibr" target="#b33">Solin et al., 2015)</ref>. Intuitively, these vector fields do not have sinks or sources, and every state always finally returns to itself after sufficient amount of time. Similarly, curl-free kernels induce curl-free vector fields that can contain sources or sinks, that is, trajectories can accelerate or decelerate. For theoretical treatment of vector field kernels, see <ref type="bibr" target="#b25">(Narcowich and Ward, 1994;</ref><ref type="bibr" target="#b3">Bhatia et al., 2013;</ref><ref type="bibr" target="#b9">Fuselier and Wright, 2017)</ref>. Non-stationary vector fields can be modeled with input-dependent lengthscales <ref type="bibr" target="#b15">(Heinonen et al., 2016)</ref>, while spectral kernels can represent stationary <ref type="bibr" target="#b40">(Wilson et al., 2013)</ref> or non-stationary <ref type="bibr" target="#b31">(Remes et al., 2017)</ref> recurring patterns in the differential function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Joint Model</head><p>We assume a Gaussian likelihood over the observations y i and the corresponding simulated responses x(t i ) of Equation (2),</p><formula xml:id="formula_10">p(Y |x 0 , U, Z, ω) = N i=1 N (y i |x(t i ), Ω),<label>(10)</label></formula><p>where x(t i ) are forward simulated responses using the integral Equation <ref type="formula" target="#formula_1">(2)</ref> and differential Equation <ref type="formula" target="#formula_7">(8)</ref>, and</p><formula xml:id="formula_11">Ω = diag(ω 2 1 . . . , ω 2 D ) collects the dimension-specific noise variances.</formula><p>The inducing vectors have a Gaussian process prior</p><formula xml:id="formula_12">p(U |Z, θ) = N (vec(U )|0, K θ (Z, Z)).<label>(11)</label></formula><p>The model posterior is then</p><formula xml:id="formula_13">p(U, x 0 , θ, ω|Y ) ∝ p(Y |x 0 , U, ω)p(U |θ) = L,<label>(12)</label></formula><p>where we have for brevity omitted the dependency on the locations of the inducing points Z and also the parameter hyperpriors p(θ) and p(ω) since we assume them to be uniform, unless there is specific domain knowledge of the priors.</p><p>The model parameters are the initial state x 0 3 , the inducing vectors U , the noise standard deviations ω = (ω 1 , . . . , ω D ), and the kernel hyperparameters θ = (σ f , 1 , . . . , D ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Noncentral Parameterisation</head><p>We apply a latent parameterisation using Cholesky decomposition L θ L T θ = K θ (Z, Z), which maps the inducing vectors to whitened domain <ref type="bibr" target="#b20">(Kuss and Rasmussen, 2005)</ref> </p><formula xml:id="formula_14">U = L θ U , U = L −1 θ U.<label>(13)</label></formula><p>The latent variables U are projected on the kernel manifold L θ to obtain the inducing vectors U . This non-centered parameterisation (NCP) transforms the hierarchical posterior L of Equation <ref type="formula" target="#formula_0">(12)</ref> into a reparameterised form</p><formula xml:id="formula_15">p(x 0 , U , θ, ω|Y ) ∝ p(Y |x 0 , U , ω, θ)p( U ),<label>(14)</label></formula><p>where all variables to be optimised are decoupled, with the latent inducing vectors having a standard normal prior U ∼ N (0, I). Optimizing U and θ is now more efficient since they have independent contributions to the vector field via U = L θ U . The gradients of the whitened posterior can be retrieved analytically as <ref type="bibr" target="#b15">(Heinonen et al., 2016</ref>)</p><formula xml:id="formula_16">∇ U log L = L T θ ∇ U log L.<label>(15)</label></formula><p>Finally, we find a maximum a posteriori (MAP) estimate for the initial state x 0 , latent vector field U , kernel parameters θ and noise variances ω by gradient ascent,</p><formula xml:id="formula_17">x 0,MAP , U MAP , θ MAP , ω MAP = arg max x0, U ,θ,ω log L,<label>(16)</label></formula><p>while keeping the inducing locations Z fixed on a sufficiently dense grid (See <ref type="figure" target="#fig_0">Figure 1)</ref>. The partial derivatives of the posterior with respect to noise parameters ω can be found analytically, while the derivative with respect to σ f is approximated with finite differences. We select the optimal lengthscales by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sensitivity Equations</head><p>The key term to carry out the MAP gradient ascent optimization is the likelihood</p><formula xml:id="formula_18">log p(Y |x 0 , U , ω)</formula><p>that requires forward integration and computing the partial derivatives with respect to the whitened inducing vectors U . Given Equation <ref type="formula" target="#formula_0">(15)</ref> we only need to compute the gradients with respect to the inducing vectors</p><formula xml:id="formula_19">u = vec(U ) ∈ R M D , d log p(Y |x 0 , u, ω) du = N s=1 d log N (y s |x(t s , u), Ω) dx dx(t s , u) du .<label>(17)</label></formula><p>This requires computing the derivatives of the simulated system response x(t, u) against the vector field parameters u,</p><formula xml:id="formula_20">dx(t, u) du ≡ S(t) ∈ R D×M D ,<label>(18)</label></formula><p>which we denote by S ij (t) = ∂x(t,u)i ∂uj , and expand the notation to make the dependency of x on u explicit. Approximating these with finite differences is possible in principle, but is highly inefficient and has been reported to cause unstability <ref type="bibr" target="#b30">(Raue et al., 2013)</ref>. We instead turn to sensitivity equations for u and x 0 that provide computationally efficient, analytical gradients S(t) <ref type="bibr" target="#b19">(Kokotovic and Heller, 1967;</ref><ref type="bibr">Fröhlich et al., 2017)</ref>. can be derived by differentiating the full nonparametric ODE system with respect to u by</p><formula xml:id="formula_21">d du dx(t, u) dt = d du f (x(t, u)).<label>(19)</label></formula><p>The sensitivity equation for the given system can be obtained by changing the order of differentiation on the left hand side and carrying out the differentiation on the right hand side. The resulting sensitivity equation can then be expressed in the forṁ</p><formula xml:id="formula_22">S(t) d dt dx(t, u) du = J(t)</formula><p>∂f (x(t, u)) ∂x</p><formula xml:id="formula_23">S(t) dx(t, u) du + R(t) ∂f (x(t, u)) ∂u ,<label>(20)</label></formula><p>where J(t) ∈ R D×D , R(t),Ṡ(t) ∈ R D×M D (See Supplements for detailed specification). For our nonparametric ODE system the sensitivity equation is fully determined by</p><formula xml:id="formula_24">J(t) = ∂K(x, Z) ∂x K(Z, Z) −1 u (21) R(t) = K(x, Z)K(Z, Z) −1 .<label>(22)</label></formula><p>The sensitivity equation provides us with an additional ODE system which describes the time evolution of the derivatives with respect to the inducing vectors S(t). The sensitivities are coupled with the actual ODE system and, thus both systems x(t) and S(t) are concatenated as the new augmented state that is solved jointly by Equation (2) driven by the differentialsẋ(t) andṠ(t) <ref type="bibr" target="#b22">(Leis and Kramer, 1988)</ref>. The initial sensitivities are computed as S(0) = dx0 du . In our implementation, we merge x 0 with u for sensitivity analysis to obtain the partial derivatives with respect to the initial state which is estimated along with the other parameters. We use the CVODES solver from the SUNDIALS package <ref type="bibr" target="#b17">(Hindmarsh et al., 2005)</ref> to solve the nonparametric ODE models and the corresponding gradients numerically. The sensitivity equation based approach is superior to the finite differences approximation because we have exact formulation for the gradients of state over inducing points, which can be solved up to the numerical accuracy of the ODE solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Simple Simulated Dynamics</head><p>As first illustration of the proposed nonparametric ODE method we consider three simulated differential systems: the Van der Pol (VDP), FitzHugh-Nagumo (FHN) and Lotka-Volterra (LV) oscillators of form</p><formula xml:id="formula_25">VDP :ẋ 1 = x 2ẋ2 = (1 − x 2 1 )x 2 − x 1 FHN :ẋ 1 = 3(x 1 − x 3 1 3 + x 2 )ẋ 2 = 0.2 − 3x 1 − 0.2x 2 3 LV :ẋ 1 = 1.5x 1 − x 1 x 2ẋ2 = −3x 2 + x 1 x 2 .</formula><p>In the conventional ODE case the coefficients of these equations can be inferred using standard statistical techniques if sufficient amount of time series data is available <ref type="bibr" target="#b12">(Girolami, 2008;</ref><ref type="bibr" target="#b30">Raue et al., 2013)</ref>. Our main goal is to infer unknown dynamics, that is, when these equations are unavailable and we instead represent the dynamics with a nonparametric vector field of Equation (8). We use these simulated models to only illustrate our model behavior against the true dynamics.</p><p>We employ 25 data points from one cycle of noisy observation data from VDP and FHN models, and 25 data points from 1.7 cycles from the LV model with a noise variance of σ 2 n = 0.1 2 . We learn the npODE model with five training sequences using M = 6 2 inducing locations on a fixed grid, and forecast between 4 and 8 future cycles starting from true initial state x 0 at time 0. Training takes approximately 100 seconds per oscillator. <ref type="figure" target="#fig_2">Figure 2 (bottom)</ref> shows the training datasets (grey regions), initial states, true trajectories (black lines) and the forecasted trajectory likelihoods (colored regions). The model accurately learns the dynamics from less than two cycles of data and can reproduce them reliably into future.</p><p>Figure 2 (top) shows the corresponding true vector field (black arrows) and the estimated vector field (grey arrows). The vector field is a continuous function, which is plotted on a 8x8 grid for visualisation. In general the most difficult part of the system is learning the middle of the loop (as seen in the FHN model), and learning the most outermost regions (bottom left in the LV model). The model learns the underlying differential f (x) accurately close to observed points, while making only few errors in the border regions with no data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Unknown System Estimation</head><p>Next, we illustrate how the model estimates realistic, unknown dynamics from noisy observations y(t 1 ), . . . , y(t N ). As in Section 4, we make no assumptions on the structure or form of the underlying system, and capture the underlying dynamics with the nonparameteric system alone. We employ no subjective priors, and assume no inputs, controls or other sources of information. The task is to infer the underlying dynamics f (x), and interpolate or extrapolate the state trajectory outside the observed data.</p><p>We use a benchmark dataset of human motion capture data from the Carnegie Mellon University motion capture (CMU mocap) database. Our dataset contains 50-dimensional pose measurements y(t i ) from humans walking, where each pose dimension records a measurement in different parts of the body during movement <ref type="bibr" target="#b38">(Wang et al., 2008)</ref>. We apply the preprocessing of <ref type="bibr" target="#b38">Wang et al. (2008)</ref> by downsampling the datasets by a factor of four and centering the data. This resulted in a total of 4303 datapoints spread across 43 trajec-tories with on average 100 frames per trajectory. In order to tackle the problem of dimensionality, we project the original dataset with PCA to a three dimensional latent space where the system is specified, following <ref type="bibr">Damianou et al. (2011)</ref> and <ref type="bibr" target="#b37">Wang et al. (2006)</ref>. We place M = 5 3 inducing vectors on a fixed grid, and optimize our model starting from 100 different initial values, which we set by perturbing the projected empirical differences y(t i ) − y(t i−1 ) to the inducing vectors. We use an L-BFGS optimizer in Matlab. The whole inference takes approximately few minutes per trajectory.</p><p>We evaluate the method with two types of experiments: imputing missing values and forecasting future cycles. For the forecasting the first half of the trajectory is reserved for model training, and the second half is to be forecasted. For imputation we remove roughly 20% of the frames from the middle of the trajectory, which are to be filled by the models. We perform model selection for lengthscales with crossvalidation split of 80/20. We record the root mean square error (RMSE) over test points in the original feature space in both cases, where we reconstruct the original dimensions from the latent space trajectories.</p><p>Due to the current lack of ODE methods suitable for this nonparametric inference task, we instead compare our method to the state-of-the-art state-space models where such problems have been previously considered <ref type="bibr" target="#b38">(Wang et al., 2008)</ref>. In a state-space or dynamical model a transition function x(t k+1 ) = g(x(t k )) moves the system forward in discrete steps. With sufficiently high sampling rate, such models can estimate and forecast finite approximations of smooth dynamics. In Gaussian process dynamical model <ref type="bibr" target="#b37">(Wang et al., 2006;</ref><ref type="bibr" target="#b7">Frigola et al., 2014;</ref><ref type="bibr" target="#b34">Svensson et al., 2016)</ref> a GP transition function is inferred in a latent space, which can be inferred with a standard GPLVM <ref type="bibr" target="#b21">(Lawrence, 2004)</ref> or with a dependent GPLVM <ref type="bibr" target="#b41">(Zhao and Sun, 2016)</ref>. In dynamical systems the transition function is replaced by a GP interpolation <ref type="bibr">(Damianou et al., 2011)</ref>. The discrete time state-space models emphasize inference of a low-dimensional manifold as an explanation of the high-dimensional measurement trajectories.</p><p>We compare our method to the dynamical model GPDM of <ref type="bibr" target="#b37">Wang et al. (2006)</ref> and to the dynamical system VGPLVM of <ref type="bibr">Damianou et al. (2011)</ref>, where we directly apply the implementations provided by the authors at inverseprobability.com/vargplvm and dgp. toronto.edu/˜jmwang/gpdm. Both methods optimize their latent spaces separately, and they are thus not directly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Forecasting</head><p>In the forecasting task we train all models with the first half of the trajectory, while forecasting the second half starting from the first frame. The models are trained and forecasted within a low-dimensional space, and subsequently projected back into the original space via inverting the PCA or with GPLVM mean predictions. As all methods optimize their latent spaces separately, they are not directly comparable. Thus, the mean errors are computed in the original highdimensional space. Note that the low-dimensional representation necessarily causes some reconstruction errors. The VGPLVM has most trouble forecasting future points, and reverts quickly after training data to a value close to zero, failing to predict future points. The GPDM model produces more realistic trajectories, but fails to predict any of the poses accurately. Finally, npODE can accurately predict five poses, and still retains adequate performance on remaining poses, except for pose 2.</p><p>Furthermore, <ref type="table" target="#tab_1">Table 1</ref> indicates that npODE is also best performing method on average over the whole dataset in the forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Imputation</head><p>In the imputation task we remove approximately 20% of the training data from the middle of the trajectory. The goals are to learn a model with the remaining data and to forecast the missing values. <ref type="figure" target="#fig_5">Figure 4</ref> highlights the performance of the three models on the trajectory 07 07.amc. The top part (a) shows the training data (black points) in the PCA space (npODE) or optimized training locations in the latent space (GPDM, VGPLVM). The middle part imputation is shown with colored points or lines. Interestingly both npODE and GPDM operate on cyclic representations, while VGPLVM is not cyclic.</p><p>The bottom panel (b) shows the first 9 reconstructed pose</p><p>Learning unknown ODE models with GP dimensions from the three models. The missing values are shown in circles, while training points are shown with black dots. All models can accurately reproduce the overall trends, while npODE seems to fit slightly worse than the other methods. The PCA projection causes the seemingly perfect fit of the npODE prediction (at the top) to lead to slightly warped reconstructions (at the bottom). All methods mostly fit the missing parts as well. <ref type="table" target="#tab_1">Table 1</ref> shows that on average the npODE and VGPLVM have approximately equal top performance on the imputing missing values task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We proposed the framework of nonparametric ODE model that can accurately learn arbitrary, nonlinear continuos-time dynamics from purely observational data without making assumptions of the underlying system dynamics. We demonstrated that the model excels at learning dynamics that can be forecasted into the future. We consider this work as the first in a line of studies of nonparametric ODE systems, and foresee several aspects as future work. Currently we do not handle non-stationary vector fields, that is time-dependent differentials f t (x). Furthermore, an interesting future avenue is the study of various vector field kernels, such as divergence-free, curl-free or spectral kernels <ref type="bibr" target="#b31">(Remes et al., 2017)</ref>. Finally, including inputs or controls to the system would allow precise modelling in interactive settings, such as robotics.</p><p>The proposed nonparametric ODE model operates along a continuous-time trajectory, while dynamic models such as hidden Markov models or state-space models are restricted to discrete time steps. These models are unable to consider system state at arbitrary times, for instance, between two successive timepoints.</p><p>Conventional ODE models have also been considered from the stochastic perspective with stochastic differential equation (SDE) models that commonly model the deterministic system drift and diffusion processes separately leading to a distribution of trajectories p(x(t)) <ref type="bibr" target="#b2">(Archambeau et al., 2007;</ref><ref type="bibr" target="#b11">García et al., 2017)</ref>. As future work we will consider stochastic extensions of our nonparametric ODE model, as well as MCMC sampling of the inducing point posterior p(U |Y ), leading to trajectory distribution as well. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) Illustration of an ODE system vector field induced by the Gaussian process. The vector field f (x) (gray arrows) at arbitrary states x is interpolated from the inducing points u, z (black arrows), with the trajectory x(t) (red points) following the differential system f (x) exactly. (b) The trajectory x(t) plotted over time t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Estimated dynamics from Van der Pol, FitzHugh-Nagumo and Lotka-Volterra systems. The top part (a-c) shows the learned vector field (grey arrows) against the true vector field (black arrows). The bottom part (d-f) shows the training data (grey region points) and forecasted future cycle likelihoods with the learned model (shaded region) against the true trajectory (black line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3 illustrates the models on one of the trajectories 35 12.amc. The top part (a) shows the training data in the PCA space for npODE, and optimized training data representation for GPDM and VGPLVM (black points). The colored lines (npODE) and points (GPDM, VGPLVM) indicate the future forecast. The bottom part (b) shows the first 9 reconstructed original pose dimensions reconstructed from the latent forecasted trajectories. The training data is shown in grey background, while test data is shown with circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Forecasting 50 future frames after 49 frames of training data of human motion dataset 35 12.amc. (a) The estimated locations of the trajectory in a latent space (black points) and future forecast (colored lines). (b) The original features reconstructed from the latent predictions with grey region showing the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Imputation of 17 missing frames in the middle of a 94-length trajectory of human motion dataset 07 07.amc (subsampled every fourth frame). (a) The estimated locations of the missing points in the latent space are colored. (b) The original features reconstructed from the latent trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Information Technology HIIT, Finland. Correspondence to: Markus Heinonen &lt;markus.o.heinonen@aalto.fi&gt;.</figDesc><table>* Equal contribution 
1 Aalto University, Finland 
2 Helsinki Insti-
tute of Proceedings of the 35 
th International Conference on Machine 
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 
by the author(s). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Means and standard deviations of RMSEs of 43 datasets in forecasting and filling experiments.</figDesc><table>MODEL 
FORECASTING IMPUTATION 

NPODE 

4.52 ± 2.31 
3.94 ± 3.50 
GPDM 
4.94 ± 3.3 
5.31 ± 3.39 
VGPLVM 
8.74 ± 3.43 
3.91 ± 1.80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>B. Calderhead, M. Girolami, and N. Lawrence. Accelerating bayesian inference over nonlinear differential equations with gaussian processes. NIPS, 2008. Andreas Damianou, Michalis K Titsias, and Neil D Lawrence. Variational gaussian process dynamical sys- tems. In Advances in Neural Information Processing Systems, pages 2510-2518, 2011. Michiel JL De Hoon, Seiya Imoto, Kazuo Kobayashi, Nao- take Ogasawara, and Satoru Miyano. Inferring gene regu- latory networks from time-ordered gene expression data of bacillus subtilis using differential equations. In Bio- computing 2003, pages 17-28. World Scientific, 2002.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The implementation is publicly available in http://www. github.com/cagatayyildiz/npode 2 We use vector field and differential function interchangeably.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In case of multiple time-series, we will use one initial state for each time-series.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The data used in this project was obtained from mocap.cs.cmu.edu. The database was created with funding from NSF EIA-0196217. This work has been supported by the Academy of Finland Center of Excellence in Systems Immunology and Physiology, the Academy of Finland grants no. 284597, 311584, 313271, 299915.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sorad: a systems biology approach to predict and modulate dynamic signaling pathway response from phosphoproteome time-course measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsi</forename><surname>Tarmoäijö</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Granberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lähdesmäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1283" to="1291" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kernels for vector-valued functions: A review. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian process approximations of stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cornford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gaussian Processes in Practice</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The helmholtz-hodge decomposition --a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Norgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Numerical methods for ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Butcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ode parameter inference using adaptive gradient matching with gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dondelinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Husmeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fitting population dynamic models to time-series data by gradient matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seifu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2256" to="2270" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational gaussian process state-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Frigola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3680" to="3688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable parameter estimation for genome-scale biochemical reaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Kaltenbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hasenauer</surname></persName>
		</author>
		<idno type="doi">10.1371/journal.pcbi.1005331</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A radial basis function method for computing helmholtz-hodge decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fuselier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA Journal of Numerical Analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaussian process modelling of latent chemical species: applications to inferring transcription factor activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Rattray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="70" to="75" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of stochastic differential equations with sparse Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Otero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Presedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">22104</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian inference for differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">408</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Solving noisy linear operator equations by gaussian processes: Application to ordinary and partial differential equations. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning nonparametric differential equations with operator-valued kernels and gradient matching. arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Telecom ParisTech</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-stationary Gaussian process regression with Hamiltonian Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mannerström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lähdesmäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="732" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Network reconstruction using nonparametric additive ode models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SUNDIALS: Suite of nonlinear and differential/algebraic equation solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hindmarsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">E</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">S</forename><surname>Shumaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="396" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devaney</forename></persName>
		</author>
		<title level="m">Differential Equations, Dynamical Systems, and an Introduction to Chaos</title>
		<imprint>
			<publisher>Elsevier Science &amp; Technology Books</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Direct and adjoint sensitivity equations for parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kokotovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="610" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Assessing approximate inference for binary gaussian process classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Kuss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1679" to="1704" />
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for visualisation of high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The simultaneous solution and sensitivity analysis of systems described by ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">R</forename><surname>Leis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Statistical inference for ordinary differential equations using gradient matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benn</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Controversy in mechanistic modelling with gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benn</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Higham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Husmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1539" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized hermite interpolation via matrix-valued conditionally positive definite functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph D</forename><surname>Narcowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">208</biblScope>
			<biblScope unit="page" from="661" to="687" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A unifying view of sparse approximate gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quiñonero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1939" to="1959" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
	<note>Learning unknown ODE models with GP</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inferring solutions of differential equations using noisy multi-fidelity data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="736" to="746" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parameter estimation for differential equations: a generalized smoothing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="741" to="796" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ursula Klingmüller, and Jens Timmer. Lessons learned from quantitative dynamical modeling in systems biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Matteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Schelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kaschek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Hug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Kreutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Harms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Non-stationary spectral kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Remes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Kernel methods for pattern analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Modeling and interpolation of the ambient magnetic field by gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wahlstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Särkkä</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04634</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Computationally efficient bayesian learning of gaussian process state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schön</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A spline least squares method for numerical parameter estimation in differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Varah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J.sci. Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="46" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling magnetic fields using gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wahlström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schön</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE conf on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1441" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gaussian process dynamical models for human motion. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<title level="m">Gaussian processes for bayesian estimation in ordinary differential equations. ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fast multidimensional pattern extrapolation with gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cunningham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Variational dependent multioutput gaussian process dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4134" to="4169" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning gene regulatory networks from gene expression measurements using nonparametric molecular kinetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Täijö</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lähdesmäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2937" to="2944" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
