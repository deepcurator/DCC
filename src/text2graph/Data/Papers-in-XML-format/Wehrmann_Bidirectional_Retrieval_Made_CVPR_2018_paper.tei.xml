<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Retrieval Made Simple</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jônatas</forename><surname>Wehrmann</surname></persName>
							<email>jonatas.wehrmann@acad.pucrs.br</email>
							<affiliation key="aff0">
								<orgName type="department">School of Technology Pontifícia</orgName>
								<orgName type="laboratory">School of Technology Pontifícia Universidade Católica do Rio Grande do Sul</orgName>
								<orgName type="institution">Universidade Católica do Rio Grande do Sul</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
							<email>rodrigo.barros@pucrs.br</email>
							<affiliation key="aff0">
								<orgName type="department">School of Technology Pontifícia</orgName>
								<orgName type="laboratory">School of Technology Pontifícia Universidade Católica do Rio Grande do Sul</orgName>
								<orgName type="institution">Universidade Católica do Rio Grande do Sul</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Retrieval Made Simple</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem that we address in this paper is bidirectional retrieval, also regarded as multimodal content retrieval or image-text alignment. In this scenario, the main target is to retrieve content from a modality (e.g., image) given some input content from another modality (e.g., textual description). Several important applications benefit from successful retrieval strategies, such as image and video retrieval, captioning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>, and navigation for the blind.</p><p>State-of-the-art results for bidirectional retrieval <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> are based on networks trained over wordembeddings <ref type="bibr" target="#b19">[20]</ref> that encode the sentences with either Recurrent Neural Networks (LSTMs <ref type="bibr" target="#b8">[9]</ref>, GRUs <ref type="bibr" target="#b2">[3]</ref>) or with handcrafted non-linear transformations such as Fisher vectors <ref type="bibr" target="#b15">[16]</ref>. Despite showing promising results, these strategies present several drawbacks. First, they are costly due to the need of training word-embeddings within a latent space to capture semantic relationships among words. Second, it takes a considerable amount of storage and memory for dealing with these embeddings depending on the size of the dictionary, in which often larger is better in terms of results.</p><p>We address those two problems by proposing an approach that does not rely on RNNs or handcrafted transformations over pre-trained word-embeddings. Our architecture learns from scratch, in a character basis, how to retrieve descriptions from images and images from descriptions, and for that it relies exclusively on convolutional layers. It does not make assumptions on specific templates, guidelines, or previous knowledge since it learns everything from scratch using the available training data.</p><p>Bearing in mind that bidirectional retrieval can be seen as a special case of a single visual-semantic hierarchy over words, sentences, and images, we employ a loss function based on order embeddings <ref type="bibr" target="#b28">[29]</ref>, which are designed to model the partial order structure of the visual-semantic hierarchy existing in image descriptions. While typical strategies for bidirectional retrieval rely on distance-preserving strategies, our approach performs order-preserving optimization, making the process of relating the naturallyhierarchical concepts within descriptions much easier.</p><p>We perform thorough experiments in order to evaluate multiple aspects of our architecture. In particular, we analyze the impact of using (1) distinct number of filters; (2) depth-wise convolutions; and (3) distinct latent embedding size. Moreover, we measure the robustness of our approach to input noise. We compare our proposed architecture with the current state-of-the-art approaches in the wellknown MS COCO <ref type="bibr" target="#b16">[17]</ref> retrieval dataset, and we show that it outperforms all other approaches while presenting a much lighter and simpler retrieval architecture. Finally, we show that our models perform well for text classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CHAIN-VSE</head><p>We propose a bidirectional retrieval architecture that relies on novel inception-based <ref type="bibr" target="#b27">[28]</ref> modules named Character-level Inception for Visual-Semantic Embeddings (CHAIN-VSE). These modules are designed to understand descriptions directly from raw characters (similarly to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref>), projecting the sentence semantics onto a R d representation that explicitly encodes information in a hierarchical fashion for leveraging both fine-grained and global-level features.</p><p>State-of-the-art approaches for multimodal alignment/retrieval are often based on word-embeddings and either RNNs or handcrafted transformations for encoding sentences. Such a strategy presents some drawbacks: (i) it requires training word-embeddings and RNNs in very large corpora (with millions or billions of words), consuming a lot of time and demanding high computational power; (ii) to encode a single word or sentence, it is necessary to have at disposal the whole word-dictionary containing all known words, largely increasing the memory requirements to store all data; (iii) for multilingual or informal domains such as tweets and search queries, the number of words in the dictionary increases with the number of languages; (iv) a preprocessing step is required for correcting typos and standardizing the words.</p><p>The idea behind CHAIN-VSE is to replace both wordembeddings and RNNs/handcrafted transformations by a simple yet effective architecture that is exclusively based on character-level convolutions. The advantage of using convolutions instead of RNNs is that one can use parallelism for convolving temporal data, while in RNNs a given temporal iteration depends on the previous one. This allows for much faster computation, especially in GPUs. By employing a character-level textual representation instead of wordlevel (as in <ref type="bibr" target="#b18">[19]</ref>), we can build descriptions across several languages with a small finite set of characters. With wordembeddings, on the other hand, we would need to store thousands or millions of word-vectors.</p><p>Our architectures depend on two main hyper-parameters: p, that regulates the number of filters in the convolutional layers, and d, which defines the latent-embedding size. Next, we describe two main variations of our architecture, namely CHAIN-VSE-[v1, v2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CHAIN-VSE-v1</head><p>Figure 1 depicts the overall structure of the first version of CHAIN-VSE. It consists of two inception modules <ref type="bibr" target="#b27">[28]</ref> for capturing distinct granularity levels of the original sentence. The first module maps the binary character-level content from the sentences to a dense representation by convolving the characters with filter sizes f ∈ {7, 5, 3}, generating word-embedding-like vectors. This dense representation is then fed to the second module, which consists in using four independent convolutional streams. The first stream comprises three convolutional layers with filter sizes f ∈ {7, 5, 3}, which are capable of learning relationships among a reduced set of words, depending on the word length. For instance, it could generalize trigram-like features. The second and fourth streams are mostly responsible for understanding fine-grained information (e.g., similar to bigrams and unigrams), being capable of learning from single words and short character sequences (e.g., :), : /, and goood). The third stream first performs an average pooling that reduces by half the temporal dimension, and is thus designed to feature-wise average sequences of characters, helping in exploiting the receptive field for learning mid-term dependencies (see <ref type="figure" target="#fig_1">Figure 2)</ref>. The final layer of each convolutional stream within CHAIN-VSE-v1 performs a Max Global Pooling (often called max[average]-over-time-pooling <ref type="bibr" target="#b12">[13]</ref>) that summarizes the temporal dimension. Each vector is then concatenated for building the final textual semantic representation. Finally, by adopting an inception-like architecture, our models (with four convolutional streams of 256 filters) present a reduced number of parameters when compared to a similar network built over a single convolutional stream of three layers with 1024 filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CHAIN-VSE-v2</head><p>We also designed lighter modules that are based on separable depth-wise convolutions. <ref type="figure" target="#fig_2">Figure 3</ref> depicts an example of a separable depth-wise convolutional layer being applied to bidimensional data such as character-based inputs. This layer comprises two steps: (i) a separable convolution, which processes each channel of the input data individually; and (ii) a regular convolution with filter size f = 1, which merges information across all features. By using this particular strategy, one can basically halve the number of parameters and floating-point operations. Note that this architecture employs separable convolutions after the first inception layer. Hence, convolutions applied directly to the character-level input use filters 7 × η, where η is the alphabet size. Formally, note that convolving a W × H input with a filter P × Q implies a complexity of O(W × H × P × Q). When using a separable filter, the first-step complexity is O(W × H × P ), while the second-step is O(W × H × Q), resulting in a total complexity of O(W × H × (P + Q)) or O(W ×H ×P ×2) if P = Q. Therefore, the computational advantage in using separable filters against regular ones is P × Q/(P + Q). For a 7 × 7 filter, it results in a theoretical speed-up of 3.5. A final note regarding the use of separable filters is that the application of 1 × 1 convolutions introduce, per se, extra non-linearity to the model, eliminating the need of additional activation functions. Nevertheless, we notice that using Maxout over the feature maps lead to far better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Overall Architecture</head><p>We approximate two encoding functions, f t (T ) and f i (I), whose goal is to project both description T and image I into the same embedding space. In such a space, correlated image-text pairs should be close to each other, and the distance of non-correlated pairs should necessarily be larger than the correlated ones. For the text encoding function f t (T ), we make use of the CHAIN-VSE modules described in Sections 2.1 and 2.2. For the image encoding function f i (I), we extract image features from the global layer of a ConvNet (VGG-19, Inception-ResNetv2 [IRv2], or ResNet-152) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> pre-trained in the ImageNet dataset <ref type="bibr" target="#b24">[25]</ref>. Each image I is then represented by c-long vectors, extracted using the 10-crop strategy. Respectively, VGG-19, IRv2 and ResNet-152 present c = 4096, 1536, 2048. Let C(I) be features extracted from image I by the convolutional neural network; images are projected onto the R d + embedding-space based on a linear mapping:</p><formula xml:id="formula_0">f i (I) = |W i · C(I)| (1)</formula><p>where W i ∈ R d×c is a learned weight matrix and d is the number of dimensions of the embedding space.</p><p>For embedding text, we use the proposed characterbased approach f t (·) with the two main variations previously discussed. Our models provide a l-long vector representation that carries the textual semantic information, where l depends on the number of filters used. For instance, CHAIN-VSE-v1 (p = 1) produces a l = 1024 vector. Similarly to f i (·), we linearly project such representation onto R d + by using a learned W t ∈ R d×l weight matrix. Note that one of the best approaches for image-text alignment <ref type="bibr" target="#b28">[29]</ref> makes use of GRU networks fed with wordembeddings, requiring ≈ 15M parameters for the wordembeddings (considering 50, 000 words) and ≈ 5M parameters for the GRU itself. Our largest architecture contains roughly the same amount of parameters, while the smallest architecture contains only 1.5M parameters, which is more than one order of magnitude lighter. Another topperforming approach employs two-way nets <ref type="bibr" target="#b4">[5]</ref>, with fullyconnected layers that result in more than 100M total parameters, which also depends on pre-trained word-embeddings and handcrafted nonlinear transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Loss function</head><p>Let f t (T ) = m be the sentence embedding vector and f i (I) = v be the image embedding. We first scale m and v to have unit norm, so that the inner product of both results in the cosine distance. Instead of directly optimizing the cosine distance as in <ref type="bibr" target="#b14">[15]</ref>, we follow <ref type="bibr" target="#b28">[29]</ref> by optimizing the alignment preserving the order relationships among the visual-semantic hierarchy, given that asymmetric distances are naturally more well-suited for image-sentence alignment. Hence, we apply an order-violation constraint by penalizing an ordered pair (x, y) of points in R N + :</p><formula xml:id="formula_1">s(x, y) = −|max{0, y − x}| 2 (2)</formula><p>The order violation penalties are used as a similarity distance, and optimized by the following constrastive pairwise ranking loss:</p><formula xml:id="formula_2">L = m k max{0, α − s(m, v) + s(m, v k )} + v k max{0, α − s(v, m) + s(v, m k )} (3)</formula><p>where m k and v k are the sentence and image contrastive examples (i.e., uncorrelated). This loss function encourages the similarity s(x, y) for proper image-text pairs to be larger than the contrastive pairs by a margin of at least α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>For analyzing the performance of our proposed approach, we make use of the Microsoft COCO dataset <ref type="bibr" target="#b16">[17]</ref>. We have used the same data splits from <ref type="bibr" target="#b11">[12]</ref>: 113,287 images for training, 5,000 images for validation, and 5,000 images for testing.</p><p>MS COCO has been extensively employed in the recent years for image-text retrieval challenges. Note that, for the 5k images in the test set, there are three distinct evaluation protocols employed by the research community, because the test images were further divided into 5 folds of 1k images each. Some studies present results on the entire test set of 5k images, a protocol we refer to as COCO-5k; others present results only for a subset of 1k images, which we refer to as COCO-1k; finally, there are studies that present the average result over the 5 folds, referred to as COCO-5cv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hyper-Parameters and Training Details</head><p>We choose hyper-parameters via non-exhaustive random search based on the results over the validation data. We employ Adam for optimization, given its capacity in adjusting per-weight learning rates during training. We use Adam's default initial learning rate of 1 × 10 −3 . In addition, we found it was beneficial to reduce the learning rate by 10× whenever the validation error plateaus. Inspired by <ref type="bibr" target="#b28">[29]</ref>, we use a batch size of 128 (127 contrastive examples) and margin α = 0.05. Neither weight decay nor dropout were used, since we believe the loss function itself is enough to regularize the model by including several contrastive examples that naturally inject some amount of noise during training.</p><p>The three convolutions in the first inception module comprise 32 filters for all of our models. For the second inception module, the width varies according to p. The default number of convolutional filters for each layer in the second module is set to 256. This means that a network with p = 1 has 256 filters in each convolutional layer, while a network with p = 0.5 comprises layers with 128 filters. In addition, the default size of the latent multimodal space is set to d = 1024. We early-stop the training when the sum of all metrics calculated in the validation data stops improving for 10 consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Measures</head><p>For evaluating the results, we use the same measures as those in <ref type="bibr" target="#b28">[29]</ref>: R@K (reads "Recall at K") is the percentage of queries in which the ground-truth term is one of the first K retrieved results. The higher its value, the better. We also show the results of Med r and Mean r, which represent respectively the median and mean of the ground-truth ranking. Since they are ranking-based measures, the smaller their values the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Analysis</head><p>In this section, we provide a thorough analysis of the performance of our proposed approach. First, we analyze the impact of different architectural choices for CHAIN-VSE by looking exclusively to results on validation data. Then, we compare our best approach with the state-of-theart in bidirectional retrieval (results over the test set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Impact of p</head><p>We also analyze the impact of the hyper-parameter p that regulates the width of the network. We vary p ∈ {0.5, 0.75, ..., 1.50}, allowing us to discover the minimum number of neurons that are required for achieving good performance. It is also useful for finding models that present a good performance-complexity trade-off.</p><p>We report in <ref type="table" target="#tab_0">Table 1</ref> results for both CHAIN-VSE-v1 (top section) and CHAIN-VSE-v2 (bottom section). Both architectures employ Maxout activations. Note that traditional approaches for separable convolutions make use of linear layers (no activations), given that the single-sized depth-wise convolution itself is responsible for increasing the non-linearity of the model. Nevertheless, we achieved top performance by using Maxout activations. The use of separable convolutions does not seem to be recommended for reaching top performance, though it becomes an interesting option when the goal is to generate faster and lighter models, requiring roughly half the floating-point operations. Also note that using p = 0.75 leads to an ≈ 2% drop in terms of R@1, but also reducing ≈ 1.6× the required floating-point operations and requiring 1.27M fewer parameters. Models with p = 0.5 perform similarly to p = 0.75 while being much lighter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Impact of the Latent Embedding Size</head><p>Our architecture for training both image and text encoders rely on the use of a contrastive loss function that optimizes a metric so that correlated image-text pairs lie close in a multimodal embedding space. As far as we know, this is the first study to investigate the impact of the resulting multimodal embedding size. We trained several models of CHAIN-VSE-v1 by varying d ∈ {2 4 , 2 6 , ..., 2 14 }. The training behavior can be observed in <ref type="figure">Figure 4</ref>, where we report values of R@1 on the validation set across all training epochs. For the text to image retrieval task, note that using d = 2048 leads to much better results than using the default embedding size (1024). In addition, it seems that d = 16384 brings little to no impact when compared to d = 8192 for that same task, but it seems to definitely help for the description retrieval. In a nutshell, we conclude that suitable high-performing d values are 512 &lt; d &lt; 16384.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness to Input Noise</head><p>Our models are built based on character-level inputs. Theoretically, this strategy is much more robust to input noise such as typos and prolonged words (e.g., thorought, gooood). This advantage is inherent to the fact that using the atomic part of the sentence for learning semantics makes it easier to learn syntactic and semantic similarities across words. Note that input noise is challenging for wordembedding-based approaches. For handling noise in those approaches, we must implement ad-hoc strategies such as dictionary look-ups to correct typos. This poses an unnecessary cost that can be avoided by character-based models.</p><p>For evaluating the robustness of our models to input noise, we randomly change a given ratio of the description characters. We compute validation set results (R@1 and R@10) while varying the noise ratio in the interval ∈ {1%, 2%, 3%, ..., 25%}. We make sure that when the noise ratio is set to &gt;= 1% at least one character is changed in the original description. This is necessary because MS COCO dataset is mostly comprised of short descriptions. <ref type="figure" target="#fig_3">Figure 5</ref> confirms our hypothesis that character-level models are far more robust to input noise. Our approach only suffers a significant drop in R@10 when we set the noise ratio to &gt; 15% (i.e., 15% of the original text is randomly changed). On the other hand, the word-embedding based approach by Vendrov et al. <ref type="bibr" target="#b28">[29]</ref> presents a dramatic drop of both evaluation measures even for very small amounts of noise. This result was expected given that by changing a single character, the word may not be found in the trained word-embedding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CHAIN-VSE vs. State-of-the-art</head><p>For comparing our models with the state-of-the-art, we selected those models from each CHAIN-VSE's variation that presented the best performance on validation data. Our models are compared to the published state-of-the- art approaches for multimodal retrieval, namely UVS <ref type="bibr" target="#b14">[15]</ref>, DVSA <ref type="bibr" target="#b11">[12]</ref>, FV <ref type="bibr" target="#b15">[16]</ref>, m-CNN <ref type="bibr" target="#b18">[19]</ref>, Order-Embeddings (OE) <ref type="bibr" target="#b28">[29]</ref>, Embedding Network <ref type="bibr" target="#b30">[31]</ref>, sm-LSTM <ref type="bibr" target="#b9">[10]</ref>, 2WayNet <ref type="bibr" target="#b4">[5]</ref>, SEAM-C <ref type="bibr" target="#b31">[32]</ref>, and RFF-Net <ref type="bibr" target="#b17">[18]</ref>. For providing a fair comparison, we replicated the OE <ref type="bibr" target="#b28">[29]</ref> results using their own source code, both with their default parameters and also with varied latent embedding sizes. Note that several baselines employ text representation based on word-embeddings, which inflicts a minimum memory cost of |V| × |D|, where V is the vocabulary that contains all known words and D is the word-vector. For instance, the vocabulary of the MS COCO dataset contains roughly 50,000 words (as in <ref type="bibr" target="#b28">[29]</ref>), which inflicts a minimum cost of 50, 000 × 300 (assuming |D| = 300). On the other hand, our approach is capable of fully learning compact vectors for sentence encoding, not requiring pretrained dictionary-based vectors nor handcrafted transformations. Note that our approach presents a constant data cost that is related to the alphabet size alone, i.e., it is independent of the number of words, which is an interesting property especially for multilingual learning (see Section 5 for results in multilingual and noisy data learning). <ref type="table" target="#tab_1">Table 2</ref> presents results for the best selected versions of CHAIN-VSE alongside the state-of-the-art approaches when considering COCO-1k. Note that CHAIN-VSE's version using ResNet-152, d = 8192, and p = 1 establishes itself as the new state-of-the-art for both description and image retrieval tasks, outperforming RFF-Net by ≈ 6.1% for image-to-text task in absolute R@1 values. Whereas using a better ConvNet definitely helps in achieving state-of-the-art results, note that the versions of CHAIN-VSE that employ either a IRv2 or a VGG-19 also outperform all baselines for all evaluation measures and retrieval tasks, with the exception of R@1 in the description retrieval task, where it is outperformed by 2WayNet (with a VGG-19). However, note that 2WayNet performs poorly regarding R@10, and that all versions of CHAIN-VSE are far superior in the image retrieval task. In addition, CHAIN-VSE is about two orders of magnitude lighter than 2WayNet.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, we present results for the best selected versions of CHAIN-VSE along OE <ref type="bibr" target="#b28">[29]</ref> (and its modified versions), OECC <ref type="bibr" target="#b33">[34]</ref>, and SEAM-C <ref type="bibr" target="#b31">[32]</ref>, since those studies explicitly provide average results on COCO-5cv. Once again CHAIN-VSE (d ∈ {4096, 8192}), provides superior results for all evaluation measures and retrieval tasks regardless of the ConvNet it uses.</p><p>For better visualizing the trade-off between model complexity and predictive performance of CHAIN-VSE, we present in <ref type="figure" target="#fig_4">Figure 6</ref> the effect of varying the amount of parameters versus R@1, and how CHAIN-VSE compares to OE <ref type="bibr" target="#b28">[29]</ref> and Embedding Network <ref type="bibr" target="#b30">[31]</ref>, which are the two baselines that present the smallest amount of parameters. The ideal position is in the upper-left position (largest recall and smallest amount of parameters). For generating this visualization, we computed the number of trainable parameters for each method. Note that the word-embeddings are considered trainable parameters as well, considering all hyper-parameters and settings defined in <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b30">[31]</ref>.</p><p>Note that our models in <ref type="figure" target="#fig_4">Figure 6</ref> present fewer parameters than the baselines. There are several models that are almost an order of magnitude lighter while presenting better results for both tasks. Our models seem to respond much better to larger embedding sizes than OE <ref type="bibr" target="#b28">[29]</ref>. Moreover, using features from IRv2 provides a large gain for CHAIN-VSE, whereas that does not seem to be the case for OE. Finally, 2WayNet <ref type="bibr" target="#b4">[5]</ref> is not included in <ref type="figure" target="#fig_4">Figure 6</ref> since it has about two orders of magnitude more parameters than our models.  Impact of text length. We also evaluate the impact of the text length in our retrieval results. In order to accomplish that, we separate 100 images that present the longest and shortest captions. Results are shown in <ref type="table" target="#tab_3">Table 4</ref>. Our findings are twofold: (i) CHAIN-VSE is much better for annotating images with long captions; and (ii) it works better for retrieving images given short captions, probably due to the fact that our approach explicitly exploits short and mid-term aspects of the sentences. Finally, it seems to be hard for both methods to retrieve the correct images given long captions. This might be due to the data distribution in MS COCO, since it presents mostly short captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Limitations</head><p>CHAIN-VSE's main limitation comes from the fact that it learns textual features from scratch rather than using external corpora for learning word semantics: CHAIN-VSE may suffer from overfitting when dealing with smaller datasets, such as Flickr30k <ref type="bibr" target="#b21">[22]</ref>. Apparently, those datasets do not present enough textual data to properly learn textual semantics from raw characters. We achieve R@1 ≈ 36 (40) for caption retrieval and R@1 ≈ 26 (31) for image retrieval using VGG-19 (IRv2), which outperforms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>, but is outperformed by some recent approaches that employ external corpora to some extent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Text Classification</head><p>We designed CHAIN-VSE in order to provide a simple yet efficient and robust method for learning textual semantics directly from characters. One of our findings was that CHAIN-VSE is far more robust to noise than state-of-theart approaches. In this section we provide an analysis regarding the suitability of CHAIN-VSE for learning multilanguage sentiment analysis models from noisy data and in widely used text classification datasets.</p><p>We test CHAIN-VSE on a multi-language Twitter corpora as well as on the widely used AGNews and DBPedia datasets. The Twitter corpora from <ref type="bibr" target="#b20">[21]</ref> does not provide the tweet itself, but rather a URL that leads to the tweets. Due to this particularity, some tweets are no longer available. We adapt CHAIN-VSE by replacing the multimodal embedding layer with a fully-connected softmax layer for performing the final classification. Since the data used in this experiment contains about 5× fewer textual instances than MS COCO, we optimized the hyper-parameters for properly regularizing the network.</p><p>We optimized the width of the network, the latent space, regularization, and activation function. CHAIN-VSE is by far the lightest method in terms of parameters (DBPedia and Tweets require a vocabulary of 200,000 words). Our total architecture comprises about 1.5M parameters, 400× fewer parameters than FastText <ref type="bibr" target="#b10">[11]</ref> and Conv <ref type="bibr" target="#b12">[13]</ref>. It is also 10× lighter than VCDNN <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Karpathy and Fei-Fei <ref type="bibr" target="#b11">[12]</ref> propose an architecture that makes use of features from detection-based systems, aligning image regions with a proper sentence fragment. Ma et al. <ref type="bibr" target="#b18">[19]</ref> propose a multimodal ConvNet for aligning image and text by jointly convolving word-embeddings and image features. The learned similarity score predicts whether a pair is correlated or not. Vendrov et al. <ref type="bibr" target="#b28">[29]</ref> propose sentence order-embeddings, which aim to preserve the partial order structure of a visual-semantic hierarchy. It allows learning ordered representation by applying order-penalties, and they show that asymmetric measures are better suited for image-sentence retrieval tasks.</p><p>Wang et al. <ref type="bibr" target="#b30">[31]</ref> introduce a two-branch neural network for learning a multimodal embedding space. They encode text based on 300-d word-embeddings, where they apply ICA and construct a codebook with 30 centers using first and second-order information, resulting in a 18k-dimensional representation. Next, they apply PCA to reduce the representation to 6k dimensions in order to reduce memory requirements and training time. The projection into the joint space is performed with dense layers.</p><p>Huang et al. <ref type="bibr" target="#b9">[10]</ref> propose a selective multimodal LSTM (sm-LSTM). They introduce a multimodal contextmodulated attention scheme at each time-step, which is capable of focusing on a text-image pair by predicting pairwise instance-aware saliency maps. Sentences are processed by a bidirectional LSTM that runs over wordembeddings. Image features are selected by using a strategy of instance candidates, which extracts local information from a 512×14×14 tensor. They also make use of the 4096-d vectors for a global image representation, leveraging local and global information from both text and image.</p><p>In <ref type="bibr" target="#b4">[5]</ref>, the authors introduce a 2-Way-Network for mapping a modality into another. Similarly to <ref type="bibr" target="#b30">[31]</ref>, they use Fisher Vectors applied over word2vec for sentence encoding. They concatenate the Fisher Vector encoding (GMM) and the Fisher Vector of the HGLMM distribution, resulting in a 36k-dimensional vector per sentence.</p><p>In <ref type="bibr" target="#b31">[32]</ref>, the authors propose a fast approach for multimodal retrieval. They employ a self-attention mechanism in order to embed word-vectors onto a sentence-level embedding space. They achieved good results while their models were much faster for training and deployment than the RNN-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we presented a simple architecture for bidirectional retrieval capable of learning textual embedding based on raw characters, namely CHAIN-VSE. Even though it is conceptually a much simpler architecture than those found in related work, our approach achieves state-ofthe-art results in both text to image and image to text tasks considering the most well-known retrieval dataset, namely MS COCO <ref type="bibr" target="#b16">[17]</ref>. CHAIN-VSE is simple, effective, requires fewer parameters, and it is robust to input noise due to the fact that it learns sentence representation from characterlevel convolutions. In addition, it presents sound performance for text classification tasks, specially in noisy and multilingual scenarios. For future work, we intend to analyze the impact of CHAIN-VSE in recent work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and its performance in other tasks, such as VQA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and video retrieval <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. CHAIN-VSE-v1 module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Impact of the pooling layer (filter size of 5 and stride of 2) in the receptive field of a character-based textual representation. For simplicity, we are ignoring the existence of the first inception module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of a depth-wise separable convolution applied to a bidimensional input of size t × ft−1, where t denotes the size of the temporal dimension (text length), and ft−1 is the number of filters of the previous convolutional layer. The filter length in the example is set to l = 5, as seen in the second convolutional layers within CHAIN-VSE streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Analysis of performance given random input noise. Continuous lines depict R@10 values whereas dotted lines depict R@1 values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Trade-off between model complexity (#parameters) and predictive performance (R@10) in the COCO-1k test set. Results are for both image-to-text (left) and text-to-image (right) retrieval. Dashed lines are architectures that make use of IRv2 networks whereas continuous lines are architectures that use VGG-19. Each point in the line indicate a distinct latent embedding size. From left to right: d = 1024, 2048, 4096, 8192.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Impact of p (width) in CHAIN-VSE. Bidirectional results on MS COCO validation set.</figDesc><table>Image to text 
Text to image 

CHAIN-VSE (p) 
R@1 
Mean r 
R@1 
Mean r 
#Params 
Flops ×10 

10 

v1 (0.50) 
48.5 
4.8 
39.7 
6.3 
1.47M 
4.55 
v1 (0.75) 
48.7 
4.4 
40.9 
6.3 
2.52M 
8.43 
v1 (1.00) 
50.6 
4.3 
41.3 
6.3 
3.79M 
13.41 
v1 (1.25) 
49.2 
4.4 
40.9 
6.9 
5.27M 
19.48 
v1 (1.50) 
48.8 
4.7 
40.6 
6.7 
6.96M 
26.64 

v2 (0.50) 
46.5 
4.8 
38.6 
6.5 
1.16M 
2.93 
v2 (0.75) 
47.0 
4.8 
39.3 
6.7 
1.80M 
4.75 
v2 (1.00) 
47.9 
4.7 
39.4 
6.5 
2.50M 
6.83 
v2 (1.25) 
48.6 
4.5 
39.8 
6.5 
3.26M 
9.15 
v2 (1.50) 
47.7 
4.6 
40.0 
6.6 
4.06M 
11.73 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Bidirectional results on COCO-1k test set. Bold values indicate the current state-of-the-art results. Underlined values outperform the best published results.</figDesc><table>Image to text 
Text to image 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Bidirectional results on COCO-5cv test set. Bold values indicate the current state-of-the-art results.</figDesc><table>Image to text 
Text to image 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Impact of text length. d=2048, cnn=ResNet152.</figDesc><table>Image to text 
Text to image 
Method 
Length 
R@1 
Mean r 
R@1 
Mean r 

OE 
100 longest 
86.0 
1.3 
76.4 
1.5 
CHAIN-VSE 
100 longest 
92.0 
1.3 
74.2 
1.6 

OE 
100 shortest 
85.0 
1.4 
69.8 
1.7 
CHAIN-VSE 
100 shortest 
80.0 
1.6 
81.0 
1.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Text classificaton results.</figDesc><table>Method 
Tweets 
AgNews 
DBPedia 

Conv [13, 33] 
71.8% 
-
-
ConvChar [35] 
70.6% 
87.2% 
98.3% 
FastText [11] 
71.3% 
91.5% 
98.1% 
VCDNN [4] 
-
91.3% 
98.7% 

CHAIN-VSE-v1 
73.5% 
91.5% 
98.6% 
CHAIN-VSE-v2 
72.1% 
91.0% 
98.2% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgments</head><p>We thank Google, Motorola, and the Brazilian research agencies CAPES, CNPq, and FAPERGS for funding this research. We also acknowledge the support of NVIDIA Corporation for the donation of the GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML&apos;15</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">VSE++: improved visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno>abs/1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06420</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilingual twitter sentiment classification: The role of human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mozetič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grčar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smailović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">155036</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="120" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Orderembeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR 2016)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Captioning images with diverse objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast self-attentive multimodal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Armani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision (WACV&apos;18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Barros. A character-based convolutional neural network for language-agnostic twitter sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cagnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2384" to="2391" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Order embeddings and character-level convolutions for multimodal alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mattjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
