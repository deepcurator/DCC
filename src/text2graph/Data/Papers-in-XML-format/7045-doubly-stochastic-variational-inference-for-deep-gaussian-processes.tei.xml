<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Doubly Stochastic Variational Inference for Deep Gaussian Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Salimbeni</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Imperial College London and PROWLER.io</orgName>
								<orgName type="institution">Imperial College London and PROWLER</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
							<email>m.deisenroth@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Imperial College London and PROWLER.io</orgName>
								<orgName type="institution">Imperial College London and PROWLER</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Doubly Stochastic Variational Inference for Deep Gaussian Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to overfitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalizations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm that does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gaussian processes (GPs) achieve state-of-the-art performance in a range of applications including robotics <ref type="bibr" target="#b25">(Ko and Fox, 2008;</ref><ref type="bibr" target="#b11">Deisenroth and Rasmussen, 2011)</ref>, geostatistics <ref type="bibr" target="#b12">(Diggle and Ribeiro, 2007)</ref>, numerics <ref type="bibr" target="#b3">(Briol et al., 2015)</ref>, active sensing <ref type="bibr" target="#b17">(Guestrin et al., 2005)</ref> and optimization <ref type="bibr" target="#b37">(Snoek et al., 2012)</ref>. A Gaussian process is defined by its mean and covariance function. In some situations prior knowledge can be readily incorporated into these functions. Examples include periodicities in climate modelling <ref type="bibr" target="#b35">(Rasmussen and Williams, 2006)</ref>, change-points in time series data <ref type="bibr" target="#b16">(Garnett et al., 2009</ref>) and simulator priors for robotics <ref type="bibr" target="#b7">(Cutler and How, 2015)</ref>. In other settings, GPs are used successfully as black-box function approximators. There are compelling reasons to use GPs, even when little is known about the data: a GP grows in complexity to suit the data; a GP is robust to overfitting while providing reasonable error bars on predictions; a GP can model a rich class of functions with few hyperparameters.</p><p>Single-layer GP models are limited by the expressiveness of the kernel/covariance function. To some extent kernels can be learned from data, but inference over a large and richly parameterized space of kernels is expensive, and approximate methods may be at risk of overfitting. Optimization of the marginal likelihood with respect to hyperparameters approximates Bayesian inference only if the number of hyperparameters is small <ref type="bibr" target="#b30">(Mackay, 1999)</ref>. Attempts to use, for example, a highly parameterized neural network as a kernel function <ref type="bibr" target="#b5">(Calandra et al., 2016;</ref><ref type="bibr" target="#b43">Wilson et al., 2016)</ref> incur the downsides of deep learning, such as the need for application-specific architectures and regularization techniques. Kernels can be combined through sums and products <ref type="bibr" target="#b13">(Duvenaud et al., 2013)</ref> to create more expressive compositional kernels, but this approach is limited to simple base kernels, and their optimization is expensive.</p><p>A Deep Gaussian Process (DGP) is a hierarchical composition of GPs that can overcome the limitations of standard (single-layer) GPs while retaining the advantages. DGPs are richer models than standard GPs, just as deep networks are richer than generalized linear models. In contrast to models with highly parameterized kernels, DGPs learn a representation hierarchy non-parametrically with very few hyperparmeters to optimize.</p><p>Unlike their single-layer counterparts, DGPs have proved difficult to train. The mean-field variational approaches used in previous work <ref type="bibr" target="#b9">(Damianou and Lawrence, 2013;</ref><ref type="bibr" target="#b33">Mattos et al., 2016;</ref> make strong independence and Gaussianity assumptions. The true posterior is likely to exhibit high correlations between layers, but mean-field variational approaches are known to severely underestimate the variance in these situations <ref type="bibr" target="#b40">(Turner and Sahani, 2011)</ref>.</p><p>In this paper, we present a variational algorithm for inference in DGP models that does not force independence or Gaussianity between the layers. In common with many state-of-the-art GP approximation schemes we start from a sparse inducing point variational framework <ref type="bibr" target="#b32">(Matthews et al., 2016)</ref> to achieve computational tractability within each layer, but we do not force independence between the layers. Instead, we use the exact model conditioned on the inducing points as a variational posterior. This posterior has the same structure as the full model, and in particular it maintains the correlations between layers. Since we preserve the non-linearity of the full model in our variational posterior we lose analytic tractability. We overcome this difficulty by sampling from the variational posterior, introducing the first source of stochasticity. This is computationally straightforward due to an important property of the sparse variational posterior marginals: the marginals conditioned on the layer below depend only on the corresponding inputs. It follows that samples from the marginals at the top layer can be obtained without computing the full covariance within the layers. We are primarily interested in large data applications, so we further subsample the data in minibatches. This second source of stochasticity allows us to scale to arbitrarily large data.</p><p>We demonstrate through extensive experiments that our approach works well in practice. We provide results on benchmark regression and classification data problems, and also demonstrate the first DGP application to a dataset with a billion points. Our experiments confirm that DGP models are never worse than single-layer GPs, and in many cases significantly better. Crucially, we show that additional layers do not incur overfitting, even with small data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we present necessary background on single-layer Gaussian processes and sparse variational inference, followed by the definition of the deep Gaussian process model. Throughout we emphasize a particular property of sparse approximations: the sparse variational posterior is itself a Gaussian process, so the marginals depend only on the corresponding inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-layer Gaussian Processes</head><p>We consider the task of inferring a stochastic function f : R D → R, given a likelihood p(y|f ) and a set of N observations y = (y 1 , . . . , y N ) at design locations X = (x 1 , . . . , x N ) . We place a GP prior on the function f that models all function values as jointly Gaussian, with a covariance function k : R D × R D → R and a mean function m : R D → R. We further define an additional set of M inducing locations Z = (z 1 , . . . , z M ) . We use the notation f = f (X) and u = f (Z) for the function values at the design and inducing points, respectively. We define also [m(</p><formula xml:id="formula_0">X)] i = m(x i ) and [k(X, Z)] ij = k(x i , z j )</formula><p>. By the definition of a GP, the joint density p(f , u) is a Gaussian whose mean is given by the mean function evaluated at every input (X, Z) , and the corresponding covariance is given by the covariance function evaluated at every pair of inputs. The joint density of y, f and u is</p><formula xml:id="formula_1">p(y, f , u) = p(f |u; X, Z)p(u; Z) GP prior N i=1 p(y i |f i ) likelihood .<label>(1)</label></formula><p>In <ref type="formula" target="#formula_1">(1)</ref> we factorized the joint GP prior p(f , u; X, Z) 1 into the prior p(u) = N (u|m(Z), k(Z, Z)) and the conditional p(f |u; X, Z) = N (f |µ, Σ), where for i, j = 1, . . . , N</p><formula xml:id="formula_2">[µ] i = m(x i ) + α(x i ) (u − m(Z)) ,<label>(2)</label></formula><formula xml:id="formula_3">[Σ] ij = k(x i , x j ) − α(x i ) k(Z, Z)α(x j ) ,<label>(3)</label></formula><formula xml:id="formula_4">with α(x i ) = k(Z, Z) −1 k(Z, x i ).</formula><p>Note that the conditional mean µ and covariance Σ defined via (2) and (3), respectively, take the form of mean and covariance functions of the inputs x i . Inference in the model (1) is possible in closed form when the likelihood p(y|f ) is Gaussian, but the computation scales cubically with N .</p><p>We are interested in large datasets with non-Gaussian likelihoods. Therefore, we seek a variational posterior to overcome both these difficulties simultaneously. Variational inference seeks an approximate posterior q(f , u) by minimizing the Kullback-Leibler divergence <ref type="bibr">KL[q||p]</ref> between the variational posterior q and the true posterior p. Equivalently, we maximize the lower bound on the marginal likelihood (evidence)</p><formula xml:id="formula_5">L = E q(f ,u) log p(y, f , u) q(f , u) ,<label>(4)</label></formula><p>where p(y, f , u) is given in (1). We follow <ref type="bibr" target="#b20">Hensman et al. (2013)</ref> and choose a variational posterior</p><formula xml:id="formula_6">q(f , u) = p(f |u; X, Z)q(u) ,<label>(5)</label></formula><p>where q(u) = N (u|m, S). Since both terms in the variational posterior are Gaussian, we can analytically marginalize u, which yields</p><formula xml:id="formula_7">q(f |m, S; X, Z) = p(f |u; X, Z)q(u)du = N (f |μ,Σ) .<label>(6)</label></formula><p>Similar to (2) and (3), the expressions forμ andΣ can be written as mean and covariance functions of the inputs. To emphasize this point we define</p><formula xml:id="formula_8">µ m,Z (x i ) = m(x i ) + α(x i ) (m − m(Z)) ,<label>(7)</label></formula><formula xml:id="formula_9">Σ S,Z (x i , x j ) = k(x i , x j ) − α(x i ) (k(Z, Z) − S)α(x j ) .<label>(8)</label></formula><p>With these functions we define</p><formula xml:id="formula_10">[μ] i = µ m,Z (x i ) and [Σ] ij = Σ S,Z (x i , x j ).</formula><p>We have written the mean and covariance in this way to make the following observation clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1.</head><p>The f i marginals of the variational posterior (6) depend only on the corresponding inputs x i . Therefore, we can write the ith marginal of q(f |m, S; X, Z) as</p><formula xml:id="formula_11">q(f i |m, S; X, Z) = q(f i |m, S; x i , Z) = N (f i |µ m,Z (x i ), Σ S,Z (x i , x i )) .<label>(9)</label></formula><p>Using our variational posterior (5) the lower bound (4) simplifies considerably since (a) the conditionals p(f |u; X, Z) inside the logarithm cancel and (b) the likelihood expectation requires only the variational marginals. We obtain</p><formula xml:id="formula_12">L = N i=1 E q(fi|m,S;xi,Z) [log p(y i |f i )] − KL[q(u)||p(u)] .<label>(10)</label></formula><p>The final (univariate) expectation of the log-likelihood can be computed analytically in some cases, with quadrature <ref type="bibr" target="#b21">(Hensman et al., 2015)</ref> or through Monte Carlo sampling <ref type="bibr" target="#b15">Gal et al., 2015)</ref>. Since the bound is a sum over the data, an unbiased estimator can be obtained through minibatch subsampling. This permits inference on large datasets. In this work we refer to a GP with this method of inference as a sparse GP (SGP).</p><p>The variational parameters (Z, m and S) are found by maximizing the lower bound (10). This maximization is guaranteed to converge since L is a lower bound to the marginal likelihood p(y|X).</p><p>We can also learn model parameters (hyperparameters of the kernel or likelihood) through the maximization of this bound, though we should exercise caution as this introduces bias because the bound is not uniformly tight for all settings of hyperparameters <ref type="bibr" target="#b40">(Turner and Sahani, 2011)</ref> So far we have considered scalar outputs y i ∈ R. In the case of D-dimensional outputs y i ∈ R D we define Y as the matrix with ith row containing the ith observation y i . Similarly, we define F and U. If each output is an independent GP we have the GP prior</p><formula xml:id="formula_13">D d=1 p(F d |U d ; X, Z)p(U d ; Z)</formula><p>, which we abbreviate as p(F|U; X, Z)p(U; Z) to lighten the notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Gaussian Processes</head><p>A DGP <ref type="bibr" target="#b9">(Damianou and Lawrence, 2013</ref>) defines a prior recursively on vector-valued stochastic functions F 1 , . . . , F L . The prior on each function F l is an independent GP in each dimension, with input locations given by the noisy corruptions of the function values at the next layer: the outputs of the GPs at layer l are F l d , and the corresponding inputs are F l−1 . The noise between layers is assumed i.i.d. Gaussian. Most presentations of DGPs (see, e.g. <ref type="bibr" target="#b9">Damianou and Lawrence, 2013;</ref><ref type="bibr" target="#b4">Bui et al., 2016)</ref> explicitly parameterize the noisy corruptions separately from the outputs of each GP. Our method of inference does not require us to parameterize these variables separately. For notational convenience, we therefore absorb the noise into the kernel k noisy (x i , x j ) = k(x i , x j ) + σ 2 l δ ij , where δ ij is the Kronecker delta, and σ 2 l is the noise variance between layers. We use D l for the dimension of the outputs at layer l. As with the single-layer case, we have inducing locations Z l−1 at each layer and inducing function values U l for each dimension.</p><p>An instantiation of the process has the joint density</p><formula xml:id="formula_14">p(Y, {F l , U l } L l=1 ) = N i=1 p(y i |f L i ) likelihood L l=1 p(F l |U l ; F l−1 , Z l−1 )p(U l ; Z l−1 ) DGP prior ,<label>(11)</label></formula><p>where we define F 0 = X. Inference in this model is intractable, so approximations must be used.</p><p>The original DGP presentation <ref type="bibr" target="#b9">(Damianou and Lawrence, 2013</ref>) uses a variational posterior that maintains the exact model conditioned on U l , but further forces the inputs to each layer to be independent from the outputs of the previous layer. The noisy corruptions are parameterized separately, and the variational distribution over these variables is a fully factorized Gaussian. This approach</p><formula xml:id="formula_15">requires 2N (D 1 + · · · + D L−1 )</formula><p>variational parameters but admits a tractable lower bound on the log marginal likelihood if the kernel is of a particular form. A further problem of this bound is that the density over the outputs is simply a single layer GP with independent Gaussian inputs. Since the posterior loses all the correlations between layers it cannot express the complexity of the full model and so is likely to underestimate the variance. In practice, we found that optimizing the objective in <ref type="bibr" target="#b9">Damianou and Lawrence (2013)</ref> results in layers being 'turned off' (the signal to noise ratio tends to zero). In contrast, our posterior retains the full conditional structure of the true model. We sacrifice analytical tractability, but due to the sparse posterior within each layer we can sample the bound using univariate Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Doubly Stochastic Variational Inference</head><p>In this section, we propose a novel variational posterior and demonstrate a method to obtain unbiased samples from the resulting lower bound. The difficulty with inferring the DGP model is that there are complex correlations both within and between layers. Our approach is straightforward: we use sparse variational inference to simplify the correlations within layers, but we maintain the correlations between layers. The resulting variational lower bound cannot be evaluated analytically, but we can draw unbiased samples efficiently using univariate Gaussians. We optimize our bound stochastically.</p><p>We propose a posterior with three properties. Firstly, the posterior maintains the exact model, conditioned on U l . Secondly, we assume that the posterior distribution of</p><formula xml:id="formula_16">{U l } L l=1</formula><p>is factorized between layers (and dimension, but we suppress this from the notation). Therefore, our posterior takes the simple factorized form</p><formula xml:id="formula_17">q({F l , U l } L l=1 ) = L l=1 p(F l |U l ; F l−1 , Z l−1 )q(U l ) .<label>(12)</label></formula><p>Thirdly, and to complete specification of the posterior, we take q(U l ) to be a Gaussian with mean m l and variance S l . A similar posterior was used in <ref type="bibr" target="#b19">Hensman and Lawrence (2014)</ref> and , but each of these works contained additional terms for the noisy corruptions at each layer.</p><p>As in the single layer SGP, we can marginalize the inducing variables from each layer analytically. After this marginalization we obtain following distribution, which is fully coupled within and between layers:</p><formula xml:id="formula_18">q({F l } L l=1 ) = L l=1 q(F l |m l , S l ; F l−1 , Z l−1 ) = L l=1 N (F l |μ l ,Σ l ) .<label>(13)</label></formula><p>Here, q(</p><formula xml:id="formula_19">F l |m l , S l ; F l−1 , Z l−1 )</formula><p>is as in (6). Specifically, it is a Gaussian with mean and varianceμ</p><formula xml:id="formula_20">l andΣ l , where [μ l ] i = µ m l ,Z l−1 (f l i ) and [Σ l ] ij = Σ S l ,Z l−1 (f l i , f l j ) (recall that f l i</formula><p>is the ith row of F l ). Since (12) is a product of terms that each take the form of the SGP variational posterior (5), we have again the property that within each layer the marginals depend on only the corresponding inputs. In particular, f , and so on. Therefore, we have the following property: Remark 2. The ith marginal of the final layer of the variational DGP posterior (12) depends only on the ith marginals of all the other layers. That is,</p><formula xml:id="formula_21">q(f L i ) = L−1 l=1 q(f l i |m l , S l ; f l−1 i , Z l−1 )df l i .<label>(14)</label></formula><p>The consequence of this property is that taking a sample from q(f L i ) is straightforward, and furthermore we can perform the sampling using only univariate unit Gaussians using the 're-parameterization trick' <ref type="bibr" target="#b36">(Rezende et al., 2014;</ref><ref type="bibr" target="#b24">Kingma et al., 2015)</ref>. Specifically, we first sample  </p><formula xml:id="formula_22">i ∼ q(f l i |m l , S l ;f l−1 i , Z l−1 ) for l = 1, . . . , L − 1 aŝ f l i = µ m l ,Z l−1 (f l−1 i ) + l i Σ S l ,Z l−1 (f l−1 i ,f l−1 i ) ,<label>(15)</label></formula><p>where the terms in (15) are D l -dimensional and the square root is element-wise. For the first layer we definef</p><formula xml:id="formula_23">0 i := x i .</formula><p>Efficient computation of the evidence lower bound The evidence lower bound of the DGP is</p><formula xml:id="formula_24">L DGP = E q({F l ,U l } L l=1 ) p(Y, {F l , U l } L l=1 ) q({F l , U l } L l=1 ) .<label>(16)</label></formula><p>Using <ref type="formula" target="#formula_1">(11)</ref> and <ref type="formula" target="#formula_1">(12)</ref> for the corresponding expressions in (16), we obtain after some re-arranging</p><formula xml:id="formula_25">L DGP = N i=1 E q(f L i ) [log p(y n |f L n )] − L l=1 KL[q(U l )||p(U l ; Z l−1 )] ,<label>(17)</label></formula><p>where we exploited the exact marginalization of the inducing variables (13) and the property of the marginals of the final layer (14). A detailed derivation is provided in the supplementary material. This bound has complexity</p><formula xml:id="formula_26">O(N M 2 (D 1 + · · · + D L )) to evaluate.</formula><p>We evaluate the bound (17) approximately using two sources of stochasticity. Firstly, we approximate the expectation with a Monte Carlo sample from the variational posterior (14), which we compute according to (15). Since we have parameterized this sampling procedure in terms of isotropic Gaussians, we can compute unbiased gradients of the bound (17). Secondly, since the bound factorizes over the data we achieve scalability through sub-sampling the data. Both stochastic approximations are unbiased.</p><p>Predictions To predict we sample from the variational posterior changing the input locations to the test location x * . We denote the function values at the test location as f l * . To obtain the density over f L * we use the Gaussian mixture</p><formula xml:id="formula_27">q(f L * ) ≈ 1 S S s=1 q(f L * |m L , S L ; f (s) * L−1 , Z L−1 ) ,<label>(18)</label></formula><p>where we draw S samples f Further Model Details While GPs are often used with a zero mean function, we consider such a choice inappropriate for the inner layers of a DGP. Using a zero mean function causes difficulties with the DGP prior as each GP mapping is highly non-injective. This effect was analyzed in <ref type="bibr" target="#b14">Duvenaud et al. (2014)</ref> where the authors suggest adding the original input X to each layer. Instead, we consider an alternative approach and include a linear mean function m(X) = XW for all the inner layers.</p><p>If the input and output dimension are the same we use the identity matrix for W, otherwise we compute the SVD of the data and use the top D l left eigenvectors sorted by singular value (i.e. the PCA mapping). With these choices it is effective to initialize all inducing mean values m l = 0. This choice of mean function is partly inspired by the 'skip layer' approach of the ResNet <ref type="bibr" target="#b18">(He et al., 2016)</ref> architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We evaluate our inference method on a number of benchmark regression and classification datasets. We stress that we are interested in models that can operate in both the small and large data regimes, with little or no hand tuning. All our experiments were run with exactly the same hyperparameters and initializations. See the supplementary material for details. We use min(30, D 0 ) for all the inner layers of our DGP models, where D 0 is the input dimension, and the RBF kernel for all layers.</p><p>Regression Benchmarks We compare our approach to other state-of-the-art methods on 8 standard small to medium-sized UCI benchmark datasets. Following common practice (e.g. Hernández-Lobato and Adams, 2015) we use 20-fold cross validation with a 10% randomly selected held out test set and scale the inputs and outputs to zero mean and unit standard deviation within the training set (we restore the output scaling for evaluation). While we could use any kernel, we choose the RBF kernel with a lengthscale for each dimension for direct comparison with <ref type="bibr" target="#b4">Bui et al. (2016)</ref>. The test log-likelihood results are shown in <ref type="figure" target="#fig_4">Fig. 1</ref>. We compare our models of 2, 3, 4 and 5 layers (DGP 2-5), each with 100 inducing points, with (stochastically optimized) sparse GPs <ref type="bibr" target="#b20">(Hensman et al., 2013)</ref> with 100 and 500 inducing points points (SGP, SGP 500). We compare also to a two-layer Bayesian neural network with ReLu activations, 50 hidden units (100 for protein and year), with inference by probabilistic backpropagation (Hernández-Lobato and Adams, 2015) (PBP). The results are taken from Hernández-Lobato and Adams <ref type="formula" target="#formula_1">(2015)</ref> and were found to be the most effective of several other methods for inferring Bayesian neural networks. We compare also with a DGP model with approximate expectation propagation (EP) for inference <ref type="bibr" target="#b4">(Bui et al., 2016)</ref>. Using the authors' code 2 we ran a DGP model with 1 hidden layer using approximate expectation propagation <ref type="bibr" target="#b4">(Bui et al., 2016)</ref> (AEPDGP 2). We used the input dimension for the hidden layer for a fair comparison with our models 3 . We found the time requirements to train a 3-layer model with this inference prohibitive. Plots for test RMSE and further results tables can be found in the supplementary material.</p><p>On five of the eight datasets, the deepest DGP model is the best. On 'wine', 'naval' and 'boston' our DGP recovers the single-layer GP, which is not surprising: 'boston' is very small, 'wine' is near-linear (note the proximity of the linear model and the scale) and 'naval' is characterized by extremely high test likelihoods (the RMSE on this dataset is less than 0.001 for all SGP and DGP models), i.e. it is a very 'easy' dataset for a GP. The Bayesian network is not better than the sparse GP for any dataset and significantly worse for six. The Approximate EP inference for the DGP models is also not competitive with the sparse GP for many of the datasets, but this may be because the initializations were designed for lower dimensional hidden layers than we used.</p><p>Our results on these small and medium sized datasets confirm that overfitting is not observed with the DGP model, and that the DGP is never worse and often better than the single layer GP. We note in particular that on the 'power', 'protein' and 'kin8nm' datasets all the DGP models outperform the SGP with five times the number of inducing points.</p><p>Rectangles Benchmark We use the Rectangle-Images dataset 4 , which is specifically designed to distinguish deep and shallow architectures. The dataset consists of 12,000 training and 50,000 testing examples of size 28 × 28, where each image consists of a (non-square) rectangular image against a different background image. The task is to determine which of the height and width is greatest. We run 2, 3 and 4 layer DGP models, and observe increasing performance with each layer. <ref type="table" target="#tab_1">Table 1</ref> contains the results. Note that the 500 inducing point single-layer GP is significantly less effective than any of the deep models. Our 4-layer model achieves 77.9% classification accuracy, exceeding the best result of 77.5% reported in <ref type="bibr" target="#b27">Larochelle et al. (2007)</ref> with a three-layer deep belief network. We also exceed the best result of 76.4% reported in  using a sparse GP with an Arcsine kernel, a leave-one-out objective, and 1000 inducing points. Large-Scale Regression To demonstrate our method on a large scale regression problem we use the UCI 'year' dataset and the 'airline' dataset, which has been commonly used by the large-scale GP community. For the 'airline' dataset we take the first 700K points for training and next 100K for testing. We use a random 10% split for the 'year' dataset. Results are shown in <ref type="table" target="#tab_2">Table 2</ref>, with the log-likelihood reported in the supplementary material. In both datasets we see that the DGP models perform better with increased depth, significantly improving in both log likelihood and RMSE over the single-layer model, even with 500 inducing points. MNIST Multiclass Classification We apply the DGP with 2 and 3 layers to the MNIST multiclass classification problem. We use the robust-max multiclass likelihood <ref type="bibr" target="#b22">(Hernández-Lobato et al., 2011)</ref> and use full unprocessed data with the standard training/test split of 60K/10K. The single-layer GP with 100 inducing points achieves a test accuracy of 97.48% and this is increased to 98.06% and 98.11% with two and three layer DGPs, respectively. The 500 inducing point single layer model achieved 97.9% in our implementation, though a slightly higher result for this model has previously been reported of 98.1% <ref type="bibr" target="#b20">(Hensman et al., 2013)</ref> and 98.4%  for the same model with 1000 inducing points. We attribute this difference to different hyperparameter initialization and training schedules, and stress that we use exactly the same initialization and learning schedule for all our models. The only other DGP result in the literature on this dataset is 94.24% <ref type="bibr" target="#b42">(Wang et al., 2016)</ref> for a two layer model with a two dimensional latent space.</p><p>Large-Scale Classification We use the HIGGS (N = 11M, D = 28) and SUSY (N = 5.5M, D = 18) datasets for large-scale binary classification. These datasets have been constructed from Monte Carlo physics simulations to detect the presence of the Higgs boson and super-symmetry <ref type="bibr" target="#b1">(Baldi et al., 2014)</ref>. We take a 10% random sample for testing and use the rest for training. We use the AUC metric for comparison with <ref type="bibr" target="#b1">Baldi et al. (2014)</ref>. Our DGP models are the highest performing on the SUSY dataset (AUC of 0.877 for all the DGP models) compared to shallow neural networks <ref type="formula">(</ref>   <ref type="formula" target="#formula_1">(2017)</ref> we use 9 features: time of day; day of the week; day of the month; month; pick-up latitude and longitude; drop-off latitude and longitude; travel distance. The target is to predict the journey time. We randomly select 1B (10 9 ) examples for training and use 1M examples for testing, and we scale both inputs and outputs to zero mean and unit standard deviation in the training data. We discard journeys that are less than 10 s or greater than 5 h, or start/end outside the New York region, which we estimate to have squared distance less than 5 o from the center of New York. The test RMSE results are the bottom row of <ref type="table" target="#tab_2">Table 2</ref> and test log likelihoods are in the supplementary material. We note the significant jump in performance from the single layer models to the DGP. As with all the large-scale experiments, we see a consistent improvement extra layers, but on this dataset the improvement is particularly striking (DGP 5 achieves a 21% reduction in RMSE compared to SGP)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The first example of the outputs of a GP used as the inputs to another GP can be found in <ref type="bibr" target="#b28">Lawrence and Moore (2007)</ref>. MAP approximation was used for inference. The seminal work of <ref type="bibr" target="#b38">Titsias and Lawrence (2010)</ref> demonstrated how sparse variational inference could be used to propagate Gaussian inputs through a GP with a Gaussian likelihood. This approach was extended in <ref type="bibr" target="#b10">Damianou et al. (2011)</ref> to perform approximate inference in the model of <ref type="bibr" target="#b28">Lawrence and Moore (2007)</ref>, and shortly afterwards in a similar model <ref type="bibr" target="#b29">Lázaro-Gredilla (2012)</ref>, which also included a linear mean function. The key idea of both these approaches is the factorization of the variational posterior between layers. A more general model (flexible in depth and dimensions of hidden layers) introduced the term 'DGP' and used a posterior that also factorized between layers. These approaches require a linearly increasing number of variational parameters in the number of data. For high-dimensional observations, it is possible to amortize the cost of this optimization with an auxiliary model. This approach is pursued in , and with a recurrent architecture in <ref type="bibr" target="#b33">Mattos et al. (2016)</ref>. Another approach to inference in the exact model was presented in <ref type="bibr" target="#b19">Hensman and Lawrence (2014)</ref>, where a sparse approximation was used within layers for the GP outputs, similar to <ref type="bibr" target="#b9">Damianou and Lawrence (2013)</ref>, but with a projected distribution over the inputs to the next layer. The particular form of the variational distribution was chosen to admit a tractable bound, but imposes a constraint on the flexibility.</p><p>An alternative approach is to modify the DGP prior directly and perform inference in a parametric model. This is achieved in <ref type="bibr" target="#b4">Bui et al. (2016)</ref> with an inducing point approximation within each layer, and in <ref type="bibr" target="#b6">Cutajar et al. (2017)</ref> with an approximation to the spectral density of the kernel. Both approaches then apply additional approximations to achieve tractable inference. In <ref type="bibr" target="#b4">Bui et al. (2016)</ref>, an approximation to expectation propagation is used, with additional Gaussian approximations to the log partition function to propagate uncertainly through the non-linear GP mapping. In <ref type="bibr" target="#b6">Cutajar et al. (2017)</ref> a fully factorized variational approximation is used for the spectral components. Both these approaches require specific kernels: in <ref type="bibr" target="#b4">Bui et al. (2016)</ref> the kernel must have analytic expectations under a Gaussian, and in <ref type="bibr" target="#b6">Cutajar et al. (2017)</ref> the kernel must have an analytic spectral density. <ref type="bibr" target="#b41">Vafa (2016)</ref> also uses the same initial approximation as <ref type="bibr" target="#b4">Bui et al. (2016)</ref> but applies MAP inference for the inducing points, such that the uncertainty propagated through the layers only represents the quality of the approximation. In the limit of infinitely many inducing points this approach recovers a deterministic radial basis function network. A particle method is used in <ref type="bibr" target="#b42">Wang et al. (2016)</ref>, again employing an online version of the sparse approximation used by <ref type="bibr" target="#b4">Bui et al. (2016)</ref> within each layer. Similarly to our approach, in <ref type="bibr" target="#b42">Wang et al. (2016)</ref> samples are taken through the conditional model, but differently from us they then use a point estimate for the latent variables. It is not clear how this approach propagates uncertainty through the layers, since the GPs at each layer have point-estimate inputs and outputs.</p><p>A pathology with the DGP with zero mean function for the inner layers was identified in <ref type="bibr" target="#b14">Duvenaud et al. (2014)</ref>. In <ref type="bibr" target="#b14">Duvenaud et al. (2014)</ref> a suggestion was made to concatenate the original inputs at each layer. This approach is followed in  and <ref type="bibr" target="#b6">Cutajar et al. (2017)</ref>. The linear mean function was original used by <ref type="bibr" target="#b29">Lázaro-Gredilla (2012)</ref>, though in the special case of a two layer DGP with a 1D hidden layer. To the best of our knowledge there has been no previous attempt to use a linear mean function for all inner layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our experiments show that on a wide range of tasks the DGP model with our doubly stochastic inference is both effective and scalable. Crucially, we observe that on the small datasets the DGP does not overfit, while on the large datasets additional layers generally increase performance and never deteriorate it. In particular, we note that the largest gain with increasing layers is achieved on the largest dataset (the taxi dataset, with 1B points). We note also that on all the large scale experiments the SGP 500 model is outperformed by the all the DGP models. Therefore, for the same computational budget increasing the number of layers can be significantly more effective than increasing the accuracy of approximate inference in the single-layer model. Other than the additional computation time, which is fairly modest (see <ref type="table" target="#tab_3">Table 3</ref>), we do not see downsides to using a DGP over a single-layer GP, but substantial advantages.</p><p>While we have considered simple kernels and black-box applications, any domain-specific kernel could be used in any layer. This is in contrast to other methods <ref type="bibr" target="#b9">(Damianou and Lawrence, 2013;</ref><ref type="bibr" target="#b4">Bui et al., 2016;</ref><ref type="bibr" target="#b6">Cutajar et al., 2017</ref>) that require specific kernels and intricate implementations. Our implementation is simple (&lt; 200 lines), publicly available 6 , and is integrated with GPflow <ref type="bibr" target="#b31">(Matthews et al., 2017)</ref>, an open-source GP framework built on top of Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a new method for inference in Deep Gaussian Process (DGP) models. With our inference we have shown that the DGP can be used on a range of regression and classification tasks with no hand-tuning. Our results show that in practice the DGP always exceeds or matches the performance of a single layer GP. Further, we have shown that the DGP often exceeds the single layer significantly, even when the quality of the approximation to the single layer is improved. Our approach is highly scalable and benefits from GPU acceleration.</p><p>The most significant limitation of our approach is the dealing with high dimensional inner layers. We used a linear mean function for the high dimensional datasets but left this mean function fixed, as to optimize the parameters would go against our non-parametric paradigm. It would be possible to treat this mapping probabilistically, following the work of <ref type="bibr" target="#b39">Titsias and Lázaro-Gredilla (2013)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>I D l ) and then recursively draw the sampled variablesf</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>15), but replacing the inputs x i with the test location x * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Regression test log-likelihood results on benchmark datasets. Higher (to the right) is better. The sparse GP with the same number of inducing points is highlighted as a baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>NN, 0.875), deep neural networks (DNN, 0.876) and boosted decision trees (BDT, 0.863). On the HIGGS dataset we see a steady improvement in additional layers (0.830, 0.837, 0.841 and 0.846 for DGP 2-4 respectively). On this dataset the DGP models exceed the performance of BDT (0.810) and NN (0.816) and both single layer GP models SGP (0.785) and SGP 500 (0.794). The best performing model on this dataset is a 5 layer DNN (0.885). Full results are reported in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Results on Rectangles-Images dataset (N = 12000, D = 784)</figDesc><table>Single layer GP 
Ours 
Larochelle [2007] Krauth [2016] 

SGP 
SGP 500 DGP 2 
DGP 3 
DGP 4 
DBN-3 
SVM 
SGP 1000 

Accuracy (%) 
76.1 
76.4 
77.3 
77.8 
77.9 
77.5 
76.96 
76.4 
Likelihood 
−0.493 −0.485 
0.475 −0.460 −0.460 
-
-
−0.478 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Regression test RMSE results for large datasets</figDesc><table>N 
D 
SGP SGP 500 DGP 2 DGP 3 DGP 4 DGP 5 

year 
463810 90 10.67 
9.89 
9.58 
8.98 
8.93 
8.87 
airline 700K 
8 
25.6 
25.1 
24.6 
24.3 
24.2 
24.1 
taxi 
1B 
9 
337.5 
330.7 
281.4 
270.4 
268.0 
266.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Typical computation 
time in seconds for a single 
gradient step. 
CPU GPU 

SGP 
0.14 0.018 
SGP 500 1.71 
0.11 
DGP 2 
0.36 0.030 
DGP 3 
0.49 0.045 
DGP 4 
0.65 0.056 
DGP 5 
0.87 0.069 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Throughout this paper we use the semi-colon notation to clarify the input locations of the corresponding function values, which will become important later when we discuss multi-layer GP models. For example, p(f |u; X, Z) indicates that the input locations for f and u are X and Z, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/thangbui/deepGP_approxEP 3 We note however that in Bui et al. (2016) the inner layers were 2D, so the results we obtained are not directly comparable to those reported in Bui et al. (2016)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/RectanglesData</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/ICL-SML/Doubly-Stochastic-DGP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We have greatly appreciated valuable discussions with James Hensman and Steindor Saemundsson in the preparation of this work. We thank Vincent Dutordoir and anonymous reviewers for helpful feedback on the manuscript. We are grateful for a Microsoft Azure Scholarship and support through a Google Faculty Research Award to Marc Deisenroth.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno>arXiv preprint:1603.04467</idno>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Searching for Exotic Particles in High-Energy Physics with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dezfouli</surname></persName>
		</author>
		<idno>arXiv preprint:1609.00577</idno>
		<title level="m">Generic Inference in Latent Gaussian Process Models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-X</forename><surname>Briol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<idno>arXiv preprint:1512.00933</idno>
		<title level="m">Probabilistic Integration: A Role for Statisticians in Numerical Analysis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<title level="m">Deep Gaussian Processes for Regression using Approximate Expectation Propagation. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Manifold Gaussian Processes for Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cutajar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<title level="m">Random Feature Expansions for Deep Gaussian Processes. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Reinforcement Learning for Robots using Informative Simulated Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Variational Auto-encoded Deep Gaussian Processes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Deep Gaussian Processes. International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational Gaussian Process Dynamical Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PILCO: A Model-Based and Data-Efficient Approach to Policy Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Model-based Geostatistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Diggle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Ribeiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<title level="m">Structure Discovery in Nonparametric Regression through Compositional Kernel Search. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Avoiding Pathologies in Very Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequential Bayesian Prediction in the Presence of Changepoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<title level="m">Near-optimal Sensor Placements in Gaussian Processes. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nested Variational Compression in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno>arXiv preprint:1412.1370</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Gaussian Processes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian Processes for Big Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fillipone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<title level="m">MCMC for Variationally Sparse Gaussian Processes. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust Multi-class Gaussian Process Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<title level="m">Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Variational Dropout and the Local Reparameterization Trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GP-BayesFilters: Bayesian Filtering using Gaussian Process Prediction and Observation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cutajar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<idno>arXiv preprint:1610.05392</idno>
		<title level="m">AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Moore</surname></persName>
		</author>
		<title level="m">Hierarchical Gaussian Process Latent Variable Models. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<title level="m">Bayesian Warped Gaussian Processes. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparison of Approximate Methods for Handling Hyperparameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GPflow: A Gaussian process library using TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boukouvalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On Sparse Variational Methods and The Kullback-Leibler Divergence Between Stochastic Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L C</forename><surname>Mattos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Recurrent Gaussian Processes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno>arXiv preprint:1704.06735</idno>
		<title level="m">Asynchronous Distributed Variational Gaussian Processes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Bayesian Gaussian Process Latent Variable Model. International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two Problems with Variational Expectation Maximisation for Time-Series Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Time Series Models</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training Deep Gaussian Processes with Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Approximate Bayesian Inference Workshop, Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequential Inference for Deep Gaussian Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Kernel Learning. Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
