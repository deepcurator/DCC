<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
							<email>m.weiler@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Qualcomm AI Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<orgName type="institution" key="instit3">CIFAR</orgName>
								<orgName type="institution" key="instit4">AI Research</orgName>
								<orgName type="institution" key="instit5">University of Copenhagen</orgName>
								<address>
									<region>Qualcomm</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Geiger</surname></persName>
							<email>epflmario.geiger@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Qualcomm AI Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<orgName type="institution" key="instit3">CIFAR</orgName>
								<orgName type="institution" key="instit4">AI Research</orgName>
								<orgName type="institution" key="instit5">University of Copenhagen</orgName>
								<address>
									<region>Qualcomm</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<email>m.welling@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Qualcomm AI Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<orgName type="institution" key="instit3">CIFAR</orgName>
								<orgName type="institution" key="instit4">AI Research</orgName>
								<orgName type="institution" key="instit5">University of Copenhagen</orgName>
								<address>
									<region>Qualcomm</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Qualcomm AI Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<orgName type="institution" key="instit3">CIFAR</orgName>
								<orgName type="institution" key="instit4">AI Research</orgName>
								<orgName type="institution" key="instit5">University of Copenhagen</orgName>
								<address>
									<region>Qualcomm</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
							<email>taco.cohen@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Qualcomm AI Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<orgName type="institution" key="instit3">CIFAR</orgName>
								<orgName type="institution" key="instit4">AI Research</orgName>
								<orgName type="institution" key="instit5">University of Copenhagen</orgName>
								<address>
									<region>Qualcomm</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R 3 . Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Increasingly, machine learning techniques are being applied in the natural sciences. Many problems in this domain, such as the analysis of protein structure, exhibit exact or approximate symmetries. It has long been understood that the equations that define a model or natural law should respect the symmetries of the system under study, and that knowledge of symmetries provides a powerful constraint on the space of admissible models. Indeed, in theoretical physics, this idea is enshrined as a fundamental principle, known as Einstein's principle of general covariance. Machine learning, which is, like physics, concerned with the induction of predictive models, is no different: our models must respect known symmetries in order to produce physically meaningful results.</p><p>A lot of recent work, reviewed in Sec. 2, has focused on the problem of developing equivariant networks, which respect some known symmetry. In this paper, we develop the theory of SE(3)-equivariant networks. This is far from trivial, because SE(3) is both non-commutative and noncompact. Nevertheless, at run-time, all that is required to make a 3D convolution equivariant using our method, is to parameterize the convolution kernel as a linear combination of pre-computed steerable basis kernels. Hence, the 3D Steerable CNN incorporates equivariance to symmetry transformations without deviating far from current engineering best practices.</p><p>The architectures presented here fall within the framework of Steerable G-CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>, which represent their input as fields over a homogeneous space (R 3 in this case), and use steerable</p><p>The most closely related works achieving full SE(3) equivariance are the Tensor Field Network (TFN) <ref type="bibr" target="#b40">[41]</ref> and the N-Body networks (NBNs) <ref type="bibr" target="#b26">[27]</ref>. The main difference between 3D Steerable CNNs and both TFN and NBN is that the latter work on irregular point clouds, whereas our model operates on regular 3D grids. Point clouds are more general, but regular grids can be processed more efficiently on current hardware. The second difference is that whereas the TFN and NBN use Clebsch-Gordan coefficients to parameterize the network, we simply parameterize the convolution kernel as a linear combination of steerable basis filters. Clebsch-Gordan coefficient tensors have 6 indices, and depend on various phase and normalization conventions, making them tricky to work with. Our implementation requires only a very minimal change from the conventional 3D CNN. Specifically, we compute conventional 3D convolutions with filters that are a linear combination of pre-computed basis filters. Further, in contrast to TFN, we derive this filter basis directly from an equivariance constraint and can therefore prove its completeness.</p><p>The two dimensional analog of our work is the SE(2) equivariant harmonic network <ref type="bibr" target="#b45">[46]</ref>. The harmonic network and 3D steerable CNN use features that transform under irreducible representations of SO(2) resp. SO <ref type="bibr" target="#b2">(3)</ref>, and use filters related to the circular resp. spherical harmonics.</p><p>SE(3) equivariant models were already investigated in classical computer vision and signal processing. In <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39]</ref>, a spherical tensor algebra was utilized to expand signals in terms of spherical tensor fields. In contrast to 3D Steerable CNNs, this expansion is fixed and not learned. Similar approaches were used for detection and crossing preserving enhancement of fibrous structures in volumetric biomedical images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional feature spaces as fields</head><p>A convolutional network produces a stack of K n feature maps f k in each layer n. In 3D, we can model the feature maps as (well-behaved) functions f k : R 3 → R. Written another way, we have a map f : R 3 → R Kn that assigns to each position x a feature vector f (x) that lives in what we call the fiber R Kn at x. In practice f will have compact support, meaning that f (x) = 0 outside of some compact domain Ω ∈ R 3 . We thus define the feature space F n as the vector space of continuous maps from R 3 to R Kn with compact support.</p><p>In this paper, we impose additional structure on the fibers. Specifically, we assume the fiber consists of a number of geometrical quantities, such as scalars, vectors, and tensors, stacked into a single</p><formula xml:id="formula_0">K n -dimensional vector.</formula><p>The assignment of such a geometrical quantity to each point in space is called a field. Thus, the feature spaces consist of a number of fields, each of which consists of a number of channels (dimensions).</p><p>Before deriving SE(3)-equivariant networks in Sec. 4 we discuss the transformation properties of fields and the kinds of fields we use in 3D Steerable CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fields, Transformations and Disentangling</head><p>What makes a geometrical quantity (e.g. a vector) anything more than an arbitrary grouping of feature channels? The answer is that under rigid body motions, information flows within the channels of a single geometrical quantity, but not between different quantities. This idea is known as Weyl's principle, and has been proposed as a way of formalizing the notion of disentangling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. • rotation g, first move each arrow to its new position (C), keeping its orientation the same, then rotate the vector itself (R). This is described by the induced representation π = Ind</p><formula xml:id="formula_1">SE(2)</formula><p>SO(3) ρ, where ρ(g) is a 3 × 3 rotation matrix that mixes the three coordinate channels.</p><p>As an example, consider the three-dimensional vector field over R 3 , shown in <ref type="figure" target="#fig_0">Figure 1</ref>. At each point x ∈ R 3 there is a vector f (x) of dimension K = 3. If the field is translated by t, each vector x − t would simply move to a new (translated) position x. When the field is rotated, however, two things happen: the vector at r −1 x is moved to a new (rotated) position x, and each vector is itself rotated by a 3 × 3 rotation matrix ρ(r). Thus, the rotation operator π(r) for vector fields is defined as [π(r)f ](x) := ρ(r)f (r −1 x). Notice that in order to rotate this field, we need all three channels: we cannot rotate each channel independently, because ρ introduces a functional dependency between them. For contrast, consider the common situation where in the input space we have an RGB image with K = 3 channels. Then f (x) ∈ R 3 , and the rotation can be described using the same formula ρ(r)f (r −1 x) if we choose ρ(r) = I 3 to be the 3 × 3 identity matrix for all r. Since ρ(r) is diagonal for all r, the channels do not get mixed, and so in geometrical terms, we would describe this feature space as consisting of three scalar fields, not a 3D vector field. The RGB channels each have an independent physical meaning, while the x and y coordinate channels of a vector do not.</p><p>The RGB and 3D-vector cases constitute two examples of fields, each one determined by a different choice of ρ. As one might guess, there is a one-to-one correspondence between the type of field and the type of transformation law (group representation) ρ. Hence, we can speak of a ρ-field.</p><p>So far, we have concentrated on the behaviour of a field under rotations and translations separately. A 3D rigid body motion g ∈ SE(3) can always be decomposed into a rotation r ∈ SO(3) and a translation t ∈ R 3 , written as g = tr. So the transformation law for a ρ-field is given by the formula</p><formula xml:id="formula_2">[π(tr)f ](x) := ρ(r)f (r −1 (x − t)).<label>(1)</label></formula><p>The map π is known as the representation of SE(3) induced by the representation ρ of SO(3), which is denoted by π = Ind</p><formula xml:id="formula_3">SE(3)</formula><p>SO <ref type="formula" target="#formula_39">(3)</ref> ρ. For more information on induced representations, see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Irreducible SO(3) features</head><p>We have seen that there is a correspondence between the type of field and the type of inducing representation ρ, which describes the rotation behaviour of a single fiber. To get a better understanding of the space of possible fields, we will now define precisely what it means to be a representation of SO(3), and explain how any such representation can be constructed from elementary building blocks called irreducible representations.</p><p>A group representation ρ assigns to each element in the group an invertible n × n matrix. Here n is the dimension of the representation, which can be any positive integer (or even infinite). For ρ to be called a representation of G, it has to satisfy ρ(gg ) = ρ(g)ρ(g ), where gg denotes the composition of two transformations g, g ∈ G, and ρ(g)ρ(g ) denotes matrix multiplication.</p><p>To make this more concrete, and to introduce the concept of an irreducible representation, we consider the classical example of a rank-2 tensor (i.e. matrix). A 3 × 3 matrix A transforms under rotations as A → R(r)AR(r) T , where R(r) is the 3 × 3 rotation matrix representation of the abstract group element r ∈ SO(3). This can be written in matrix-vector form using the Kronecker / tensor product:</p><formula xml:id="formula_4">vec(A) → [R(r) ⊗ R(r)] vec(A) ≡ ρ(r) vec(A)</formula><p>. This is a 9-dimensional representation of SO(3).</p><p>One can easily verify that the symmetric and anti-symmetric parts of A remain symmetric respectively anti-symmetric under rotations. This splits R 3×3 into 6-and 3-dimensional linear subspaces that transform independently. According to Weyl's principle, these may be considered as distinct quantities, even if it is not immediately visible by looking at the coordinates A ij . The 6-dimensional space can be further broken down, because scalar matrices A ij = αδ ij (which are invariant under rotation) and traceless symmetric matrices also transform independently. Thus a rank-2 tensor decomposes into representations of dimension 1 (trace), 3 (anti-symmetric part), and 5 (traceless symmetric part). In representation-theoretic terms, we have reduced the 9-dimensional representation ρ into irreducible representations of dimension 1, 3 and 5. We can write this as</p><formula xml:id="formula_5">ρ(r) = Q −1 2 l=0 D l (r) Q,<label>(2)</label></formula><p>where we use to denote the construction of a block-diagonal matrix with blocks D l (r), and Q is a change of basis matrix that extracts the trace, symmetric-traceless and anti-symmetric parts of A.</p><p>More generally, it can be shown that any representation of SO(3) can be decomposed into irreducible representations of dimension 2l + 1, for l = 0, 1, 2, . . . , ∞. The irreducible representation acting on this 2l + 1 dimensional space is known as the Wigner-D matrix of order l, denoted D l (r). Note that the Wigner-D matrix of order 4 is a representation of dimension 9, it has the same dimension as the representation ρ acting on A but these are two different representations.</p><p>Since any SO(3) representation can be decomposed into irreducibles, we only use irreducible features in our networks. This means that the feature vector f (x) in layer n is a stack of</p><formula xml:id="formula_6">F n features f i (x) ∈ R 2li+1 , so that K n = Fn i=1 2l in + 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SE(3)-Equivariant Networks</head><p>Our general approach to building SE(3)-equivariant networks will be as follows: First, we will specify for each layer n a linear transformation law π n (g) : F n → F n , which describes how the feature space F n transforms under transformations of the input by g ∈ SE(3). Then, we will study the vector space Hom SE(3) (F n , F n+1 ) of equivariant linear maps (intertwiners) Φ between adjacent feature spaces:</p><formula xml:id="formula_7">Hom SE(3) (F n , F n+1 ) = {Φ ∈ Hom(F n , F n+1 ) | Φπ n (g) = π n+1 (g)Φ, ∀g ∈ SE(3)} (3)</formula><p>Here Hom(F n , F n+1 ) is the space of linear (not necessarily equivariant) maps from F n to F n+1 .</p><p>By finding a basis for the space of intertwiners and parameterizing Φ n as a linear combination of basis maps, we can make sure that layer n + 1 transforms according to π n+1 if layer n transforms according to π n , thus guaranteeing equivariance of the whole network by induction.</p><p>As explained in the previous section, fields transform according to induced representations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. In this section we show that equivariant maps between induced representations of SE(3) can always be expressed as convolutions with equivariant / steerable filter banks. The space of equivariant filter banks turns out to be a linear subspace of the space of filter banks of a conventional 3D CNN. The filter banks of our network are expanded in terms of a basis of this subspace with parameters corresponding to expansion coefficients.</p><p>Sec. 4.1 derives the linear constraint on the kernel space for arbitrary induced representations. From Sec. 4.2 on we specialize to representations induced from irreducible representations of SO <ref type="formula" target="#formula_39">(3)</ref> and derive a basis of the equivariant kernel space for this choice analytically. Subsequent sections discuss choices of equivariant nonlinearities and the actual discretized implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Subspace of Equivariant Kernels</head><p>A continuous linear map between F n and F n+1 can be written using a continuous kernel κ with signature κ :</p><formula xml:id="formula_8">R 3 × R 3 → R Kn+1×Kn</formula><p>, as follows:</p><formula xml:id="formula_9">[κ · f ](x) = R 3 κ(x, y)f (y)dy (4)</formula><p>Lemma 1. The map f → κ · f is equivariant if and only if for all g ∈ SE(3),</p><formula xml:id="formula_10">κ(gx, gy) = ρ 2 (r)κ(x, y)ρ 1 (r) −1 ,<label>(5)</label></formula><p>Proof. For this map to be equivariant, it must satisfy</p><formula xml:id="formula_11">κ · [π 1 (g)f ] = π 2 (g)[κ · f ].</formula><p>Expanding the left hand side of this constraint, using g = tr, and the substitution y → gy, we find:</p><formula xml:id="formula_12">κ · [π 1 (g)f ](x) = R 3 κ(x, gy)ρ 1 (r)f (y)dy<label>(6)</label></formula><p>For the right hand side,</p><formula xml:id="formula_13">π 2 (g)[κ · f ](x) = ρ 2 (r) R 3 κ(g −1 x, y)f (y)dy.<label>(7)</label></formula><p>Equating these, and using that the equality has to hold for arbitrary f ∈ F n , we conclude:</p><formula xml:id="formula_14">ρ 2 (r)κ(g −1 x, y) = κ(x, gy)ρ 1 (r).<label>(8)</label></formula><p>Substitution of x → gx and right-multiplication by ρ 1 (r) −1 yields the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.</head><p>A linear map from F n to F n+1 is equivariant if and only if it is a cross-correlation with a rotation-steerable kernel.</p><p>Proof. Lemma 1 implies that we can write κ in terms of a one-argument kernel, since for g = −x :</p><formula xml:id="formula_15">κ(x, y) = κ(0, y − x) ≡ κ(y − x).<label>(9)</label></formula><p>Substituting this into Equation 4, we find</p><formula xml:id="formula_16">[κ · f ](x) = R 3 κ(x, y)f (y)dy = R 3 κ(y − x)f (y)dy = [κ f ](x).<label>(10)</label></formula><p>Cross-correlation is always translation-equivariant, but Eq. 5 still constrains κ rotationally:</p><formula xml:id="formula_17">κ(rx) = ρ 2 (r)κ(x)ρ 1 (r) −1 .<label>(11)</label></formula><p>A kernel satisfying this constraint is called rotation-steerable.</p><p>We note that κ f (Eq. 10) is exactly the operation used in a conventional convolutional network, just written in an unconventional form, using a matrix-valued kernel ("propagator") κ :</p><formula xml:id="formula_18">R 3 → R Kn+1×Kn .</formula><p>Since Eq. 11 is a linear constraint on the correlation kernel κ, the space of equivariant kernels (i.e. those satisfying Eq. 11) forms a vector space. We will now proceed to compute a basis for this space, so that we can parameterize the kernel as a linear combination of basis kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solving for the Equivariant Kernel Basis</head><p>As mentioned before, we assume that the</p><formula xml:id="formula_19">K n -dimensional feature vectors f (x) = ⊕ i f i (x) consist of irreducible features f i (x) of dimension 2 l in + 1.</formula><p>In other words, the representation ρ n (r) that acts on fibers in layer n is block-diagonal, with irreducible representation D lin (r) as the i-th block. This implies that the kernel κ :</p><formula xml:id="formula_20">R 3 → R Kn+1×Kn splits into blocks 1 κ jl : R 3 → R (2j+1)×(2l+1)</formula><p>mapping between irreducible features. The blocks themselves are by Eq. 11 constrained to transform as</p><formula xml:id="formula_21">κ jl (rx) = D j (r)κ jl (x)D l (r) −1 .<label>(12)</label></formula><p>1 For more details on the block structure see Sec. 2.7 of <ref type="bibr" target="#b9">[10]</ref> Figure 2: Angular part of the basis for the space of steerable kernels κ jl (for j = l = 1, i.e. 3D vector fields as input and output). From left to right we plot three 3 × 3 matrices, for j − l ≤ J ≤ j + l i.e. J = 0, 1, 2. Each 3 × 3 matrix corresponds to one learnable parameter per radial basis function ϕ m . A seasoned eye will see the identity, the curl (∇∧) and the gradient of the divergence (∇∇·).</p><p>To bring this constraint into a more manageable form, we vectorize these kernel blocks to vec(κ jl (x)), so that we can rewrite the constraint as a matrix-vector equation</p><formula xml:id="formula_22">2 vec(κ jl (rx)) = [D j ⊗ D l ](r) vec(κ jl (x)),<label>(13)</label></formula><p>where we used the orthogonality of D l . The tensor product of representations is itself a representation, and hence can be decomposed into irreducible representations. For irreducible SO(3) representations D j and D l of order j and l it is well known <ref type="bibr" target="#b16">[17]</ref> that D j ⊗ D l can be decomposed in terms of 2 min(j, l) + 1 irreducible representations of order 3 |j − l| ≤ J ≤ j + l. That is, we can find a change of basis matrix <ref type="bibr" target="#b3">4</ref> Q of shape (2l + 1)(2j + 1) × (2l + 1)(2j + 1) such that the representation becomes block diagonal:</p><formula xml:id="formula_23">[D j ⊗ D l ](r) = Q T j+l J=|j−l| D J (r) Q<label>(14)</label></formula><p>Thus, we can change the basis to η jl (x) := Q vec(κ jl (x)) such that constraint 12 becomes</p><formula xml:id="formula_24">η jl (rx) = j+l J=|j−l| D J (r) η jl (x).<label>(15)</label></formula><p>The block diagonal form of the representation in this basis reveals that η jl decomposes into 2 min(j, l) + 1 invariant subspaces of dimension 2J + 1 with separated constraints:</p><formula xml:id="formula_25">η jl (x) = j+l J=|j−l| η jl,J (x) , η jl,J (rx) = D J (r)η jl,J (x)<label>(16)</label></formula><p>This is a famous equation for which the unique and complete solution is well-known to be given by the spherical harmonics</p><formula xml:id="formula_26">Y J (x) = (Y J −J (x), . . . , Y J J (x)) ∈ R 2J+1</formula><p>. More specifically, since x lives in R 3 instead of the sphere, the constraint only restricts the angular part of η jl but leaves its radial part free. Therefore, the solutions are given by spherical harmonics modulated by an arbitrary continuous radial function ϕ :</p><formula xml:id="formula_27">R + → R as η jl,J (x) = ϕ( x )Y J (x/ x ).</formula><p>To obtain a complete basis, we can choose a set of radial basis functions ϕ m : R + → R, and define kernel basis functions</p><formula xml:id="formula_28">η jl,Jm (x) = ϕ m ( x ) Y J (x/ x ). Following [43], we choose a Gaussian radial shell ϕ m ( x ) = exp (− 1 2 ( x − m) 2 /σ 2 ) in our implementation.</formula><p>The angular dependency at a fixed radius of the basis for j = l = 1 is shown in <ref type="figure">Figure 2</ref>.</p><p>By mapping each η jl,Jm back to the original basis via Q T and unvectorizing, we obtain a basis κ jl,Jm for the space of equivariant kernels between features of order j and l. This basis is indexed by the radial index m and frequency index J. In the forward pass, we linearly combine the basis kernels as κ jl = Jm w jl,Jm κ jl,Jm using learnable weights w, and stack them into a complete kernel κ, which is passed to a standard 3D convolution routine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Equivariant Nonlinearities</head><p>In order for the whole network to be equivariant, every layer, including the nonlinearities, must be equivariant. In a regular G-CNN, any elementwise nonlinearity will be equivariant because the regular representation acts by permuting the activations. In a steerable G-CNN however, special equivariant nonlinearities are required.</p><p>Trivial irreducible features, corresponding to scalar fields, do not transform under rotations. So for these features we use conventional nonlinearities like ReLUs or sigmoids. For higher order features we considered tensor product nonlinearities <ref type="bibr" target="#b26">[27]</ref> and norm nonlinearities <ref type="bibr" target="#b45">[46]</ref>, but settled on a novel gated nonlinearity. For each non-scalar irreducible feature κ i n f n−1 (x) = f i n (x) ∈ R 2lin+1 in layer n, we produce a scalar gate σ(γ i n f n−1 (x)), where σ denotes the sigmoid function and γ i n is another learnable rotation-steerable kernel. Then, we multiply the feature (a non-scalar field) by the gate (a scalar field):</p><formula xml:id="formula_29">f i n (x) σ(γ i n f n−1 (x)).</formula><note type="other">Since γ i n f n−1 is a scalar field, σ(γ i n f n−1 ) is a scalar field, and multiplying any feature by a scalar is equivariant. See Section 1.3 and Figure 5 in the Supplementary Material for details.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discretized Implementation</head><p>In a computer implementation of SE(3) equivariant networks, we need to sample both the fields / feature maps and the kernel on a discrete sampling grid in Z 3 . Since this could introduce aliasing artifacts, care is required to make sure that high-frequency filters, corresponding to large values of J, are not sampled on a grid of low spatial resolution. This is particularly important for small radii since near the origin only a small number of pixels is covered per solid angle. In order to prevent aliasing we hence introduce a radially dependent angular frequency cutoff. Aliasing effect originating from the radial part of the kernel basis are counteracted by choosing a smooth Gaussian radial profile as described above. Below we describe how our implementation works in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Kernel space precomputation</head><p>Before training, we compute basis kernels κ jl,Jm (x i ) sampled on a s × s × s cubic grid of points x i ∈ Z 3 , as follows. For each pair of output and input orders j and l we first sample spherical harmonics Y J , |j − l| ≤ J ≤ j + l in a radially independent manner in an array of shape (2J + 1) × s × s × s. Then, we transform the spherical harmonics back to the original basis by multiplying by Q J ∈ R (2j+1)(2l+1)×(2J+1) , consisting of 2J + 1 adjacent columns of Q, and unvectorize the resulting array to unvec(Q J Y J (x i )) which has shape (2j + 1)</p><formula xml:id="formula_30">× (2l + 1) × s × s × s.</formula><p>The matrix Q itself could be expressed in terms of Clebsch-Gordan coefficients <ref type="bibr" target="#b16">[17]</ref>, but we find it easier to compute it by numerically solving Eq. 14.</p><p>The radial dependence is introduced by multiplying the cubes with each windowing function ϕ m . We use integer means m = 0, . . . , s/2 and a fixed width of σ = 0.6 for the radial Gaussian windows.</p><p>Sampling high-order spherical harmonics will introduce aliasing effects, particularly near the origin. Hence, we introduce a radius-dependent bandlimit J m max , and create basis functions only for |j − l| ≤ J ≤ J m max . Each basis kernel is scaled to unit norm for effective signal propagation <ref type="bibr" target="#b42">[43]</ref>. In total we get B = s/2 m=0 J m max |j−l| 1 ≤ ( s/2 + 1)(2 min(j, l) + 1) basis kernels mapping between fields of order j and l, and thus a basis array of shape B × (2j + 1) × (2l + 1) × s × s × s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Spatial dimension reduction</head><p>We found that the performance of the Steerable CNN models depends critically on the way of downsampling the fields. In particular, the standard procedure of downsampling via strided convolutions performed poorly compared to smoothing features maps before subsampling. We followed <ref type="bibr" target="#b0">[1]</ref> and experiment with applying a low pass filtering before performing the downsampling step which can be implemented either via an additional strided convolution with a Gaussian kernel or via an average pooling. We observed significant improvements of the rotational equivariance by doing so. See <ref type="table">Table 2</ref> in the Supplementary Material for a comparison between performances with and without low pass filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Forward pass</head><p>At training time, we linearly combine the basis kernels using learned weights, and stack them together into a full filter bank of shape K n+1 × K n × s × s × s, which is used in a standard convolution routine. Once the network is trained, we can convert the network to a standard 3D CNN by linearly combining the basis kernels with the learned weights, and storing only the resulting filter bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We performed several experiments to gauge the performance and data efficiency of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tetris</head><p>In order to confirm the equivariance of our model, we performed a variant of the Tetris experiments reported by <ref type="bibr" target="#b40">[41]</ref>. We constructed a 4-layer 3D Steerable CNN and trained it to classify 8 kinds of Tetris blocks, stored as voxel grids, in a fixed orientation. Then we test on Tetris blocks rotated by random rotations in SO(3). As expected, the 3D Steerable CNN generalizes over rotations and achieves 99±2% accuracy on the test set. In contrast, a conventional CNN is not able to generalize over larger unseen rotations and gets a result of only 27±7%. For both networks we repeated the experiment over 17 runs.  <ref type="table">Table 4</ref> in the Supplementary Material for all the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">3D model classification</head><p>Moving beyond the simple Tetris blocks, we next considered classification of more complex 3D objects. The SHREC17 task <ref type="bibr" target="#b35">[36]</ref>, which contains 51300 models of 3D shapes belonging to 55 classes (chair, table, light, oven, keyboard, etc), has a 'perturbed' category where images are arbitrarily rotated, making it a well-suited test case for our model. We converted the input into voxel grids of size 64x64x64, and used an architecture similar to the Tetris case, but with an increased number of layers (see <ref type="table">Table 3</ref> in the Supplementary Material). Although we have not done extensive fine-tuning on this dataset, we find our model to perform comparably to the current state of the art, see <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="table">Table 4</ref> in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualization of the equivariance property</head><p>We made a movie to show the action of rotating the input on the internal fields. We found that the action are remarkably stable. A visualization is provided in https://youtu.be/ENLJACPHSEA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Amino acid environments</head><p>Next, we considered the task of predicting amino acid preferences from the atomic environments, a problem which has been studied by several groups in the last year <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref>. Since physical forces are primarily a function of distance, one of the previous studies argued for the use of a concentric grid, investigated strategies for conducting convolutions on such grids, and reported substantial gains when using such convolutions over a standard 3D convolution in a regular grid (0.56 vs 0.50 accuracy) <ref type="bibr" target="#b3">[4]</ref>.</p><p>Since the classification of molecular environments involves the recognition of particular interactions between atoms (e.g. hydrogen bonds), one would expect rotational equivariant convolutions to be more suitable for the extraction of relevant features. We tested this hypothesis by constructing the exact same network as used in the original study, merely replacing the conventional convolutional layers with equivalent 3D steerable convolutional layers. Since the latter use substantially fewer parameters per channel, we chose to use the same number of fields as the number of channels in the original model, which still only corresponds to roughly half the number of parameters (32.6M vs 61.1M (regular grid), and 75.3M (concentric representation)). Without any alterations to the model and using the same training procedure (apart from adjustment of learning rate and regularization factor), we obtained a test accuracy of 0.58, substantially outperforming the conventional CNN on this task, and also providing an improvement over the state-of-the-art on this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">CATH: Protein structure classification</head><p>The molecular environments considered in the task above are oriented based on the protein backbone. Similar to standard images, this implies that the images have a natural orientation. For the final experiment, we wished to investigate the performance of our Steerable 3D convolutions on a problem domain with full rotational invariance, i.e. where the images have no inherent orientation. For this purpose, we consider the task of classifying the overall shape of protein structures.</p><p>We constructed a new data set, based on the CATH protein structure classification database <ref type="bibr" target="#b10">[11]</ref>, version 4.2 (see http://cathdb.info/browse/tree). The database is a classification hierarchy containing millions of experimentally determined protein domains at different levels of structural detail. For this experiment, we considered the CATH classification-level of "architecture", which splits proteins based on how protein secondary structure elements are organized in three dimensional space. Predicting the architecture from the raw protein structure thus poses a particularly challenging task for the model, which is required to not only detect the secondary structure elements at any orientation in the 3D volume, but also detect how these secondary structures orient themselves relative to one another. We limited ourselves to architectures with at least 500 proteins, which left us with 10 categories. For each of these, we balanced the data set so that all categories are represented by the same number of structures <ref type="formula" target="#formula_2">(711)</ref> Following the same ResNet template, we then constructed a 3D Steerable network by replacing each layer by an equivariant version, keeping the number of 3D channels fixed. The channels are allocated such that there is an equal number of fields of order l = 0, 1, 2, 3 in each layer except the last, where we only used scalar fields (l = 0). This network contains only 143, 560 parameters, more than a factor hundred less than the baseline.</p><p>We used the first seven of the ten splits for training, the eighth for validation and the last two for testing. The data set was augmented by randomly rotating the input proteins whenever they were presented to the model during training. Note that due to their rotational equivariance, 3D Steerable CNNs benefit only marginally from rotational data augmentation compared to the baseline CNN. We train the models for 100 epochs using the Adam optimizer <ref type="bibr" target="#b25">[26]</ref>, with an exponential learning rate decay of 0.94 per epoch starting after an initial burn-in phase of 40 epochs. Despite having 100 times fewer parameters, a comparison between the accuracy on the test set shows a clear benefit to the 3D Steerable CNN on this dataset <ref type="figure" target="#fig_2">(Figure 4</ref>, leftmost value). We proceeded with an investigation of the dependency of this performance on the size of the dataset by considering reductions of the size of each training split in the dataset by increasing powers of two, maintaining the same network architecture but re-optimizing the regularization parameters of the networks. We found that the proposed model outperforms the baseline even when trained on a fraction of the training set size. The results further demonstrate the accuracy improvements across these reductions to be robust <ref type="figure" target="#fig_2">(Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have presented 3D Steerable CNNs, a class of SE(3)-equivariant networks which represents data in terms of various kinds of fields over R 3 . We have presented a comprehensive theory of 3D Steerable CNNs, and have proven that convolutions with SO(3)-steerable filters provide the most general way of mapping between fields in an equivariant manner, thus establishing SE(3)-equivariant networks as a universal class of architectures. 3D Steerable CNNs require only a minor adaptation to the code of a 3D CNN, and can be converted to a conventional 3D CNN after training. Our results show that 3D Steerable CNNs are indeed equivariant, and that they show excellent accuracy and data efficiency in amino acid propensity prediction and protein structure classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material 3D Steerable CNNs: Learning Rotationally Equivariant</head><p>Features in Volumetric Data 1 Design choices</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Feature types and multiplicities</head><p>The choice of the types and multiplicities of the features is a hyperparameter of our network comparable to the choice of channels in a conventional CNN. As in the latter we follow the logic of doubling the number of multiplicities when downsampling the feature maps. The types and multiplicities of the network's input and output are prescribed by the problem to be solved. If one uses only scalar fields, then the kernels can only be isotropic, higher order representation allows more complex kernels. A more detailed investigation of the choice of these hyperparameters is left open for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Normalization</head><p>We implemented an equivariant version of batch normalization <ref type="bibr" target="#b20">[21]</ref>. For scalar fields, our implementation matches with the usual batch normalization. For the nonscalar fields we normalize them with the average of their norms:</p><formula xml:id="formula_31">f i (x) → f i (x)   1 |B| j∈B 1 V dx||f j (x)|| 2 +   −1/2<label>(17)</label></formula><p>where B is the batch and i, j are the batch indices.</p><p>In order to reduce the memory consumption, we merged the batch normalization operation with the convolution κ (Af + B)</p><formula xml:id="formula_32">BN = (Aκ) f + κ B.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Nonlinearities</head><p>The nonlinearities of an equivariant network need to be adapted to be equivariant themselves. Note that the domain and codomain of the nonlinearities might transform under different representations. We give an overview over the nonlinearities with which we experimented in the following paragraphs.</p><p>Elementwise nonlinearities Scalar features do not transform under rotations. As a consequence, they can be acted on by elementwise nonlinearities as in conventional CNNs. We chose ReLU nonlinearities for all scalar features except those which are used as gates (see below).</p><formula xml:id="formula_33">F scalar n F scalar n F scalar n+1 F scalar n+1 ReLU Ind SE(3) SO(3) [id](g) Ind SE(3) SO(3) [id](g) ReLU</formula><p>Norm nonlinearity The representations we are considering are all orthogonal and hence preserve the norm of feature vectors:</p><formula xml:id="formula_34">ρ(r)f (x) = f T (x)ρ T (r)ρ(r)f (x) = f T (x)f (x) = f (x) ∀r ∈ SO(3), f ∈ F</formula><p>It follows that any nonlinearity applied to the norm of the feature commutes with the group transformation. Denoting a positive bias by β ∈ R + , we experimented with norm nonlinearites of the form</p><formula xml:id="formula_35">f (x) → σ norm (f )(x) := ReLU ( f (x) − β) f (x) f (x) .</formula><p>Intuitively, the bias acts as a threshold on the norm of the feature vectors, setting small vectors to zero and preserving the orientation of large feature vectors. In practice, this kind of nonlinearity tended to converge slower than the gated nonlinearities, therefore we did not use them in our final experiments. This issue might be related to the problem of finding a suitable initialization of the learned biases for which we could not derive a proper scale. Norm nonlinearities were considered before in <ref type="bibr" target="#b45">[46]</ref>.</p><formula xml:id="formula_36">F n F n F n+1 F n+1 σ norm Ind SE(3) SO(3) [ρ](g) Ind SE(3) SO(3) [ρ](g) σ norm</formula><p>Tensor product nonlinearity The tensor product of two fields f 1 and f 2 is in index notation defined by</p><formula xml:id="formula_37">[f 1 ⊗ f 2 ] µν (x) = f 1 µ (x)f 2 ν (x)</formula><p>. This operation is nonlinear and equivariant and hence can be used in neural networks. We denote this nonlinearity by σ ⊗ : F n ⊕ F n → F n+1 := F n ⊗ F n . Note that the output of this operation transforms under the tensor product representation ρ ⊗ ρ of the input representations ρ. In our framework we could perform a change of basis Q defined by Q[ρ ⊗ ρ]Q −1 = j D j to obtain features transforming under irreducible representations.</p><formula xml:id="formula_38">F n ⊕ F n F n ⊕ F n F n+1 = F n ⊗ F n F n+1 = F n ⊗ F n σ ⊗ Ind SE(3) SO(3) [ρ ⊕ ρ](g) Ind SE(3) SO(3) [ρ ⊗ ρ](g) σ ⊗</formula><p>Gated nonlinearity The gated nonlinearity acts on any feature vector by scaling it with a data dependent gate. We compute the gating scalars for each output feature via a sigmoid nonlinearity σ : F scalar n → F scalar n acting on an associated scalar feature. <ref type="figure">Figure 5</ref> shows how the gated nonlinaritiy is coupled with the convolution operation. One can view the gated nonlinearity as a special case of the norm nonlinearity since it operates by changing the length of the feature vector. Simultaneously it can also be seen as a tensor product nonlinearity where one of the two fields as a scalar field. We found that the gated nonlinearities work in practice better than the the other options described above.</p><formula xml:id="formula_39">F scalar n ⊕ F n F scalar n ⊕ F n F n+1 F n+1 σ gate Ind SE(3) SO(3) [id ⊕ρ](g) Ind SE<label>(3)</label></formula><p>SO <ref type="formula">(</ref>  <ref type="figure">with a ρ)</ref>. Specifically, the number of scalar output channels for the preceding convolution operator is increased by the number of features acted on by gated nonlinearities, and the extra scalar fields are computed in the same way as any other scalar field. We use sigmoid for the gate fields. In this picture, there is one scalar field in the output. It is activated with a ReLU.</p><p>2 Reduced parameter cost of 3D Steerable CNNs In the main paper, we demonstrated that the 3D Steerable CNN outperforms a conventional CNN despite having many fewer parameters. To ensure that the reduced number of parameters would not be an advantage also for the conventional CNN (due to overfitting with the highcapacity network), we trained a series of conventional CNNs with reduced number of filters in each layer ( <ref type="figure" target="#fig_4">Figure 6</ref>). Note that the relative performance gain of our model increases dramatically if we restrict the conventional CNN to use the same number of parameters as the Steerable CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Tetris experiment</head><p>The architecture used for the Tetris experiment has 4 hidden layers, the kernel size is 5 and the padding is 4. We didn't use batch normalization. <ref type="table">Table 1</ref> shows the multiplicities of the fields representations and the sizes of the fields. We compare with a regular CNN that has the same feature map sizes. The CNN is like the SE3 network simply without the constraint of being equivariant for rotation. It has therefore much more parameters since its kernels are unconstrained. The SE3 network has 41k parameters and the CNN has 6M parameters. </p><formula xml:id="formula_40">l = 0 l = 1 l = 2 l = 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">3D Model classification</head><p>To find the model we ran 10 different models by changing depth, multiplicities, dropout, low pass filter or stride and two initialization method.</p><p>For this experiment we used a kernel size of 5 and a padding of 4. We used batch normalization. In this architecture we did't used the low pass filters. <ref type="table">Table 3</ref> shows the multiplicities of the fields representations and the sizes of the fields. This network has 142k parameters.</p><p>We converted the 3d models into voxels of size 64 × 64 × 64 with the following code https: //github.com/mariogeiger/obj2voxel. <ref type="table">Table 4</ref> compares our results with results of the original competition and two other articles <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>.  <ref type="table">Table 4</ref>: Results of the SHREC17 experiment.</p><formula xml:id="formula_41">l = 0 l = 1 l = 2</formula><p>5 The CATH experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The data set</head><p>The protein structures used in the CATH study were simplified to include only C α atoms (one atom per amino acid in the backbone), and placed at the center of a 50 3 vx grid, where each voxel spans 0.2 nm. The values of the voxels were set to the densities arising from placing a Gaussian at each atom position, with a standard deviation of half the voxel width. Since we limit ourselves to grids of size 5 nm, we exclude proteins which expand beyond a 5 nm sphere centered around their center of mass. This constraint is only violated by a small fraction of the original dataset, and thus constitutes no severe restriction.</p><p>For training purposes, we constructed a 10-fold split of the data. To rule out any overlap between the splits (in addition to the 40% homology reduction), we further introduce a constraint that any two members from different splits are guaranteed to originate from different categories at the "superfamily" level in the CATH hierarchy (the lowest level in the hierarchy), and all splits are guaranteed to have members from all 10 architectures. Further details about the data set are provided on the website (https://github.com/wouterboomsma/cath_datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Establishing a state-of-the-art baseline</head><p>The baseline 3D CNN architecture for the CATH task was determined through a range of experiments, ultimately converging on a ResNet34-like architecture, with half the number of channels compared to the original implementation (but with an extra spatial dimension), and using a global pooling at the end to obtain translational invariance. After establishing the architecture, we conducted additional experiments to establish good values for the learning and drop-out rates (both in the linear and in the convolutional layers). We settled on a 0.01 dropout rate in the convolutional layers, and L1 and L2 regularization values of 10 −7 . The final model consists of 15, 878, 764 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Architecture details</head><p>Following the same ResNet template, we then constructed a 3D Steerable network, by replacing each layer with its equivariant equivalent. In contrast to the model architecture for the amino acid environment, we here opted for a minimal architecture, where we use exactly the same number of 3D channels as in the baseline model, which leads to a model with the following block structure: The final block deviates slightly from the rest, since we wish to reduce to a scalar representation prior to the pooling. Optimal regularization settings were found to be a capsule-wide convolutional dropout rate of 0.1, and L1 and L2 regularization values of 10 −8.5 . In this minimal setup, the model contains only 143, 560 parameters, more than a factor hundred less than the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: To transform a vector field (L) by a 90</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Shrec17 results[2, 7, 14, 16, 25, 36, 40]. Comparison of different architectures by number of parameters and score. See Table 4 in the Supplementary Material for all the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy on the CATH test set as a function of increasing reduction in training set size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 )Figure 5 :</head><label>35</label><figDesc>Figure 5: A gated nonlinearity requires one extra scalar field (represented by gray circles with an I) per nonscalar output fields (represented by circles with a ρ). Specifically, the number of scalar output channels for the preceding convolution operator is increased by the number of features acted on by gated nonlinearities, and the extra scalar fields are computed in the same way as any other scalar field. We use sigmoid for the gate fields. In this picture, there is one scalar field in the output. It is activated with a ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of our 3D Steerable CNN compared to a conventional 3D CNN with varying numbers of filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Here the 4-tuples represent fields of order l = 0, 1, 2, 3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>, also ensuring that no two proteins within the set have more than 40% sequence identity. See Supplementary Material for details. The new dataset is available at https://github.com/wouterboomsma/cath_datasets.</figDesc><table>We first established a state-of-the-art baseline consisting of a conventional 3D CNN, by conducting a 
range of experiments with various architectures. We converged on a ResNet34-inspired architecture 
with half as many channels as the original, and global pooling at the end. The final model consists of 
15, 878, 764 parameters. For details on the experiments done to obtain the baseline, see Supplementary 
Material. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>: Architecture of the network for the Tetris experiment. Between layer 1-2 and 2-3 there is a stride of 2. Between layer 4 and the output there is a global average pooling.6% 99% ± 2% Table 2: Test accuracy to classify rotated pieces of Tetris. Average and standard deviation over 17 runs.</figDesc><table>size CNN features 
input 
1 
36 

3 

1 
layer 1 
4 
4 
4 
1 
40 

3 

43 
layer 2 
16 
16 
16 
22 

3 

144 
layer 3 
32 
16 
16 
13 

3 

160 
layer 4 
128 
17 

3 

128 
output 
8 
1 
8 
Table 1low pass filter 

disabled 
enabled 
CNN 
24% ± 4% 27% ± 7% 
SE3 
36% ± </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>: Architecture of the network for the 3D Model experiment. Where the size decrease we used a stride of 2. Between the last hidden layer and the output there is a global average pooling. micro macro total P@R R@N mAP P@R R@N mAP score input size params Furuya [16] 0.814 0.683 0.656 0.607 0.539 0.476 1.13 126 × 10</figDesc><table>size 
input 
1 
64 

3 

layer 1 
8 
4 
2 
34 

3 

layer 2 
8 
4 
2 
38 

3 

layer 3 
16 
8 
4 
21 

3 

layer 4 
16 
8 
4 
25 

3 

layer 5 
32 
16 
8 
15 

3 

layer 6 
32 
16 
8 
19 

3 

layer 7 
32 
16 
8 
12 

3 

layer 8 
512 
16 

3 

output 
55 
1 
Table 33 

8.4M 
Esteves [14] 
0.717 0.737 0.685 0.450 0.550 0.444 1.13 
2 × 64 

2 

0.5M 
Tatsuma [40] 
0.705 0.769 0.696 0.424 0.563 0.418 1.11 38 × 224 

2 

3M 
Ours 
0.704 0.706 0.661 0.490 0.549 0.449 1.11 
1 × 64 

3 

142k 
Cohen [7] 
0.701 0.711 0.676 
-
-
-
-
6 × 128 

2 

1.4M 
Zhou [2] 
0.660 0.650 0.567 0.443 0.508 0.406 0.97 50 × 224 

2 

36M 
Kanezaki [25] 0.655 0.652 0.606 0.372 0.393 0.327 0.93 
-
61M 
Deng [36] 
0.418 0.717 0.540 0.122 0.667 0.339 0.85 
-
138M 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">vectorize correspond to flatten it in numpy and the tensor product correspond to np.kron 3 There is a fascinating analogy with the quantum states of a two particle system for which the angular momentum states decompose in a similar fashion. 4 Q can be expressed in terms of Clebsch-Gordan coefficients, but here we only need to know it exists.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Why do deep convolutional networks generalize so poorly to small image transformations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12177</idno>
		<idno>abs/1805.12177</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gift: A real-time and scalable 3d shape search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Roto-translation covariant convolutional networks for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maxime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitko</forename><surname>Lafarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pluim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03393</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spherical convolutions and their application in molecular modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes</forename><surname>Frellsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3436" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Induced representations and mackey theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tullio</forename><surname>Ceccherini-Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Machì</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Scarabotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="28" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the irreducible representations of commutative lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1755" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Intertwiners between induced representations (with applications to the theory of equivariant neural networks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10743</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>The 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CATH: an expanded resource to predict protein function through structure and sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">E</forename><surname>Natalie L Dawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayoni</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">A</forename><surname>Ashford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Orengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sillitoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="289" to="295" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting cyclic symmetry in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Left-invariant diffusions on the space of positions and orientations and their application to crossing-preserving smoothing of hardi images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remco</forename><surname>Duits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Franken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="264" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06721</idno>
		<idno>abs/1711.06721</idno>
		<title level="m">Ameesh Makadia, and Kostas Daniilidis. 3D object classification and retrieval with Spherical CNNs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The design and use of steerable filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="906" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep aggregation of local 3d geometric features for 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiko</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutarou</forename><surname>Ohbuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="121" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Symmetries and Laplacians: Introduction to Harmonic Analysis, Group Representations and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gurarie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabour</forename><surname>Sara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W T</forename><surname>Jorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hexaconv</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting molecular properties with covariant compositional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Truong Son Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241745</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The hessian of axially symmetric functions on se (3) and application in 3d image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Michiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom Cj Dela</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haije</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remco</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Scale Space and Variational Methods in Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="643" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design and processing of invertible orientation scores of 3d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Michiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><forename type="middle">Jem</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Oliván</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remco</forename><surname>Bescós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Group-Theoretical Methods in Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenichi</forename><surname>Kanatani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, Inc., Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<idno type="arXiv">arXiv:1803.01588</idno>
		<title level="m">Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clebsch-gordan nets: a fully fourier space spherical convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy</forename><surname>Truong Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03690</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Groups and group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://colah.github.io/posts/2014-12-Groups-Convolution/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Equivariance through parametersharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08389</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient tensor voting with 3d tensorial harmonics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Reisert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>CVPRW&apos;08</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-Scale 3D Shape Retrieval from ShapeNet Core55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiko</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutarou</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thermos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Axenopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Johan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval. The Eurographics Association</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Ioannis Pratikakis, Florent Dupont, and Maks Ovsjanikov</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The steerable pyramid: A flexible architecture for multi-scale derivative computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings., International Conference on</title>
		<meeting>International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="444" to="447" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Spherical Tensor Algebra for Biomedical Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Skibbe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-fourier spectra descriptor and augmentation with spectral clustering for 3d shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="785" to="804" />
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor Field Networks: Rotation-and Translation-Equivariant Neural Networks for 3D Point Clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D deep convolutional neural networks for amino acid environment similarity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russ B Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">302</biblScope>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marysia</forename><surname>Winkels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taco S Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04656</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">3D G-CNNs for Pulmonary Nodule Detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">CubeNet: Equivariance to 3D Rotation and Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04458</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
