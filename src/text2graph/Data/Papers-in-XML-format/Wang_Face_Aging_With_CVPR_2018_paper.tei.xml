<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Aging with Identity-Preserved Conditional Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Wang</surname></persName>
							<email>wangzw@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghaitech University</orgName>
								<orgName type="institution" key="instit2">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tang Baidu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghaitech University</orgName>
								<orgName type="institution" key="instit2">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghaitech University</orgName>
								<orgName type="institution" key="instit2">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghaitech University</orgName>
								<orgName type="institution" key="instit2">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Face Aging with Identity-Preserved Conditional Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face aging, also known as aging synthesis of the human face, is a task of synthesizing faces of a certain person under a given age. It is attracting more and more researchers' attention because of its various applications in cross-age face recognition and entertainment. For example, it could be applied to help find lost children or to predict what someone will look like in the future. Extensive studies have been made on face aging <ref type="bibr" target="#b27">[28]</ref> [11] <ref type="bibr" target="#b4">[5]</ref>. However, the lack of training samples for a given person over a long range of years <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b19">[20]</ref> [3] <ref type="bibr" target="#b20">[21]</ref> makes face aging still an extremely challenging task in computer vision. * Corresponding author.</p><p>Traditional face aging methods can roughly be categorized into prototype-based approaches <ref type="bibr" target="#b10">[11]</ref> and physical model-based approaches <ref type="bibr" target="#b24">[25]</ref>. Prototype-based approaches usually compute an average face within a face age group first, and the difference between different average faces from different groups would be treated as aging pattern which would be used for synthesizing an aged face <ref type="bibr" target="#b10">[11]</ref>. Consequently, person-specific information of each person would be lost, which results in the synthesized faces look unrealistic. By contrast, physical model-based approaches model the shape and texture changes with age in terms of hair colors, muscles, and wrinkles, etc. with a parametric model, which usually requires lots of training samples and is computationally expensive.</p><p>Recently, Generative Adversarial Networks(GANs) based approaches have been demonstrated their successes in generating high quality images <ref type="bibr" target="#b6">[7]</ref> [14] <ref type="bibr" target="#b17">[18]</ref>. Of which, as a special GANs, Conditional GANs (cGANs) <ref type="bibr" target="#b6">[7]</ref> [14] <ref type="bibr" target="#b8">[9]</ref> take prior information in image generation and make generated images be with certain desired property. Inspired by the success of CGANs, we propose an Identity-Preserved Conditional Generative Adversarial Networks (IPCGANs) for face aging. Specifically, our IPCGANs consists of three modules: a CGANs module, an identity-preserved module and an age classifier. The generator of CGANs takes an input image and a target age code as its input and generates a face with the target age. The generated face is expected to be indistinguishable from real faces in that target age group by the discriminator. To keep identity information, we introduce a perceptual loss <ref type="bibr" target="#b3">[4]</ref> in the objective of IPCGANs. Finally, to guarantee the synthesized faces fall into the target age group, we send the generated aged faces to a pre-trained age classifier and add an age classification loss to the objective. Since all components of our IPCAGANs are differentiable with respect to the model parameters, the whole network can be trained in an end-to-end fashion.</p><p>The contributions of this paper are summarized as follows:</p><p>1. We propose to impose an identity-preserved term and an age classification term into the objective of our IPC-GANs. The former lets the aged faces keep the same identity with the input face. The latter is to make sure the generated faces be with the target age. Extensive experiments validate the effectiveness of both terms for preserving the identity information and making the face aging effect evident.</p><p>2. Other than quantitatively evaluate the quality of the synthesized faces, we also propose to conduct face verification and face age classification for the generated aged faces by means of user study. Our proposed data augmentation experiment also validates the effectiveness of IPCGANs.</p><p>3. IPCGANs is not limited to face aging problem, it is a general framework. Without any modification, IPCGANs can be applied to multi-attribute generation task, like hair colors, facial expressions, etc, which can be used for imbalanced data classification scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face Aging</head><p>As aforementioned, traditional face aging approaches can be categorized into prototype-based approaches and physical model based approaches. We refer readers to <ref type="bibr" target="#b4">[5]</ref> for a comprehensive survey of these approaches. Specifically, physical model-based approaches usually focus on the change of skin's anatomy structure, facial muscle changes and some other physical measurements for aged face modeling <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b26">[27]</ref>. These models are usually very complex, and require lots of training data. Prototype-based approaches leverage the differences between the average faces of different age groups for age pattern transfer <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b10">[11]</ref>. However, such strategy neglects the differences between different persons, which makes the generated faces look unrealistic. Further, some important age clues, say wrinkles, may be averaged out. To avoid this, in <ref type="bibr" target="#b22">[23]</ref>[30] <ref type="bibr" target="#b28">[29]</ref>, sparse representation based approaches have been adopted to model the person-specific facial properties for synthesizing aged faces. Though the identity information can be preserved to some extent by these methods, the reconstruction procedure makes the synthesized faces suffer from the ghost artifacts. Recently, a recurrent face aging framework <ref type="bibr" target="#b27">[28]</ref> has been proposed for face aging by leveraging a Recurrent Neural Network model. Thus the change of synthesized faces between neighboring age groups is more smooth, but the identity information is not explicitly preserved in this work. <ref type="bibr" target="#b1">[2]</ref> is the first to apply conditional GANs to face aging. Their training process is three-stage. This method is not efficient at inference time because they have to solve a LBFGS optimization problem for each image. To better preserve the identity information, they propose a Local Manifold Adaptation approach in <ref type="bibr" target="#b0">[1]</ref>. Combined with <ref type="bibr" target="#b1">[2]</ref>, they boost the cross-age face verification via age normalization. Similar to us, <ref type="bibr" target="#b30">[31]</ref> proposed an auto-encoder conditional GANs which encodes the input image to a manifold and then reconstructs aged images. However, their aged faces seem little change given different age conditions. Recently, <ref type="bibr" target="#b23">[24]</ref> proposed a face editing method which can be extended to face aging task. Their results show some aging effect, but the aged faces look blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative Adversarial Networks (GANs)</head><p>Generative Adversarial Networks(GANs) <ref type="bibr" target="#b6">[7]</ref> has been widely used for image generation. It has two components: a generator and a discriminator. Given a noise vector z which is sampled from a normal distribution or a uniform distribution p z (z), the generator maps z to a synthesized imagex. The discriminator takes eitherx or x (x is images sampled from real image distribution p data (x)) as input and tries to tell them apart. The generator is trained to let the discriminator be unable to discriminate them. The objective function of original GANs is given as follows:</p><formula xml:id="formula_0">min G max D E x∼p data (x) [log D(x)]+E z∼pz(z) [log(1−D(G(z)))]</formula><p>(1) To facilitate the training of GANs, Radford et al. <ref type="bibr" target="#b17">[18]</ref> propose a Deep Convolutional GANs (DCGANs) framework, which promotes the application of GANs in many tasks, such as video prediction <ref type="bibr" target="#b12">[13]</ref>, cross domain image generation <ref type="bibr" target="#b25">[26]</ref>, etc. Arjovsky et al. provide a rigorous analysis on the objective of GAN and its instability in training phase, which leads to a Wasserstein GANs (WGANs). Soon after WGANs, an improved version of WGANs is proposed <ref type="bibr" target="#b7">[8]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, a Conditional GANs(CGANs) model which employs prior information in image generation is proposed. Reed et al. <ref type="bibr" target="#b18">[19]</ref> demonstrate its capability in generating realistic images from text descriptions. Recently, CycleGANs <ref type="bibr" target="#b31">[32]</ref> has also been successfully applied to image-to-image translation task and achieves good performance. These work greatly boosts the performance of GANs in image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Style transfer</head><p>The objective of synthesizing a face with a target age is also related to the work of style transfer <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b5">[6]</ref>. Given one input image (to be transferred with some artistic style) and one artistic style image, the goal of style transfer is to generate one image whose contents are taken from the former while the style is from the latter. To reach this goal, a content loss and a style loss in feature space are jointly optimized <ref type="bibr" target="#b9">[10]</ref>. Specifically, both the content loss and the style loss are called as perceptual loss because they depend on features extracted from a pre-trained neural network. A neural network extracts more abstract and perceptual meanings features than raw pixel features. While <ref type="bibr" target="#b5">[6]</ref> can generate high quality images, it is slow in testing phase because the inference needs to solve a LBFGS optimization problem. 512 512 <ref type="figure">Figure 1</ref>. The pipeline of our proposed IPCGANs for face aging. The input image and target age label are concatenated together and then is fed into the generator G. The label is of size 128 × 128 × 5. The discriminator D tries to separate the synthesized faces and faces within the target age group. To preserve the identity information, we enforce the features of synthesized face and input to be similar. We also use an age classifier to force the synthesized face to be with the target age.</p><p>To avoid this, in <ref type="bibr" target="#b9">[10]</ref>, a feed-forward network is adopted. Different from style transfer, which transfers the style of one image to another image, in face aging, it is desirable to transfer the age pattern in the target age group to one face. Therefore, style transfer cannot be directly applied to face aging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Identity-Preserved Conditional Generative Adversarial Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We divide faces with different ages into 5 nonoverlapping groups. The faces within these 5 groups corresponding to aged 11-20, 21-30, 31-40, 41-50, and 50+, respectively. Given a face image x, we use a code C s ∈ R h×w×5 to indicate the age group that x belongs to, in which, h and w represent the height and width of a feature map, 5 is the age group number. Like one-hot code, only one feature map is filled with ones while the rest feature maps are all filled with zeros. Face aging aims to generate a synthesized facex that lies in target age group C t . It is desirable that the generated facex has the following characteristics: i)x looks realistic; ii)x has the same identity as x; iii) the age ofx lies in the target age group C t . To reach these goals, we propose an Identity-Preserved Conditional GANs (IPCGANs) framework. In our implementation, we train multiple models based on the age group that x belongs to. In other words, the model is only related to the C s . The model corresponds to C s can map any face in group C s to any target age group C t . For simplicity, We slightly abuse notions and do not specify which C s age group the model corresponding to in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Identity-Preserved Conditional Generative Adversarial Networks</head><p>The whole framework of our Identity-Preserved Conditional Generative Adversarial Networks (IPCGANs) is illustrated in <ref type="figure">Figure 1</ref>. It contains three modules: i) A CGANs module which generates a synthesized face with target age C t and guaranteesx looks realistic; ii) An Identity-Preserved module which guaranteesx has the same identity with x; iii) An age classifier module which further enforcesx with the desired age C t .</p><p>CGANs based face generation module. Since face aging aims at generating a synthesized face with a target age, we adopt Conditional GANs (CGANs) for face generation. Specifically, we denote y as real faces within the target age group, and denote the distribution of x and y as p x (x) and p y (y), respectively. With CGANs, the synthesized face with the target age C t should not be classified as a faked sample by discriminator D. For real faces, the probability that they belong to real face D(x|C t ) should be high. Besides, D is also responsible for aligning the input label C t with the generated images. Consequently, we arrive at the following objective function:</p><formula xml:id="formula_1">min G max D E x∼px(x) [log D(x|C t )] +E y∼py(y) [log(1 − D(G(y|C t ))]<label>(2)</label></formula><p>Similar to the standard GANs <ref type="bibr" target="#b6">[7]</ref>, the optimization of CGANs in Equation (2) also suffers from instability. Consequently, the generated images are unrealistic and of bad quality. In <ref type="bibr" target="#b11">[12]</ref>, a Least Squares Generative Adversarial Networks (LSGANs) model is proposed. As shown in <ref type="bibr" target="#b11">[12]</ref>, the objective of standard GANs can easily get stacked into a very small loss for the faked samples because the discriminator can easily tell the generated faces and real faces apart. By contrast, LSGANs tries to push both the generated faces and real faces close to the decision boundary and make them indistinguishable. Thus LSGANs is shown to be able to generate high quality images and training is more stable. Therefore, we choose a Conditional LSGANs for our face generation task, which is a special CGANs. Mathematically, the Conditional LSGANs can be formulated as follows:</p><formula xml:id="formula_2">L D = 1 2 E x∼px(x) [(D(x|C t ) − 1) 2 ] + 1 2 E y∼py(y) [(D(G(y|C t )) 2 ] L G = 1 2 E y∼py(y) [(D(G(y)|C t ) − 1) 2 ]<label>(3)</label></formula><p>To optimize Conditional LSGANs, we use the matchingaware discriminator proposed in <ref type="bibr" target="#b18">[19]</ref> which is shown effective for aligning conditions with the generated images.</p><p>Identity-preserved module. It is important to preserve the identity information for the synthesized faces. However, the adversarial loss only makes the generator generate samples that follow the target data distribution, consequently, the generated samples can be like any person in the target age group. In other words, adversarial loss alone can not guarantee that the generated samples can preserve the identity information. To keep the identity information for the generated faces, we introduce the following perceptual loss into our face aging objective:</p><formula xml:id="formula_3">L identity = x∈px(x) h(x) − h(G(x|C t ) 2<label>(4)</label></formula><p>Here h(·) corresponds to features extracted by a specific feature layer in a pre-trained neural network. The reason of not using mean square error (MSE) between x and its aged face G(x|C t ) in pixel space is that the aged face contains changes in terms of hair color, beard, wrinkles, receding hairline, etc., therefore it is different from x any more. An MSE loss will force G(x|C t ) to be the same as x. However, a perceptual loss encourages the generated images to be close to the features of input face in the same feature space.</p><p>Choosing features extracted from a proper layer h(·) is of great importance for preserving the identity information. Experiments in style transfer <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref> indicate that lower feature layers are good at keeping the content, while higher layers help keep style related things like color, texture, etc. Even though aged face has the change in terms of hair color, wrinkles, etc., the identity information should not change.</p><p>Based on this, here we argue that the face content itself represents the identity information, lower feature layer of a pretrained neural network should be adopted as h(·). To balance the quality of aged images and the identity information of the faces, in Sec. 4 we line search from f c7 to conv2 of Alexnet pre-trained on ImageNet and empirically set h(x) as the features of conv5 layer. Qualitative and quantitative results show that this setting can preserve the identity well and generate diverse aged faces.</p><p>Age classification module. To further guarantee the generated faces fall into the target age group C t , we pre-train an age classifier and use it to identify which age group the face comes from. During the training of our IPCGANs, we fix the parameters of this age classifier and use it to classify the generated face, G(x|C t ). If the generated face is indeed in group C t , our age classifier gives a small penalty. On the contrary, if G(x|C t ) is not in group C t , the age classifier will give a big penalty. Here we introduce an age classification loss L age into the objective of IPCGANs. We use L age to represent the age classification loss. We define L age as follows:</p><formula xml:id="formula_4">L age = x∈px(x) ℓ(G(x|C t ), C t )<label>(5)</label></formula><p>Here ℓ(·) corresponds to a softmax loss. Through backpropagation, age classification loss forces the parameters of generator to change and generate faces that lie in the correct age group.</p><p>Objective function Overall, to generate a face with the target age and the same identity with CGANs, we arrive at the following objective function:</p><formula xml:id="formula_5">G loss = λ 1 L G + λ 2 L identity + λ 3 L age D loss = L D<label>(6)</label></formula><p>where λ 1 controls to want extent the input image is aged. λ 2 and λ 3 controls to what extent we want to keep the identity information and let the generated samples fall into the right age group, respectively. In Sec. 4 we empirically find the optimal λ 1 , λ 2 and λ 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>The generator and discriminator networks Inspired by the impressive results of style transfer <ref type="bibr" target="#b9">[10]</ref> and unpaired image-to-image translation <ref type="bibr" target="#b31">[32]</ref>, our generator is the same with <ref type="bibr" target="#b31">[32]</ref> except the first convolution layer. Our generator receives 128 × 128 × 3 images and 128 × 128 × 5 condition feature maps as input, so we adopt 6 residual blocks in our generator. Like one-hot code, only one feature map is filled with ones while the rest feature maps are all filled with zeros. We inject the conditions before the first convolution <ref type="figure">Figure 2</ref>. The aging effect of different age classification loss weights. We set λ1 = 75, λ2 = 5e − 5, input age lies in 11-20 group, target age lies is in 50+ group, throughout. Here we use conv4 as the feature layer. ax means λ3 = x. As we can see that as λ3 grows, the aging effect gets more and more evident. But this trend is limited by the age of the aged images. I.e., if the target age lies in 30-40 group, as λ3 grows, the aging effect gets more obvious but the aged images will not look like images of group 40-50 or 50+.</p><p>layer. The input images and condition feature maps are concatenated together and then the combined feature maps are sent to the first convolution layer.</p><p>The architecture of discriminator is adapted from invertible conditional GANs <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b31">[32]</ref>. The way we inject conditions into the discriminator is exactly the same with <ref type="bibr" target="#b16">[17]</ref>, which is shown to be able to generate high quality images that are consistent with the conditions. Specifically, as <ref type="bibr" target="#b31">[32]</ref> did, we follow the naming convention used in <ref type="bibr" target="#b9">[10]</ref>. Let Conv k represents a 4 × 4 convolutionBatchnorm-leakyRelu layer with stride 2 and k output channels. The architecture of discriminator is Conv 64 − Conv 128 − Conv 256 − Conv 512 − Conv 512 . As <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b16">[17]</ref> suggested, we do not apply Batchnorm on the first Conv 64 layer and we inject the conditions after this layer. LeakyRelu is with slope 0.2. As the feature map after the Conv 64 layer is of size 64 × 64, the size of the condition feature maps that fed into the generator changes from 128 to 64, correspondingly. The Conv 64 feature maps and conditions are stacked together. The combined feature maps are sent to the Conv 128 layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Age classification network</head><p>Our age classifier is adapted from Alexnet. The age classifier shares the same architecture from conv1 to pool5. After that, we add two fully connected layers and a softmax layer, dropout is used to prevent from overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Evaluation</head><p>In this section, we first introduce the training dataset and image pre-processing details. Then we will evaluate our proposed IPCGANs both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>Following the work <ref type="bibr" target="#b27">[28]</ref>[31], we choose the Cross-Age Celebrity Dataset (CACD) <ref type="bibr" target="#b2">[3]</ref> for training and evaluation. CACD contains more than 160,000 face images of 2,000 celebrities with age ranging from 16 to 62. All the images are annotated with age, though not accurate. There are large variations in pose, illumination, expression and even style in this dataset. After face detection, aligning and center cropping, we get 163,104 CACD images whose resolution is 400 × 400 pixels. We split CACD into two parts, 90% for training and the rest for test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We compare our method with the following latest work: age conditional Generative Adversarial Networks (acGANs) <ref type="bibr" target="#b1">[2]</ref> and Conditional Adversarial Autoencoder Network (CAAE) <ref type="bibr" target="#b30">[31]</ref> which achieve state-of-the-art performance for face aging. All of these methods are based on conditional GANs and are closely related to our method. For acGANs we change the number of age groups from 6 to 5 and replace FaceNet with the pre-trained face VGG net <ref type="bibr" target="#b15">[16]</ref>. We use the Tensorflow implementation of L-BFGS-B algorithm and set the maximal iterations to be 1000. CAAE originally has 10 age groups and uses gender information. For fair comparison, we remove the gender information and use 5 age groups instead. The age classifier is finetuned based on Alexnet on the CACD training set with 200, 000 steps. The learning rate is set as 0.01 at first and is exponentially decreased every 15 epochs. The learning rate decay factor = 0.1, weight de-cay factor = 0.0005 and batch size = 64. For the training of IPCGANs, we fix the learning rate as 0.001 and use a batch size of 32. The whole training process takes 500,000 steps. As we use BatchNorm throughout, in order to avoid problems with BatchNorm (Running mean and running var from BatchNorm layers in test mode might be a bit off from training, which causes significant differences between images generated on training mode and test mode.), we follow the method in <ref type="bibr" target="#b16">[17]</ref> to stabilize the BatchNorm layers before using the generator models for image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative comparison</head><p>Following previous work <ref type="bibr" target="#b1">[2]</ref> [31] <ref type="bibr" target="#b27">[28]</ref>, we first qualitatively compare the synthesized faces of different methods. We randomly choose 6 persons from the 11-20 CACD test age group. <ref type="figure">Figure 3</ref> shows the aged faces of different methods. Since the source code of acGAN is not available, we try our best to implement the original work and tune the parameters to improve the performance. We think we reproduce the same image quality as presented in the original paper of acGANs. For CAAE, we retrain a model with their released code. We can see that images generated by acGANs have lots of artifacts. Besides, acGANs has the danger of losing identities when the target age grows. By contrast, images generated by CAAE look blurry and unrealistic. Due to the use of pixel loss between the input image and its aged ones, the aging effect is not evident. Compared with CAAE and acGANS, the synthesized images of IPCGANs have fewer artifacts, higher image quality and lower possibilities of losing identities. <ref type="figure">Figure 4</ref> shows the objective with/without identitypreserved module. Without identity-preserved term, although the adversarial loss makes the input face aged, sometimes the generated images have lots of artifacts and have the danger to lose their identities. With L identity , the quality of synthesized images is closely related to which layer is chosen as the feature layer. We fix the other factors unchanged and line search the optimal feature layer from f c7 to conv2. We can see that as feature layer goes from shallow to deep, the aging effect gets more and more evident and artifacts and distortions will appear. To balance between the image quality and the face aging effect, we empirically set h(x) as the features of conv5 layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">The effect of identity-preserved module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The effect of age classifier module</head><p>L age pushes the generator to generate samples that lie in the target age group. <ref type="figure">Figure 2</ref> shows the effect of age classification term. Keep the other factors unchanged, as the age loss weight λ 3 grows, the aging effect gets more evident.  <ref type="bibr" target="#b27">[28]</ref>. Following these work, we also conduct user study experiments to compare the quality of faces generated by different methods. Specifically, we invite 80 volunteers who have no knowledge about our work to rate the faces generated by different methods. Different from previous work, besides the image quality evaluation <ref type="bibr" target="#b30">[31]</ref>[28], we also ask users to conduct face verification task and age classification task for synthesized faces. We randomly select 100 images in the 11-20 age group. Then for each image, we generate 4 aged faces with different target age conditions. Finally, we get 400 aged faces for each aging method. Throughout this part, we use these images for user study evaluation. So the same input → output mappings are generated for each model and images presented to all volunteers are the same, which guarantees the fairness of comparison. Image quality. In this part, we ask volunteers to rate the quality of each face (good or bad). Then we calculate the percentage of images rated as good. Age classification. Given a synthesized face, we ask volunteers to vote which age group that this face belongs to. By repeating this process for each method, finally, the percentage of faces whose target age agrees with that of user estimation is reported. Face verification. For each input image, we generate 4 aged images given different age labels. We denote the 4 aged images as age1-4. We form 3 pairs here. (input, age1), (age2, age3), (age4, one randomly selected generated image of other persons). The first 2 pairs are to verify whether the generated images are the same person as the input. The last pair is to verify whether the generated image seems like the other person. Then we ask the users to do face verification task and report the accuracy of different methods. Here the accuracy = (t p + t n )/(N p + N n ). If the trained model is not identity preserved or generates the same person given different inputs, face verification score must be low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Inception score.</head><p>Inception score is another metric used for evaluating the quality of generated images <ref type="bibr" target="#b21">[22]</ref>. However, the inception <ref type="figure">Figure 3</ref>. Some synthesized faces generated by different methods. Each dotted box denotes one person's images. In each box, from top to bottom, they are images generated by IPCGANs, CAAE and acGANs. The input age lies in 11-20 age group and target age lies is in 50+ age group. conv5 is chosen as the feature layer of L identity . network on ImageNet is trained on 1000 object classes that exclude human faces or human categories. Using the inception score calculated by the network trained on ImageNet to measure the image quality of faces is inappropriate. Instead, we use the pre-trained face VGG net for evaluation. We run the OpenAI source code to compute score. We term this score as VGG-face score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Computational cost</head><p>For fair comparison, here we evaluate the average time cost of generating 4 aged images conditioned on one input image by different methods. We set the maximal iterations of L-BFGS-B to be 1000 and keep the same settings for all methods. Each method is repeated 5 times then the average time is computed. The performance of all the measurements by different methods is reported in <ref type="table" target="#tab_1">Table 1</ref>. We can see that our IPCGANs achieves the highest performance on image quality and face verification. Further, our IPCGAN also achieves the highest VGG-face score, which validates the effectiveness of our method for generating a high quality face with the same identity and target age. As for the computation efficiency, <ref type="figure">Figure 4</ref>. The aging effect with different feature layers. Here we set λ3 = 0. The input age lies in 11-20 age group and target age lies is in 50+ age group. 'no' means without the identity-preserving module. As we can see, the lower the layer, the stronger the ability to keep the source image content. As the feature layer goes deeper, the aging effect gets more evident.</p><p>our method is nearly 100× faster than acGANs and 2.5× faster than CAAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Face recognition with synthesized data</head><p>To avoid the suspicion that the limited images used for user study is carefully selected and to further validate the effectiveness of our method, we use the synthesized data to augment the real faces and train a classifier for face recognition. Because of the slow speed of acGANs, We only compare the performance of classifiers trained by using data augmented with synthesized faces generated by CAAE and IPCGANs. For each training image, we generate 4 images given different age conditions. Thus the augmented training set is 4× larger than the original training set. Without data augmentation, the face recognition accuracy with face VGG classifier on the test set is 84.9%. We finetune on this model with the augmented training set, then the best performance achieved by CAAE and IPCGANs is 78.2% and 84.8%, respectively. It shows that after data augmentation, IPCGANs model keeps the face recognition performance while the CAAE model degrades the accuracy greatly. The performance does not drop after finetuning with 4 times more synthesized data validates that our synthesized images preserve the identity information and the image quality. If synthesized faces cannot keep identities well, so many synthesized faces would reduce the performance (as CAAE does). So face recognition accuracy is another measurement for comparing different face aging models. Meanwhile, this is the first work to use face recognition as a measurement to measure different algorithms. This experiment shows that as a data augmentation method, aged faces generated by our method can not improve the face recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">The evaluation of identity-preserved module and age classifier module</head><p>We also quantitatively evaluate the models with/without identity-preserved term and age classification module by conducting user study based face verification and age classification. The experimental setup is the same with Section 4.4. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. We can see that with age classifier, the age classification accuracy is boosted than that without the age classifier. The identity-preserved module also improves the face verification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In light of the success of GANs for image generation, we propose a conditional GANs based face aging approach. The discriminator in CGANs guarantees the consistency between the aged faces and the corresponding target age. To preserve the identity of input images, we force the high level features of input faces and the synthesized faces to be similar. Further, we introduce an age classifier module to force the synthesized faces to be with the target age. In this way, our method can generate high quality faces with the same identity and target age. Both of qualitative and quantitative experiments validate the effectiveness of our approach. Besides, IPCGANs is a general framework. Without any modification, it can be applied to multi-attribute transfer tasks like brown hair to black/blond/gray hair, no beard to beard/5 o'clock shadow/mustache/sideburns, etc. If we remove the condition part, our framework can be used for image translation task, like from RGB domain to near infrared domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>The performance of different methods.</figDesc><table>CAAE 
acGANs 
IPCGANs 
Face verification (%) 
91.53 
85.83 
96.90 
Image quality (%) 
68.85 
39.67 
71.74 
Age classification (%) 
24.84 
32.70 
31.74 
VGG-face score 
19.53±1.76 23.42±1.82 36.33±1.85 
Time cost (s) 
0.71 
38.68 
0.28 

4.4. Quantitative comparison 

4.4.1 User study evaluation 

Most existing work quantitatively evaluate the performance 
of different methods based on user study [31]</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>The effect of with/without identity-preserved module and age classifier module(%) age classification face verification with age classifier w/o age classifier with identity-preserved term w/o identity-preserved term</figDesc><table>31.37 
28.73 
99.07 
98.15 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by NSFC (NO. 61502304 ) and Shanghai Subject Chief Scientist (A type) (No. 15XD1502900).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boosting cross-age face verification via generative age normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face aging with conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-age reference coding for age-invariant face recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Age synthesis and estimation via faces: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1955" to="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3334" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<idno>ArXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An overview of research activities in facial age estimation using the fg-net aging database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="737" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06355</idno>
		<title level="m">Invertible conditional gans for image editing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Personalized age progression with aging dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3970" to="3978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural face editing with intrinsic image disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5541" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A compositional and dynamic model for face aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="401" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised crossdomain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial aging simulator considering geometry and patch-tiled texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tazoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maejima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2012 Posters</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent face aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2378" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Category specific dictionary learning for attribute specific feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1465" to="1478" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face aging effect simulation using hidden factor analysis joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2493" to="2507" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
