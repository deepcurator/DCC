<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeply-Recursive Convolutional Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
							<email>j.kim@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deeply-Recursive Convolutional Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose an image super-resolution method (SR)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For image super-resolution (SR), receptive field of a convolutional network determines the amount of contextual information that can be exploited to infer missing highfrequency components. For example, if there exists a pattern with smoothed edges contained in a receptive field, it is plausible that the pattern is recognized and edges are appropriately sharpened. As SR is an ill-posed inverse problem, collecting and analyzing more neighbor pixels can possibly give more clues on what may be lost by downsampling.</p><p>Deep convolutional networks (DCN) succeeding in various computer vision tasks often use very large receptive fields (224x224 common in ImageNet classification <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24]</ref>). Among many approaches to widen the receptive field, increasing network depth is one possible way: a convolutional (conv.) layer with filter size larger than a 1 × 1 or a pooling (pool.) layer that reduces the dimension of intermediate representation can be used. Both approaches have drawbacks: a conv. layer introduces more parameters and a pool. layer typically discards some pixel-wise information.</p><p>For image restoration problems such as super-resolution and denoising, image details are very important. Therefore, most deep-learning approaches for such problems do not use pooling. Increasing depth by adding a new weight layer basically introduces more parameters. Two problems can arise. First, overfitting is highly likely. More data are now required. Second, the model becomes too huge to be stored and retrieved.</p><p>To resolve these issues, we use a deeply-recursive convolutional network (DRCN). DRCN repeatedly applies the same convolutional layer as many times as desired. The number of parameters do not increase while more recursions are performed. Our network has the receptive field of 41 by 41 and this is relatively large compared to SRCNN <ref type="bibr" target="#b5">[5]</ref> (13 by 13). While DRCN has good properties, we find that DRCN optimized with the widely-used stochastic gradient descent method does not easily converge. This is due to exploding/vanishing gradients <ref type="bibr" target="#b0">[1]</ref>. Learning long-range dependencies between pixels with a single weight layer is very difficult.</p><p>We propose two approaches to ease the difficulty of training <ref type="figure" target="#fig_0">(Figure 3(a)</ref>). First, all recursions are supervised. Feature maps after each recursion are used to reconstruct the target high-resolution image (HR). Reconstruction method (layers dedicated to reconstruction) is the same for all recursions. As each recursion leads to a different HR prediction, we combine all predictions resulting from different levels of recursions to deliver a more accurate final prediction. The second proposal is to use a skip-connection from input to the reconstruction layer. In SR, a low-resolution image (input) and a high-resolution image (output) share the same information to a large extent. Exact copy of input, however, is likely to be attenuated during many forward passes. We explicitly connect the input to the layers for output reconstruction. This is particularly effective when input and output are highly correlated.</p><p>Contributions In summary, we propose an image superresolution method deeply recursive in nature. It utilizes a very large context compared to previous SR methods with only a single recursive layer. We improve the simple recursive network in two ways: recursive-supervision and skipconnection. Our method demonstrates state-of-the-art performance in common benchmarks.  <ref type="figure" target="#fig_1">Figure 1</ref>: Architecture of our basic model. It consists of three parts: embedding network, inference network and reconstruction network. Inference network has a recursive layer and its unfolded version is in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single-Image Super-Resolution</head><p>We apply DRCN to single-image super-resolution (SR) <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8]</ref>. Many SR methods have been proposed in the computer vision community. Early methods use very fast interpolations but yield poor results. Some of the more powerful methods utilize statistical image priors <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b12">12]</ref> or internal patch recurrence <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b10">10]</ref>. Recently, sophisticated learning methods have been widely used to model a mapping from LR to HR patches. Many methods have paid attention to find better regression functions from LR to HR images. This is achieved with various techniques: neighbor embedding <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b19">19]</ref>, sparse coding <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>, convolutional neural network (CNN) <ref type="bibr" target="#b5">[5]</ref> and random forest <ref type="bibr" target="#b23">[23]</ref>.</p><p>Among several recent learning-based successes, convolutional neural network (SRCNN) <ref type="bibr" target="#b5">[5]</ref> demonstrated the feasibility of an end-to-end approach to SR. One possibility to improve SRCNN is to simply stack more weight layers as many times as possible. However, this significantly increases the number of parameters and requires more data to prevent overfitting. In this work, we seek to design a convolutional network that models long-range pixel dependencies with limited capacity. Our network recursively widens the receptive field without increasing model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recursive Neural Network in Computer Vision</head><p>Recursive neural networks, suitable for temporal and sequential data, have seen limited use on algorithms operating on a single static image. Socher et al. <ref type="bibr" target="#b25">[25]</ref> used a convolutional network in a separate stage to first learn features on RGB-Depth data, prior to hierarchical merging. In these models, the input dimension is twice that of the output and recursive convolutions are applied only two times. Similar dimension reduction occurs in the recurrent convolutional neural networks used for semantic segmentation <ref type="bibr" target="#b22">[22]</ref>. As SR methods predict full-sized images, dimension reduction is not allowed.</p><formula xml:id="formula_0">3 × 3 × F × F Filters W Conv /</formula><p>In Eigen et al. <ref type="bibr" target="#b6">[6]</ref>, recursive layers have the same input and output dimension, but recursive convolutions resulted in worse performances than a single convolution due to overfitting. To overcome overfitting, Liang and Hu <ref type="bibr" target="#b17">[17]</ref> uses a recurrent layer that takes feed-forward inputs into all unfolded layers. They show that performance increases up to three convolutions. Their network structure, designed for object recognition, is the same as the existing CNN architectures.</p><p>Our network is similar to the above in the sense that recursive or recurrent layers are used with convolutions. We further increase the recursion depth and demonstrate that very deep recursions can significantly boost the performance for super-resolution. We apply the same convolution up to 16 times (the previous maximum is three).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local outputs</head><formula xml:id="formula_1">H d-1 Recon Net d-1 Output d-1 (a) Recon Net Recon Net d Recon Net d＋1 Final Output Output d Output d+1 Output d-1 Output d Output d＋1 Output d Output d-1 Output d＋1</formula><p>Output D </p><formula xml:id="formula_2">H D Recon Net 1 Output d-1 Output 1 Recon Net D Output d+1 Output D Final Output (b) Input Embed Net 1 Input (c) Embed Net d Embed Net d-1 Embed Net d＋1 Embed Net D H d-1 H1 H1 H d H1 H1 H d＋1 H1 H1 H D H1 H1 H d-1 Hd-1 H d Hd-1 H H d＋1 Hd-1 H D Hd-1 H d Hd H d＋1 Hd H D Hd H d＋1 H D Hd＋1 H D HD Hd＋1 Recon Net 1 Recon Net d Recon Net d-1 Recon Net d＋1 Recon Net D Output d Output d-1 Output d＋1</formula><p>Output D </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Model</head><p>Our first model, outlined in <ref type="figure" target="#fig_1">Figure 1</ref>, consists of three sub-networks: embedding, inference and reconstruction networks. The embedding net is used to represent the given image as feature maps ready for inference. Next, the inference net solves the task. Once inference is done, final feature maps in the inference net are fed into the reconstruction net to generate the output image.</p><p>The embedding net takes the input image (grayscale or RGB) and represents it as a set of feature maps. Intermediate representation used to pass information to the inference net largely depends on how the inference net internally represent its feature maps in its hidden layers. Learning this representation is done end-to-end altogether with learning other sub-networks. Inference net is the main component that solves the task of super-resolution. Analyzing a large image region is done by a single recursive layer. Each recursion applies the same convolution followed by a rectified linear unit <ref type="figure">(Figure 2</ref>). With convolution filters larger than 1 × 1, the receptive field is widened with every recursion. While feature maps from the final application of the recursive layer represent the high-resolution image, transforming them (multi-channel) back into the original image space (1 or 3-channel) is necessary. This is done by the reconstruction net.</p><p>We have a single hidden layer for each sub-net. Only the layer for the inference net is recursive. Other sub-nets are vastly similar to the standard mutilayer perceptrons (MLP) with a single hidden layer. For MLP, full connection of F neurons is equivalent to a convolution with 1 × 1 × F × F . In our sub-nets, we use 3×3×F ×F filters. For embedding net, we use 3×3 filters because image gradients are more informative than the raw intensities for super-resolution. For inference net, 3 × 3 convolutions imply that hidden states are passed to adjacent pixels only. Reconstruction net also takes direct neighbors into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematical Formulation</head><p>The network takes an interpolated input image (to the desired size) as input x and predicts the target image y as in SRCNN <ref type="bibr" target="#b5">[5]</ref>. Our goal is to learn a model f that predicts valuesŷ = f (x), whereŷ is its estimate of ground truth output y. Let f 1 , f 2 , f 3 denote subnet functions: embedding, inference and reconstruction, respectively. Our model is the composition of three functions:</p><formula xml:id="formula_3">f (x) = f 3 (f 2 (f 1 (x))).</formula><p>Embedding net f 1 (x) takes the input vector x and computes the matrix output H 0 , which is an input to the inference net f 2 . Hidden layer values are denoted by H −1 . The formula for embedding net is as follows:</p><formula xml:id="formula_4">H −1 = max(0, W −1 * x + b −1 ) (1) H 0 = max(0, W 0 * H −1 + b 0 ) (2) f 1 (x) = H 0 ,<label>(3)</label></formula><p>where the operator * denotes a convolution and max(0, ·) corresponds to a ReLU. Weight and bias matrices are W −1 , W 0 and b −1 , b 0 . Inference net f 2 takes the input matrix H 0 and computes the matrix output H D . Here, we use the same weight and bias matrices W and b for all operations. Let g denote the function modeled by a single recursion of the recursive layer: g(H) = max(0, W * H +b). The recurrence relation is</p><formula xml:id="formula_5">H d = g(H d−1 ) = max(0, W * H d−1 + b),<label>(4)</label></formula><p>for d = 1, ..., D. Inference net f 2 is equivalent to the composition of the same elementary function g:</p><formula xml:id="formula_6">f 2 (H) = (g • g • · · · •)g(H) = g D (H),<label>(5)</label></formula><p>where the operator • denotes a function composition and g d denotes the d-fold product of g. Reconstruction net f 3 takes the input hidden state H D and outputs the target image (high-resolution). Roughly speaking, reconstruction net is the inverse operation of embedding net. The formula is as follows:</p><formula xml:id="formula_7">H D+1 = max(0, W D+1 * H D + b D+1 ) (6) y = max(0, W D+2 * H D+1 + b D+2 ) (7) f 3 (H) =ŷ.<label>(8)</label></formula><p>Model Properties Now we have all components for our model. The recursive model has pros and cons. While the recursive model is simple and powerful, we find training a deeply-recursive network very difficult. This is in accordance with the limited success of previous methods using at most three recursions so far <ref type="bibr" target="#b17">[17]</ref>. Among many reasons, two severe problems are vanishing and exploding gradients <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>Exploding gradients refer to the large increase in the norm of the gradient during training. Such events are due to the multiplicative nature of chained gradients. Long term components can grow exponentially for deep recursions. The vanishing gradients problem refers to the opposite behavior. Long term components approach exponentially fast to the zero vector. Due to this, learning the relation between distant pixels is very hard. Another known issue is that storing an exact copy of information through many recursions is not easy. In SR, output is vastly similar to input and recursive layer needs to keep the exact copy of input image for many recursions. These issues are also observed when we train our basic recursive model and we did not succeed in training a deeply-recursive network.</p><p>In addition to gradient problems, there exists an issue with finding the optimal number of recursions. If recursions are too deep for a given task, we need to reduce the number of recursions. Finding the optimal number requires training many networks with different recursion depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Advanced Model</head><p>Recursive-Supervision To resolve the gradient and optimal recursion issues, we propose an improved model. We supervise all recursions in order to alleviate the effect of vanishing/exploding gradients. As we have assumed that the same representation can be used again and again during convolutions in the inference net, the same reconstruction net is used to predict HR images for all recursions. Our reconstruction net now outputs D predictions and all predictions are simultaneously supervised during training ( <ref type="figure" target="#fig_0">Figure  3 (a)</ref>). We use all D intermediate predictions to compute the final output. All predictions are averaged during testing. The optimal weights are automatically learned during training.</p><p>A similar but a different concept of supervising intermediate layers for a convolutional network is used in Lee et al <ref type="bibr" target="#b16">[16]</ref>. Their method simultaneously minimizes classification error while improving the directness and transparency of the hidden layer learning process. There are two significant differences between our recursive-supervision and deep-supervision proposed in Lee et al. <ref type="bibr" target="#b16">[16]</ref>. They associate a unique classifier for each hidden layer. For each additional layer, a new classifier has to be introduced, as well as new parameters. If this approach is used, our modified network would resemble that of <ref type="figure" target="#fig_0">Figure 3(b)</ref>. We would then need D different reconstruction networks. This is against our original purpose of using recursive networks, which is avoid introducing new parameters while stacking more layers. In addition, using different reconstruction nets no longer effectively regularizes the network. The second difference is that Lee et al. <ref type="bibr" target="#b16">[16]</ref> discards all intermediate classifiers during testing. However, an ensemble of all intermediate predictions significantly boosts the performance. The final output from the ensemble is also supervised.</p><p>Our recursive-supervision naturally eases the difficulty of training recursive networks. Backpropagation goes through a small number of layers if supervising signal goes directly from loss layer to early recursion. Summing all gradients backpropagated from different prediction losses gives a smoothing effect. The adversarial effect of vanishing/exploding gradients along one backpropagation path is alleviated.</p><p>Moreover, the importance of picking the optimal number of recursions is reduced as our supervision enables utilizing predictions from all intermediate layers. If recursions are too deep for the given task, we expect the weight for late predictions to be low while early predictions receive high weights.</p><p>By looking at weights of predictions, we can figure out the marginal gain from additional recursions.</p><p>We present an expanded CNN structure of our model for illustration purposes in <ref type="figure" target="#fig_0">Figure 3(c)</ref>. If parameters are not allowed to be shared and CNN chains vary their depths, the number of free parameters grows fast (quadratically).</p><p>Skip-Connection Now we describe our second extension: skip-connection. For SR, input and output images are highly correlated. Carrying most if not all of input values until the end of the network is inevitable but very inefficient. Due to gradient problems, exactly learning a simple linear relation between input and output is very difficult if many recursions exist in between them.</p><p>We add a layer skip <ref type="bibr" target="#b3">[3]</ref> from input to the reconstruction net. Adding layer skips is successfully used for a semantic segmentation network <ref type="bibr" target="#b18">[18]</ref> and we employ a similar idea. Now input image is directly fed into the reconstruction net whenever it is used during recursions. Our skip-connection has two advantages. First, network capacity to store the input signal during recursions is saved. Second, the exact copy of input signal can be used during target prediction.</p><p>Our skip-connection is simple yet very effective. In super-resolution, LR and HR images are vastly similar. In most regions, differences are zero and only small number of locations have non-zero values. For this reason, several super-resolution methods <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b1">2]</ref> predict image details only. Similarly, we find that this domain-specific knowledge significantly improves our learning procedure.</p><p>Mathematical Formulation Each intermediate prediction under recursive-supervision <ref type="figure" target="#fig_0">(Figure 3(a)</ref>) iŝ</p><formula xml:id="formula_8">y d = f 3 (x, g (d) (f 1 (x))),<label>(9)</label></formula><p>for d = 1, 2, . . . , D, where f 3 now takes two inputs, one from skip-connection. Reconstruction net with skipconnection can take various functional forms. For example, input can be concatenated to the feature maps H d . As the input is an interpolated input image (roughly speaking,</p><formula xml:id="formula_9">y ≈ x), we find f 3 (x, H d ) = x + f 3 (H d )</formula><p>is enough for our purpose. More sophisticated functions for merging two inputs to f 3 will be explored in the future. Now, the final output is the weighted average of all intermediate predictions:ŷ</p><formula xml:id="formula_10">= D d=1 w d ·ŷ d .<label>(10)</label></formula><p>where w d denotes the weights of predictions reconstructed from each intermediate hidden state during recursion. These weights are learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Objective We now describe the training objective used to find optimal parameters of our model. Given a training dataset</p><formula xml:id="formula_11">{x (i) , y (i) } N i=1</formula><p>, our goal is to find the best model f that accurately predicts valuesŷ = f (x).</p><p>In the least-squares regression setting, typical in SR, the mean squared error </p><formula xml:id="formula_12">l 1 (θ) = D d=1 N i=1 1 2DN ||y (i) −ŷ (i) d || 2 ,<label>(11)</label></formula><p>where θ denotes the parameter set andŷ</p><formula xml:id="formula_13">(i)</formula><p>d is the output from the d-th recursion. For the final output, we have</p><formula xml:id="formula_14">l 2 (θ) = N i=1 1 2N ||y (i) − D d=1 w d ·ŷ (i) d || 2<label>(12)</label></formula><p>Now we give the final loss function L(θ). The training is regularized by weight decay (L 2 penalty multiplied by β).</p><formula xml:id="formula_15">L(θ) = αl 1 (θ) + (1 − α)l 2 (θ) + β||θ|| 2 ,<label>(13)</label></formula><p>where α denotes the importance of the companion objective on the intermediate outputs and β denotes the multiplier of weight decay. Setting α high makes the training procedure stable as early recursions easily converge. As training progresses, α decays to boost the performance of the final output. Training is carried out by optimizing the regression objective using mini-batch gradient descent based on backpropagation (LeCun et al. <ref type="bibr" target="#b15">[15]</ref>). We implement our model using the MatConvNet 1 package <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we evaluate the performance of our method on several datasets. We first describe datasets used  <ref type="figure">Figure 4</ref>: Super-resolution results of "img082"(Urban100) with scale factor ×4. Line is straightened and sharpened in our result, whereas other methods give blurry lines. Our result seems visually pleasing.  <ref type="figure">Figure 7</ref>: Super-resolution results of "58060" (B100) with scale factor ×2. A three-line stripe in ground truth is also observed in DRCN, whereas it is not clearly seen in results of other methods.</p><p>for training and testing our method. Next, our training setup is given. We give several experiments for understanding our model properties. The effect of increasing the number of recursions is investigated. Finally, we compare our method with several state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For training, we use 91 images proposed in Yang et al. <ref type="bibr" target="#b31">[31]</ref> for all experiments. For testing, we use four datasets. Datasets Set5 <ref type="bibr" target="#b19">[19]</ref> and Set14 <ref type="bibr" target="#b32">[32]</ref> are often used for benchmark <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b5">5]</ref>. Dataset B100 consists of natural images in the Berkeley Segmentation Dataset <ref type="bibr" target="#b20">[20]</ref>. Finally, dataset Urban100, urban images recently provided by Huang et al. <ref type="bibr" target="#b10">[10]</ref>, is very interesting as it contains many challenging images failed by existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Setup</head><p>We use 16 recursions unless stated otherwise. When unfolded, the longest chain from the input to the output passes 20 conv. layers (receptive field of 41 by 41). We set the momentum parameter to 0.9 and weight decay to 0.0001. We use 256 filters of the size 3 × 3 for all weight layers. Training images are split into 41 by 41 patches with stride 21 and 64 patches are used as a mini-batch for stochastic gradient descent.</p><p>For initializing weights in non-recursive layers, we use the method described in He et al. <ref type="bibr" target="#b9">[9]</ref>. For recursive convolutions, we set all weights to zero except self-connections (connection to the same neuron in the next layer) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b14">14]</ref>. Biases are set to zero.</p><p>Learning rate is initially set to 0.01 and then decreased by a factor of 10 if the validation error does not decrease for 5 epochs. If learning rate is less than 10 −6 , the procedure is terminated. Training roughly takes 6 days on a machine using one Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Study of Deep Recursions</head><p>We study the effect of increasing recursion depth. We trained four models with different numbers of recursions: 1, 6, 11, and 16. Four models use the same number of parameters except the weights used for ensemble. In <ref type="figure">Figure   8</ref>, it is shown that as more recursions are performed, PSNR measures increase. Increasing recursion depth with a larger image context and more nonlinearities boosts performance. The effect of ensemble is also investigated. We first evaluate intermediate predictions made from recursions ( <ref type="figure">Figure 9</ref>). The ensemble output significantly improves performances of individual predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with State-of-the-Art Methods</head><p>We provide quantitative and qualitative comparisons. For benchmark, we use public code for A+ <ref type="bibr" target="#b29">[29]</ref>, SRCNN <ref type="bibr" target="#b5">[5]</ref>, RFL <ref type="bibr" target="#b23">[23]</ref> and SelfEx <ref type="bibr" target="#b10">[10]</ref>. We deal with luminance components only as similarly done in other methods because human vision is much more sensitive to details in intensity than in color.</p><p>As some methods such as A+ <ref type="bibr" target="#b29">[29]</ref> and RFL <ref type="bibr" target="#b23">[23]</ref> do not predict image boundary, they require cropping pixels near borders. For our method, this procedure is unnecessary as our network predicts the full-sized image. For fair comparison, however, we also crop pixels to the same amount. PSNRs can be slightly different from original papers as existing methods use slightly different evaluation frameworks. We use the public evaluation code used in <ref type="bibr" target="#b10">[10]</ref>.</p><p>In <ref type="table">Table 1</ref>, we provide a summary of quantitative evaluation on several datasets. Our method outperforms all existing methods in all datasets and scale factors (both PSNR and SSIM). In <ref type="figure">Figures 4, 5, 6</ref> and 7, example images are given. Our method produces relatively sharp edges respective to patterns. In contrast, edges in other images are blurred. Our method takes a second to process a 288 × 288 image on a GPU Titan X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have presented a super-resolution method using a deeply-recursive convolutional network. Our network efficiently reuses weight parameters while exploiting a large image context. To ease the difficulty of training the model, we use recursive-supervision and skipconnection. We have demonstrated that our method outperforms existing methods by a large margin on benchmarked images. In the future, one can try more recursions in order to use image-level context. We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a): Our final (advanced) model with recursive-supervision and skip-connection. The reconstruction network is shared for recursive predictions. We use all predictions from the intermediate recursion to obtain the final output. (b): Applying deep-supervision [16] to our basic model. Unlike in (a), the model in (b) uses different reconstruction networks for recursions and more parameters are used. (c): An example of expanded structure of (a) without parameter sharing (no recursion). The number of weight parameters is proportional to the depth squared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 2</head><label>1</label><figDesc>||y − f (x)|| 2 averaged over the train- ing set is minimized. This favors high Peak Signal-to-Noise Ratio (PSNR), a widely-used evaluation criteria. With recursive-supervision, we have D + 1 objectives to minimize: supervising D outputs from recursions and the final output. For intermediate outputs, we have the loss function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIMBenchmark results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, B100 and Urban100. Red color indicates the best performance and blue color refers the second best.</figDesc><table>Ground Truth 
A+ [29] 
SRCNN [5] 
RFL [23] 
SelfEx [10] 
DRCN (Ours) 
(PSNR, SSIM) 
(23.53, 0.6977) 
(23.79, 0.7087) 
(23.53, 0.6943) 
(23.52, 0.7006) 
(24.36, 0.7399) 

Figure 5: Super-resolution results of "134035" (B100) with scale factor ×4. Our result shows a clear separation between branches 
while in other methods, branches are not well separated. 

Ground Truth 
A+ [29] 
SRCNN [5] 
RFL [23] 
SelfEx [10] 
DRCN (Ours) 
(PSNR, SSIM) 
(26.09, 0.9342) 
(27.01, 0.9365) 
(25.91, 0.9254) 
(27.10, 0.9483) 
(27.66, 0.9608) 

Figure 6: Super-resolution results of "ppt3" (Set14) with scale factor ×3. Texts in DRCN are sharp while, in other methods, 
character edges are blurry. Dataset Scale 

Bicubic 
A+ [29] 
SRCNN [5] 
RFL [23] 
SelfEx [10] DRCN (Ours) 
PSNR/Set5 

×2 33.66/0.9299 36.54/0.9544 36.66/0.9542 36.54/0.9537 36.49/0.9537 37.63/0.9588 
×3 30.39/0.8682 32.58/0.9088 32.75/0.9090 32.43/0.9057 32.58/0.9093 33.82/0.9226 
×4 28.42/0.8104 30.28/0.8603 30.48/0.8628 30.14/0.8548 30.31/0.8619 31.53/0.8854 

Set14 

×2 30.24/0.8688 32.28/0.9056 32.42/0.9063 32.26/0.9040 32.22/0.9034 33.04/0.9118 
×3 27.55/0.7742 29.13/0.8188 29.28/0.8209 29.05/0.8164 29.16/0.8196 29.76/0.8311 
×4 26.00/0.7027 27.32/0.7491 27.49/0.7503 27.24/0.7451 27.40/0.7518 28.02/0.7670 

B100 

×2 29.56/0.8431 31.21/0.8863 31.36/0.8879 31.16/0.8840 31.18/0.8855 31.85/0.8942 
×3 27.21/0.7385 28.29/0.7835 28.41/0.7863 28.22/0.7806 28.29/0.7840 28.80/0.7963 
×4 25.96/0.6675 26.82/0.7087 26.90/0.7101 26.75/0.7054 26.84/0.7106 27.23/0.7233 

Urban100 

×2 26.88/0.8403 29.20/0.8938 29.50/0.8946 29.11/0.8904 29.54/0.8967 30.75/0.9133 
×3 24.46/0.7349 26.03/0.7973 26.24/0.7989 25.86/0.7900 26.44/0.8088 27.15/0.8276 
×4 23.14/0.6577 24.32/0.7183 24.52/0.7221 24.19/0.7096 24.79/0.7374 25.14/0.7510 
Table 1: Ground Truth 
A+ [29] 
SRCNN [5] 
RFL [23] 
SelfEx [10] 
DRCN (Ours) 
(PSNR, SSIM) 
(24.24, 0.8176) 
(24.48, 0.8267) 
(24.24, 0.8137) 
(24.16, 0.8145) 
(24.76, 0.8385) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Figure 8: Recursion versus Performance for the scale factor ×3 on the dataset Set5. More recursions yielding larger receptive fields lead to better performances.Figure 9: Ensemble effect. Prediction made from intermediate recursions are evaluated. There is no single recursion depth that works the best across all scale factors. Ensemble of intermediate predictions significantly improves performance.</figDesc><table>Recursion 

1 
6 
11 
16 

PSNR (dB) 

32.6 

32.8 

33 

33.2 

33.4 

33.6 

33.8 

34 

Recursion d 

1 
3 
5 
7 
9 
11 
13 
15 

PSNR (dB) 

26 

27.2 

28.4 

29.6 

30.8 

32 

33.2 

34.4 

35.6 

36.8 

38 

#2 Single 
#3 Single 
#4 Single 

#2 Ensemble 
#3 Ensemble 
#4 Ensemble 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.vlfeat.org/matconvnet/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult. Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution using neighbor embedding of backprojection residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Image superresolution using deep convolutional networks. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding deep architectures using a recursive convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image superresolution using transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving resolution by image registration. CVGIP: Graphical models and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno>1998. 5</idno>
		<imprint>
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Deeplysupervised nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Marco Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4564</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On single image scaleup using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
