<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning via Learning to Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Transfer Learning via Learning to Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In transfer learning, what and how to transfer are two primary issues to be addressed, as different transfer learning algorithms applied between a source and a target domain result in different knowledge transferred and thereby the performance improvement in the target domain. Determining the optimal one that maximizes the performance improvement requires either exhaustive exploration or considerable expertise. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer are the best for a future pair of domains by optimizing the reflection function. We also theoretically analyse the algorithmic stability and generalization bound of L2T, and empirically demonstrate its superiority over several state-ofthe-art transfer learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inspired by human beings' capabilities to transfer knowledge across tasks, transfer learning aims to leverage knowledge from a source domain to improve the learning performance or minimize the number of labeled examples required in a target domain. It is of particular significance when tackling tasks with limited labeled examples. Transfer learning has proved its wide applicability in, for example, <ref type="bibr">1</ref> Hong Kong University of Science and Technology, Hong Kong 2 Tencent AI Lab, Shenzhen, China. Correspondence to: <ref type="bibr">Ying Wei &lt;judyweiying@gmail.com&gt;, Qiang Yang &lt;qyang@cse.ust.hk&gt;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s).</p><p>image classification <ref type="bibr" target="#b19">(Long et al., 2015)</ref>, sentiment classification <ref type="bibr" target="#b6">(Blitzer et al., 2006)</ref>, dialog systems <ref type="bibr" target="#b22">(Mo et al., 2016)</ref>, and urban computing <ref type="bibr" target="#b33">(Wei et al., 2016)</ref>.</p><p>Three key research issues in transfer learning, pointed by <ref type="bibr">Pan &amp; Yang, are</ref> when to transfer, how to transfer, and what to transfer. Once transfer learning from a source domain is considered to benefit a target domain (when to transfer), an algorithm (how to transfer) discovers the transferable knowledge across domains (what to transfer). Different algorithms are likely to discover different transferable knowledge, and thereby lead to uneven transfer learning effectiveness which is evaluated by the performance improvement over non-transfer baselines in a target domain. To achieve the optimal performance improvement for a target domain given a source domain, researchers may try tens to hundreds of transfer learning algorithms covering instance <ref type="bibr" target="#b9">(Dai et al., 2007)</ref>, parameter <ref type="bibr" target="#b31">(Tommasi et al., 2014)</ref>, and feature <ref type="bibr" target="#b25">(Pan et al., 2011)</ref> based algorithms. Such bruteforce exploration is computationally expensive and practically impossible. As a tradeoff, a sub-optimal improvement is usually obtained from a heuristically selected algorithm, which unfortunately requires considerable expertise in an ad-hoc and unsystematic manner.</p><p>Exploring different algorithms is not the only way to optimize what to transfer. Previous transfer learning experiences do also help, which has been widely accepted in educational psychology <ref type="bibr" target="#b20">(Luria, 1976;</ref><ref type="bibr" target="#b4">Belmont et al., 1982)</ref>. Human beings sharpen transfer learning skills of deciding what to transfer by conducting meta-cognitive reflection on diverse transfer learning experiences. For example, children who are good at playing chess may transfer mathematical skills, visuospatial skills, and decision making skills learned from chess to solve arithmetic problems, to solve pattern matching puzzles, and to play basketball, respectively. At a later age, it will be easier for them to decide to transfer mathematical and decision making skills learned from chess, rather than visuospatial skills, to market investment. Unfortunately, all existing transfer learning algorithms transfer from scratch and ignore previous transfer learning experiences.</p><p>Motivated by this, we propose a novel transfer learning framework called Learning to Transfer (L2T). The key idea of the L2T is to enhance the transfer learning effectiveness from a source to a target domain by leveraging previous transfer learning experiences to optimize what and how to transfer between them. To achieve the goal, we establish the L2T in two stages. During the first stage, we encode each transfer learning experience into three components: a pair of source and target domains, the transferred knowledge between them parameterized as latent feature factors, and performance improvement. We learn from all experiences a reflection function which maps a pair of domains and the transferred knowledge between them to the performance improvement. The reflection function, therefore, is believed to encrypt transfer learning skills of deciding what and how to transfer. In the second stage, what to transfer between a newly arrived pair of domains is optimized so that the value of the learned reflection function, matching to the performance improvement, is maximized.</p><p>The contribution of this paper lies in that we propose a novel transfer learning framework which opens a new door to improve transfer learning effectiveness by taking advantage of previous transfer learning experiences. The L2T can discover more transferable knowledge in a systematic and automatic fashion without requiring considerable expertise. We have also provided theoretic analyses to its algorithmic stability and generalization bound, and conducted comprehensive empirical studies showing the L2T's superiority over state-of-the-art transfer learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transfer Learning Pan &amp; Yang identified three key research issues in transfer learning as what, how, and when to transfer. Parameters <ref type="bibr" target="#b34">(Yang et al., 2007a;</ref><ref type="bibr" target="#b31">Tommasi et al., 2014)</ref>, instances <ref type="bibr" target="#b9">(Dai et al., 2007)</ref>, or latent feature factors <ref type="bibr" target="#b25">(Pan et al., 2011)</ref> can be transferred between domains. A few works <ref type="bibr" target="#b34">(Yang et al., 2007a;</ref><ref type="bibr" target="#b31">Tommasi et al., 2014)</ref> transfer parameters from source domains to regularize parameters of SVM-based models in a target domain. In <ref type="bibr" target="#b9">(Dai et al., 2007)</ref>, a basic learner in a target domain is boosted by borrowing the most useful source instances. Various techniques capable of learning transferable latent feature factors between domains have been investigated extensively. These techniques include manually selected pivot features <ref type="bibr" target="#b6">(Blitzer et al., 2006)</ref>, dimension reduction <ref type="bibr" target="#b25">(Pan et al., 2011;</ref><ref type="bibr" target="#b2">Baktashmotlagh et al., 2013;</ref>, collective matrix factorization <ref type="bibr" target="#b18">(Long et al., 2014)</ref>, dictionary learning and sparse coding <ref type="bibr" target="#b27">(Raina et al., 2007;</ref><ref type="bibr" target="#b37">Zhang et al., 2016)</ref>, manifold learning <ref type="bibr" target="#b13">(Gopalan et al., 2011;</ref><ref type="bibr" target="#b12">Gong et al., 2012)</ref>, and deep learning <ref type="bibr" target="#b36">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b19">Long et al., 2015;</ref><ref type="bibr" target="#b32">Tzeng et al., 2015)</ref>. Unlike L2T, all existing transfer learning studies transfer from scratch, i.e., only considering the pair of domains of interest but ignoring previous transfer learning experiences. Better yet, L2T can even collect all algorithms' wisdom together, considering that any algorithm mentioned above can be applied in a transfer learning experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lifelong Learning</head><p>Task 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to Transfer</head><p>Task 2  <ref type="figure">Figure 1</ref>. Illustration of the differences between our work and the other three lines of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Testing</head><p>Multi-task Learning Multi-task learning <ref type="bibr" target="#b8">(Caruana, 1997;</ref><ref type="bibr" target="#b1">Argyriou et al., 2007)</ref> trains multiple related tasks simultaneously and learns shared knowledge among tasks, so that all tasks reinforce each other in generalization abilities. However, multi-task learning assumes that training and testing examples follow the same distribution, as <ref type="figure">Figure 1</ref> shows, which is different from transfer learning we focus on.</p><p>Lifelong Learning Assuming a new learning task to lie in the same environment as training tasks, learning to learn <ref type="bibr" target="#b30">(Thrun &amp; Pratt, 1998)</ref> or meta-learning <ref type="bibr" target="#b21">(Maurer, 2005;</ref><ref type="bibr" target="#b11">Finn et al., 2017;</ref><ref type="bibr" target="#b0">Al-Shedivat et al., 2018)</ref> transfers the knowledge shared among training tasks to the new task. <ref type="bibr" target="#b28">(Ruvolo &amp; Eaton, 2013;</ref><ref type="bibr" target="#b26">Pentina &amp; Lampert, 2015)</ref> consider lifelong learning as online meta-learning. Though L2T and lifelong (meta) learning both aim to improve a learning system by leveraging histories, L2T differs from them in that each historical experience we consider is a transfer learning task rather than a traditional learning task as <ref type="figure">Figure 1</ref> illustrates. Thus we learn transfer learning skills instead of task-sharing knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning to Transfer</head><p>We begin by first briefing the proposed L2T framework. Then we detail the two stages in L2T, i.e., learning transfer learning skills from previous transfer learning experiences and applying those skills to infer what and how to transfer for a future pair of source and target domains. With N e transfer learning experiences {E 1 , · · · , E Ne } as the input, the L2T agent learns a function f such that f (S e , T e , W e ) approximates l e as shown in the training stage of <ref type="figure">Figure 2</ref>. We call f a reflection function which encrypts meta-cognitive transfer learning skills -what and how to transfer can maximize the improvement ratio given a pair of domains. Whenever a new pair of domains S Ne+1 , T Ne+1 arrives, the L2T agent can optimize the knowledge to be transferred, i.e., W * Ne+1 , by maximizing the value of f (see step 2 of the testing stage in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parameterizing What to Transfer</head><p>Transfer learning algorithms applied can vary from experience to experience. Uniformly parameterizing "what to transfer" for any algorithm out of the base algorithm set A is a prerequisite for learning the reflection function. In this work, we consider A to contain algorithms transferring single-level latent feature factors, because existing parameter-based and instance-based algorithms cannot address the transfer learning setting we focus on (i.e.,  <ref type="bibr" target="#b34">(Yang et al., 2007a;</ref><ref type="bibr" target="#b31">Tommasi et al., 2014)</ref> can transfer across domains in heterogeneous label spaces, they can only handle binary classification problems. Deep neural network based algorithms <ref type="bibr" target="#b36">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b19">Long et al., 2015;</ref><ref type="bibr" target="#b32">Tzeng et al., 2015)</ref> transferring latent feature factors in multiple levels are left for our future research. As a result, we parameterize what to transfer with a latent feature factor matrix W which is elaborated in the following.</p><p>Latent feature factor based algorithms aim to learn domaininvariant feature factors across domains. Consider classifying dog pictures as a source domain and cat pictures as a target domain. The domain-invariant feature factors may include eyes, mouth, tails, etc. What to transfer, in this case, is the shared feature factors across domains. The way of defining domain-invariant feature factors dictates two groups of latent feature factor based algorithms, i.e., common latent space based and manifold ensemble based algorithms.</p><p>Common Latent Space Based This line of algorithms, including but not limited to TCA <ref type="bibr" target="#b25">(Pan et al., 2011)</ref>, LS-DT , and DIP <ref type="bibr" target="#b2">(Baktashmotlagh et al., 2013)</ref>, assumes that domain-invariant feature factors lie in a single shared latent space. We denote by ϕ the function mapping original feature representation into the latent space. If ϕ is linear, it can be represented as an embedding matrix W ∈ R m×u where u is the dimensionality of the latent space. Therefore, we can parameterize what to transfer we focus on with W which describes u latent feature factors. Otherwise, if ϕ is nonlinear, what to transfer can still be parameterized with W. Though a nonlinear ϕ is not explicitly specified in most cases such as LSDT using sparse coding, target examples represented in the latent space Z t e = ϕ(X t e ) ∈ R n t e ×u are always available. Consequently, we obtain the similarity metric matrix <ref type="bibr" target="#b7">(Cao et al., 2013)</ref>  </p><formula xml:id="formula_0">in the latent space, i.e., G = (X t e ) † Z t e (Z t e ) T [(X t e ) T ] † ∈ R m×m according to X t e G(X t e ) T = Z t e (Z t e ) T ,</formula><formula xml:id="formula_1">) T = X t e G(X t e )</formula><p>T where G is the similarity metric matrix. For computational details of G, please refer to <ref type="bibr" target="#b12">(Gong et al., 2012)</ref>. W = LD 1/2 with L and D obtained from performing LDL decomposition on G = LDL T , therefore, is also qualified to represent latent feature factors distributed in a series of subspaces on a manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning from Experiences</head><p>The goal here is to learn a reflection function f such that f (S e , T e , W e ) can approximate l e for all experiences {E 1 , · · · , E Ne }. The improvement ratio l e is closely related to two aspects: 1) the difference between a source and a target domain in the shared latent space, and 2) the discriminative ability of a target domain in the latent space. The smaller difference guarantees more overlap between domains in the latent space, which signifies more transferable latent feature factors and higher improvement ratios as a result. The discriminative ability of a target domain in the latent space is also vital to improve performances. Therefore, we build f to take both aspects into consideration.</p><p>The Difference between a Source and a Target Domain We follow <ref type="bibr" target="#b25">(Pan et al., 2011)</ref> and adopt the maximum mean discrepancy (MMD) <ref type="bibr" target="#b15">(Gretton et al., 2012b)</ref>  </p><formula xml:id="formula_2">K(x s ei We, x t ej We),<label>(1)</label></formula><p>where x t ej is the j-th example in X t e , and φ maps from the u-dimensional latent space to the RKHS H.</p><formula xml:id="formula_3">K(·, ·) = φ(·), φ(·)</formula><p>is the kernel function. Different kernels K lead to different MMD distances and thereby different values of f . Thus learning the reflection function f is equivalent to optimizing K so that the MMD distance can well characterize the improvement ratio l e for all pairs of domains. Inspired by multi-kernel MMD <ref type="bibr" target="#b15">(Gretton et al., 2012b)</ref>, we parameterize K as a linear combination of </p><formula xml:id="formula_4">N k PSD kernels, i.e., K = N k k=1 β k K k (β k ≥ 0,</formula><formula xml:id="formula_5">K k (a, b) = exp(− a−b 2 /δ k ) by varying the bandwidth δ k .</formula><p>Unfortunately, the MMD alone is insufficient to measure the difference between domains. The distance variance among all pairs of instances across domains is also required to fully characterize the difference. A pair of domains with small MMD but extremely high variance still have little overlap. Equation <ref type="formula" target="#formula_2">(1)</ref>  To be consistent with the MMD characterized with N k PSD kernels, we rewrite σ where h k 1 is calculated using the k 1 -th kernel. We detail the empirical estimateQ e of Q e in the supplementary due to page limit.</p><formula xml:id="formula_6">2 e = β T Qeβ where Qe = cov(h) = σ e(1,1) ··· σ e(1,N k ) ··· ··· ··· σ e(N k ,1) ··· σ e(N k ,N k ) . Each element σ e(k 1 ,k 2 ) = cov(h k 1 , h k 2 ) = E [(h k 1 −Eh k 1 )(h k 2 −Eh k 2 )]</formula><p>The Discriminative Ability of a Target Domain In view of limited labeled examples in a target domain, we resort to unlabeled examples to evaluate the discriminative ability. The principles of the unlabeled discriminant criterion are two-fold: 1) similar examples should still be neighbours after being embedded into the latent space; and 2) dissimilar examples should be far away. We adopt the unlabeled discriminant criterion proposed in <ref type="bibr" target="#b35">(Yang et al., 2007b)</ref> T , the non-local scatter covariance matrix, enforces the second principle. τ e also depends on kernels which in this case indicate different neighbour information and different degrees of similarity between neighboured examples. With τ e(k) obtained from the k-th kernel K k , the unlabeled discriminant criterion τ e can be written as</p><formula xml:id="formula_7">τ e = N k k=1 β k τ e(k) = β T τ e where τ e = [τ e(1) , · · · , τ e(N k ) ].</formula><p>The Optimization Problem Combining the two aspects abovementioned to model the reflection function f , we finally formulate the optimization problem as follows,</p><formula xml:id="formula_8">β * , λ * , μ * , b * = arg min β,λ,μ,b Ne e=1 L h β Td e + λβ TQ eβ + μ β T τ e + b, 1 le + γ1R(β, λ, μ, b), s.t. β k ≥ 0, ∀k ∈ {1, · · · , N k }, λ ≥ 0, μ ≥ 0,<label>(2)</label></formula><p>where 1/f = β  <ref type="bibr" target="#b17">(Huber et al., 1964)</ref> constraining the value of 1/f to be as close to 1/l e as possible. γ 1 controls the complexity of the parameters by l2-regularization. Minimizing the difference between domains, including the MMD distance β Td e and the distance variance β TQ eβ, and meanwhile maximizing the discriminant criterion β T τ e in the target domain will contribute a large performance improvement ratio l e (i.e., a small 1/l e ). λ and μ balance the importance of the three terms in f , and b is the bias term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inferring What to Transfer</head><p>Once the L2T agent has learned the reflection function f (S, T , W; β * , λ * , μ * , b * ), it takes advantage of the function to optimize what to transfer, i.e., the latent feature factor matrix W, for a newly arrived source domain S Ne+1 and a target domain T Ne+1 . The optimal latent feature factor matrix W * Ne+1 should maximize the value of f . To this end, we optimize the following objective with regard to W,</p><formula xml:id="formula_9">W * Ne+1 = arg max W f (SN e+1 , TN e +1 , W; β * , λ * , μ * , b * ) − γ2 W 2 F = arg min W (β * ) Td W + λ * (β * ) TQ W β * + μ * 1 (β * ) T τ W + γ2 W 2 F ,<label>(3)</label></formula><p>where · F denotes the matrix Frobenius norm and γ 2 controls the complexity of W. The first and second terms in problem (3) can be calculated as</p><formula xml:id="formula_10">(β * ) Td W = N k k=1 β * k 1 a 2 a i,i =1 K k (viW, v i W)+ 1 b 2 b j,j =1 K k (wj W, w j W) − 2 ab a,b i,j=1 K k (viW, wj W) , (β * ) TQ W β * = 1 n 2 − 1 n i,i =1 N k k=1 β * k K k (viW, v i W)+ K k (wiW, w i W) − 2K k (viW, w i W) − 1 n 2 n i,i =1 K k (viW, v i W) + K k (wiW, w i W) − 2K k (viW, w i W) 2 ,</formula><p>where the shorthand  </p><formula xml:id="formula_11">vi = x s (Ne+1)i , v i = x s (Ne+1)i , wj = x t (Ne+1)j , w j = x t (Ne+1)j , a = n</formula><formula xml:id="formula_12">) T τ W = N k k=1 β * k tr(W T S N k W) tr(W T S L k W)</formula><p>. We optimize the non-convex problem (3) w.r.t W by employing a conjugate gradient method in which the gradient is listed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stability and Generalization Bounds</head><p>In this section, we would theoretically investigate how previous transfer learning experiences influence a transfer learning task of interest. We also provide and prove the algorithmic stability and generalization bound for latent feature factor based transfer learning algorithms without experiences considered in the supplementary.</p><p>Consider S = { S1, T1 ,· · ·, SN e , TN e } to be N e transfer learning experiences or the so-called meta-samples <ref type="bibr" target="#b21">(Maurer, 2005)</ref>. Let L(S) be our algorithm that learns meta-cognitive knowledge from N e transfer learning experiences in S and applies the knowledge to the (N e +1)-th transfer learning task SN e+1 , TN e+1 . To analyse the stability and give the generalization bound, we make an assumption on the distribution from which all N e transfer learning experiences as meta-samples are sampled. For every environment E we have, all N e pairs of source and target domains in S are drawn according to an algebraic β-mixing stationary distribution (D E ) Ne , which is not i.i.d.. Intuitively, the algebraical β-mixing stationary distribution (see Definition 2 in <ref type="bibr" target="#b23">(Mohri &amp; Rostamizadeh, 2010)</ref>) with the β-mixing coefficient β(m) ≤ β 0 /m r models the dependence between future samples and past samples by a distance of at least m. The independent block technique <ref type="bibr" target="#b5">(Bernstein, 1927)</ref>  </p><formula xml:id="formula_13">l emp (L(S), (S, T )) − l emp (L(S e0 ), (S, T )) ≤ 4(4N e − 3 + r /r W )B 2 r x λN 2 e ∼ O B 2 r x λN e ,<label>(4)</label></formula><p>where By generalizing S to be meta-samples S and h S to be L2T L(S), we apply Corollary 21 in <ref type="bibr" target="#b23">(Mohri &amp; Rostamizadeh, 2010)</ref> to give the generalization bound of our algorithm L(S) in Theorem 2.</p><formula xml:id="formula_14">S = { S1, T1 , · · · , Se 0 −1, Te 0 −1 , Se 0 , Te 0 , Se 0 +1, Te 0 +1 , · · · ,</formula><formula xml:id="formula_15">Theorem 2. Let δ = δ −(N e ) 1 2(r+1) − 1 4 (r &gt; 1 is required).</formula><p>Then for any sample S of size N e drawn according to an algebraic β-mixing stationary distribution, and δ ≥ 0 such that δ ≥ 0, the following generalization bound holds with probability at least 1 − δ:</p><formula xml:id="formula_16">R(L(S)) − RN e (L(S)) &lt; O (Ne) 1 2(r+1) − 1 4 log( 1 δ ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>where R(L(S)) and R Ne (L(S)) denote the expected risk and the empirical risk of L2T over meta-samples, respectively. A larger mixing parameter r, indicating more independence, would lead to a tighter bound.</head><p>Theorem 2 tells that as the number of transfer learning experiences, i.e., N e , increases, L2T tends to produce a tighter generalization bound. This fact lays the foundation for further conducting L2T in an online manner which can gradually assimilate transfer learning experiences and continuously improve. The detailed proofs for Theorem 1 and 2 can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets We evaluate the L2T framework on two image datasets, Caltech-256 <ref type="bibr" target="#b16">(Griffin et al., 2007)</ref> and Sketches <ref type="bibr" target="#b10">(Eitz et al., 2012)</ref>. Caltech-256, collected from Google Images, contains a total of 30,607 images in 256 categories. The Sketches dataset, however, consists of 20,000 unique sketches by human beings that are evenly distributed over 250 different categories. We construct each pair of source and target domains by randomly sampling three categories from Caltech-256 as the source domain and randomly sampling three categories from Sketches as the target domain, which we give an example in the supplementary material. Consequently, there are 20, 000/250 × 3 = 720 examples in a target domain of each pair. In total, we generate 1,000 training pairs for preparing transfer learning experiences, 500 validation pairs to determine hyperparameters of the reflection function, and 500 testing pairs to evaluate the reflection function. We characterize each image from both datasets with 4,096-dimensional features extracted by a convolutional neural network pre-trained by ImageNet.</p><p>In this paper we generate transfer learning experiences by ourselves, because we are the first to consider transfer learning experiences and there exists no off-the-shelf datasets. In real-world applications, either the number of labeled examples in a target domain or the transfer learning algorithm could vary from experience to experience. In order to mimic the real environment, we prepare each transfer learning experience by randomly selecting a transfer learning algorithm from a base set A and randomly setting the number of labeled target examples in the range of <ref type="bibr">[3,</ref><ref type="bibr">120]</ref>. The randomly generated training experiences, lying in the same environment (generated by one dataset), are non i.i.d., which fit the algebraical β-mixing assumption theoretically in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Evaluation Metrics</head><p>We compare L2T with the following nine baseline algorithms in three classes:</p><p>• Non-transfer: Original builds a model using labeled data in a target domain only.</p><p>• Common latent space based transfer learning algorithms: TCA <ref type="bibr" target="#b25">(Pan et al., 2011)</ref>, ITL , CMF <ref type="bibr" target="#b18">(Long et al., 2014)</ref>, LSDT , STL <ref type="bibr" target="#b27">(Raina et al., 2007)</ref>, DIP <ref type="bibr" target="#b2">(Baktashmotlagh et al., 2013)</ref> and SIE <ref type="bibr" target="#b3">(Baktashmotlagh et al., 2014</ref>).</p><p>• Manifold ensemble based algorithms: GFK <ref type="bibr" target="#b12">(Gong et al., 2012)</ref>.</p><p>The eight feature-based transfer learning algorithms also constitute the base set A. Based on feature representations obtained by different algorithms, we use the nearestneighbor classifier to perform three-class classification for the target domain.</p><p>One evaluation metric is classification accuracy on testing examples of a target domain. However, accuracies are incomparable for different target domains at different levels of difficulty. The other evaluation metric we adopt is the performance improvement ratio defined in Section 3.1, so as to compare the L2T over different pairs of domains. Performance Comparison In this experiment, we learn a reflection function from 1,000 transfer learning experiences, and evaluate the reflection function on 500 testing pairs of source and target domains by comparing the average performance improvement ratio to the baselines.  <ref type="bibr" target="#b14">(Gretton et al., 2012a</ref>). As <ref type="figure" target="#fig_3">Figure 4</ref> shows, on average the proposed L2T framework outperforms the baselines up to 10% when varying the number of labeled samples in the target domain. As the number of labeled target examples increases from 3 to 120, the performance improvement ratio becomes smaller because the accuracy of Original without transfer tends to increase. The baseline algorithms behave differently. The transferable knowledge learned by LSDT helps a target domain a lot when training examples are scarce, while GFK performs poorly until training examples become more. STL is almost the worst baseline because it learns a dictionary from the source domain only but ignores the target domain. It runs at a high risk of failure especially when two domains are distant. DIP and SIE, which minimize the MMD and Hellinger distance between domains subject to manifold constraints, are competent. Note that we have run the paired t-test between L2T and each baseline with all the p-values in the order of 10 −12 , concluding that the L2T is significantly superior.</p><p>We also randomly select six of the 500 testing pairs and compare classification accuracies by different algorithms for each pair in <ref type="figure" target="#fig_4">Figure 3</ref>. The performance of all baselines varies from pair to pair. Among all the baseline methods, TCA performs the best when transferring between domains in <ref type="figure" target="#fig_4">Figure 3a</ref> and LSDT is the most superior in <ref type="figure" target="#fig_4">Figure 3c</ref>. However, L2T consistently outperforms the baselines on all the settings. For some pairs, e.g., <ref type="figure" target="#fig_4">Figures 3a, 3c</ref> and 3f, the three classes in a target domain are comparably easy to tell apart, hence Original without transfer can achieve even better results than some transfer learning algorithms. In this case, L2T still improves by discovering the best transferable knowledge from the source domain, especially when the number of labeled examples is small (see <ref type="figure" target="#fig_4">Figure 3c</ref> and 3f). If two domains are very related, e.g., the source with "galaxy" and "saturn" and the target with "sun" in <ref type="figure" target="#fig_4">Figure 3a</ref>, L2T even finds out more transferable knowledge and contributes more significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying the Experiences</head><p>We further investigate how transfer learning experiences used to learn the reflection function influence the performance of L2T. In this experiment, we evaluate on 50 randomly sampled pairs out of the 500 testing pairs in order to efficiently investigate a wide range of cases in the following. The sampled set is unbiased and sufficient to characterize such influence, evidenced by the asymptotic consistency between the average performance improvement ratio on the 500 pairs in <ref type="figure" target="#fig_3">Figure 4</ref> and that on the 50 pairs in the last line of <ref type="table" target="#tab_12">Table 1</ref>. First, we fix the number of transfer learning experiences to be 1,000 and vary the set of base transfer learning algorithms. The results are shown in <ref type="table" target="#tab_12">Table 1</ref>. Even with experiences generated by single base algorithm, e.g., ITL or DIP, the L2T can still learn a reflection function that significantly better (p-value &lt; 0.05) decides what to transfer than using ITL or DIP directly. With more base algorithms involved, the transfer learning experiences are more diverse to cover more situations of source-target pairs and the knowledge transferred between them. As a result, the L2T learns a better reflection function and thereby achieves higher performance improvement ratios, which coincides with Theorem 2 where a larger r indicating more independence between experiences gives a tighter bound. Second, we fix the set of base algorithms to include all the eight baselines and vary the number of transfer learning experiences used for training. As shown in <ref type="figure">Figure 5</ref>, the average performance improvement ratio achieved by L2T tends to increase as the number of labeled examples in the target domain decreases, given that Original without transfer performs extremely poor with scarce labeled examples.   More importantly, it increases as the number of experiences increases, which coincides with Theorem 2.</p><p>Varying the Reflection Function We also study the influence of different configurations of the reflection function on the performance of L2T. First, we vary the components to be considered in building the reflection function f as shown in <ref type="figure">Figure 6</ref>. Considering single type, either MMD, variance, or the discriminant criterion, brings inferior performance and even negative transfer. L2T taking all the three factors into consideration outperforms the others, demonstrating that the three components are all necessary and mutually reinforcing. With all the three components included, we plot values of the learned β * in the supplementary material. Second, we change the kernels used. In <ref type="figure" target="#fig_6">Figure 7,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel L2T framework for transfer learning which automatically optimizes what and how to transfer between a source and a target domain by leveraging previous transfer learning experiences. In particular, L2T learns a reflection function mapping a pair of domains and the knowledge transferred between them to the performance improvement ratio. When a new pair of domains arrives, L2T optimizes what and how to transfer by maximizing the value of the learned reflection function. We believe that L2T opens a new door to improve transfer learning by leveraging transfer learning experiences. Many research issues, e.g., incorporating hierarchical latent feature factors as what to transfer and designing online L2T, can be further examined.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The third term in problem (3) can be computed as (β *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Average performance improvement ratio comparison over 500 testing pairs of source and target domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Classification accuracies on six pairs of source and target domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Varying the number of transfer learning experiences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Varying the number of kernels considered in the f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>e , where the superscript * can be either s or t to denote a source or a target domain. y * e ∈ Y * e denotes the vector of labels with the length being n * le . The num- ber of target labeled examples is much smaller than that of source labeled examples, i.e., n∈ A = {a 1 , · · · , a Na } denotes a transfer learning algo- rithm having been applied between S e and T e . Suppose that the transferred knowledge by the algorithm a e can be param- eterized as W e . Finally, each transfer learning experience is labeled by the performance improvement ratio l e = pis the learning performance (e.g., classification accuracy) on a test dataset in T e without transfer and p st e is that on the same test dataset after transferring W e from S e .</figDesc><table>3.1. The L2T Framework 

A L2T agent previously conducted transfer learning sev-
eral times, and kept a record of N e transfer learning ex-
periences. We define each transfer learning experience 
as E e = ( S e , T e , a e , l e ) in which S e = {X 

s 

e , y 

s 

e } and 
T e = {X 

t 

e , y 

t 

e } denote a source domain and a target domain, 
respectively. X 

 *  

e ∈ R 

n 

 *  

e ×m represents the feature matrix 
if either domain has n 

 *  

e examples in a m-dimensional fea-
ture space X 

 *  

t 
le 

n 

s 

le . We focus on the optimize what to transfer for a target pair of source and target domains 

source 
domain 

target 
domain 

transfer 
algorithm 

performance 
improvement 
learn transfer 
learning skills 

1 

e 

N 
1 

e 

N 

e 

N 

1 
1 
1 
1 

( ) 
a W 

( 
) 

e 
e 

N 
N 

a W 

1 

l 

e 

N 

l 

* 
1 
1 
1 

arg max ( 
, 
, ) 

e 
ee 

N 
N 
N 

f 
W 
W 

( , , ) 

e 
e 
e 
e 

f 
l 
W 

e 

W 

, 

e 

e 

e 

l 
Training 

Testing 
( , , ) 

e 
e 
e 
e 

f 
l 
W 

* 
1 

e 

N 

W 

1 

1 

2 
2 

e 

N 

1 

E 

e 

N 

E 

Figure 2. Illustration of the L2T framework: in the training stage, we have Ne transfer learning experiences {E1, · · · , EN e } from which 
we learn a reflection function f encrypting transfer learning skills; in the testing stage, given the (Ne + 1)-th source-target pair and the 
learned reflection function f ( 1 ), we optimize the transferred knowledge between them, i.e., W 

 *  

Ne+1 , by maximizing the value of f ( 2 ). 

setting X 

s 

e = X 

t 

e and Y 

s 

e = Y 

t 

e for each pair of domains. 
a e st 

e /p 

t 

e , 
where p 

t 

e </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>t e . LDL decomposition on G = LDL T brings the latent feature factor matrix W = LD 1/2 . Manifold Ensemble Based Initiated by Gopalan et al., manifold ensemble based algorithms consider that a source and a target domain share multiple subspaces (of the same dimension) as points on the Grassmann manifold be- tween them. The representation of target examples on u domain-invariant latent factors turns to Z×nuu , if n u subspaces on the mani- fold are sampled. When all continuous subspaces on the manifold are sampled, i.e., n u → ∞, Gong et al. proved that Z</figDesc><table>where (X 

t 

e ) 
 † is the 
pseudo-inverse of X 

t(nu) 
e 

= [ϕ 1 (X 

t 

e ), 
· · ·, ϕ nu (X 

t 

e )] ∈ R 

n 

t 

e t(∞) 
e 

(Z 

t(∞) 
e 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>to measure the difference between domains. By mapping two domains into the reproducing kernel Hilbert space (RKHS), MMD em- pirically evaluates the distance between the mean of source examples and that of target examples:</figDesc><table>d 

2 

e (X 

s 

e We, X 

t 

e We) 

= 
1 
n s e 

n 

s 
e 

i=1 

φ(x 

s 

ei We) − 

1 
n t e 

n 

t 
e 

j=1 

φ(x 

t 

ej We) 

2 

H 

= 
1 
(n s e ) 2 

n 

s 
e 

i,i =1 

K(x 

s 

ei We, x 

s 

ei We) 

+ 
1 
(n t e ) 2 

n 

t 
e 

j,j =1 

K(x 

t 

ej We, x 

t 

ej We) 

− 
2 
n s e n t e 

n 

s 

e ,n 

t 
e 

i,j=1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>∀k), and learn the coefficients β = [β1,· · ·, βN k ] instead. Using β, the MMD can be rewrit- ten asdcomputed by the k-th kernel K k . In this paper, we consider RBF kernels</figDesc><table>2 

e (X 

s 

e We, X 

t 

e We)= 

N k 

k=1 β kd 

2 

e(k) (X 

s 

e We, X 

t 

e We) = 

β 

Td 

e, wherede = [d 

2 

e(1) ,· · ·,d 

2 

e(N k ) ] withd 

2 

e(k) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>. Note that Eh k 1 is shorthand for E x s</figDesc><table>e x s 
e x t 
e x t 

e 

h k1 (x 

s 

e , x 

s 

e , x 

t 

e , x 

t 

e ) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>,</head><label></label><figDesc>τe = tr(Wis the local scatter covariance matrix with the neighbour information H jj defined as</figDesc><table>T 

e S 

N 

e We)/tr(W 

T 

e S 

L 

e We), 

where S 

L 
e 

= 

n 

t 
e 

j,j =1 

H jj 

(n t 

e ) 2 (x 

t 

ej − x 

t 

ej )(x 

t 

ej − x 

t 

ej ) 

T 

H jj 
= 

K(x 

t 

ej , x 

t 

ej ), if x 

t 

ej ∈ Nr(x 

t 

ej ) and x 

t 

ej ∈ Nr(x 

t 

ej ) 
0, 
otherwise 
. 

If x 

t 

ej and x 

t 

ej are mutual r-nearest neighbours to each 
other, H jj equals the kernel value K(x 

t 

ej , x 

t 

ej ). By max-
imizing the unlabeled discriminant criterion τ e , the local 
scatter covariance matrix guarantees the first principle, while 

S 

N 
e 

= 

n 

t 
e 

j,j =1 

K(x 

t 

ej ,x 

t 
ej 

)−H jj 

(n t 
e ) 2 

(x 

t 

ej − x 

t 

ej )(x 

t 

ej − x 

t 

ej ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>has been widely adopted to deal with non-i.i.d. learning problems. Under this assumption, L(S) is uniformly stable. Theorem 1. Suppose that for any x| ≤ B. Meanwhile, for any e-th trans- fer learning experience, we assume that the latent feature factor matrix We ≤ rW . To meet the assumption above, we reasonably simplify L(S) so that the latent feature fac- tor matrix for the (Ne + 1)-th transfer learning task is a linear combination of all N e historical latent factor feature matrices plus a noisy latent feature matrix W satisfying W ≤r , i.e., WN e +1 = Ne e=1 ceWe +W with each coef- ficient 0 ≤ c e ≤ 1. Our algorithm L(S) is uniformly stable. For any S, T as the coming transfer learning task, the following inequality holds:</figDesc><table>t 

e and for any y 

t 

e we 
have x 

t 
e 

2 ≤ rx and |y 

t 

e </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="true"><head>Table 1 .</head><label>1</label><figDesc>The performance improvement ratios by varying different approaches used to generate transfer learning experiences. For example, "ITL+L2T" denotes the L2T learning from experiences generated by ITL only, and the second line of results for "ITL+L2T" is the p-value compared to ITL.</figDesc><table># of labeled 
examples 
3 
1 5 
3 0 
4 5 
6 0 
7 5 
9 0 
1 0 5 
1 2 0 

TCA 
1.0181 1.0024 0.9965 0.9973 0.9941 0.9933 0.9938 0.9927 0.9928 
ITL 
1.0188 1.0248 1.0250 1.0254 1.0250 1.0224 1.0232 1.0224 1.0224 
CMF 
0.9607 1.0203 1.0224 1.0218 1.0190 1.0158 1.0144 1.0142 1.0125 
LSDT 
1.0828 1.0168 0.9988 0.9940 0.9895 0.9867 0.9854 0.9834 0.9837 
GFK 
0.9729 1.0180 1.0232 1.0243 1.0246 1.0219 1.0239 1.0229 1.0225 
STL 
0.9973 0.9771 0.9715 0.9713 0.9715 0.9694 0.9705 0.9693 0.9693 
DIP 
1.0875 1.0633 1.0518 1.0465 1.0425 1.0372 1.0365 1.0343 1.0317 
SIE 
1.0745 1.0579 1.0485 1.0448 1.0412 1.0359 1.0359 1.0334 1.0318 

ITL + L2T 
1.1210 1.0737 1.0577 1.0506 1.0456 1.0398 1.0394 1.0361 1.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head></head><label></label><figDesc>we present results by either narrowing down or extending the range [212 η]), capable of encrypting better trans- fer learning skills in the reflection function, achieve larger performance improvement ratios.</figDesc><table>−8 η : 2 
0.5 η : 2 
8 η]. Obviously, more kernels (e.g., 
[2 
−12 η : 2 
0.5 η : 2 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the reviewers for their valuable comments to improve this paper. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mordatch, I., and Abbeel, P. Continuous adaptation via metalearning in nonstationary and competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation on the statistical manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2481" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">To secure transfer of training instruct self-management skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Belmont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Butterfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Ferretti</surname></persName>
		</author>
		<editor>Detterman, D. K. and Sternberg, R. J. P.</editor>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Ablex</publisher>
			<biblScope unit="page" from="147" to="154" />
			<pubPlace>Norwood, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>How and How Much Can Intelligence be Increased</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sur l&apos;extension du théorème limite du calcul des probabilités aux sommes de quantités dépendantes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity metric learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multitask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>44:1-44:10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1205" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transfer learning with graph co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1805" to="1818" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cognitive Development: Its Cultural and Social Foundations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Luria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithmic stability and meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="967" to="994" />
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Personalizing a dialogue system with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02891</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stability bounds for stationary ϕ-mixing and β-mixing processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="789" to="814" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lifelong learning with non-iid tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1540" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ELLA: An efficient lifelong learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Information-theoretical learning of discriminative clusters for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1079" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning categories from few examples with multi model knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="928" to="941" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transfer knowledge between cities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1905" to="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adapting SVM classifiers to data with shifted distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Globally maximizing, locally minimizing: unsupervised discriminant projection with applications to face and palm biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LSDT: Latent sparse domain transfer learning for visual adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1177" to="1191" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
