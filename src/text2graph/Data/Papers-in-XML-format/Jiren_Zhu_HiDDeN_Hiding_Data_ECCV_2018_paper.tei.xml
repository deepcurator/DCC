<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HiDDeN: Hiding Data With Deep Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiren</forename><surname>Zhu</surname></persName>
							<email>jirenz@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Kaplan</surname></persName>
							<email>rjkaplan@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HiDDeN: Hiding Data With Deep Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial Networks</term>
					<term>Steganography</term>
					<term>Robust blind water- marking</term>
					<term>Deep Learning</term>
					<term>Convolutional Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixelwise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sometimes there is more to an image than meets the eye. An image may appear normal to a casual observer, but knowledgeable recipients can extract more information. Two common settings exist for hiding information in images. In steganography, the goal is secret communication: a sender (Alice) encodes a message in an image such that the recipient (Bob) can decode the message, but an adversary (Eve) cannot tell whether any given image contains a message or not; Eve's task of detecting encoded images is called steganalysis. In digital watermarking, the goal is to encode information robustly: Alice wishes to encode a fingerprint in an image; Eve will then somehow distort the image (by cropping, blurring, etc), and Bob should be able to detect the fingerprint in the distorted image. Digital watermarking can be used to identify image ownership: if Alice is a photographer, then by embedding digital watermarks in her images she can prove ownership of those images even if versions posted online are modified.  <ref type="figure">Fig. 1</ref>. Given a cover image and a binary message, the HiDDeN encoder produces a visually indistinguishable encoded image that contains the message, which can be recovered with high accuracy by the decoder.</p><p>Interestingly, neural networks are also capable of "detecting" information from images that are not visible to human eyes. Recent research have showed that neural networks are susceptible to adversarial examples: given an image and a target class, the pixels of the image can be imperceptibly modified such that it is confidently classified as the target class <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Moreover, the adversarial nature of these generated images is preserved under a variety of image transformations <ref type="bibr" target="#b2">[3]</ref>. While the existence of adversarial examples is usually seen as a disadvantage of neural networks, it can be desirable for information hiding: if a network can be fooled with small perturbations into making incorrect class predictions, it should be possible to extract meaningful information from similar perturbations.</p><p>We introduce HiDDeN, the first end-to-end trainable framework for data hiding which can be applied to both steganography and watermarking. HiDDeN uses three convolutional networks for data hiding. An encoder network receives a cover image and a message (encoded as a bit string) and outputs an encoded image; a decoder network receives the encoded image and attempts to reconstruct the message. A third network, the adversary, predicts whether a given image contains an encoded message; this provides an adversarial loss that improves the quality of encoded images. In many real world scenarios, images are distorted between a sender and recipient (e.g. during lossy compression). We model this by inserting optional noise layers between the encoder and decoder, which apply different image transformations and force the model to learn encodings that can survive noisy transmission. We model the data hiding objective by minimizing (1) the difference between the cover and encoded images, (2) the difference between the input and decoded messages, and (3) the ability of an adversary to detect encoded images.</p><p>We analyze the performance of our method by measuring capacity, the size of the message we can hide; secrecy, the degree to which encoded images can be detected by steganalysis tools (steganalyzers); and robustness, how well our encoded messages can survive image distortions of various forms. We show that our methods outperform prior work in deep-learning-based steganography, and that our methods can also produce robust blind watermarks. The networks learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Though JPEG is not differentiable, we can reliably train networks that are robust to its perturbations using a differentiable approximation at training time.</p><p>Classical data hiding methods typically use heuristics to decide how much to modify each pixel. For example, some algorithms manipulate the least significant bits of some selected pixels <ref type="bibr" target="#b3">[4]</ref>; others change mid-frequency components in the frequency domain <ref type="bibr" target="#b4">[5]</ref>. These heuristics are effective in the domains for which they are designed, but they are fundamentally static. In contrast, HiDDeN can easily adapt to new requirements, since we directly optimize for the objectives of interest. For watermarking, one can simply retrain the model to gain robustness against a new type of noise instead of inventing a new algorithm. End-to-end learning is also advantageous in steganography, where having a diverse class of embedding functions (the same architecture, trained with different random initializations, produces very different embedding strategies) can stymie an adversary's ability to detect a hidden message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial examples. Adversarial examples were shown to disrupt classification accuracy of various networks with minimal perturbation to the original images <ref type="bibr" target="#b1">[2]</ref>. They are typically computed by adding a small perturbation to each pixel in the direction that maximizes one output neuron <ref type="bibr" target="#b0">[1]</ref>. Adversarial examples generated for one network can transfer to another network <ref type="bibr" target="#b5">[6]</ref>, suggesting that they come from a universal property of commonly used networks. Kurakin et al. showed that adversarial examples are robust against image transformations; when an adversarial example is printed and photographed, the network still misclassifies the photo <ref type="bibr" target="#b2">[3]</ref>. Instead of injecting perturbations that lead to misclassification, we consider the possibility of transmitting useful information through adding the appropriate perturbations.</p><p>Steganography. A wide variety of steganography settings and methods have been proposed in the literature; most relevant to our work are methods for blind image steganography, where the message is encoded in an image and the decoder does not have access to the original cover image. Least-Significant Bit (LSB) methods modify the lowest-order bits of each image pixel depending on the bits of the secret message; several examples of LSB schemes are described in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. By design, LSB methods produce image perturbations which are not visually apparent. However, they can systematically alter the statistics of the image, leading to reliable detection <ref type="bibr" target="#b8">[9]</ref>.</p><p>Many steganography algorithms differ only in how they define a particular distortion metric to minimize during encoding. Highly Undetectable Steganography (HUGO) <ref type="bibr" target="#b3">[4]</ref> measures distortion by computing weights for local pixel neighborhoods, resulting in lower distortion costs along edges and in high-texture regions. WOW (Wavelet Obtained Weights) <ref type="bibr" target="#b9">[10]</ref> penalizes distortion to predictable regions of the image using a bank of directional filters. S-UNIWARD <ref type="bibr" target="#b10">[11]</ref> is similar to WOW but can be used for embedding in an arbitrary domain.</p><p>Watermarking. Watermarking is similar to steganography: both aim to encode a secret message into an image. However, while the goal of steganography is se- <ref type="figure">Fig. 2</ref>. Model overview. The encoder E receives the secret message M and cover image Ico as input and produces an encoded image Ien. The noise layer N distorts the encoded image, producing a noised image Ino. The decoder produces a predicted message from the noised image. The adversary is trained to detect if an image is encoded. The encoder and decoder are jointly trained to minimize loss LI from difference between the cover and encoded image, loss LM from difference between the input and predicted message and loss LG from encoded image Ien being detected by the adversary.</p><p>cret communication, watermarking is frequently used to prove image ownership as a form of copyright protection. As such, watermarking methods prioritize robustness over secrecy: messages should be recoverable even after the encoded image is modified or distorted. Non-blind methods assumes access to the unmodified cover image <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>; more relevant to us are blind methods <ref type="bibr" target="#b4">[5]</ref> where the decoder does not assume access to the cover image. Some watermarking methods encode information in the least significant bits of image pixels <ref type="bibr" target="#b6">[7]</ref>; however for more robust encoding many methods instead encode information in the frequency domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. Other methods combine frequency-domain encoding with log-polar mapping <ref type="bibr" target="#b15">[16]</ref> or template matching <ref type="bibr" target="#b13">[14]</ref> to achieve robustness against spatial domain transformations.</p><p>Data Hiding with Neural Networks. Neural networks have been used for both steganography and watermarking <ref type="bibr" target="#b16">[17]</ref>. Until recently, prior work has typically used them for one stage of a larger pipeline, such as determining watermarking strength per image region <ref type="bibr" target="#b17">[18]</ref>, or as part of the encoder <ref type="bibr" target="#b18">[19]</ref> or the decoder <ref type="bibr" target="#b19">[20]</ref>.</p><p>In contrast, we model the entire data hiding pipeline with neural networks and train them end-to-end. Different from <ref type="bibr" target="#b17">[18]</ref>, HiDDeN is a blind method: it does not require the recipient to have access to the original image, which is more useful than non-blind methods in many practical scenarios. <ref type="bibr" target="#b19">[20]</ref> uses gradient descent to do encoding, whereas HiDDeN hides information in a single forward pass. <ref type="bibr" target="#b20">[21]</ref> is a recent end-to-end approach to steganography using adversarial networks, with which we compare results in Section 4.1. <ref type="bibr" target="#b21">[22]</ref> train networks to hide an entire image within another image. In contrast to our work, neither of <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> consider encoding robustness; their focus is on steganography whereas HiDDeN can be used for watermarking as well.</p><p>Neural networks have also been applied to other forms of data hiding. Abadi and Anderson <ref type="bibr" target="#b22">[23]</ref> show that adversarial networks can be trained for cryptography; Uchida et al. <ref type="bibr" target="#b23">[24]</ref> embed watermarks into the weights of trained neural networks; Fang et al. <ref type="bibr" target="#b24">[25]</ref> use LSTMs to embed secret messages into text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to develop a learnable, end-to-end model for image steganography and watermarking that can be made robust to arbitrary types of image distortion. To this end, our system comprises four main components: an encoder E θ , a parameterless noise layer N , a decoder D φ and an adversarial discriminator A γ . θ, φ and γ are trainable parameters. The encoder E θ receives a cover image I co of shape C × H × W and a binary secret message</p><formula xml:id="formula_0">M in ∈ {0, 1}</formula><p>L of length L and produces an encoded image (also called the stego image) I en of the same shape as I co . The noise layer N receives I co and I en as input and distorts the encoded image to produce a noised image I no . The decoder D recovers a message M out from I no . At the same time, given an imageĨ ∈ {I co , I en }, i.e. either a cover image or encoded image, the discriminator predicts A(Ĩ) ∈ [0, 1], the probability thatĨ is an encoded image.</p><p>Encoded images should look visually similar to cover images. We characterize the "similarity" with image distortion loss, the l 2 distance between I co and I en : L I (I co , I en ) = I co − I en 2 2 /(CHW ), and adversarial loss, the ability of the discriminator to detect an encoded image I en : L G (I en ) = log(1 − A(I en )) <ref type="bibr" target="#b25">[26]</ref>. The discriminator incurs a classification loss from its predictions L A (I co , I en ) = log(1 − A(I co )) + log(A(I en )). The decoded message should be the same as the encoded message. We impose a message distortion loss using the l 2 distance between the original and decoded messages</p><formula xml:id="formula_1">L M (M in , M out ) = M in −M out 2 2 /L.</formula><p>We perform stochastic gradient descent on θ, φ to minimize the following loss over the distribution of input messages and images:</p><formula xml:id="formula_2">E Ico,Min L M (M in , M out ) + λ I L I (I co , I en ) + λ G L G (I en )<label>(1)</label></formula><p>where λ I and λ G control the relative weights of the losses. At the same time, we train discriminator A γ to minimize the following loss over the same distribution:</p><formula xml:id="formula_3">E Ico,Min [L A (I co , I en )] .<label>(2)</label></formula><p>Network Architecture. A diagram for our system setup is shown in <ref type="figure">Figure 2</ref>, and details can be found in Appendix A. The encoder first applies convolutions to input I co to form some intermediate representation. Next, we aim to incorporate the message input (of length L) in such a way that the encoder can easily learn to embed parts of it at any spatial location of the output. For this reason, we replicate the message spatially, and concatenate this "message volume" to the encoder's intermediary representation. This ensures that each convolutional filter in the next layer has access to the entire message as it convolves across each spatial location. After more convolutional layers, the encoder produces I en , the encoded image. The noise layer, given I co , I en , applies noise and produces I no . We do not require I no to have the same dimension as I en , so that we can model dimension-altering noise like cropping. The decoder first applies several convolutional layers to produce L feature channels in the intermediate representation.</p><p>It then applies global spatial average pooling to produce a vector L of the same size of the message -the average pooling ensures that it can handle inputs of different spatial dimensions. It then applies a single linear layer to produce the predicted message. The adversary has a structure similar to the decoder, but it outputs a binary classification instead.</p><formula xml:id="formula_4">Dropout Cropout Crop Gaussian real JPEG JPEG JPEG p =30% p =30% p =3.5% σ = 2 Q = 50 Mask Drop Ien Ino 15|Ien − Ino|</formula><p>As we shall see, by varying message length L, image size C, H, W , noise layer N and weight factors λ I and λ G , the network setup described above can be adapted well to both steganography and watermarking. The specific choices will be discussed in their respective experiments.</p><p>Noise Layers. Our model should be able to hide images robustly against a variety of image distortions; for this reason we consider six different types of noise layers which apply different types of image distortion. Examples of all distortion types are shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The Identity layer is the simplest: it leaves I en unchanged. The Dropout and Cropout layers undo some of the changes made by the encoder, producing the noisy image I no by combining pixels from the cover image I co and the encoded image I en . Both types of noise keep a percentage p of pixels from I en and use the rest from I co , but Dropout makes this choice independently per pixel while Cropout keeps a random square crop from I en . The Gaussian layer blurs the encoded image I en with a Gaussian kernel of width σ, and the Crop layer produces a random square H ′ × W ′ crop of the encoded image, where the ratio of image sizes</p><formula xml:id="formula_5">H ′ ×W ′</formula><p>H×W is p ∈ (0, 1). The JPEG layer applies JPEG compression to I en with quality factor Q ∈ (0, 100). Note that all non-identity noise layers have a scalar hyperparameter governing the intensity of the distortion: Dropout, Cropout, and Crop keep a fraction p of the pixels from I en , Gaussian has a kernel width σ; and JPEG has a quality factor Q.</p><p>We experiment with an extensive family of distortions for the noise layer, N . Dropout, Crop and Cropout mainly affect the spatial domain, while Gaussian and JPEG act on the frequency domain. Granting the noise layer access to the  <ref type="figure">Fig. 4</ref>. JPEG compresses an image by performing a discrete cosine transform (DCT) to give an 8 × 8 grid of frequency components (left) which are then quantized, with higher frequency components quantized more aggressively (middle left; bright red signifies stronger quantization). The DCT transform can be implemented as a single 8x8 stride 8 convolution layer with 64 filters fixed to the DCT basis, but due to the quantization step, JPEG compression is not differentiable. We therefore train models using two differentiable approximations: JPEG-Mask (middle right) zeros a fixed set of highfrequency coefficients, and JPEG-Drop (right) zeros channels with higher drop probabilities for high-frequency coefficients. Models trained against either approximation perform well against true JPEG compression at test time; see <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>cover image makes it more challenging as well. For an LSB algorithm, a noise layer that replaces each tampered pixel with a fixed value is analogous to a binary erasure channel, whereas a noise layer that replaces encoded pixels with original pixels acts as a binary symmetric channel. It is harder to be robust against the latter since the decoder has no information about where the tampering happens. Similarly, not only does the crop layer require the decoder to be input size agnostic, it also provides no information about where the H ′ × W ′ crop came from, further limiting the knowledge of the decoder.</p><p>"Differentiating" the JPEG compression. Although the network is trained with gradient descent, we do not need the test-time noise to be differentiable. We successfully obtain robustness against non-differentiable JPEG compression by training on differentiable approximations. JPEG compression divides the image into 8 × 8 regions, computes a discrete cosine transformation (DCT) within each region (essentially a change of basis in R 64 ), then quantizes the resulting frequency-domain coefficients to different coarseness. Thus, only perceptually relevant information is preserved <ref type="bibr" target="#b26">[27]</ref>. The quantization step is non-differentiable, rendering JPEG compression unfit for gradient-based optimization.</p><p>Nevertheless, quantizing a value is information-theoretically equivalent to limiting the amount of information that can be transmitted though that "channel". To limit the amount of information being passed through specific frequency domain channels, we created noise layers that simulate the JPEG compression. These layers apply the DCT transformation using an 8 × 8 stride 8 convolution layer with each filter corresponding to a basis vector in the DCT transformation. The network activations thus represent DCT domain coefficients of the encoded image. Masking/Dropout is then applied to the DCT coefficients to limit information flow; higher frequency coefficients are more likely to be masked/dropped, see <ref type="figure">Figure 4</ref>. The noised image I no is then produced using a transpose convolution to implement the inverse DCT transform. We call the corresponding layers JPEG-Mask and JPEG-Drop. JPEGMask applies a fixed masking that only keeps 25 low frequency DCT coefficients in the Y channel and 9 in the U, V channels (following JPEG, which also preserves more information in the Y channel). The other coefficients are set to zero. JPEG-Drop applies a progressive dropout on the coefficients. The coarser the quantization for a coefficient in actual JPEG compression, the more likely this coefficient is zeroed in our simulation. Both methods successfully produce models that are robust against actual JPEG compression, see <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>Implementation details. All models are trained on 10,000 cover images from the COCO <ref type="bibr" target="#b27">[28]</ref> training set, resized to experiment-specific dimensions. Evaluation is performed on a 1000 image test set unseen during training. Messages are sampled with each bit drawn uniformly at random. For gradient descent, we use Adam <ref type="bibr" target="#b28">[29]</ref> with a learning rate of 10 −3 and default hyperparameters. All models are trained with batch size 12. Models are trained for 200 epochs, or 400 epochs if being trained on multiple noise layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model on three axes: capacity, the number of message bits which can be hidden per image bit; secrecy, the difficulty of detecting our encoded images; and robustness, the degree to which our method can succeed in the face of image distortions.</p><p>Metrics. We use a number of metrics to measure our performance along these axes. We measure capacity primarily with bits per pixel (BPP), which is the number of message bits hidden per pixel of the encoded image; this is simply L/(HW C) using the notation of Section 3. We measure secrecy primarily with detection rate; we train a modern steganalyzer <ref type="bibr" target="#b29">[30]</ref> which attempts to separate cover from stego images; its test-set accuracy is the detection rate. As a proxy to secrecy, we can also measure the image distortion between the cover and stego image using peak signal-to-noise ratio (PSNR); methods which distort the cover image more are likely to be more detectable. We measure robustness using bit accuracy, which is the number of identical bits between the input message M and the predicted message M ′ , as we subject the model to varying image distortions between the encoder and decoder during testing.</p><p>Many of these metrics compete against each other. Models which have higher capacity typically sacrifice secrecy, since hiding more information in images naturally leads to larger image distortions; models that are very robust to noise typically sacrifice capacity or secrecy, since the message must be encoded in the image redundantly. In some sense, steganography and watermarking are at two ends of a spectrum of problems prioritizing these different axes; steganography stresses secrecy while watermarking emphasizes robustness. Since HiDDeN encompasses a whole family of data hiding algorithms, we can flexibly train models emphasizing different axes by the varying message length L, using different noise layers, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Steganography: Capacity and Secrecy</head><p>We train our model to encode binary messages of length L = 52 in grayscale images of size 16×16, giving our trained model a capacity of 52/(16×16) ≈ 0.203 BPP. For larger images, we use a longer message to preserve encoding capacity. Our model encodes an L-bit chunk of the message on each 16 × 16 patch of the image. Steganography typically assumes noiseless transmission, so we set N to be the identity layer. We first compare with traditional methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> in <ref type="figure" target="#fig_4">Figure 6</ref> and then with a neural-network-based approach <ref type="bibr" target="#b20">[21]</ref> in <ref type="figure">Figure 7</ref>.</p><p>Image quality We compare with three recent steganography methods: HUGO <ref type="bibr" target="#b3">[4]</ref>, WOW <ref type="bibr" target="#b9">[10]</ref>, and S-UNIWARD <ref type="bibr" target="#b10">[11]</ref>. Each is configured with a capacity of 0.2 BPP to closely match our model. We test all methods on 512 × 512 cover images from the BOSS dataset <ref type="bibr" target="#b30">[31]</ref>, which is a common testbed for steganography algorithms. Since our model was trained on COCO, this showcases its generality to new data. <ref type="figure" target="#fig_4">Figure 6</ref> shows example cover and stego images from our method and from HUGO (other classical baselines give results indistinguishable from HUGO). Both HUGO and our method generate encoded images which are visually indistinguishable from the cover image. Capacity. Our model closely matches the baselines on BPP (0.203 vs 0.200). One disadvantage of neural networks in steganography is the possibility for the decoder network to make mistakes. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, our method reconstructs the message accurately but not perfectly, having an error rate of &lt; 10 −5 . One can use error correcting codes if higher accuracy is desired.</p><p>Secrecy. We measure secrecy by training the steganalyzer ATS <ref type="bibr" target="#b29">[30]</ref> to distinguish between cover and stego images for all methods; results are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Following the recommendation of the authors for best performance <ref type="bibr" target="#b29">[30]</ref>, we provide ATS with small datasets for each algorithm: 250 cover and 250 stego images.</p><p>ATS generates an eponymous artificial training set from this unlabeled test set by repeatedly applying the steganography algorithm to the data, then uses this artificial training set to train a supervised classifier. Finally, the trained classifier is evaluated on its ability to recognize stego images in the test set. With comparable capacity to baselines as measured by bits per pixel and bit accuracy, our method is undetectable (50%, no better than chance) by ATS under the assumption that an adversary has access to the algorithm but not the trained model weights. For our model we evaluate two threat scenarios. We first train one encoderdecoder pair and evaluate ATS assuming it has access to the learned model directly. In this setting (model weights known), ATS achieves a detection accuracy of 98%, showing the susceptibility of our model to statistical analysis when the precise weights are known. In the second threat scenario, we consider the case where the attacker knows HiDDeN is being used (and furthermore, knows the precise architecture, hyperparameters and dataset) but does not have access to the trained model itself (which is not exactly reproducible without the random seed used to initialize the parameters). To test this scenario, we train five identical HiDDeN models with different seeds and provide these models to ATS for building its artificial training set. Finally, we train a sixth model with a new random initialization for evaluation. When we test ATS on this sixth model (Ours -different model), it achieves a detection accuracy of 50%, no better than random guessing. In comparison, ATS achieves detection accuracies of 70% for HUGO and 68% for WOW and S-UNIWARD. This result shows that image distortion patterns produced by each trained model are unique, which makes our approach undetectable by ATS without access to the trained model directly. This hints at an advantage of learned steganography models: unlike static classical algorithms, they can employ a diverse set of encoding strategies.</p><p>Against other neural network based methods Compared to <ref type="bibr" target="#b20">[21]</ref> which uses a fully connected network to generate encoded images, our method uses convolutional networks, greatly improving encoded image quality. <ref type="figure">Figure 7</ref> compares our results with <ref type="bibr" target="#b20">[21]</ref>; at double their bit rate we achieve lower error and generate images much closer to the cover image. Comparison of our encoding method against <ref type="bibr" target="#b20">[21]</ref>. We encode twice as many bits in images of the same size, while having smaller decoding error and better encoded image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Watermarking: Robustness</head><p>Digital watermarking prioritizes robustness over capacity and secrecy; it hides only a small message in the image, but that information should remain even after significant distortions are applied to the encoded image. By varying the type of image distortion applied at training time, we show that our model can learn robustness to a variety of different image distortions. We train our model on YUV color images of size C × H × W = 3 × 128 × 128 with message length L = 30 and weight factors λ I = 0.7 and λ G = 0.001. A model is trained on the identity noise layer as reference. Six models are trained using different noise layers N at training time: Dropout (p = 0.3), Cropout (p = 0.3), Crop (p = 0.035), Gaussian (σ = 2.0), JPEG-mask and JPEGdrop. We call these types of noise-resistant models specialized models as they are trained to be robust against a particular kind of noise. We additionally train a noise-resistant combined model by using a different noise layer for each minibatch during training. See the supplementary materials for details.</p><p>Baseline. To the best of our knowledge, there are no open source implementations of recent methods for digital watermarking. As a baseline we compare to Digimarc <ref type="bibr" target="#b31">[32]</ref>, a closed source commercial package for digital watermarking. Since Digimarc is closed source, there are certain limitations when comparing HiDDeN against it, especially for comparing transmission accuracy. Detailed analysis and comparison methodology are provided in the appendix.</p><p>Qualitative Results. <ref type="figure" target="#fig_7">Figure 8 shows</ref>  and the encoded image I en . We see that encoded images from our models are visually indistinguishable from the cover image, and that we can train a single model (Combined) that is simultaneously robust to all types of noise without sacrificing image quality.</p><p>Adversary. Robustness. The intensity of an image distortion can be controlled with a scalar: keep percentage p for Dropout, Cropout, and Crop, kernel width σ for Gaussian, and quality Q for JPEG compression. <ref type="figure" target="#fig_8">Figure 9</ref> shows the bit accuracy of models when they are tested on various noise layers. For each tested noise layer, we evaluate the model trained with the identity noise layer, i.e. no noise (blue), the model trained on the same noise layer (orange), and the model trained on combined noise layers (green). Bit accuracies are measured on 1000 images unseen during training. <ref type="figure" target="#fig_9">Figure 10</ref> reports bit accuracy as a function of test time distortion intensity. The model trained without noise unsurprisingly performs poorly when tested against different noise layers, and fails completely (50% bit accuracy, no better than chance) when tested on Crop and JPEG. Since this model enjoyed lossless transmission between the encoder and decoder during training, it has no incentive to learn robustness to any type of image distortion.  However, the high bit accuracies of the Specialized models (orange bars) in <ref type="figure" target="#fig_8">Figure 9</ref> demonstrate that models can learn robustness to many different types of image distortion when these distortions are introduced into the training process. This remains true even when the distortion is non-differentiable: Models trained without noise have 50% bit accuracy when tested against true JPEG compression, but this improves to 85% when trained with simulated JPEG noise.</p><p>Finally, we see that in most cases the Combined model, which is trained on all types of noise, is competitive with specialized models despite its increased generality. For example, it achieves 94% accuracy against Cropout, close to the 97% accuracy of the specialized model.</p><p>Comparison with Digimarc. Digimarc is closed source, and it only reports success or failure for decoding a fixed-size watermark. It provides no information about its bit error rate, which makes comparing it with HiDDeN difficult.</p><p>To ensure a fair comparison, we first estimate the capacity of Digimarc, and then apply an error correcting code that matches HiDDeN's bit rate with Digimarc. This also allows us to converts bit accuracy to decode success rate since a few errors can be corrected (see Appendix B for full methodology). From this analysis, we consider ≥ 95% bit accuracy for our model to be comparable to a successful Digimarc decoding, and ≤ 90% bit accuracy to be a failed decoding. <ref type="figure">Fig. 11</ref>. Model performance under different distortions and intensities. We compare the model trained with no noise (blue), models specialized to a particular distortion (orange), and a Combined model trained on all distortion types (green). We also show Digimarc's decoding success rate for 256 × 256 images (purple). The two axis are scaled to translate bit accuracy into full reconstruction rate. See Appendix B for detail.</p><p>We report the comparison in <ref type="figure">Figure 11</ref>, with the y-axis clipped according to our analysis. For spatial domain noise, our model exceeds the performance of Digimarc at high noise intensities. Against Dropout (p = 0.1), our specialized model has bit accuracy ≥ 95%, yet Digimarc fails completely. Against Crop (p = 0.1), both the specialized and combined models have bit accuracy ≥ 95%, but Digimarc cannot reconstruct any of the ten watermarks tested. For frequency domain noise, our model performs worse than Digimarc. This is likely due to the fact that we baked no assumptions about frequency domain transformations into the architecture, whereas watermarking tools commonly work directly in the frequency domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have developed a framework for data hiding in images which is trained endto-end using neural networks. Compared to classical data hiding methods, ours allows flexibly trading off between capacity, secrecy, and robustness to different types of noise by varying parameters or noise layers at training-time. Compared to deep learning methods for steganography, we demonstrate improved quantitative and qualitative performance. For robust watermarking, HiDDeN is to our knowledge the first end-to-end method using neural networks. Ultimately, end-to-end methods like HiDDeN have a fundamental advantage in robust datahiding: new distortions can be incorporated directly into the training procedure, with no need to design new, specialized algorithms. In future work, we hope to see improvements in message capacity, robustness to more diverse types of image distortions -such as geometric transforms, contrast change, and other lossy compression schemes -and procedures for data hiding in other input domains, such as audio and video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of non-identity noise layers. JPEG-Mask and JPEG-Drop are differentiable approximations of the JPEG compression (see Figure 4). Top: Encoded image Ien. Middle: Noised image Ino. Bottom: Magnified difference |Ien − Ino|. Even under heavy distortion, such as a Crop layer which retains only 3.5% of the original image, our model still learns to recover the watermark with high accuracy (see Section 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Bit accuracy for models trained with JPEG-Mask (blue, zero-masking on DCT coefficients) / JPEG-Drop (red, dropout on DCT coefficients). When trained against these approximations (dashed lines), both become robust against actual JPEG compression (solid lines, quality Q = 50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Top: Capacity and secrecy of HiDDeN versus classical steganography methods. With comparable capacity to baselines as measured by bits per pixel and bit accuracy, our method is undetectable (50%, no better than chance) by ATS under the assumption that an adversary has access to the algorithm but not the trained model weights. Bottom: Example of 512×512 encoded images. (a) Cover, (b) encoded by our method, (c) encoded by HUGO [4]. (d), (e), (f) are 32 × 32 crops of (a), (b), (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 6. Top: Capacity and secrecy of HiDDeN versus classical steganography methods. With comparable capacity to baselines as measured by bits per pixel and bit accuracy, our method is undetectable (50%, no better than chance) by ATS under the assumption that an adversary has access to the algorithm but not the trained model weights. Bottom: Example of 512×512 encoded images. (a) Cover, (b) encoded by our method, (c) encoded by HUGO [4]. (d), (e), (f) are 32 × 32 crops of (a), (b), (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 7. Comparison of our encoding method against [21]. We encode twice as many bits in images of the same size, while having smaller decoding error and better encoded image quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8</head><label>8</label><figDesc>also compares generated images of two models, one trained with the adversary and the other trained without the adversary. Both models are trained on the combined noise layer and tuned individually. The model trained with l 2 loss alone has visible artifacts, as shown in the rightmost image of Fig- ure 8. The model trained against an adversarial discriminator produces images with no visible artifacts (Figure 8, second image from the right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Robustness of our models against different test time distortions. Each cluster uses a different test time distortion. Identity (blue) is trained with no image distortion; Specialized (orange) is trained on the same type of distortion used during testing; Combined (green) is trained on all types of distortions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Bit accuracy under various distortions and intensities. Stars denote the noise intensity used during training. The specialized JPEG model is trained on the differentiable approximation JPEG-Mask, and the plot shows performance on actual JPEG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>qualitative examples of 128 × 128 images encoded with each of our trained models, as well as a 128 × 128 image encoded with Digimarc. For each image we report the PSNR between the cover image I co Digimarc Identity Dropout Cropout Crop Gaussian JPEG-mask JPEG-drop Combined</figDesc><table>PSNR(Y) 62.12 
44.63 
42.52 
47.24 35.20 40.55 
30.09 
28.79 
33.55 
PSNR(U) 38.23 
45.44 
38.52 
40.97 33.31 41.96 
35.33 
32.51 
38.92 
PSNR(V) 52.06 
46.90 
41.05 
41.88 35.86 42.88 
36.27 
33.42 
39.38 

Trained with Adversary 
No Adversary 

Cover 
Digimarc 
Crop 
Gaussian 
Combined 
Combined 

Fig. 8. Image distortions for watermarking algorithms. Top: Mean PSNR between 
cover and encoded images for Digimarc and our model trained with different noise 
layers. Bottom: A cover image and encoded images from both Digimarc and our 
model trained with Crop, Gaussian, and Combined noise layers. Bottom Right: An 
encoded image from a model trained with combined noise but without an adversary. 
Adversarial training significantly improves the visual quality of the encoded images. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">⋆ These authors contributed equally.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments Our work is supported by an ONR MURI grant. We would like to thank Ehsan Adeli, Rishi Bedi, Jim Fan, Kuan Fang, Adithya Ganesh, Agrim Gupta, De-An Huang, Ranjay Krishna, Damian Mrowca, Ben Zhang and anonymous reviewers for their feedback on our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Using High-Dimensional Image Models to Perform Highly Undetectable Steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="161" to="177" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust image watermarking based on multiband wavelets and empirical mode decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A digital watermark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Van Schyndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Tirkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Converence on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A watermark for digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Wolfgang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="219" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A review on detection of LSB matching steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Technology Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1725" to="1738" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Designing steganographic distortion using directional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal distortion function for steganography in an arbitrary domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Secure spread spectrum watermarking for multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shamoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1673" to="1687" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hiding digital watermarks using multiresolution wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on industrial electronics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="875" to="882" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust template matching for affine resistant image watermarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1123" to="1129" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of digital image watermarking techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IEEE International Conference on Industrial Informatics (INDIN 2005)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="709" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Saddik</surname></persName>
		</author>
		<title level="m">Rst-invariant digital image watermarking based on log-polar mapping and phase correlation. IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A study on digital image and video watermarking schemes using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Isac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Santhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Applications of a neural network to estimate watermark embedding strength</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Image Analysis for Multimedia Interactive Services</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the learning capabilities of convolutional neural networks for robust image watermarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R S</forename><surname>Gorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03248</idno>
		<title level="m">A robust blind watermarking using convolutional neural network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generating steganographic images via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Danezis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hiding images in plain sight: Deep steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06918</idno>
		<title level="m">Learning to protect communications with adversarial neural cryptography</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Embedding watermarks into deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating steganographic text with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Argyraki</surname></persName>
		</author>
		<idno>EPFL-CONF-229881</idno>
	</analytic>
	<monogr>
		<title level="m">ACL Student Research Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets. In: NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The jpeg still picture compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on consumer electronics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="xviii" to="xxxiv" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context. In: ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised steganalysis based on artificial training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lerch-Hostalot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Megas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="45" to="59" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">break our steganographic system: The ins and outs of organizing BOSS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pevnỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Hiding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Digimarc</surname></persName>
		</author>
		<ptr target="https://www.digimarc.com/home" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
