<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Streaming Weak Submodularity: Interpreting Neural Networks on the Fly</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
							<email>elenberg@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of Mathematics and Computer Science Open</orgName>
								<orgName type="department" key="dep4">Department of Electrical Engineering Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit3">University of Israel</orgName>
								<orgName type="institution" key="instit4">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
							<email>dimakis@austin.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of Mathematics and Computer Science Open</orgName>
								<orgName type="department" key="dep4">Department of Electrical Engineering Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit3">University of Israel</orgName>
								<orgName type="institution" key="instit4">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moran</forename><surname>Feldman</surname></persName>
							<email>moranfe@openu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of Mathematics and Computer Science Open</orgName>
								<orgName type="department" key="dep4">Department of Electrical Engineering Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit3">University of Israel</orgName>
								<orgName type="institution" key="instit4">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
							<email>amin.karbasi@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of Mathematics and Computer Science Open</orgName>
								<orgName type="department" key="dep4">Department of Electrical Engineering Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit3">University of Israel</orgName>
								<orgName type="institution" key="instit4">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Streaming Weak Submodularity: Interpreting Neural Networks on the Fly</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of <ref type="bibr" target="#b33">Ribeiro et al. [2016]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set function f : 2 N 7 ! R 0 , find the set S of size k which maximizes f (S). This formulation is at the heart of many machine learning applications such as sparse regression, data summarization, facility location, and graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular then many approximation algorithms have been shown to perform provably within a constant factor from the best solution.</p><p>Some disadvantages of the standard greedy algorithm of  for this problem are that it requires repeated access to each data element and a large total number of function evaluations. This is undesirable in many large-scale machine learning tasks where the entire dataset cannot fit in main memory, or when a single function evaluation is time consuming. In our main application, each function evaluation corresponds to inference on a large neural network and can take a few seconds. In contrast, streaming algorithms make a small number of passes (often only one) over the data and have sublinear space complexity, and thus, are ideal for tasks of the above kind. nonlinear sparse regression. Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular functions <ref type="bibr" target="#b3">[Badanidiyuru et al., 2014</ref><ref type="bibr" target="#b10">, Buchbinder et al., 2015</ref><ref type="bibr" target="#b13">, Chekuri et al., 2015</ref> extend to the weakly submodular setting. This paper answers the above question by providing the first analysis of a streaming algorithm for any class of approximately submodular functions. We use key algorithmic components of SIEVE-STREAMING <ref type="bibr" target="#b3">[Badanidiyuru et al., 2014]</ref>, namely greedy thresholding and binary search, combined with a novel analysis to prove a constant factor approximation for -weakly submodular functions (defined in Section 3). Specifically, our contributions are as follows.</p><p>• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized streaming algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set elements arrive in a worst case order.</p><p>• STREAK: a greedy, deterministic streaming algorithm for maximizing -weakly submodular functions which uses O(" 1 k log k) memory and has an approximation ratio of</p><formula xml:id="formula_0">(1 ") 2 · (3 e /2 2 p 2 e /2</formula><p>) when the ground set elements arrive in a random order.</p><p>• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using pairwise products of features and interpretability of black-box neural network classifiers.</p><p>The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst case stream order.</p><p>One advantage of our approach is that, while our approximation guarantees are in terms of , our algorithm STREAK runs without requiring prior knowledge about the value of . This is important since the weak submodularity parameter is hard to compute, especially in streaming applications, as a single element can alter drastically.</p><p>We use our streaming algorithm for neural network interpretability on Inception V3 <ref type="bibr" target="#b36">[Szegedy et al., 2016]</ref>. For that purpose, we define a new set function maximization problem similar to LIME <ref type="bibr" target="#b33">[Ribeiro et al., 2016]</ref> and apply our framework to approximately maximize this function. Experimentally, we find that our interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Monotone submodular set function maximization has been well studied, starting with the classical analysis of greedy forward selection subject to a matroid constraint . For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio of 1 1 /e , and a more involved algorithm obtains this ratio also for general matroid constraints <ref type="bibr" target="#b11">[Cȃlinescu et al., 2011]</ref>. In general, no polynomial-time algorithm can have a better approximation ratio even for a uniform matroid constraint <ref type="bibr">Wolsey, 1978, Feige, 1998</ref>]. However, it is possible to improve upon this bound when the data obeys some additional guarantees <ref type="bibr" target="#b14">[Conforti and Cornuéjols, 1984</ref><ref type="bibr" target="#b37">, Vondrák, 2010</ref><ref type="bibr" target="#b35">, Sviridenko et al., 2015</ref>. For maximizing nonnegative, not necessarily monotone, submodular functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an approximation ratio of 0.385 <ref type="bibr" target="#b9">[Buchbinder and Feldman, 2016b]</ref>. Moreover, for uniform matroids there is also a deterministic algorithm achieving a slightly worse approximation ratio of 1 /e <ref type="bibr" target="#b8">[Buchbinder and Feldman, 2016a]</ref>. The reader is referred to <ref type="bibr" target="#b2">Bach [2013]</ref> and <ref type="bibr" target="#b26">Krause and Golovin [2014]</ref> for surveys on submodular function theory.</p><p>A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for large-scale machine learning applications. Algorithmic advances of this kind include STOCHASTIC-GREEDY <ref type="bibr" target="#b29">[Mirzasoleiman et al., 2015]</ref>, SIEVE-STREAMING <ref type="bibr" target="#b3">[Badanidiyuru et al., 2014]</ref>, and several distributed approaches <ref type="bibr" target="#b28">[Mirzasoleiman et al., 2013</ref><ref type="bibr" target="#b5">, Barbosa et al., 2015</ref><ref type="bibr" target="#b32">, Pan et al., 2014</ref><ref type="bibr" target="#b24">, Khanna et al., 2017b</ref>. Our algorithm extends ideas found in SIEVE-STREAMING and uses a different analysis to handle more general functions. Additionally, submodular set functions have been used to prove guarantees for online and active learning problems <ref type="bibr" target="#b21">[Hoi et al., 2006</ref><ref type="bibr" target="#b38">, Wei et al., 2015</ref><ref type="bibr" target="#b10">, Buchbinder et al., 2015</ref>. Specifically, in the online setting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint), <ref type="bibr" target="#b12">Chan et al. [2017]</ref> achieve a competitive ratio of about 0.3178 when the function is submodular.</p><p>The concept of weak submodularity was introduced in <ref type="bibr" target="#b25">Krause and Cevher [2010]</ref>, <ref type="bibr" target="#b15">Das and Kempe [2011]</ref>, where it was applied to the specific problem of feature selection in linear regression. Their main results state that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue assumptions), then maximizing the goodness of fit f (S) = R 2 S as a function of the feature set S is weakly submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak submodularity was connected with Restricted Strong Convexity in <ref type="bibr">Elenberg et al. [2016a,b]</ref>. This showed that the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms. This framework was later used for additional algorithms and applications <ref type="bibr">[Khanna et al., 2017a,b]</ref>. Other approximate versions of submodularity were used for greedy selection problems in <ref type="bibr" target="#b22">Horel and Singer [2016]</ref>, <ref type="bibr" target="#b20">Hassidim and Singer [2017]</ref>, <ref type="bibr" target="#b1">Altschuler et al. [2016]</ref>, Bian et al. <ref type="bibr">[2017]</ref>. To the best of our knowledge, this is the first analysis of streaming algorithms for approximately submodular set functions.</p><p>Increased interest in interpretable machine learning models has led to extensive study of sparse feature selection methods. For example, <ref type="bibr" target="#b4">Bahmani et al. [2013]</ref> consider greedy algorithms for logistic regression, and <ref type="bibr" target="#b39">Yang et al. [2016]</ref> solve a more general problem using`1 regularization. Recently, Ribeiro et al. <ref type="bibr">[2016]</ref> developed a framework called LIME for interpreting black-box neural networks, and <ref type="bibr" target="#b34">Sundararajan et al. [2017]</ref> proposed a method that requires access to the network's gradients with respect to its inputs. We compare our algorithm to variations of LIME in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>First we establish some definitions and notation. Sets are denoted with capital letters, and all big O notation is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function f , we often use the discrete derivative </p><formula xml:id="formula_1">f (B | A) , f (A [ B) f (A). f is monotone if f (B | A) 0,</formula><formula xml:id="formula_2">P j2S\L f (j | L) f (S | L) ,</formula><p>where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.</p><p>This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives. It is easy to show that f is submodular if and only if |N | = 1. Definition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S has approximation ratio</p><formula xml:id="formula_3">R 2 [0, 1] if E[f (S)] R · f (OP T )</formula><p>, where OP T is the optimal solution and the expectation is over the random decisions of the algorithm and the randomness of the input stream order (when it is random).</p><p>Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at either random or worst case order. The goal is then to design a one pass streaming algorithm that given oracle access to a nonnegative set function f : 2 N 7 ! R 0 maintains at most o(N ) elements in memory and returns a set S of size at most k approximating</p><formula xml:id="formula_4">max |T |k f (T ) ,</formula><p>up to an approximation ratio R( k ). Ideally, this approximation ratio should be as large as possible, and we also want it to be a function of k and nothing else. In particular, we want it to be independent of k and N .</p><p>To simplify notation, we use in place of k in the rest of the paper. Additionally, proofs for all our theoretical results are deferred to the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Impossibility Result</head><p>To prove our negative result showing that no streaming algorithm for our problem has a constant approximation ratio against a worst case stream order, we first need to construct a weakly submodular set function f k . Later we use it to construct a bad instance for any given streaming algorithm.</p><p>Fix some k 1, and consider the ground set</p><formula xml:id="formula_5">N k = {u i , v i } k i=1</formula><p>. For ease of notation, let us define for every subset S ✓ N k</p><formula xml:id="formula_6">u(S) = |S \ {u i } k i=1 | , v(S) = |S \ {v i } k i=1</formula><p>| . Now we define the following set function:</p><formula xml:id="formula_7">f k (S) = min{2 · u(S) + 1, 2 · v(S)} 8 S ✓ N k .</formula><p>Lemma 4.1. f k is nonnegative, monotone and 0.5-weakly submodular for the integer |N k |.</p><formula xml:id="formula_8">Since |N k | = 2k, the maximum value of f k is f k (N k ) = 2 · v(N k ) = 2k</formula><p>. We now extend the ground set of f k by adding to it an arbitrary large number d of dummy elements which do not affect f k at all. Clearly, this does not affect the properties of f k proved in Lemma 4.1. However, the introduction of dummy elements allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of the next theorem. In a nutshell, this proof is based on the observation that the elements of {u i } k i=1 are indistinguishable from the dummy elements as long as no element of</p><formula xml:id="formula_9">{v i } k i=1</formula><p>has arrived yet. Theorem 4.2. For every constant c 2 (0, 1] there is a large enough k such that no randomized streaming algorithm that uses o(N ) memory to solve max |S|2k f k (S) has an approximation ratio of c for a worst case stream order.</p><p>We note that f k has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular for every 0  r  |N|. In contrast, the algorithm we show later assumes weak submodularity only for the cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant approximation ratio even for functions with much stronger properties compared to what is necessary for getting a constant approximation ratio when the order is random.</p><p>The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other words, once the algorithm decides to discard an element from its memory, this element is gone forever, which is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis remains an interesting open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Streaming Algorithms</head><p>In this section we give a deterministic streaming algorithm for our problem which works in a model in which the stream contains the elements of N in a random order. We first describe in Section 5.1 such a streaming algorithm assuming access to a value ⌧ which approximates a · f (OP T ), where a is a shorthand for a = ( p 2 e /2 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain STREAK and bound its approximation ratio, space complexity, and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Algorithm with access to ⌧</head><p>Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter ⌧ 2 [0, a · f (OP T )]. One should think of ⌧ as close to a · f (OP T ), although the following analysis of the algorithm does not rely on it. We provide an outline of the proof, but defer the technical details to the Supplementary Material. Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least</p><formula xml:id="formula_10">⌧ a · 3 e /2 2 p 2 e /2 2 = ⌧ · ( p 2 e /2 1) . Algorithm 1 THRESHOLD GREEDY(f, k, ⌧ )</formula><p>Let S ?. while there are more elements do Let u be the next element.</p><formula xml:id="formula_11">if |S| &lt; k and f (u | S) ⌧ /k then Update S S [ {u}. end if end while return: S Algorithm 2 STREAK(f, k, ")</formula><p>Let m 0, and let I be an (originally empty) collection of instances of Algorithm 1. while there are more elements do Let u be the next element. if f (u) m then Update m f (u) and u m u. end if Update I so that it contains an instance of Algorithm 1 with ⌧ = x for every x 2 {(1 ") i | i 2 Z and (1 ")m/(9k 2 )  (1 ") i  mk}, as explained in Section 5.2. Pass u to all instances of Algorithm 1 in I. end while return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set {u m }.</p><p>Proof (Sketch). Let E be the event that f (S) &lt; ⌧, where S is the output produced by Algorithm 1. Clearly f (S) ⌧ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S) using E as follows.  . An important ingredient of the proof of this proposition is the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than k when E happens.</p><p>Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ⌧ .</p><p>The proof of Proposition 5.4 is based on the above observation and on the observation that the random arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a random element out of all the OP T elements that did not arrive yet.</p><p>Proposition 5.4. For the set S produced by Algorithm 1, </p><formula xml:id="formula_12">E[f (S)] 1 2 · ⇣ · [Pr[E] e /2 ] · f (OP T ) 2⌧ ⌘ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Algorithm without access to ⌧</head><p>In this section we explain how to get an algorithm which does not depend on ⌧ . Instead, STREAK (Algorithm 2) receives an accuracy parameter " 2 (0, 1). Then, it uses " to run several instances of Algorithm 1 stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is the maximum value of a singleton set corresponding to an element that the algorithm already observed, and u m references an arbitrary element satisfying f (u m ) = m.</p><p>The collection I is updated as follows after each element arrival. If previously I contained an instance of Algorithm 1 with a given value for ⌧ , and it no longer should contain such an instance, then the instance is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for ⌧ , and it should now contain such an instance, then a new instance with this value for ⌧ is created. Finally, if I contained an instance of Algorithm 1 with a given value for ⌧ , and it should continue to contain such an instance, then this instance remains in I as is. Theorem 5.5. The approximation ratio of STREAK is at least</p><formula xml:id="formula_13">(1 ") · 3 e /2</formula><p>2 p 2 e /2 2 . The proof of Theorem 5.5 shows that in the final collection I there is an instance of Algorithm 1 whose ⌧ provides a good approximation for a · f (OP T ), and thus, this instance of Algorithm 1 should (up to some technical details) produce a good output set in accordance with Theorem 5.1.</p><p>It remains to analyze the space complexity and running time of STREAK. We concentrate on bounding the number of elements STREAK keeps in its memory at any given time, as this amount dominates the space complexity as long as we assume that the space necessary to keep an element is at least as large as the space necessary to keep each one of the numbers used by the algorithm. Theorem 5.6. The space complexity of STREAK is O(" 1 k log k) elements.</p><p>The running time of Algorithm 1 is O(Nf) where, abusing notation, f is the running time of a single oracle evaluation of f . Therefore, the running time of STREAK is O(Nf" 1 log k) since it uses at every given time only O(" 1 log k) instances of the former algorithm. Given multiple threads, this can be improved to O(Nf + " 1 log k) by running the O(" 1 log k) instances of Algorithm 1 in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the performance of our streaming algorithm on two sparse feature selection applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sparse Regression with Pairwise Features</head><p>In this experiment, a sparse logistic regression is fit on 2000 training and 2000 test observations from the Phishing dataset <ref type="bibr" target="#b27">[Lichman, 2013]</ref>. This setup is known to be weakly submodular under mild data assumptions <ref type="bibr" target="#b16">[Elenberg et al., 2016a]</ref>. First, the categorical features are one-hot encoded, increasing Streaming maximization runs 10 times faster than the LIME framework.</p><p>Results averaged over 40 total iterations using 8 example explanations, error bars show 1 standard deviation.</p><p>the feature dimension to 68. Then, all pairwise products are added for a total of N = 4692 features.</p><p>To reduce computational cost, feature products are generated and added to the stream on-the-fly as needed. We compare with 2 other algorithms. RANDOMSUBSET selects the first k features from the random stream. LOCALSEARCH first fills a buffer with the first k features, and then swaps each incoming feature with the feature from the buffer which yields the largest nonnegative improvement.</p><p>Figure 1(a) shows both the final log likelihood and the generalization accuracy for RANDOMSUBSET, LOCALSEARCH, and our STREAK algorithm for " = {0.75, 0.1} and k = {20, 40, 80}. As expected, the RANDOMSUBSET algorithm has much larger variation since its performance depends highly on the random stream order. It also performs significantly worse than LOCALSEARCH for both metrics, whereas STREAK is comparable for most parameter choices. <ref type="figure">Figure 1</ref> <ref type="bibr">(b)</ref> shows two measures of computational cost: running time and the number of oracle evaluations (regression fits). We note STREAK scales better as k increases; for example, STREAK with k = 80 and " = 0.1 (" = 0.75) runs in about 70% (5%) of the time it takes to run LOCALSEARCH with k = 40. Interestingly, our speedups are more substantial with respect to running time. In some cases STREAK actually fits more regressions than LOCALSEARCH, but still manages to be faster. We attribute this to the fact that nearly all of LOCALSEARCH's regressions involve k features, which are slower than many of the small regressions called by STREAK.</p><p>Figure 2(a) shows the final log likelihood versus running time for k = 80 and " 2 [0.05, 0.75]. By varying the precision ", we achieve a gradual tradeoff between speed and performance. This shows that STREAK can reduce the running time by over an order of magnitude with minimal impact on the final log likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Black-Box Interpretability</head><p>Our next application is interpreting the predictions of black-box machine learning models. Specifically, we begin with the Inception V3 deep neural network <ref type="bibr" target="#b36">[Szegedy et al., 2016]</ref> trained on ImageNet. We use this network for the task of classifying 5 types of flowers via transfer learning. This is done by adding a final softmax layer and retraining the network.</p><p>We compare our approach to the LIME framework <ref type="bibr" target="#b33">[Ribeiro et al., 2016]</ref> for developing sparse, interpretable explanations. The final step of LIME is to fit a k-sparse linear regression in the space of interpretable features. Here, the features are superpixels determined by the SLIC image segmentation algorithm <ref type="bibr" target="#b0">[Achanta et al., 2012]</ref> (regions from any other segmentation would also suffice). The number of superpixels is bounded by N = 30. After a feature selection step, a final regression is performed on only the selected features. The following feature selection methods are supplied by LIME: 1. Highest Weights: fits a full regression and keep the k features with largest coefficients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:`1 regularization.</p><p>We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new image that contains only these regions and feed this into the black-box classifier. For a given model M , an input image I, and a label L 1 we ask for an explanation: why did model M label image I with label L 1 . We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that image I(S) has label L 1 . We approximately solve</p><formula xml:id="formula_14">max |S|k f (S) ,</formula><p>using STREAK. Intuitively, we are limiting the number of superpixels to k so that the output will include only the most important superpixels, and thus, will represent an interpretable explanation. In our experiments we set k = 5.</p><p>Note that the set function f (S) depends on the black-box classifier and is neither monotone nor submodular in general. Still, we find that the greedy maximization algorithm produces very good explanations for the flower classifier as shown in <ref type="figure" target="#fig_5">Figure 3</ref> and the additional experiments in the Supplementary Material. <ref type="figure" target="#fig_4">Figure 2</ref> <ref type="bibr">(b)</ref> shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on generating and classifying a large set of randomly perturbed example images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We propose STREAK, the first streaming algorithm for maximizing weakly submodular functions, and prove that it achieves a constant factor approximation assuming a random stream order. This is useful when the set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set. Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground set size has a constant factor approximation. We formulate interpretability of black-box neural networks as set function maximization, and show that STREAK provides interpretable explanations faster than previous approaches. We also show experimentally that STREAK trades off accuracy and running time in nonlinear sparse regression.</p><p>One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-of-the-art algorithm for submodular functions and our bound for = 1. However, as our algorithm performs the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been able to prove in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award (D16AP00046). We have used transfer learning to extract features from Inception and train a flower classifier. In these four input images the flower types were correctly classified (from (a) to (d): rose, sunflower, daisy, and daisy). We ask the question of interpretability: why did this model classify this image as rose. We are using our framework (and the recent prior work LIME <ref type="bibr" target="#b33">[Ribeiro et al., 2016]</ref>) to see which parts of the image the neural network is looking at for these classification tasks. As can be seen STREAK correctly identifies the flower parts of the images while some LIME variations do not. More importantly, STREAK is creating subsampled images on-the-fly, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls to the black-box model, the running times can be quite significant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>8A, B and nonnegative if f (A) 0, 8A. Using this notation one can define weakly submodular functions based on the following ratio. Definition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set function f : 2 N 7 ! R 0 is called -weakly submodular for an integer r if  r , min L,S✓N : |L|,|S\L|r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Observation 5 . 2 .</head><label>52</label><figDesc>Let S denote the output of Algorithm 1, then E[f (S)] (1 Pr[E]) · ⌧ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower bound for E[f (S)] which increases with Pr[E]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 FeaturesFigure 1 :</head><label>11</label><figDesc>Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable to LOCALSEARCH in both log likelihood and generalization accuracy, with much lower running time and number of model fits in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation. 6.1 Sparse Regression with Pairwise Features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features. By varying the parameter ", our algorithm captures a time-accuracy tradeoff between RANDOMSUBSET and LOCALSEARCH. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example explanations, error bars show 1 standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have used transfer learning to extract features from Inception and train a flower classifier. In these four input images the flower types were correctly classified (from (a) to (d): rose, sunflower, daisy, and daisy). We ask the question of interpretability: why did this model classify this image as rose. We are using our framework (and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network is looking at for these classification tasks. As can be seen STREAK correctly identifies the flower parts of the images while some LIME variations do not. More importantly, STREAK is creating subsampled images on-the-fly, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls to the black-box model, the running times can be quite significant.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code for these experiments is available at https://github.com/eelenberg/streak.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC Superpixels Compared to State-of-the-art Superpixel Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza Zadimoghaddam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Altschuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Bhaskara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2539" to="2548" />
		</imprint>
	</monogr>
	<note>Greedy Column Subset Selection: New Bounds and Distributed Algorithms</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning with Submodular Functions: A Convex Optimization Perspective. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Streaming Submodular Maximization: Massive Data Summarization on the Fly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwinkumar</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Greedy Sparsity-Constrained Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohail</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><forename type="middle">T</forename><surname>Boufounos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="807" to="841" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Power of Randomization: Distributed Submodular Maximization on Massive Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ponte</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Ene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1236" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A New Framework for Distributed Submodular Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ponte</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Ene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Andrew An Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deterministic Algorithms for Submodular Maximization Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Buchbinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moran</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="392" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Constrained Submodular Maximization via a Non-symmetric Technique. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Buchbinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moran</forename><surname>Feldman</surname></persName>
		</author>
		<idno>abs/1611.03253</idno>
		<ptr target="http://arxiv.org/abs/1611.03253" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online Submodular Maximization with Preemption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Buchbinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moran</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1202" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximizing a Monotone Submodular Function Subject to a Matroid Constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gruia</forename><surname>Cȃlinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Vondrák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1740" to="1766" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online Submodular Maximization with Free Disposal: Randomization Beats 1 /4 for Partition Matroids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-H. Hubert</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shaofeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao Gavin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1204" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Streaming Algorithms for Submodular Function Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalmoli</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kent</forename><surname>Quanrud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICALP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="318" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Conforti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Cornuéjols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="274" />
			<date type="published" when="1984-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Submodular meets Spectral: Greedy Algorithms for Subset Selection, Sparse Approximation and Dictionary Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Restricted Strong Convexity Implies Weak Submodularity. CoRR, abs/1612.00804</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1612.00804" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Restricted Strong Convexity Implies Weak Submodularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning in High Dimensions with Structure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Threshold of ln n for Approximating Set Cover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uriel</forename><surname>Feige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="634" to="652" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions-II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Polyhedral Combinatorics: Dedicated to the memory of D.R. Fulkerson</title>
		<editor>M. L. Balinski and A. J. Hoffman</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1978" />
			<biblScope unit="page" from="73" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Submodular Optimization Under Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1069" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch Mode Active Learning and its Application to Medical Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maximization of Approximately Submodular Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Horel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On Approximation Guarantees for Greedy Low Rank Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1837" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable Greedy Support Selection via Weak Submodularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1560" to="1568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Submodular Dictionary Selection for Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Submodular Function Maximization. Tractability: Practical Approaches to Hard Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<title level="m">Distributed Submodular Maximization: Identifying Representative Elements in Massive Data. NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2049" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lazier Than Lazy Greedy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwinkumar</forename><surname>Baharan Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vondrák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1812" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Best Algorithms for Approximating the Maximum of a Submodular Set Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="188" />
			<date type="published" when="1978-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions-I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parallel Double Greedy Submodular Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">K</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explaining the Predictions of Any Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why Should I Trust You?</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Axiomatic Attribution for Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimal approximation for submodular and supermodular optimization with bounded curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Sviridenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Vondrák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Submodularity and curvature: the optimal algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Vondrák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RIMS Kôkyûroku Bessatsu B23</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="253" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Iyer Rishabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
		<title level="m">Submodularity in Data Subset Selection and Active Learning. ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonina</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Sparse Nonlinear Regression: Parameter Estimation and Asymptotic Inference. ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
