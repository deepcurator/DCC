<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MemNet: A Persistent Memory Network for Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MemNet: A Persistent Memory Network for Image Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recently, very deep convolutional neural networks (CNNs)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image restoration <ref type="bibr" target="#b28">[29]</ref> is a classical problem in low-level computer vision, which estimates an uncorrupted image from a noisy or blurry one. A corrupted low-quality image x can be represented as: x = D(x) + n, wherex is a highquality version of x, D is the degradation function and n is * This work was supported by the National Science Fund of China under Grant Nos.   The blue circles denote a recursive unit with an unfolded structure which generates the short-term memory. The green arrow denotes the long-term memory from the previous memory blocks that is directly passed to the gate unit.</p><p>network named RED for image denoising and SISR. Moreover, Zhang et al. <ref type="bibr" target="#b39">[40]</ref> propose a denoising convolutional neural network (DnCNN) to tackle image denoising, SISR and JPEG deblocking simultaneously.</p><p>The conventional plain CNNs, e.g., VDSR <ref type="bibr" target="#b19">[20]</ref>, DRCN <ref type="bibr" target="#b20">[21]</ref> and DnCNN <ref type="bibr" target="#b39">[40]</ref>  <ref type="figure" target="#fig_2">(Fig. 1(a)</ref>), adopt the singlepath feed-forward architecture, where one state is mainly influenced by its direct former state, namely short-term memory. Some variants of CNNs, RED <ref type="bibr" target="#b26">[27]</ref> and ResNet <ref type="bibr" target="#b11">[12]</ref> ( <ref type="figure" target="#fig_2">Fig. 1(b)</ref>), have skip connections to pass information across several layers. In these networks, apart from the short-term memory, one state is also influenced by a specific prior state, namely restricted long-term memory. In essence, recent evidence suggests that mammalian brain may protect previously-acquired knowledge in neocortical circuits <ref type="bibr" target="#b3">[4]</ref>. However, none of above CNN models has such mechanism to achieve persistent memory. As the depth grows, they face the issue of lacking long-term memory.</p><p>To address this issue, we propose a very deep persistent memory network (MemNet), which introduces a memory block to explicitly mine persistent memory through an adaptive learning process. In MemNet, a Feature Extraction Net (FENet) first extracts features from the low-quality image. Then, several memory blocks are stacked with a densely connected structure to solve the image restoration task. Finally, a Reconstruction Net (ReconNet) is adopted to learn the residual, rather than the direct mapping, to ease the training difficulty.</p><p>As the key component of MemNet, a memory block contains a recursive unit and a gate unit. Inspired by neuroscience <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref> that recursive connections ubiquitously exist in the neocortex, the recursive unit learns multi-level representations of the current state under different receptive fields (blue circles in <ref type="figure" target="#fig_2">Fig. 1(c)</ref>), which can be seen as the short-term memory. The short-term memory generated from the recursive unit, and the long-term memory generated from the previous memory blocks 1 (green arrow in <ref type="figure" target="#fig_2">Fig. 1(c)</ref>) are concatenated and sent to the gate unit, which is a non-linear function to maintain persistent memory. Further, we present an extended multi-supervised MemNet, which fuses all intermediate predictions of memory blocks to boost the performance.</p><p>In summary, the main contributions of this work include: ⋄ A memory block to accomplish the gating mechanism to help bridge the long-term dependencies. In each memory block, the gate unit adaptively learns different weights for different memories, which controls how much of the longterm memory should be reserved, and decides how much of the short-term memory should be stored.</p><p>⋄ A very deep end-to-end persistent memory network (80 convolutional layers) for image restoration. The densely connected structure helps compensate mid/high-frequency signals, and ensures maximum information flow between memory blocks as well. To the best of our knowledge, it is by far the deepest network for image restoration.</p><p>⋄ The same MemNet structure achieves the state-of-theart performance in image denoising, super-resolution and JPEG deblocking. Due to the strong learning ability, our MemNet can be trained to handle different levels of corruption even using a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The success of AlexNet <ref type="bibr" target="#b21">[22]</ref> in ImageNet <ref type="bibr" target="#b30">[31]</ref> starts the era of deep learning for vision, and the popular networks, GoogleNet <ref type="bibr" target="#b32">[33]</ref>, Highway network <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b11">[12]</ref>, reveal that the network depth is of crucial importance.</p><p>As the early attempt, Jain et al. <ref type="bibr" target="#b16">[17]</ref> proposed a simple CNN to recover a clean natural image from a noisy observation and achieved comparable performance with the wavelet methods. As the pioneer CNN model for SISR, superresolution convolutional neural network (SRCNN) <ref type="bibr" target="#b7">[8]</ref> predicts the nonlinear LR-HR mapping via a fully deep convolutional network, which significantly outperforms classical shallow methods. The authors further proposed an extended CNN model, named Artifacts Reduction Convolutional Neural Networks (ARCNN) <ref type="bibr" target="#b6">[7]</ref>, to effectively handle JPEG compression artifacts.</p><p>To incorporate task-specific priors, Wang et al. adopted a cascaded sparse coding network to fully exploit the natural sparsity of images <ref type="bibr" target="#b35">[36]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, a deep dual-domain approach is proposed to combine both the prior knowledge in the JPEG compression scheme and the practice of dual-domain sparse coding. Guo et al. <ref type="bibr" target="#b9">[10]</ref> also proposed a dual-domain convolutional network that jointly learns a very deep network in both DCT and pixel domains.</p><p>Recently, very deep CNNs become popular for image restoration. Kim et al. <ref type="bibr" target="#b19">[20]</ref> stacked 20 convolutional layers to exploit large contextual information. Residual learning and adjustable gradient clipping are used to speed up the training. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> introduced batch normalization into a DnCNN model to jointly handle several image restoration tasks. To reduce the model complexity, the DRCN model introduced recursive-supervision and skipconnection to mitigate the training difficulty <ref type="bibr" target="#b20">[21]</ref>. Using symmetric skip connections, Mao et al. <ref type="bibr" target="#b26">[27]</ref> proposed a very deep convolutional auto-encoder network for image denoising and SISR. Very Recently, Lai et al. <ref type="bibr" target="#b22">[23]</ref> proposed LapSRN to address the problems of speed and accuracy for SISR, which operates on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type="bibr" target="#b33">[34]</ref> proposed deep recursive residual network (DRRN) to address the problems of model parameters and accuracy, which recursively learns the residual unit in a multi-path model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MemNet for Image Restoration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Network Architecture</head><p>Our MemNet consists of three parts: a feature extraction net FENet, multiple stacked memory blocks and finally a reconstruction net ReconNet <ref type="figure" target="#fig_3">(Fig. 2</ref>). Let's denote x and y as the input and output of MemNet. Specifically, a convolutional layer is used in FENet to extract the features from the noisy or blurry input image,</p><formula xml:id="formula_0">B 0 = f ext (x),<label>(1)</label></formula><p>where f ext denotes the feature extraction function and B 0 is the extracted feature to be sent to the first memory block. Supposing M memory blocks are stacked to act as the feature mapping, we have</p><formula xml:id="formula_1">B m = M m (B m−1 ) = M m (M m−1 (...(M 1 (B 0 ))...)),<label>(2)</label></formula><p>where M m denotes the m-th memory block function and B m−1 and B m are the input and output of the m-th memory block respectively. Finally, instead of learning the direct mapping from the low-quality image to the high-quality image, our model uses a convolutional layer in ReconNet to reconstruct the residual image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. Therefore, our basic MemNet can be formulated as,</p><formula xml:id="formula_2">y = D(x) = f rec (M M (M M −1 (...(M 1 (f ext (x)))...))) + x,<label>(3)</label></formula><p>where f rec denotes the reconstruction function and D denotes the function of our basic MemNet. Given a training set {x</p><formula xml:id="formula_3">(i) ,x (i) } N i=1</formula><p>, where N is the number of training patches andx (i) is the ground truth highquality patch of the low-quality patch x (i) , the loss function of our basic MemNet with the parameter set Θ, is</p><formula xml:id="formula_4">L(Θ) = 1 2N N ∑ i=1 ∥x (i) − D(x (i) )∥ 2 ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Block</head><p>We now present the details of our memory block. The memory block contains a recursive unit and a gate unit. Recursive Unit is used to model a non-linear function that acts like a recursive synapse in the brain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. Here, we use a residual building block, which is introduced in ResNet <ref type="bibr" target="#b11">[12]</ref> and shows powerful learning ability for object recognition, as a recursion in the recursive unit. A residual building block in the m-th memory block is formulated as, F denotes the residual function, W m is the weight set to be learned and R denotes the function of residual building block. Specifically, each residual function contains two convolutional layers with the pre-activation structure <ref type="bibr" target="#b12">[13]</ref>,</p><formula xml:id="formula_5">H r m = R m (H r−1 m ) = F(H r−1 m , W m ) + H r−1 m ,<label>(5)</label></formula><formula xml:id="formula_6">F(H r−1 m , W m ) = W 2 m τ (W 1 m τ (H r−1 m )),<label>(6)</label></formula><p>where τ denotes the activation function, including batch normalization <ref type="bibr" target="#b15">[16]</ref> followed by ReLU <ref type="bibr" target="#b29">[30]</ref>, and W i m , i = 1, 2 are the weights of the i-th convolutional layer. The bias terms are omitted for simplicity.</p><p>Then, several recursions are recursively learned to generate multi-level representations under different receptive fields. We call these representations as the short-term memory. Supposing there are R recursions in the recursive unit, the r-th recursion in recursive unit can be formulated as,</p><formula xml:id="formula_7">H r m = R (r) m (Bm−1) = Rm(Rm(...(Rm r (Bm−1))...)), (7)</formula><p>where r-fold operations of R m are performed and {H </p><p>Gate Unit is used to achieve persistent memory through an adaptive learning process. In this paper, we adopt a 1 × 1 convolutional layer to accomplish the gating mechanism that can learn adaptive weights for different memories,  and B m denote the function of the 1 × 1 convolutional layer (parameterized by W gate m ) and the output of the m-th memory block, respectively. As a result, the weights for the long-term memory controls how much of the previous states should be reserved, and the weights for the short-term memory decides how much of the current state should be stored. Therefore, the formulation of the m-th memory block can be written as,</p><formula xml:id="formula_9">B m = f gate m (B gate m ) = W gate m τ (B gate m ),<label>(9)</label></formula><formula xml:id="formula_10">B m = M m (B m−1 ) = f gate ([R m (B m−1 ), ..., R (R) m (B m−1 ), B 0 , ..., B m−1 ]).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Supervised MemNet</head><p>To further explore the features at different states, inspired by <ref type="bibr" target="#b20">[21]</ref>, we send the output of each memory block to the same reconstruction netf rec to generate</p><formula xml:id="formula_11">y m =f rec (x, B m ) = x + f rec (B m ),<label>(11)</label></formula><p>where {y m } M m=1 are the intermediate predictions. All of the predictions are supervised during training, and used to compute the final output via weighted averaging: y = ∑ M m=1 w m · y m <ref type="figure" target="#fig_6">(Fig. 3)</ref>. The optimal weights {w m } M m=1</p><p>are automatically learned during training and the final output from the ensemble is also supervised. The loss function of our multi-supervised MemNet can be formulated as,</p><formula xml:id="formula_12">L(Θ) = α 2N N ∑ i=1 ∥x (i) − M ∑ m=1 w m · y (i) m ∥ 2 + 1 − α 2M N M ∑ m=1 N ∑ i=1 ∥x (i) − y (i) m ∥ 2 ,<label>(12)</label></formula><p>where α denotes the loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dense Connections for Image Restoration</head><p>Now we analyze why the long-term dense connections in MemNet may benefit the image restoration. In very deep networks, some of the mid/high-frequency information can get lost at latter layers during a typical feedforward CNN process, and dense connections from previous layers can compensate such loss and further enhance  high-frequency signals. To verify our intuition, we train a 80-layer MemNet without long-term connections, which is denoted as MemNet NL, and compare with the original MemNet. Both networks have 6 memory blocks leading to 6 intermediate outputs, and each memory block contains 6 recursions. <ref type="figure" target="#fig_8">Fig. 4(a)</ref> shows the 4th and 6th outputs of both networks. We compute their power spectrums, center them, estimate spectral densities for a continuous set of frequency ranges from low to high by placing concentric circles, and plot the densities of four outputs in <ref type="figure" target="#fig_8">Fig. 4(b)</ref>.</p><p>We further plot differences of these densities in <ref type="figure" target="#fig_8">Fig. 4(c)</ref>. From left to right, the first case indicates the earlier layer does contain some mid-frequency information that the latter layers lose. The 2nd case verifies that with dense connections, the latter layer absorbs the information carried from the previous layers, and even generate more mid-frequency information. The 3rd case suggests in earlier layers, the frequencies are similar between two models. The last case demonstrates the MemNet recovers more high frequency than the version without long-term connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussions</head><p>Difference to Highway Network First, we discuss how the memory block accomplishes the gating mechanism and present the difference between MemNet and Highway Network -a very deep CNN model using a gate unit to regulate information flow <ref type="bibr" target="#b31">[32]</ref>.</p><p>To avoid information attenuation in very deep plain networks, inspired by LSTM, Highway Network introduced the bypassing layers along with gate units, i.e.,</p><formula xml:id="formula_13">b = A(a) · T (a) + a · (1 − T (a)),<label>(13)</label></formula><p>where a and b are the input and output, A and T are two non-linear transform functions. T is the transform gate to control how much information produced by A should be stored to the output; and 1 − T is the carry gate to decide how much of the input should be reserved to the output. In MemNet, the short-term and long-term memories are concatenated. The 1 × 1 convolutional layer adaptively learns the weights for different memories. Compared to Highway Network that learns specific weight for each pixel, our gate unit learns specific weight for each feature map, which has two advantages: (1) to reduce model parameters and complexity; (2) to be less prone to overfitting. Difference to DRCN There are three main differences between MemNet and DRCN <ref type="bibr" target="#b20">[21]</ref>. The first is the design of the basic module in network. In DRCN, the basic module is a convolutional layer; while in MemNet, the basic module is a memory block to achieve persistent memory. The second is in DRCN, the weights of the basic modules (i.e., the convolutional layers) are shared; while in MemNet, the weights of the memory blocks are different. The third is there are no dense connections among the basic modules in DRCN, which results in a chain structure; while in MemNet, there are long-term dense connections among the memory blocks leading to the multi-path structure, which not only helps information flow across the network, but also encourages gradient backpropagation during training. Benefited from the good information flow ability, MemNet could be easily trained without the multi-supervision strategy, which is imperative for training DRCN <ref type="bibr" target="#b20">[21]</ref>. Difference to DenseNet Another related work to MemNet is DenseNet <ref type="bibr" target="#b13">[14]</ref>, which also builds upon a densely connected principle. In general, DenseNet deals with object recognition, while MemNet is proposed for image restoration. In addition, DenseNet adopts the densely connected structure in a local way (i.e., inside a dense block), while MemNet adopts the densely connected structure in a global way (i.e., across the memory blocks). In Secs.  For the curve of the mth block, the left (m × 64) elements denote the long-term memories and the rest (Lm − m × 64) elements denote the short-term memories. The bar diagrams illustrate the average norm of long-term memories, short-term memories from the first R − 1 recursions and from the last recursion, respectively. E.g., each yellow bar is the average norm of the short-term memories from the last recursion in the recursive unit (i.e., the last 64 elements in each curve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Datasets For image denoising, we follow <ref type="bibr" target="#b26">[27]</ref> to use 300 images from the Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b27">[28]</ref>, known as the train and val sets, to generate image patches as the training set. Two popular benchmarks, a dataset with 14 common images and the BSD test set with 200 images, are used for evaluation. We generate the input noisy patch by adding Gaussian noise with one of the three noise levels (σ = 30, 50 and 70) to the clean patch.</p><p>For SISR, by following the experimental setting in <ref type="bibr" target="#b19">[20]</ref>, we use a training set of 291 images where 91 images are from Yang et al. <ref type="bibr" target="#b37">[38]</ref>   <ref type="table">Table 2</ref>. SISR comparisons with start-of-the-art networks for scale factor ×3 on Set5. Red indicates the fewest number or best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MemNet_M6</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MemNet_M5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MemNet_M4</head><p>MemNet_M3 VDSR DRCN (sec.) <ref type="figure">Figure 6</ref>. PSNR, complexity vs. speed.</p><p>BSD100 <ref type="bibr" target="#b27">[28]</ref> and Urban100 <ref type="bibr" target="#b14">[15]</ref> are used. Three scale factors are evaluated, including ×2, ×3 and ×4. The input LR image is generated by first bicubic downsampling and then bicubic upsampling the HR image with a certain scale. For JPEG deblocking, the same training set for image denoising is used. As in <ref type="bibr" target="#b6">[7]</ref>, Classic5 and LIVE1 are adopted as the test datasets. Two JPEG quality factors are used, i.e., 10 and 20, and the JPEG deblocking input is generated by compressing the image with a certain quality factor using the MATLAB JPEG encoder. Training Setting Following the method <ref type="bibr" target="#b26">[27]</ref>, for image denoising, the grayscale image is used; while for SISR and JPEG deblocking, the luminance component is fed into the model. The input image size can be arbitrary due to the fully convolution architecture. Considering both the training time and storage complexities, training images are split into 31 × 31 patches with a stride of 21. The output of MemNet is the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type="bibr" target="#b33">[34]</ref> to do data augmentation. For each task, we train a single model for all different levels of corruption. E.g., for image denoising, noise augmentation is used. Images with different noise levels are all included in the training set. Similarly, for super-resolution and JPEG deblocking, scale and quality augmentation are used, respectively.</p><p>We use Caffe <ref type="bibr" target="#b18">[19]</ref> to implement two 80-layer MemNet networks, the basic and the multi-supervised versions. In both architectures, 6 memory blocks, each contains 6 recursions, are constructed (i.e., M6R6). Specifically, in multisupervised MemNet, 6 predictions are generated and used to compute the final output. α balances different regularizations, and is empirically set as α = 1/(M + 1).</p><p>The objective functions in Eqn. 4 and Eqn. 12 are optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type="bibr" target="#b23">[24]</ref>. We set the mini-batch size of SGD to 64, momentum parameter to 0.9, and weight decay to 10 −4 . All convolutional layer has 64 filters. Except the 1 × 1 convolutional layers in the gate units, the kernel size of other convolutional layers is 3 × 3. We use the method in <ref type="bibr" target="#b10">[11]</ref> for weight initialization. The initial learning rate is set to 0.1 and then divided 10 every 20 epochs. Training a 80-layer basic MemNet by 91 images <ref type="bibr" target="#b37">[38]</ref> for SISR roughly takes 5 days using 1 Tesla P40 GPU. Due to space constraint and more recent baselines, we focus on SISR in Sec. 5.2, 5.4 and 5.6, while all three tasks in Sec. 5.3 and 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>Tab. 1 presents the ablation study on the effects of longterm and short-term connections. Compared to MemNet, MemNet NL removes the long-term connections (green curves in <ref type="figure" target="#fig_6">Fig. 3</ref>) and MemNet NS removes the short-term connections (black curves from the first R − 1 recursions to the gate unit in <ref type="figure" target="#fig_2">Fig. 1</ref>. Connection from the last recursion to the gate unit is reserved to avoid a broken interaction between recursive unit and gate unit). The three networks have the same depth (80) and filter number (64). We see that, long-term dense connections are very important since MemNet significantly outperforms MemNet NL. Further, MemNet achieves better performance than MemNet NS, which reveals the short-term connections are also useful for image restoration but less powerful than the long-term connections. The reason is that the long-term connections skip much more layers than the short-term ones, which can carry some mid/high frequency signals from very early layers to latter layers as described in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Gate Unit Analysis</head><p>We now illustrate how our gate unit affects different kinds of memories. Inspired by <ref type="bibr" target="#b13">[14]</ref>, we adopt a weight norm as an approximate for the dependency of the current layer on its preceding layers, which is calculated by the corresponding weights from all filters w.r.t. each feature map: Basically, the larger the norm is, the stronger dependency it has on this particular feature map. For better visualization, we normalize the norms to the range of 0 to 1.  <ref type="formula" target="#formula_2">(3)</ref> In general, the short-term memories from the last recursion in recursive unit (the last 64 elements in each curve) contribute most than the other two memories, and the long-term memories seem to play a more important role in late memory blocks to recover useful signals than the short-term memories from the first R − 1 recursions.   <ref type="table">Table 5</ref>. Benchmark JPEG deblocking results. Average PSNR/SSIMs for quality factor 10 and 20 on datasets Classic5 and LIVE1.</p><formula xml:id="formula_14">v l m = √ ∑ 64 i=1 (W gate m (1, 1, l, i)) 2 , l = 1, 2, ..., L m ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparision with Non-Persistent CNN Models</head><p>In this subsection, we compare MemNet with three existing non-persistent CNN models, i.e., VDSR <ref type="bibr" target="#b19">[20]</ref>, DRCN <ref type="bibr" target="#b20">[21]</ref> and RED <ref type="bibr" target="#b26">[27]</ref>, to demonstrate the superiority of our persistent memory structure. VDSR and DRCN are two representative networks with the plain structure and RED is representative for skip connections. Tab. 2 presents the published results of these models along with their training details. Since the training details are different among different work, we choose DRCN as a baseline, which achieves good performance using the least training images. But, unlike DRCN that widens its network to increase the parameters (filter number: 256 vs. 64), we deepen our MemNet by stacking more memory blocks (depth: 20 vs. 80). It can be seen that, using the fewest training images (91), filter number (64) and relatively few model parameters (667K), our basic MemNet already achieves higher PSNR than the prior networks. Keeping the setting unchanged, our multi-supervised MemNet further improves the performance. With more training images (291), our MemNet significantly outperforms the state of the arts.</p><p>Since we aim to address the long-term dependency problem in networks, we intend to make our MemNet very deep. However, MemNet is also able to balance the model complexity and accuracy. <ref type="figure">Fig. 6</ref> presents the PSNR of different intermediate predictions in MemNet (e.g., MemNet M3 denotes the prediction of the 3rd memory block) for scale ×3 on Set5, in which the colorbar indicates the inference time (sec.) when processing a 288 × 288 image on GPU P40. Results of VDSR <ref type="bibr" target="#b19">[20]</ref> and DRCN <ref type="bibr" target="#b20">[21]</ref> are cited from their papers. RED <ref type="bibr" target="#b26">[27]</ref> is skipped here since its high number of parameters may reduce the contrast among other methods. We see that our MemNet already achieve comparable result at the 3rd prediction using much fewer parameters, and significantly outperforms the state of the arts by slightly increasing model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparisons with State-of-the-Art Models</head><p>We compare multi-supervised 80-layer MemNet with the state of the arts in three restoration tasks, respectively. Image Denoising Tab. 3 presents quantitative results on two benchmarks, with results cited from <ref type="bibr" target="#b26">[27]</ref>. For BSD200 dataset, by following the setting in RED <ref type="bibr" target="#b26">[27]</ref>, the original image is resized to its half size. As we can see, our MemNet achieves the best performance on all cases. It should be noted that, for each test image, RED rotates and mirror flips the kernels, and performs inference multiple times. The outputs are then averaged to obtain the final result. They claimed this strategy can lead to better performance. However, in our MemNet, we do not perform any post-processing. For qualitative comparisons, we use public codes of PCLR <ref type="bibr" target="#b1">[2]</ref>, PGPD <ref type="bibr" target="#b36">[37]</ref> and WNNM <ref type="bibr" target="#b8">[9]</ref>. The results are shown in <ref type="figure">Fig. 7</ref>  Since LapSRN doesn't report the results on scale ×3, we use the symbol '−' instead. <ref type="figure" target="#fig_12">Fig. 8</ref> shows the visual comparisons for SISR. SRCNN <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b19">[20]</ref> and DnCNN <ref type="bibr" target="#b39">[40]</ref> are compared using their public codes. MemNet recovers relatively sharper edges, while others have blurry results. JPEG Deblocking Tab. 5 shows the JPEG deblocking results on Classic5 and LIVE1, by citing the results from <ref type="bibr" target="#b39">[40]</ref>. Our network significantly outperforms the other methods, and deeper networks do improve the performance compared to the shallow one, e.g., ARCNN. <ref type="figure">Fig. 9</ref> shows the JPEG deblocking results of these three methods, which are generated by their corresponding public codes. As it can be seen, MemNet effectively removes the blocking artifact and recovers higher quality images than the previous methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison on Different Network Depths</head><p>Finally, we present the comparison on different network depths, which is caused by stacking different numbers of memory blocks or recursions. Specifically, we test four network structures: M4R6, M6R6, M6R8 and M10R10, which have the depth 54, 80, 104 and 212, respectively. Tab. 6 shows the SISR performance of these networks on Set5 with scale factor ×3. It verifies deeper is still better and the proposed deepest network M10R10 achieves 34.23 dB, with the improvement of 0.14 dB compared to M6R6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, a very deep end-to-end persistent memory network (MemNet) is proposed for image restoration, where a memory block accomplishes the gating mechanism for tackling the long-term dependency problem in the previous CNN architectures. In each memory block, a recursive unit is adopted to learn multi-level representations as the short-term memory. Both the short-term memory from the recursive unit and the long-term memories from the previous memory blocks are sent to a gate unit, which adaptively learns different weights for different memories. We use the same MemNet structure to handle image denoising, superresolution and JPEG deblocking simultaneously. Comprehensive benchmark evaluations well demonstrate the superiority of our MemNet over the state of the arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>91420201, 61472187, 61502235, 61233011, 61373063 and 61602244, the 973 Program No. 2014CB349303, Program for Changjiang Scholars, and partially sponsored by CCF-Tencent Open Re- search Fund. Jian Yang and Xiaoming Liu are corresponding authors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Prior network structures (a,b) and our memory block (c). The blue circles denote a recursive unit with an unfolded structure which generates the short-term memory. The green arrow denotes the long-term memory from the previous memory blocks that is directly passed to the gate unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Basic MemNet architecture. The red dashed box represents multiple stacked memory blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the input and output of the r-th resid- ual building block respectively. When r = 1, H 0 m = B m−1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the multi-level representations of the re- cursive unit. These representations are concatenated as the short-term memory:In addition, the long-term memory coming from the pre- vious memory blocks can be constructed as: B long m = [B 0 , B 1 , ..., B m−1 ]. The two types of memories are then concatenated as the input to the gate unit,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Multi-supervised MemNet architecture. The outputs with purple color are supervised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) ×4 super-resolved images and PSNR/SSIMs of different networks. (b) We convert 2-D power spectrums to 1-D spectral densities by integrating the spectrums along each concentric circle. (c) Differences of spectral densities of two networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The norm of filter weights v l m vs. feature map index l. For the curve of the mth block, the left (m × 64) elements denote the long-term memories and the rest (Lm − m × 64) elements denote the short-term memories. The bar diagrams illustrate the average norm of long-term memories, short-term memories from the first R − 1 recursions and from the last recursion, respectively. E.g., each yellow bar is the average norm of the short-term memories from the last recursion in the recursive unit (i.e., the last 64 elements in each curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>where L m is the number of the input feature maps for the m-th gate unit, l denotes the feature map index, W gate m stores the weights with the size of 1 × 1 × L m × 64, and v l m is the weight norm of the l-th feature map for the m-th gate unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>vs. fea- ture map index l. We have three observations: (1) Different tasks have different norm distributions. (2) The average and variance of the weight norms become smaller as the mem- ory block number increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Qualitative comparisons of SISR. The first row shows image "108005" from BSD100 with scale factor ×3. Only MemNet correctly recovers the pattern. The second row shows image "img 002" from Urban100 with scale factor ×4. MemNet recovers sharper lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>3.4 and 5.2, we analyze and demonstrate the long-term dense connec- tions in MemNet indeed play an important role in image restoration tasks.Table 1. Ablation study on effects of long-term and short-term con- nections. Average PSNR/SSIMs for the scale factor ×2, ×3 and ×4 on dataset Set5. Red indicates the best performance.</figDesc><table>Methods 
MemNet NL 
MemNet NS 
MemNet 

×2 
37.68/0.9591 
37.71/0.9592 
37.78/0.9597 
×3 
33.96/0.9235 
34.00/0.9239 
34.09/0.9248 
×4 
31.60/0.8878 
31.65/0.8880 
31.74/0.8893 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>and other 200 are from BSD train set. For testing, four benchmark datasets, Set5 [1], Set14 [39],</figDesc><table>Dataset 

VDSR [20] DRCN [21] 
RED [27] 
MemNet 
Depth 
20 
20 
30 
80 
Filters 
64 
256 
128 
64 
Parameters 
665K 
1, 774K 
4, 131K 
677K 
Traing images 
291 
91 
300 
91 
91 
291 
Multi-supervision 
No 
Yes 
No 
No 
Yes 
Yes 
PSNR 
33.66 
33.82 
33.82 
33.92 
33.98 
34.09 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Benchmark SISR results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100 and Urban100.</figDesc><table>Dataset 
Quality 
JPEG 
ARCNN [7] 
TNRD [3] 
DnCNN [40] 
MemNet 

Classic5 
10 
27.82/0.7595 
29.03/0.7929 
29.28/0.7992 
29.40/0.8026 
29.69/0.8107 
20 
30.12/0.8344 
31.15/0.8517 
31.47/0.8576 
31.63/0.8610 
31.90/0.8658 

LIVE1 
10 
27.77/0.7730 
28.96/0.8076 
29.15/0.8111 
29.19/0.8123 
29.45/0.8193 
20 
30.07/0.8512 
31.29/0.8733 
31.46/0.8769 
31.59/0.8802 
31.83/0.8846 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>. As we can see, our MemNet handles Gaussian noise better than the previous state of the arts. Super-Resolution Tab. 4 summarizes quantitative results on four benchmarks, by citing the results of prior methods. MemNet outperforms prior methods in almost all cases. (PSNR/SSIM) (18.56/0.2953) (29.89/0.8678) (29.80/0.8652) (29.93/0.8702) (30.48/0.8791)Figure 7. Qualitative comparisons of image denoising. The first row shows image "10" from 14-image dataset with noise level 30. Only MemNet recovers the fold. The second row shows image "206062" from BSD200 with noise level 70. Only MemNet cor- rectly recovers the pillar. Please zoom in to see the details.</figDesc><table>Ground Truth 
Noisy 
PCLR 
PGPD 
WNNM 
MemNet (ours) 

(PSNR/SSIM) (11.19/0.1082) (24.67/0.6691) (24.49/0.6559) (24.50/0.6632) (25.37/0.6964) 

(PSNR/SSIM) (26.43/0.7606) (27.74/0.8194) (28.18/0.8341) (28.19/0.8349) (28.35/0.8388) 

Ground Truth 
Bicubic 
SRCNN 
VDSR 
DnCNN 
MemNet (ours) 

(PSNR/SSIM) (21.68/0.6491) (22.85/0.7249) (23.91/0.7859) (23.89/0.7838) (24.62/0.8167) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Figure 9. Qualitative comparisons of JPEG deblocking. The first row shows image "barbara" from Classic5 with quality factor 10. MemNet recovers the lines, while others give blurry results. The second row shows image "lighthouse" from LIVE1 with quality factor 10. MemNet accurately removes the blocking artifact.Table 6. Comparison on different network depths.</figDesc><table>(PSNR/SSIM) (25.79/0.7621) (26.92/0.7971) (27.24/0.8104) (27.59/0.8161) (28.15/0.8353) 

Ground Truth 
JPEG 
ARCNN 
TNRD 
DnCNN 
MemNet (ours) 

(PSNR/SSIM) (28.29/0.7636) (29.63/0.7977) (29.76/0.8018) (29.82/0.8008) (30.13/0.8088) 

Network 
M4R6 M6R6 M6R8 M10R10 
Depth 
54 
80 
104 
212 
PSNR (dB) 34.05 
34.09 
34.16 
34.23 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the first memory block, its long-term memory comes from the output of FENet.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">External patch prior guided internal clustering for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Branch-specific dendritic ca2+ spikes cause persistent synaptic plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cichon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">520</biblScope>
			<biblScope unit="issue">7546</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on IP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Theoretical neuroscience. Cambridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
			<pubPlace>MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data-driven sparsity-based restoration of jpeg-compressed images in dual transform-pixel domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tour of modern image filtering: new insights and methods, both practical and theoretical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="128" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">D 3 : Deep dual-domain based fast restoration of jpegcompressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Patch group based nonlocal self-similarity prior learning for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on IP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On single image scaleup using sparse-representations. Curves and Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on IP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
