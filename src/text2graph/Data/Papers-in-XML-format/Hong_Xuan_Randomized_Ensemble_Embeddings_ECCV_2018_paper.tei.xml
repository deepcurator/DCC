<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Randomized Ensembles for Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
							<email>xuanhong@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
							<email>souvenir@temple.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">Temple University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
							<email>pless@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Washington University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Randomized Ensembles for Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Learning embedding functions, which map semantically related inputs to nearby locations in a feature space supports a variety of classification and information retrieval tasks. In this work, we propose a novel, generalizable and fast method to define a family of embedding functions that can be used as an ensemble to give improved results. Each embedding function is learned by randomly bagging the training labels into small subsets. We show experimentally that these embedding ensembles create effective embedding functions. The ensemble output defines a metric space that improves state of the art performance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes Retrieval  and VehicleID.   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image embeddings are commonly optimized to map semantically similar inputs to nearby locations in feature space. Thereafter, tasks such as classification and image retrieval can be recast as simple operations, such as neighborhood lookups, in the learned feature space. This approach has been applied across many problems.</p><p>Deep learning approaches to embedding are trained with data from many classes and optimize loss functions based on pairs of images from the same or different classes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, triplets of images where inputs from the same class are forced to be closer than inputs from different classes <ref type="bibr" target="#b15">[16]</ref>, or functions of large collections of images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>For many of these optimization functions, embedding the input images into a high dimensional space leads to poor performance due to over-fitting. Some recent work <ref type="bibr" target="#b12">[13]</ref> suggests an approach to high-dimensional embedding with an ensemble approach, learning to map images to a collection of independent output spaces, using boosting to re-weight input examples to make each output space independent.</p><p>We propose a different approach to learning a robust, high-dimensional embedding space. Instead of re-weighting input examples to create independent output embeddings, we propose to group class labels. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the idea. We learn a collection of embeddings all trained with the same input data, but differing in the label assigned to the data points. We group classes into metaclasses (each containing a few classes), and learn embeddings with inputs labelled by their meta-class. We show a visual example of the meta-classes in <ref type="figure">Figure 2</ref> based on data from the CARS196 dataset. The first meta-class groups together images from particular models of Porches and Audi, so the first embedding will seek to map all these images to the same location.</p><p>For each grouping of classes into meta-classes, an embedding is learned that embeds all elements of the same meta-class similarly. We train many such embeddings, with different random groupings of classes into meta-classes. The final embedding is the concatenation of the coordinates of each low-dimensional embedding.</p><p>This approach fits to many choices of embedding architectures. We show experimental results using the ResNet-18 <ref type="bibr" target="#b5">[6]</ref> and Inception V3 <ref type="bibr" target="#b18">[19]</ref> architectures, and demonstrate that our ensemble method improves upon the state of the art across a number of problem domains. Our contributions are as follows.</p><p>-We introduce the idea of randomly grouping labels as an approach to making a large family of related embedding models that can be used as an ensemble. -We illustrate the effect of different parameter choices relating to the embedding size and number of embeddings within the ensemble. -We demonstrate improvement over the state of the art for retrieval tasks for the CUB-200-2011 <ref type="bibr" target="#b20">[21]</ref>, Cars-196 <ref type="bibr" target="#b7">[8]</ref>, In-Shop Clothes Retrieval <ref type="bibr" target="#b23">[24]</ref> and VehicleID <ref type="bibr" target="#b9">[10]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Embeddings</head><p>Image embedding falls under the umbrella of distance metric learning. There has been quite a bit of work in this area from both the machine learning and computer vision perspectives. Here, we focus on recent methods which employ convolutional neural networks for image embedding.</p><p>There are many ways to learn embedding functions. Triplet loss (e.g. <ref type="bibr" target="#b15">[16]</ref>) defines a loss function based on triplets of images (two from the same class, and one from a different class), and penalizes the network if it does not map the same class inputs to be closer than the different classes. Because training is often performed in batches, it is natural to consider loss functions that optimize the embedded location of all images in the batch, either by considering all triplets (defined by the batch) simultaneously or by penalizing the histograms of distance between same-class and different class images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>No Fuss Embeddings <ref type="bibr" target="#b10">[11]</ref> shows that using the output layer of classification networks provide very useful embedding functions for one-shot learning and image retrieval tasks. This has the advantage of faster convergence (because each input image has a specific label, and the loss function does not depend on where other inputs are mapped), which removes some challenges in hard-example mining that plague some triplet-loss approaches. While triplet-loss approaches can be designed to mitigate these challenges <ref type="bibr" target="#b6">[7]</ref>, we choose to use <ref type="bibr" target="#b10">[11]</ref> as our embedding approach in our experiments primarily for its speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ensemble CNNs</head><p>Ensemble algorithms have been more widely used for classification problems. One example applies a variation of the boosting model that adds extra network layers trained on examples for which a smaller network fails <ref type="bibr" target="#b21">[22]</ref>. Other approaches to create diversity in the ensemble is to train a collection of networks of different architectures to solve the same problem and combine the results <ref type="bibr" target="#b3">[4]</ref>.</p><p>To the best of our knowledge, the only work that creates an ensemble for embedding is BIER <ref type="bibr" target="#b12">[13]</ref>. This follows a boosting model to incrementally create an ensemble embedding by re-weighting examples so that subsequent embeddings are driven to correct errors in earlier embeddings. Compared to this approach, our method is not sequential, and therefore trivially parallelized. Additionally, as we demonstrate in Section 4, our method outperforms BIER on many benchmark datasets. <ref type="figure">Fig. 2</ref>. Meta-classes for the CAR196 dataset. It may be counter-intuitive that it is helpful to group specific models of Porches and Audis into one class and learn embeddings where those classes are mapped to the same location, but this approach makes it easy to define many different but related embedding problems that become an effective ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Randomized Ensemble Embedding</head><p>Our training approach is to create a collection of related models and learn an embedding for each one. To create one member of our ensemble, model i, we partition the set of class labels from the training set Y into a set of meta-classes M i , where the number of meta-classes is a parameter D, and each meta-class is roughly the same size. All model in the ensemble are computed the same way, with the only difference being that the mapping, φ i , based on meta-classes, M i , come from a different random partitions of Y .</p><p>We define the number of embeddings as L. To compute the final embedding for a new input x, we concatenate the output of each embedding to get a final output vector, Φ = φ 1 (x), φ 2 (x), . . . φ L (x) . For one-shot learning or image retrieval tasks, this function Φ takes the place of standard embedding functions.</p><p>Overall this approach has a collection of parameters and choices, with the two most prominent being:</p><p>1. D, the number of meta-classes into which the class label set Y is partitioned, and 2. L, the number of embeddings functions included in the ensemble.</p><p>There are a collection of choices that relate to learning φ i based on the embedding problem defined by the class partition M i . If φ i is represented as a deep neural network, we consider the following questions:</p><p>1. What is output embedding dimension of φ i ? 2. What is the network architecture that represents the function φ i ? 3. What is the loss function used to train φ i ?</p><p>We chose to experiment with ResNet-18 and Inception V3 architectures and follow the no-fuss-embedding approach <ref type="bibr" target="#b10">[11]</ref> with an output dimension equal to the number of meta-classes, D (except where noted). Given these choices, Section 4 characterizes performance as a function of the number of meta-classes (and therefore the size of each meta-class) and the size of the ensemble used.</p><p>All tests are run on the PyTorch platform <ref type="bibr" target="#b13">[14]</ref>. For our experiments, we use the ResNet18 and Inception V3 implementations from the PyTorch model zoo, which are pretrained on ILSVRC 2012-CLS data <ref type="bibr" target="#b14">[15]</ref>. The input images are re-sized to 256 by 256 pixels. We adopt a standard data augmentation scheme (random horizontal flip and random crops padded by 10 pixels on each side). For pre-processing, we normalize the images using the channel means and standard deviations. All networks are trained using stochastic gradient descent (SGD). On all datasets we train using a batch size of 128 for 9 epochs. The initial learning rate is set to 0.01 and divided by 10 every 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We compare our method, Deep Randomized Ensembles for Metric Learning (DREML) with 7 state-of-art methods (using published results where available): Triplet Learning with semi-hard negative mining <ref type="bibr" target="#b16">[17]</ref>, N-Pairs deep metric loss <ref type="bibr" target="#b17">[18]</ref>, Proxy-based method <ref type="bibr" target="#b10">[11]</ref>, Hard-Aware Deeply Cascaded Embedding (HDC) <ref type="bibr" target="#b22">[23]</ref>, Boosting Independent Embeddings Robustly (BIER) <ref type="bibr" target="#b12">[13]</ref>, the FashionNet benchmark <ref type="bibr" target="#b23">[24]</ref> and Group Sensitive Triplet Sampling (GS-TRS) <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> shows the performance tradeoffs for different choices of the number of embeddings to include in the ensemble (our parameter L), and the number of meta-classes (our parameter D), for the CAR196 dataset. The left graph shows a dramatic improvement as the ensemble size grows while it is small, and a clear asymptotic behavior beyond which adding new embeddings does not help. The right graph shows that the performance also depends on the size of the metaclass. When D is small, the number of classes per meta-class is large making a harder embedding problem; when D is large, the number of classes per meta-class is small leading to less diversity in the ensemble. <ref type="figure" target="#fig_2">Figure 4</ref> explores the effect of increasing ensemble size for a fixed metaclass size. We see that the distribution of dot-products between embeddings of objects in the same class (solid) and different classes (dashed) becomes more separated for the CAR196 dataset, for both the training and validation datasets. Additionally, the number of pairs from different classes that have a large dotproduct (for example, greater than 0.75) decreases. This is consistent with the observed improvement in the recall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter Selection</head><p>For the remaining experiments, we employ multiple DREML models, denoted as DREML ({I,R}, D, L) where the tuple indicates the architecture (I)nceptionV3 or (R)esNet18 and values for D and L.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retrieval performance</head><p>We follow the evaluation protocol described in <ref type="bibr" target="#b11">[12]</ref> to evaluate the Recall@K and Normalized Mutual Information (NMI) values on two datasets, CUB200 and CAR196. For the In-Shop Clothes Retrieval and PKU VehicleID datasets, we follow the evaluation protocol described in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b9">[10]</ref> and evaluate Recall@K. <ref type="table" target="#tab_0">Table 1</ref> shows retrieval performance results on the CUB200 and CAR196 datasets. The CUB200 dataset <ref type="bibr" target="#b20">[21]</ref> contains 200 classes of birds with 11,788 images. We split the first 100 classes for training (5,864 images) and the rest of the classes for testing (5,924 images). The CAR196 dataset <ref type="bibr" target="#b7">[8]</ref> contains 196 classes of cars with 16,185 images. We use the standard split with the first 98 classes for training (8,054 images) and the rest of the classes for testing <ref type="bibr" target="#b7">(8,</ref><ref type="bibr">131</ref> images). For each dataset, <ref type="table" target="#tab_0">Table 1</ref> shows the Recall@K for K = 1, 2, 4, 8. Additionally, the normalized mutual information (NMI) score is included as a measure of clustering performance, as suggested in <ref type="bibr" target="#b11">[12]</ref>. The results show dramatic improvement in both retrieval accuracy and clustering performance across both datasets. Boths datasets contain substantial intra-class variability; in CUB200, birds are shown in different poses in front of very different backgrounds. <ref type="figure" target="#fig_4">Figure 6</ref> (top left) highlights this variability and shows example retrieval results from our method for CUB200 (top left) and CAR196 (top right).</p><p>The In-Shop Clothes Retrieval (ICR) dataset <ref type="bibr" target="#b23">[24]</ref> contains 11,735 classes of clothing items with 54,642 images. Following the settings in <ref type="bibr" target="#b23">[24]</ref>, only 7,982 classes of clothing items with 52,712 images are used for training and testing. 3,997 classes are for training (25,882 images) and 3,985 classes are for testing (28,760 images). The test set are partitioned to query set and gallery set, where query set contains 14,218 images of 3,985 classes and gallery set contains 12,612 images of 3,985 classes. Then, given a target image in test set, we retrieve the most similar image in the gallery set. <ref type="table" target="#tab_1">Table 2</ref> shows retrieval and clustering results showing a slight improvement over the BIER results. In absolute terms, DREML underperforms on the In Shop Clothes dataset compared to other datasets; this dataset has more classes, fewer examples per class, and substantial intra-class variation. Example results showing this variation are shown in <ref type="figure" target="#fig_4">Figure 6</ref> (bottom left). The PKU VehicleID (VID) <ref type="bibr" target="#b9">[10]</ref> dataset contains 221,763 images of 26,267 vehicles captured by surveillance cameras. The training set contains 110,178 images of 13,134 vehicles and the testing set contains 111,585 images of 13,133 vehicles. We follow the standard experimental protocol <ref type="bibr" target="#b9">[10]</ref> to test on the small, medium and large test set which contains 7,332 images of 800 vehicles, 12,995 images of 1,600 vehicles and 20,038 images of 2,400 vehicles respectively. Table 3 shows retrieval and clustering results for the PKU Vehicle-ID dataset. This dataset has substantially less intra-class variability, but some nearby classes are quite similar. Example retrieval results and images are shown in <ref type="figure" target="#fig_4">Figure 6</ref> (bottom right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding Unseen Classes</head><p>Our approach performs well to embed unseen classes, scattering new examples more effectively across the feature space. We believe that this property helps explain the improved performance of our method on retrieval tasks. We use the No Fuss Embedding approach with 96 training classes to define a 96 dimensional embedding. We then map inputs from the 98 standard testing classes onto this embedding. Because the No Fuss Embedding forces points to lie on a hypersphere we use the dot-product as a measure of similarity, and compute the similarity of each point in an unseen class to the most sim ilar point from any of the training classes.  <ref type="figure" target="#fig_3">Figure 5</ref> shows this distribution. The blue line is the distribution for a single network, with most unseen examples having a maximum similarity score to a training class of greater than 0.9. This shows that when mapping new classes into the embedding space, they are often mapped very close to existing classes, and this crowding within the embedding space may limit recall performance.</p><p>We repeat this experiment with 3 other networks. For the same 96 training categories, we group them into 24 meta-classes, each of size 4 and perform the same experiment (shown in the orange curved, shifted second farthest to the right). This is not an ensemble embedding, but we hypothesize that the metaclasses comprised of dis-similar input encourages an embedding that pushes novel images farther away from existing images.</p><p>The final two curves show the results of the ensemble embedding, using 4 (green) and 24 (red) total embedding functions respectively. In these embeddings, new images are mapped to locations where they tend to be much farther from the training images, and because they are more spread out the embedding may be more effective at representing unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Deep Randomized Ensembles for Metric Learning (DREML) is a simple approach to creating an ensemble of diverse embedding functions. We think this is a handy tool that may have broad applicability and have demonstrated results on four datasets spanning problem domains from a medium number of categories in CUB200 and CAR196, to the In-Shop Clothes and Vehicle ID datasets with tens of thousands of categories. Ensemble based approaches, both ours and BIER paper outperform the non-ensemble approaches by a dramatic margin on all four datasets.</p><p>The CARS196 and CUB200 datasets have a moderate amount of training data, and we believe that our approach of building meta-classes creates a version of "label augmentation" that effectively allows our ensemble to have more independent embeddings. For larger datasets, our approach is similar in performance to BIER for the In-Shop Clothes Retrieval dataset, which has substantial in-class variation due to color and pose changes, and is overall less balanced with many classes that have few examples per class. We outperform BIER for the PKU Vehicle ID dataset for all dataset sizes, perhaps because our ensemble approach is more robust that BIER to the relatively smaller intra-class variation.</p><p>The downside to our approach is that we train a large number of networks, in the cases where we outperform BIER, we showed results with an ensemble of 12 networks (for In-Shop Clothes dataset) and 48 networks (for the CUB, CAR, and VID dataset), something which affects both the training phase and the test time computational requirements. It is interesting to explore if the benefits of this ensemble approach can be replicated within a single network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Standard deep embedding approaches (left) train a network to map all images from a class to nearby locations in an output space. Our approach (right) learns an ensemble of mappings. Each model of the ensemble learns a mapping that groups small subsets of classes. Images are mapped by each model in the ensemble, and the output coordinates are concatenated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Left: Recall@1 accuracy on the CAR196 dataset with various D (different lines) and L (on the x-axis). Right: Recall@1 accuracy of the largest ensemble models with various D. Extreme cases have poorer performance because the individual models must deal with either many meta-classes per class (small D) or a lack of diversity (large D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Pair-wise dot product distribution of image feature vectors in the same classes (solid) and in different classes (dashes), for images from the training classes (red) and validation classes (blue). Shown are the distributions for 1, 4, 24 network ensembles, all with an output dimension of 24. As the number of networks grow, the distributions for same and different categories separate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Dot product for each testing image to the closest training image on CAR196 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Retrieval results on the CUB200, CAR196, In-Shop Clothes Retrieval and PKU VehicleID dataset. We retrieve the 4 most similar images to the query image. Correct results are highlighted green and incorrect results are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Retrieval and Clustering Performance on the CUB200 and CAR196 dataset.</figDesc><table>CUB200 
CAR196 
Method 
R@1 R@2 R@4 R@8 NMI R@1 R@2 R@4 R@8 NMI 
TRIPLET 
42.6 55.0 66.4 77.2 55.4 51.5 63.8 73.5 81.4 53.4 
N-PAIRS 
51.0 63.3 74.3 83.2 60.4 71.1 79.7 86.5 91.6 64.0 
PROXY 
49.2 61.9 67.9 72.4 59.5 73.2 82.4 86.4 88.7 64.9 
HDC 
53.6 65.7 77.0 85.6 − 73.7 83.2 89.5 93.8 − 
BIER 
55.3 67.2 76.9 85.1 − 78.0 85.8 91.1 95.1 − 
DREML (I,12,48) 78.5 86.2 91.3 94.4 79.3 89.8 94.0 96.6 97.9 79.1 
DREML (R,12,48) 80.5 87.4 91.9 94.7 79.4 86.0 91.7 95.0 97.2 76.4 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Retrieval Performance on the In-Shop Clothes dataset.</figDesc><table>Method 
R@1 R@10 R@20 R@30 
FashionNet 
53.0 73.0 76.0 77.0 
HDC 
62.1 84.9 89.0 91.2 
BIER 
76.9 92.8 95.2 96.2 
DREML (R,192,48) 78.4 93.7 95.8 96.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Retrieval Performance on the VID dataset.</figDesc><table>Data Size 
small 
medium 
large 
Method 
R@1 R@5 R@1 R@5 R@1 R@5 
GS-TRS 
75.0 83.0 74.1 82.6 73.2 81.9 
BIER 
82.6 90.6 79.3 88.3 76.0 86.4 
DREML (R,192,12) 88.5 94.8 87.2 94.2 83.1 92.4 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Incorporating intraclass variance to fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<idno>CoRR abs/1703.00196</idno>
		<ptr target="http://arxiv.org/abs/1703.00196" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep cnn ensemble with data augmentation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07224</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quadruplet-wise image similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bier -boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="doi">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4170" to="4178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1611.05720</idno>
		<ptr target="http://arxiv.org/abs/1611.05720" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q X W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
