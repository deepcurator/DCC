<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Representations for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Learning Semantic Representations for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>It is important to transfer the knowledge from label-rich source domain to unlabeled target domain due to the expensive cost of manual labeling efforts. Prior domain adaptation methods address this problem through aligning the global distribution statistics between source domain and target domain, but a drawback of prior methods is that they ignore the semantic information contained in samples, e.g., features of backpacks in target domain might be mapped near features of cars in source domain. In this paper, we present moving semantic transfer network, which learn semantic representations for unlabeled target samples by aligning labeled source centroid and pseudo-labeled target centroid. Features in same class but different domains are expected to be mapped nearby, resulting in an improved target classification accuracy. Moving average centroid alignment is cautiously designed to compensate the insufficient categorical information within each mini batch. Experiments testify that our model yields state of the art results on standard datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning approaches have gained prominence in various machine learning problems and applications. However, the recent success of deep learning depends on massive labeled data. Manual large scale labeled data on the target domain are too expensive or impossible to collect in practice. Therefore, there is a strong motivation to build an effective classification model using available labeled data from other domains. But, this learning paradigms suffers from the domain shift problem, which is an huge obstacle for adapting predictive models to the target domain <ref type="bibr" target="#b24">(Pan &amp; Yang, 2010)</ref>.</p><p>Learning a discriminative predictor in the presence of the shift between source domain and target domain is known as domain adaptation <ref type="bibr" target="#b24">(Pan &amp; Yang, 2010)</ref>. In recent years, deep learning has shown its potential to produce transferable features for domain adaptation. Fruitful line of works have been done in deep domain adaptation <ref type="bibr" target="#b22">(Motiian et al., 2017b;</ref><ref type="bibr" target="#b33">Tzeng et al., 2014;</ref><ref type="bibr" target="#b15">Long et al., 2015)</ref>. These methods aim at matching the marginal distributions across domains while <ref type="bibr" target="#b37">(Zhang et al., 2013;</ref><ref type="bibr" target="#b8">Gong et al., 2016)</ref> considers the conditional distribution shift problem. Recently adversarial adaptation methods <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b35">Tzeng et al., 2017;</ref><ref type="bibr" target="#b21">Motiian et al., 2017a;</ref><ref type="bibr" target="#b2">Bousmalis et al., 2016)</ref> have shown promising results in domain adaptation. Adversarial adaptation methods is analogous to generative adversarial networks (GAN) <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref>. A domain classifier is trained to tell whether the sample comes from source domain or target domain. The feature extractor is trained to minimize the classification loss and maximize the domain confusion loss. Domain-invariant yet discriminative features are seemingly obtainable through the principled lens of adversarial training.</p><p>Prior adversarial adaptation methods suffer a main limitation: as the discriminator only enforces the alignment of global domain statistics, crucial semantic information for each category might be lost. Even with perfect confusion alignment, there is no guarantee that samples from different domains but with the same class label will map nearby in the feature space, e.g, features of backpacks in the target domain may be mapped near features of cars in the source domain. This lack of semantic alignment is an important source of performance reduction <ref type="bibr" target="#b21">(Motiian et al., 2017a;</ref><ref type="bibr" target="#b10">Hoffman et al., 2017;</ref><ref type="bibr" target="#b19">Luo et al., 2017)</ref>. Recently, semantic transfer for supervised domain adaptation has received wide attention <ref type="bibr" target="#b21">(Motiian et al., 2017a;</ref><ref type="bibr" target="#b19">Luo et al., 2017)</ref>. To date, semantic alignment has not been addressed in unsupervised domain adaptation due to the lack of target label information.</p><p>In this paper, we propose a novel moving semantic transfer network (MSTN) for unsupervised domain adaptation, where our feature extractor learns to align the distributions semantically without any labeled target samples. We large-ly extend the ability of prior adversarial adaptation methods by our proposed semantic representation learning module. We firstly assign pseudo labels to target samples to fix the problem of lacking target label information. Since there are obviously false labels in pseudo labels, we wish to use correctly-pseudo-labeled samples to reduce the bias caused by falsely-pseudo-labeled samples. So we propose to align the centroid for each class in source and target domains instead of treating the pseudo-labeled samples as true directly. In particular, as we use mini batch SGD in practice, categorical information is usually insufficient and even one false label could lead to extremely biased estimation of the true centroid, moving average centroid is designed for safer semantic representation learning. Experiments have proven that MSTN yields state of the art results on standard datasets. Furthermore, we also find that MSTN stabilizes the adversarial learning for unsupervised domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, adversarial learning has been widely adopted in domain adaptation <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b34">Tzeng et al., 2015;</ref><ref type="bibr" target="#b10">Hoffman et al., 2017;</ref><ref type="bibr" target="#b21">Motiian et al., 2017a;</ref><ref type="bibr" target="#b35">Tzeng et al., 2017;</ref><ref type="bibr" target="#b28">Saito et al., 2017b;</ref><ref type="bibr" target="#b17">Long et al., 2017a;</ref><ref type="bibr" target="#b19">Luo et al., 2017;</ref><ref type="bibr" target="#b30">Sankaranarayanan et al.)</ref>. Most of adversarial adaptation methods are based on generative adversarial networks (GAN) <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref>. A discriminator is trained to tell whether the sampled feature comes from the source domain or target domain while the feature extractor is trained to fool the discriminator. However, prior unsupervised adversarial domain adaptation methods only enforce embedding alignment in domain-level instead of class-level transfer. Lacking the semantic alignment hurts the performance of domain adaptation significantly <ref type="bibr" target="#b21">(Motiian et al., 2017a;</ref>.</p><p>Semantic transfer is much easier in supervised domain adaptation as labeled target samples are available. In recent years, few-shot adversarial learning <ref type="bibr" target="#b34">(Tzeng et al., 2015;</ref><ref type="bibr" target="#b21">Motiian et al., 2017a;</ref><ref type="bibr" target="#b19">Luo et al., 2017)</ref> have been explored in domain adaptation. Few-shot domain adaptation considers the task where very few labeled target data are available in training. <ref type="bibr" target="#b34">(Tzeng et al., 2015)</ref> computes the average output probability with source training samples for each category, then for each labeled target sample, they optimize the model to match the distributions over classes to the average probability. FADA <ref type="bibr" target="#b21">(Motiian et al., 2017a)</ref> pairs the labeled target sample and labeled source sample and the discriminator is trained to tell whether the pair comes from same domain and same class. <ref type="bibr" target="#b19">(Luo et al., 2017)</ref> proposes cross category similarity for semantic transfer.</p><p>In this paper, we consider a more challenging task: unsupervised semantic transfer where there is no labeled target samples. <ref type="bibr" target="#b7">(Ghifary et al., 2016)</ref> proposes to add a decoder after the feature extractor to enforce the feature extractor preserving semantic information. <ref type="bibr" target="#b2">(Bousmalis et al., 2016)</ref> propose to decouple the representation into the shared representation and private representation. It encourages the shared and private representation to be orthogonal while both the representations should be able to be decoded back to images.  adapts representations at both the pixel-level and feature-level. It encourages the feature extractor to preserve semantic information by using the cycle consistency constraints. <ref type="bibr" target="#b28">(Saito et al., 2017b)</ref> uses the dropout to obtain two different views of input and if the prediction results are different, these target samples are regarded as near decision boundary. They use the boundary information to achieve low-density separation of aligned points. <ref type="bibr" target="#b29">(Saito et al., 2017c)</ref> proposes to use two classifiers as discriminators to detect target samples that are far from the support of the source. These two classifiers are trained adversarial to view input differently. <ref type="bibr" target="#b25">(Pinheiro, 2017)</ref> classifies the input samples by computing the distances between prototype representations of each category.</p><p>Previous unsupervised adaptation methods do not necessarily align distributions semantically across domains as they can not ensure features in same class but different domains are mapped nearby owing to the huge gap for semantic alignment: no labeled information for target samples. It means that explicit matching the distributions for each category is impossible. To fill this gap, we assign pseudo labels to target samples. Contrary to prior domain adaptation methods that assign pseudo labels <ref type="bibr" target="#b4">(Chen et al., 2011;</ref><ref type="bibr" target="#b27">Saito et al., 2017a)</ref>, we doubt the pseudo labels and propose to align the centroid to reduce the shift brought by false labels instead of direct matching distributions using pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we provide details of the proposed model for domain adaptation. In unsupervised domain adaptation, we are given by n s labeled samples (x</p><formula xml:id="formula_0">(i) S , y (i) S ) ns i=1 from the source domain D S , where x (i) S ∈ X S and y (i) S ∈ Y S .</formula><p>Additionally, we are also given with n t unlabeled target</p><formula xml:id="formula_1">samples (x (i) T ) nt i=1</formula><p>from the target domain D T , where</p><formula xml:id="formula_2">x (i)</formula><p>T ∈ X T . X S and X T are assumed to be different but related (referred as covariate shift in literature <ref type="bibr" target="#b31">(Shimodaira, 2000)</ref>). Target task is assumed to be same with source task. Our ultimate goal is to develop a deep neural network f : X T → Y T that is able to predict labels for the samples from target domain.  <ref type="figure">Figure 1</ref>. Besides the standard source classification loss, we also employ the domain adversarial loss to align distributions for two domains. In particular, to learn semantic representations, we maintain global centroids C k S and C k T for each class k in two domains at feature level, i.e., G(X). In each step, source centroids will be updated with the labeled features (G(Xs), Ys) while target centroids will be updated with pseudo-labeled features (G(Xt), F•G(Xt)). Our model learns to semantically align the embedding by explicitly restricting the distance between centroids in same class but different domains.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The model</head><p>For unsupervised domain adaptation, in the presence of covariate shift, a visual classifier f = F • G is trained by minimizing the source classification error and the discrepancy between source domain and target domain:</p><formula xml:id="formula_3">L = E (x,y)∼D S [J(f (x), y)] L C (X S ,Y S ) +λ d(X S , X T ) L DC (X S ,X T )</formula><p>( <ref type="formula">1)</ref> where J(., .) is typically the cross entropy loss, λ is the balance parameter, d(., .) represents the divergence between two domains. Typically maximum mean discrepancy (M-MD) <ref type="bibr" target="#b15">(Long et al., 2015;</ref><ref type="bibr" target="#b33">Tzeng et al., 2014)</ref> or domain adversarial similarity loss <ref type="bibr" target="#b2">(Bousmalis et al., 2016;</ref><ref type="bibr" target="#b6">Ganin &amp; Lempitsky, 2015)</ref> are used to measure the divergence. We opt to use the domain adversarial similarity loss in our model. In other words, we employ an additional domain classifier D to tell whether the features from feature extractor G arise from source or target domain while G is trained to fool D. This two-player game is expected to reach an equilibrium where features from G are domain-invariant. Formally,</p><formula xml:id="formula_4">d(X S , X T ) =E x∼D S [log(1 − D • G(x))] E x∼D T [log(D • G(x))] (2)</formula><p>However, domain-invariance does not mean discriminability. Features of target backpacks can be mapped near features of source cars while satisfying the condition of domain-invariant. Separately, it has been shown that supervised domain adaptation (SDA) method improves upon unsupervised domain adaptation (UDA) by making the alignment semantic since SDA can ensure features of same class in different domains are mapped nearby <ref type="bibr" target="#b22">(Motiian et al., 2017b)</ref>. Motivated by this key observation, we endeavor to learn semantic representations for UDA.</p><p>Before we go further, we will stop to see how SDA achieves semantic transfer. For SDA, one could easily align the embeddings semantically by adding following objective,</p><formula xml:id="formula_5">L SDA SM (X S , X T , Y S , Y T ) = K k=1 d(X k S , X k T ),<label>(3)</label></formula><p>where K is the number of classes. It means that one can match the distributions for each class directly in SDA.</p><p>Unfortunately, for UDA, we do not have label information from target domain. To circumvent the impossibility of distribution matching at class-level, we resort to pseudo labels <ref type="bibr" target="#b13">(Lee, 2013)</ref>. We firstly assign pseudo labels to target samples with the training classifier f and we obtain a pseudolabeled target domain. But obviously there must be some false labels and they may harm the performance of adaptation heavily. A natural question then arises as how to suppress the noisy signals conveyed in those false pseudolabeled samples?</p><p>We approach the question by centroid alignment. Centroid has long been favored for its simplicity and effectiveness to represent a set of samples <ref type="bibr" target="#b19">(Luo et al., 2017;</ref><ref type="bibr" target="#b32">Snell et al., 2017)</ref>. When computing the centroid for each class, pseudo-labeled ( correct or wrong ) samples are being used together and the detrimental influences brought by false pseudo labels are expected be neutralized by correct pseudo labels. Inspired by this, we propose following semantic transfer objective for unsupervised domain adaptation:</p><formula xml:id="formula_6">L U DA SM (X S , Y S , X T ) = K k=1 Φ(C k S , C k T ) L SM (X S ,Y S ,X T ) ,<label>(4)</label></formula><p>where C k S and C k T are centroid for each class in feature space, Φ(., .) is any appropriate distance measure function. We use the squared Euclidean distance Φ(x, x ) = ||x − x || 2 in our experiments. In total, we obtain 2K centroids. Through explicitly restricting the distance between centroids with same class label but different domains, we can ensure that features in the same class will be mapped nearby. More importantly, false signals in pseudo-labeled target domain are suppressed through centroid alignment.</p><p>More formally, our totally objective can be written as follows:</p><formula xml:id="formula_7">L(X S , Y S , X T ) =L C (X S , Y S ) + λL DC (X S , X T ) + γL SM (X S , Y S , X T ),<label>(5)</label></formula><p>where λ and γ are parameters that balance the classification loss, domain confusion loss and semantic loss. As we can see, our model is simple and the semantic transfer objective can be computed in linear time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Moving Semantic Transfer Network</head><p>Algorithm 1 Moving semantic transfer loss computation in iteration t in our model. K is the number of classes. Input: Labeled set S, Unlabeled set T, N is the batch size, Training classifier f , Global centroids for two domains:</p><formula xml:id="formula_8">C k S K k=1 and C k T K k=1 1: S t = RANDOMSAMPLE(S, N ) 2: T t = RANDOMSAMPLE(T, N ) 3: T t =Labeling(G,f ,T t ) 4: L SM = 0 5: for k = 1 to K do 6: C k S (t) ← 1 |S k t | (xi,yi)∈S k t G(x i ) (From Scratch) 7: C k T (t) ← 1 | T k t | (xi,yi)∈ T k t G(x i) (From Scratch) 8: C k S ← θC k S + (1 − θ)C k S (t) (Moving Average) 9: C k T ← θC k T + (1 − θ)C k T (t) (Moving Average) 10: L SM ← L SM + Φ(C k S , C k T ) 11: end for 12: return L SM</formula><p>The proposed model achieves semantic transfer in very simple form but it suffers two limitations in practice: (1) As we always uses mini batch SGD for optimization in practice, categorical information in each batch is usually insufficient. For instance, it is possible that some classes are missing in the current batch of target data since the batch is randomly selected. (2) If the batch size is small, even one false pseudo label will lead to the huge deviation between the pseudo-labeled centroid and true centroid. For example, when there is one pseudo-labeled car sample in a target batch but the true label is backpack. Then it will wrongly guide the alignment between source car features and target backpack features.</p><p>Instead of aligning those newly obtained centroids in each iteration directly, we propose to align exponential moving average centroids to address the two aforementioned problems. As shown in algorithm 1, we maintain global centroids for each class. In each iteration, source centroids are updated by the labeled source samples while target centroids are updated by pseudo-labeled target samples. Then we can align those moving average centroids following equation (4).</p><p>Moving average centroid alignment works in an intuitive way: When backpack are missing in current source batch, we can align the target backpack centroid with the global source backpack centroid updated in last iteration. Under the reasonable assumption that centroids change by a limited step in each iteration, we can still ensure features of backpacks in two domains are mapped nearby. Meanwhile, when there is one pseudo-labeled car sample in a target mini batch but the true label is backpack, moving average centroids can avoid the aforementioned misalignment as it also considers the pseudo-labeled backpacks in the past mini batches.</p><p>Our method attempts to align the centroids in same class but different domains to achieve semantic transfer for unsupervised domain adaptation. We use pseudo labels from F to guide the semantic alignment for G. As the learning proceeds, G will learn semantic representations for target samples, resulting in an improved accuracy of F. This cycle will gradually enhance the accuracy for target domain. In addition, we suppress the noisy semantic information by assigning a small weight to γ in early training phase .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis</head><p>In this section, we show the relationship between our method and the theory of domain adaptation <ref type="bibr" target="#b1">(Ben-David et al., 2010)</ref>. The theory bounds the expected error on the target samples ε T (h) by three terms as follows. Theorem 1. (Ben-David et al., 2010) Let H be the hypothesis class. Given two domains S and T , we have</p><formula xml:id="formula_9">∀h ∈ H, εT (h) ≤ εS (h) + 1 2 dH∆H(S, T ) + C,<label>(6)</label></formula><p>where ε S (h) is the expected error on the source samples which can be minimized easily with source label information, d H∆H (S, T ) defines a discrepancy distance between two distributions S and T w.r.t. a hypothesis set H. C is the shared expected loss and is expected to be negligibly small, thus usually disregarded by previous methods <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b15">Long et al., 2015)</ref>. But it is very important and we cannot expect to learn a good target classifier by minimizing the source error if C is large <ref type="bibr" target="#b1">(Ben-David et al., 2010)</ref>.</p><p>It is defined as C = min</p><formula xml:id="formula_10">h∈H ε S (h, f S ) + ε T (h, f T )</formula><p>where f S and f T are labeling functions for source and target domain respectively. We show that our method is trying to optimize the upper bound for C. Recall the triangle inequality for classification error <ref type="bibr" target="#b1">(Ben-David et al., 2010;</ref><ref type="bibr" target="#b5">Crammer et al., 2008)</ref>, which implies that for any labeling functions f 1 , f 2 and f 3 , we have ε(</p><formula xml:id="formula_11">f 1 , f 2 ) ≤ ε(f 1 , f 3 ) + ε(f 2 , f 3 ). Then C = min h∈H εS (h, fS ) + εT (h, fT ) ≤ min h∈H εS (h, fS ) + εT (h, fS ) + εT (fS , fT ) ≤ min h∈H εS (h, fS ) + εT (h, fS ) + εT (fS , f T ) + εT (fT , f T )<label>(7)</label></formula><p>The first and second term denotes the disagreement between h and the source labeling function f S . These two terms should be small as we can easily find such a h in our hypothesis space to approximate the f S since we have source labels. Therefore, we seek to minimize the last two terms. Obviously the last term denotes the false pseudo rate in our method which would be minimized as learning proceeds. Now our focus should be the third term ε T (f S , f T ). This term denotes the disagreement between the source labeling function and pseudo target labeling function on target samples.</p><formula xml:id="formula_12">ε T (f S , f T ) = E x∼T [l(f S (x), f T (x))],</formula><p>where l(., .) is typically the 0-1 loss function.</p><p>Our method aligns the centroid for class k in source domain S k and pseudo-labeled target domain T k . We can decompose the hypothesis h into the feature extractor G and classifier F. Then we have</p><formula xml:id="formula_13">E x∼S k G(x) = E x∼ T k G(x). For ε T (f S , f T ), it could be rewritten as E x∼T [l(F S • G(x), F T • G(x))]<label>(8)</label></formula><p>Now the relationship is clear: for source samples in class k, the source labeling function should return k. We wish to have target features in class k to be similar with source features in class k, so the source labeling function would also predict those target samples as k, which is consistent with the prediction results made by pseudo target labeling function. Consequently, ε T (f S , f T ) is expected to be small.</p><p>In summary, the premise for the success of domain adaptation methods is that the shared expected loss C should be small. Our method attempts to minimize this item through aligning the centroid between source domain and pseudolabeled target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Setup</head><p>We evaluate the semantic transfer network with state of art transfer learning methods. Codes are available at https://github.com/Mid-Push/ Moving-Semantic-Transfer-Network.</p><p>Office-31 <ref type="bibr" target="#b26">(Saenko et al., 2010</ref>) is a standard dataset used for domain adaptation. It contains three distinct domains: Amazon (A) with 2817 images, Webcam (W) with 795 images and DSLR (D) with 498 images. Each domain contains 31 categories. We examine our methods by employing the frequently used network structures: AlexNet <ref type="bibr" target="#b11">(Krizhevsky et al., 2012)</ref>. For fair comparison, we report results of methods that are also based on AlexNet.</p><p>ImageCLEF-DA is a benchmark dataset for ImageCLE-F 2014 domain adaptation challenges. Three domains including Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P) share 12 categories. Each domain contains 600 images and 50 images for each category. Images in ImageCLEF-DA are of equal size. This dataset has been used by JAN <ref type="bibr" target="#b18">(Long et al., 2017b)</ref>. Same, we also examine our method in AlexNet <ref type="bibr" target="#b11">(Krizhevsky et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST-USPS-SVHN.</head><p>We explore three digits datasets of varying difficulty: MNIST (LeCun et al., 1998), USPS and SVNH <ref type="bibr" target="#b23">(Netzer et al., 2011)</ref>. Different from Office-31, M-NIST consists grey digits images of size 28x28, USPS contains 16x16 grey digits and SVHN composes color 32x32 digits images which might contain more than one digit in each image. MNIST-USPS-SVHN makes a good complement to previous datasets for diverse domain adaptation scenarios. We conduct experiments in a resolution-goingdown way, SVHN→ MNIST and MNIST →USPS.</p><p>Baseline Methods For Office-31 and ImageCLEF-DA datasets, we compare with state-of-art transfer learning methods: Deep Domain Confusion (DDC) <ref type="bibr" target="#b33">(Tzeng et al., 2014)</ref>, Deep Reconstruction Classification Network (DRCN) <ref type="bibr" target="#b7">(Ghifary et al., 2016)</ref>, Gradient Reversal (RevGrad) <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b16">(Long et al., 2016)</ref>, Joint Adaptation Network (JAN) <ref type="bibr" target="#b18">(Long et al., 2017b)</ref>, Automatic Domain Alignment Layer (AutoDIAL) <ref type="bibr" target="#b3">(Carlucci et al., 2017)</ref>. We cite the results of AlexNet, DDC, RevGrad, RTN, JAN from <ref type="bibr" target="#b18">(Long et al., 2017b)</ref>. For DRCN and AutoDIAL, we cite the results in their papers. For ImageCLEF-DA, we compare with AlexNet, RTN, RevGrad and JAN. Results are cited from <ref type="bibr" target="#b17">(Long et al., 2017a)</ref>. To further validate our method, we also conduct experiments on MNIST-USPS-SVHN. We compare with Domain of Confusion (DOC) <ref type="bibr" target="#b33">(Tzeng et al., 2014)</ref>, RevGrad <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>, Asymmetric Tri-Training (AsmTri) <ref type="bibr" target="#b27">(Saito et al., 2017a)</ref>, Couple GAN (CoGAN) <ref type="bibr" target="#b14">(Liu &amp; Tuzel, 2016)</ref>, Label Efficient Learning (LEL) <ref type="bibr" target="#b19">(Luo et al., 2017)</ref> and Adversarial Discriminative Domain Adaptation (ADDA) . Results of source only, DOC, RevGrad, CoGAN and ADDA are cited from . For the rest, we cite the result in their papers respectively.</p><p>We follow standard evaluation protocols for unsupervised domain adaptation as <ref type="bibr" target="#b15">(Long et al., 2015;</ref><ref type="bibr" target="#b6">Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b18">Long et al., 2017b)</ref>. We use all labeled source examples and all unlabeled target examples. We repeat each transfer task three times and report the mean accuracy as well as the standard error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Detail</head><p>CNN architecture. In our experiments on Office and ImageCLEF-DA, we employed the AlexNet architecture. Following RTN <ref type="bibr" target="#b16">(Long et al., 2016)</ref> and RevGrad <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>, a bottleneck layer f cb with 256 units is added after the f c7 layer for safer transfer representation learning. We use f cb as inputs to the discriminator  <ref type="bibr" target="#b11">(Krizhevsky et al., 2012)</ref> 61.6±0.  <ref type="bibr" target="#b11">(Krizhevsky et al., 2012)</ref> 66 as well as the centroid computation. Image random flipping and cropping are adopted following JAN <ref type="bibr" target="#b18">(Long et al., 2017b)</ref>. For a fair comparison with other methods, we also finetune the conv1, conv2, conv3, conv4, conv5, f c6, f c7 layers with pretrained AlexNet. For discriminator, we use same architecture with RevGrad, x→1024→1024→1, dropout is used.</p><formula xml:id="formula_14">Method A → W D → W W → D A → D D → A W → A Avg AlexNet</formula><formula xml:id="formula_15">Method I → P P → I I → C C → I C → P P → C Avg AlexNet</formula><p>For digit classification datasets, we use same architecture with ADDA : two convolution layers followed by max pool layers and two fully connected layers are placed behind. Digit images are also cast to 28x28x1 in all experiments for fair comparison. For discriminator, we also use same architecture with ADDA, x→500→500→1. Batch Normalization is inserted in convolutional layers.</p><p>Hyper-parameters tuning. A good unsupervised domain adaptation method should provide ways to tune hyperparameters in an unsupervised way. Therefore, no labeled target samples are referred for tuning hyper-paramters. We essentially tune the three hyper-parameters: weight balance parameter λ, γ and moving average coefficient θ. For θ, we first apply reverse validation <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref> on the experiments MNIST→USPS. Then we use the optimal value for θ in all experiments. We set θ=0.7 in all our experiments. For the weight balance parameter, we set λ = 2 1+exp(−γ.p) − 1, where γ is set to 10 and p is training progress changing from 0 to 1. It is optimized by <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref> to suppress noisy signal from the discriminator at the early stages of training. Considering that our pseudo-labeled semantic loss would be inaccurate in early training phase, we also set γ = λ to suppress the noisy information brought by false labels. Stochastic gradient descent with 0.9 momentum is used. The learning rate is annealed by µ p = µ0 (1+α.p) β , where µ 0 =0.01, α=10 and β=0.75 <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>. We set the learning rate for finetuned layers to be 0.1 times of that from scratch. We set the batch size to 128 for each domain. Domain adversarial loss is scaled by 0.1 following <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>We now discuss the experiment settings and results.</p><p>Office-31 We follow the fully transductive evaluation protocol in <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>. Results of office-31 are shown in <ref type="table" target="#tab_1">Table 1</ref>. The proposed model outperforms all comparison methods on all transfer tasks. It is noteworthy that MSTN results in improved accuracies on four hard transfer task: A→W, A→D, D→A and W→A. On these four difficult tasks, our method promote classification accuracies substantially. The encouraging improvement on hard transfer tasks proves the importance of semantic alignment and suggests that our method is able to learn semantic representations effectively despite of its simplicity.</p><p>The results reveal several interesting observations. (1) Deep transfer learning methods outperform standard deep learning methods. It validates that the idea that domain shift in two distributions can not be removed by deep networks <ref type="bibr" target="#b36">(Yosinski et al., 2014)</ref>. (2) DRCN <ref type="bibr" target="#b7">(Ghifary et al., 2016)</ref> trains an extra decoder to enforce the extracted features contain semantic information and thus outperformed standard deep learning methods by about 5%. This improvement also indicates the importance to learn seman- <ref type="figure">Figure 2</ref>. SVHN→MNIST and D→A. We confirmed the effects our method through a visualization of the learned representations using tdistributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b20">(Maaten &amp; Hinton, 2008)</ref>. Blue points are source samples and red are target samples. (a) are trained without any adaptation. (b)(d) are trained with previous adversarial domain adaptation methods. (c)(e) Adaptation using our proposed method. As we can see, compared to non-adapted method, adversarial adaptation methods successfully fuse the source features and target features. But semantic information are ignored and ambiguous features are generated near class boundary, which is catastrophic for classification task. Our model attempts to fuse features in the same class while separate features in different classes. tic representations. (3) Separately, distribution matching methods RevGrad, RTN and JAN, also bring significant improvement over source only. Our method combines the advantages of DRCN and distribution matching methods in a very simple form. In particular, in contrast to using a decoder to extract semantic information, our method also ensures that the features in same classes but different domains are similar, which has not been addressed by any existing methods. For completeness, we also conduct a visualization over transfer task D→ A for comparison between our learned representation and prior adversarial adaptation method RevGrad <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>. <ref type="figure">See Fig (2d)</ref> and (2e). Representations learned by our model are better behaved compared to RevGrad and representations in different classes are dispersed instead of mixing up.</p><formula xml:id="formula_16">SVHN→MNIST (a) Non-adapted SVHN→MNIST (b) Adversarial Adapted SVHN→MNIST (c) Semantic Adapted D→A (d) Adversarial Adapted D→A (e) Semantic Adapted</formula><p>To dive deeper into our method, we present the results of one variants of MSTN: MSTN with centroid from scratch. We try to align the centroids directly computed in each iteration instead of using moving average. The results are interesting, for the simple transfer task D→W, W→D, this variant are comparable or outperforms the moving average. This phenomenon is plausible since the prediction accuracy for target domains is already very high and introducing the past semantic information might introduce noisy information too. But take a look at the hard transfer task D→A and A→D, the improvement carried by the moving average centroid is obvious. This curious result provides us two training instructions: (1) for easy transfer tasks or large batch size, one could just align the centroids directly to learn semantic representation in each iteration. (2) for hard transfer tasks or small batch size, one could effectively pass the semantic information by aligning the moving average centroids. Note that our method does not introduce any extra network architecture but only few memory that are used to keep these global centroids.</p><p>ImageCLEF-DA For ImageCLEF-DA, results are shown in <ref type="table">Table 2</ref>. Images are balanced in ImageCLEF-DA, so  <ref type="bibr" target="#b33">(Tzeng et al., 2014)</ref> 68.1±0.3 79.1±0.5 RevGrad <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref> 73.9 77.1±1.8</p><p>AsmTri <ref type="bibr" target="#b27">(Saito et al., 2017a)</ref> 86.0 -coGAN <ref type="bibr" target="#b14">(Liu &amp; Tuzel, 2016)</ref> -91.2±0.8 ADDA  76.0±1.8 89.4±0.2 LEL <ref type="bibr" target="#b19">(Luo et al., 2017)</ref> 81.0±0.3 -MSTN (ours) 91.7±1.5 92.9±1.1 our model could be more focused on transfer learning by avoiding the class imbalance problem. But the domain size is limited to 600, which might not be sufficient for training the network. Our model outperforms existing methods in most transfer tasks, but with less improvement compared to Office-31. This result also validates hypothesis in <ref type="bibr" target="#b18">(Long et al., 2017b</ref>) that the domain size may cause shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST-USPS-SVHN</head><p>We follow the protocols in : For adaptation between SVHN and MNIST, we use the training set of SVHN and test set of MNIST for evaluation. For adaptation between MNIST and USPS, we randomly sample 2000 images from MNIST and 1800 from USPS. For SVHN→MNIST, the transfer gap is huge since images in SVHN might contain multiple digits. Thus, to avoid ending up in a local minimum, we do not use the learning rate annealing as suggested by <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>.</p><p>Results of MNIST-USPS-SVHN are shown in <ref type="table" target="#tab_4">Table 3</ref>. It shows that our model outperforms all comparison methods. For MNIST → USPS, our method obtains a desirable performance. On the difficult transfer task SVHN → M-NIST, Our model outperforms existing methods by about 6.6%. In <ref type="figure">Fig. 2</ref>, the representations in SVHN→MNIST are visualized. <ref type="figure">Fig (2a)</ref> shows the representations without any adapt. As we can see, the distributions are separated between domains. This highlights the importance for transfer learning. <ref type="figure">Fig (2b)</ref> shows the result for RevGrad <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>, a typical adversarial domain adaptation method. Features are successfully fused but it also exhibits a serious problem: features generated are near class boundary. Features of digit 1 in target domain could be easily mapped to the intermediate space between class 1 and class 2, which is obviously a damage to classification tasks. In contrast, <ref type="figure">Fig (2c)</ref> shows the representations that learned by our method. Features in the same class are mapped closer. In particular, features with different classes are dispersed, making the features more discriminative. The well-behaved learned features suggests that our model successfully pass the semantic information to the feature generator and our model is capable to learn semantic representations without any label information for target domain.</p><p>A-distance. Based on the theory in <ref type="bibr" target="#b1">(Ben-David et al., 2010)</ref>, A-distance is usually used to measure domain discrepancy. The empirical A-distance is simple to compute: d A = 2(1 − 2 ), where is the generalization error of a classifier trained with the binary classification task of discriminating the source and target. Results are shown in <ref type="figure" target="#fig_1">Fig  (3e)</ref>. We compared our method with domain adaptation methods RevGrad <ref type="bibr" target="#b6">(Ganin &amp; Lempitsky, 2015)</ref>. We use a kernel SVM as the classifier. We compare our model to the standard CNN and RevGrad. From this graph, we can see that with the adversarial adaptation module embedded, our model reduces the A distances compared to CNN. But when compared to RevGrad, the results are close. This finding tells us that our semantic representation module is not focusing on reducing the global distribution discrepancy. The superior performance lead by our method shows that only reducing the global distribution discrepancy for domain adaptation is far from enough.</p><p>Convergence As our model involves the adversarial adaptation module, we testify their performance on convergence from two different aspects. The first is the testing accuracy as shown in <ref type="figure" target="#fig_1">Fig (3b)(3d)</ref>. Our model has similar convergence speed as RevGrad.</p><p>Since the adversarial module in our model and RevGrad works analogous to GAN <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref>, we will check our model from GAN's perspective. We adopt the min-max game in GAN. It has been proved that when the discriminator is optimal, the generator involved in the min-max game in a GAN is reducing the Jenson-Shannon Divergence (JSD). For the discriminator in adversarial adaptation, it is trained to maximize</p><formula xml:id="formula_17">L D = E x∼D S [log1 − D(x)] + E x∼D T [logD(x)]</formula><p>, which is a lower bound of 2JS(D S , D T )-2log2. Therefore, following <ref type="bibr" target="#b0">(Arjovsky &amp; Bottou, 2017)</ref>, we plot the quantity of 1 2 L D + log2, which is the lower bound of the JS distance. Results are shown in <ref type="figure" target="#fig_1">Fig (3a)(3c)</ref>. We can make following observations: (1) different from the vanishing generator gradient problem in traditional GANs, the manifolds where features generated by adversarial adaptation methods lies seems to be perfectly aligned. So the gradients for the feature extractor will not vanish but towards reducing the JS distance. This justifies the feasibility for adversarial domain adaptation methods.</p><p>(2) Compared to RevGrad, our model is more stable and accelerate the minimization process for JSD. It indicates that our method stabilize the notorious unstable adversarial training through semantic alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel method which aims at learning semantic representations for unsupervised domain adaptation. Unlike previous domain adaptation methods that solely match distribution at domain-level, we proposes to match distribution at class-level and align features semantically without any target labels. We use centroid alignment to guide the feature extractor to preserve class information for target samples in aligning domains and moving average centroid is cautiously designed to tackle the problem where a mini-batch may be insufficient for covering all class distribution in each training step. Experiments on three different domain adaptation scenarios testify the efficacy of our proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Standard CNN in grey, Revgrad (Ganin &amp; Lempitsky, 2015) in green, our model MSTN in red. (a)(c): Comparison of JensenShannon divegence (JSD) estimate during training for RevGrad and our proposed method MSTN. Our model stabilizes and accelerates the adversarial learning process. (b)(d): Comparison of testing accuracies of different models. (e): Comparison of A-distance of different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracies (%) on office-31 datasets.(AlexNet)</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracies (%) on digit recognitions tasks</figDesc><table>Source 
SVHN 
MNIST 
Target 
MNIST USPS 
Source Only 
60.1±1.1 75.2±1.6 
DOC </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work described in this paper was supported by the National Key R&amp;D Program of China2018YFB1003800), the Guangdong Province Universities and Colleges Pearl River Scholar Funded Scheme 2016, the National Natural Science Foundation of China (No.61722214), and and the Program for Guangdong Introducing Innovative and Enterpreneurial Teams(No.2016ZT06D211). We thank Peifeng Wang, Weili Chen, Fanghua Ye, Honglin Zheng and Jie Hu for discussions over the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1757" to="1774" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional transferable components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2839" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cycada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycleconsistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10667</idno>
		<title level="m">Domain adaptation with randomized multilinear adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia, 6-11 Au</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Label efficient learning of transferable representations acrosss domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="164" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using tsne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Few-shot adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02536</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08995</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adapting visual category models to new domains. Computer Vision-ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Asymmetric tritraining for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>arX- iv:1711.01575</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generate to adapt: Unsupervised domain adaptation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D C R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>arX- iv:1703.05175</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05464</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="827" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
