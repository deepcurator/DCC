<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Adversarial Perturbations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isay</forename><surname>Katsman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Adversarial Perturbations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In spite of their impressive performance on challenging tasks in computer vision such as image classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b19">20]</ref>and semantic segmentation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, deep neural networks are shown to be highly vulnerable to adversarial examples, i.e. carefully crafted samples looking similar to natural images but designed to mislead a pretrained model. This phenomenon was first studied in <ref type="bibr" target="#b52">[53]</ref>, and may hinder the applications of deep networks on visual tasks, or pose security concerns.</p><p>Two types of adversarial perturbations can be considered: Universal and Image-dependent. Image-dependent perturbations can vary for different images in the dataset. To generate these perturbations, we require a function which takes a natural image, and outputs an adversarial image. We approximate this function with a deep neural network. Universal perturbations are fixed perturbations which when added to natural images can significantly degrade the accuracy of the pre-trianed network. In this case, we seek a perturbation U with small magnitude such that for most natural images x, x + U can fool the pre-trained model. Unlike the iterative approaches proposed in the literature, we consider trainable networks for learning the universal perturbation.</p><p>From another viewpoint, adversarial attacks can be categorized as targeted and non-targeted. In targeted adversarial attacks, we seek adversarial images that can change the prediction of a model to a specific target label. In nontargeted attacks we want to generate adversarial examples for which the model's prediction is any label other than the ground-truth label. Considering all the possible combinations, we can have four types of adversarial examples: targeted universal, non-targeted universal, targeted imagedependent and non-targeted image-dependent. We elaborate on each of them in the following sections.</p><p>Our main contributions can be summarized as follows:</p><p>• We present a unifying framework for creating universal and image-dependent perturbations for both classification and semantic segmentation tasks, considering targeted and non-targeted attacks with L ∞ and L 2 norms as the metric.</p><p>• We improve the state-of-the-art performance in universal perturbations by leveraging generative models in lieu of current iterative methods.</p><p>• We are the first to present effective targeted universal perturbations. This is the most challenging task as we are constrained to have a single perturbation pattern and the prediction should match a specific target.</p><p>• Our attacks are considerably faster than iterative and optimization-based methods at inference time. We can generate perturbations in the order of milliseconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Universal Perturbations</head><p>First introduced in <ref type="bibr" target="#b34">[35]</ref>, universal perturbations are fixed perturbations which after being added to natural images can mislead a pre-trained model for most of the images. The algorithm in <ref type="bibr" target="#b34">[35]</ref> iterates over samples in a target set, and gradually builds the universal perturbation by aggregating image-dependent perturbations and normalizing the result.</p><p>[38] presents a data independent approach for generating image-agnostic perturbations. Its objective is to maximize the product of mean activations at multiple layers of the network when the input is the universal perturbation. While this method obviates the need for training data, the results are not as strong as <ref type="bibr" target="#b34">[35]</ref>. A method for generating targeted universal adversarial perturbations for semantic segmentation models is presented in <ref type="bibr" target="#b33">[34]</ref>. Their approach is similar to <ref type="bibr" target="#b34">[35]</ref> in that they also create the universal perturbation by adding image-dependent perturbations and clipping the result to limit the norm. <ref type="bibr" target="#b35">[36]</ref> proposes a quantitative analysis of the robustness of classifiers to universal perturbations based on the geometric properties of decision boundaries. A defense method against universal adversarial perturbations is proposed in <ref type="bibr" target="#b0">[1]</ref>. It learns a Perturbation Rectifying Network (PRN) from real and synthetic universal perturbations, without needing to modify the target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image-dependent Perturbations</head><p>Various approaches have been proposed for creating image-dependent perturbations. Optimization-based methods such as <ref type="bibr" target="#b52">[53]</ref> and <ref type="bibr" target="#b7">[8]</ref> define a cost function based on the perturbation norm and the model's loss. Then they use gradient ascent in pixel space with optimizers such as L-BFGS or Adam <ref type="bibr" target="#b23">[24]</ref> to create the perturbation. While these approaches yield better results than other methods, they are slow at inference time as they need to forward the input to the model several times.</p><p>[18] proposes a Fast Gradient Sign Method (FGSM) to generate adversarial examples. It computes the gradient of the loss function with respect to pixels, and moves a single step based on the sign of the gradient. While this method is fast, using only a single direction based on the linear approximation of the loss function often leads to sub-optimal results. Based on this work, <ref type="bibr" target="#b36">[37]</ref> presents an iterative algorithm to compute the adversarial perturbation by assuming that the loss function can be linearized around the current data point at each iteration. <ref type="bibr" target="#b25">[26]</ref> introduces the Iterative Least-Likely Class method, an iterative gradient-based method choosing the least-likely prediction as the desired class. This method is applied to ImageNet in <ref type="bibr" target="#b26">[27]</ref>. It also discusses how to effectively include adversarial examples in training to increase model's robustness. <ref type="bibr" target="#b10">[11]</ref> proposes a method for directly optimizing performance measures, even when they are combinatorial and non-decomposable. <ref type="bibr" target="#b38">[39]</ref> generates images unrecognizable to humans but classified with high confidence as members of a recognizable class. It uses evolutionary algorithms and gradient ascent to fool deep neural networks. Our work bears a resemblance to <ref type="bibr" target="#b5">[6]</ref> in that it also considers training a network for generating adversarial examples. However, <ref type="bibr" target="#b5">[6]</ref> does not provide a fixed bound on the perturbation magnitude, which might make perturbations detectable at inference time. It is also limited to targeted image-dependent perturbations. <ref type="bibr" target="#b57">[58]</ref> extends adversarial examples from the task of image classification to semantic segmentation and object detection. For each image, it applies gradient ascent in an iterative procedure until the number of correctly predicted targets becomes zero or a maximum iteration is reached. Similar to <ref type="bibr" target="#b52">[53]</ref> and <ref type="bibr" target="#b7">[8]</ref>, this method suffers from being slow at inference time. <ref type="bibr" target="#b1">[2]</ref> evaluates the robustness of segmentation models against common attacks. <ref type="bibr" target="#b30">[31]</ref> suggests that adversarial examples are sensitive to the angle and distance at which the perturbed picture is viewed. <ref type="bibr" target="#b3">[4]</ref> presents a method for generating adversarial examples that are robust across various transformations.</p><p>Several methods have been proposed for defending against adversarial attacks. While our focus is on efficient attacks, we refer the reader to <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56]</ref> for recent works on defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generative Adversarial Perturbations</head><p>Consider a classification network K trained on natural images from C different classes. It assigns a label K(x) ∈ {1, . . . , C} to each input image x 1 . We assume that images are normalized to</p><formula xml:id="formula_0">[0, 1] range. Let N ⊂ [0, 1]</formula><p>n represent the space of natural images <ref type="bibr" target="#b1">2</ref> . We assume that K achieves a high accuracy on natural images. Therefore, if we denote the correct class for image x by c x , K(x) = c x for most x ∈ N. Let A K stand for the space of adversarial examples for the network K. Images in A K must resemble a natural image yet be able to fool the network K. Hence, for each a ∈ A K there exists x ∈ N such that d(a, x) is small and K(a) = c x , where d(·, ·) is a distance metric.</p><p>This framework can be easily extended to the task of semantic segmentation in which the correct class for each pixel needs to be determined. In this case, the segmentation network K assigns a label map</p><formula xml:id="formula_1">K(x) = (K(x 1 ), . . . , K(x n )) ∈ {1, . . . , C}</formula><p>n to each image x = (x 1 , . . . , x n ). The ground-truth prediction for image x is c x = (c x1 , . . . , c xn ), and the set of adversarial examples is</p><formula xml:id="formula_2">A K = {a ∈ [0, 1] n \N | ∃ x ∈ N : d(a, x) &lt; ǫ , ∀ i ∈ {1, . . . , n} : K(a i ) = c xi },</formula><p>where ǫ is a fixed threshold 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Universal Perturbations</head><p>Universal Perturbations were first proposed in the seminal work of Dezfooli et al. <ref type="bibr" target="#b34">[35]</ref>. The paper proposes an iterative algorithm to generate the universal perturbation. It constructs the universal perturbation by adding imagedependent perturbations obtained from <ref type="bibr" target="#b36">[37]</ref> and scaling the result. Unlike the iterative approach of <ref type="bibr" target="#b34">[35]</ref>, we seek an end-to-end trainable model for generating the universal perturbation. Let us denote the set of universal perturbations <ref type="figure">Figure 1</ref>: Training architecture for generating universal adversarial perturbations. A fixed pattern, sampled from a uniform distribution, is passed through the generator. The scaled result is the universal perturbation which, when added to natural images, can mislead the pre-trained model. We consider both U-Net (illustrated here) and ResNet Generator architectures.</p><formula xml:id="formula_3">for the network K by U K = {U ∈ [0, 1] n | for most x ∈ N : x + U ∈ A K }.</formula><p>We do not want the perturbation to directly depend on any input image from the dataset. We seek a function f : <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref> n → U K which can transform a random pattern to the universal perturbation. By changing the input pattern, we can obtain a diverse set of universal perturbations. In practice, we approximate f (·) with a deep neural network f Θ (·) with weights Θ. This setting resembles Generative Adversarial Networks (GANs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21]</ref> in which a random vector is sampled from a latent space, and is transformed to a natural-looking image by a generator. In our case the range of the mapping is U K instead of N, and the generator is trained with a fooling loss instead of the discriminative loss used in GANs. We also tried using a combination of fooling and discriminative losses; however, it led to sub-optimal results.</p><p>There are several options for the architecture of the image transformation network f Θ (·). We consider two architectures used in recent image-to-image translation networks such as <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b60">[61]</ref>. The U-Net architecture <ref type="bibr" target="#b45">[46]</ref> is an encoder-decoder network with skip connections between the encoder and the decoder. The other architecture is ResNet Generator which was introduced in <ref type="bibr" target="#b22">[23]</ref>, and is also used in <ref type="bibr" target="#b60">[61]</ref> for transforming images from one domain to another. It consists of several downsampling layers, residual blocks and upsampling layers. In most of our experiments, the ResNet Generator outperforms U-Net. <ref type="figure">Figure 1</ref> illustrates the architecture for generating uni-</p><formula xml:id="formula_4">versal perturbations. A fixed pattern Z ∈ [0, 1] n , sampled from a uniform distribution U [0, 1]</formula><p>n , is fed to a generator f Θ to create the perturbation. The output of the generator f Θ (Z) is then scaled to have a fixed norm. More specifically, we multiply it by min 1,</p><formula xml:id="formula_5">ǫ fΘ(Z) p</formula><p>in which ǫ is the maximum permissible L p norm. Similar to related works in the literature, we consider p = 2 and p = ∞ in experiments. The resulting universal perturbation U is added to natural images to create the perturbed ones. Before feeding the perturbed image to the generator, we clip it to keep it in the valid range of images on which the network is trained. We feed the clipped imagex to the network K to obtain the output probabilities k(x) <ref type="bibr" target="#b3">4</ref> . Let 1 cx denote the one-hot encoding of the ground-truth for image x. In semantic segmentation, c x ∈ {1, . . . , C} n is the ground-truth label map, and k(x) contains the class probabilities for each pixel inx. For non-targeted attacks we want the prediction k(x) to be different from 1 cx , so we define the loss to be a decreasing function of the cross-entropy H(k(x), 1 cx ). We found that the following fooling loss gives good results in experiments:</p><formula xml:id="formula_6">l non−targeted = l f ool = − log(H(k(x), 1 cx ))<label>(1)</label></formula><p>Alternatively, as proposed by <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref>, we can consider the least likely class k ll (x) = arg min k(x), and set it as the target for training the model:</p><formula xml:id="formula_7">l non−targeted = l f ool = log(H(k(x), 1 k ll (x) )) (2)</formula><p>In practice, the losses in equations 1 and 2 lead to competitive results. We also found that for the Inception model, the logit-based loss used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> yields optimal results. For targeted perturbations we consider the cross-entropy with the one-hot encoding of the target:</p><formula xml:id="formula_8">l targeted = l f ool = log(H(k(x), 1 t ))<label>(3)</label></formula><p>where t represents the target. Note that for the classification task, t ∈ {1, . . . , C} is the target class while in semantic segmentation, t ∈ {1, . . . , C} n is the target label map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image-dependent Perturbations</head><p>We consider the task of perturbing images as a transformation from the domain of natural images to the domain of adversarial images. In other words, we require a mapping f : N → A K which generates a perturbed image f (x) ∈ A K for each natural image x ∈ N. A desirable function f (·) must result in a low accuracy and a high fooling ratio. Accuracy denotes the proportion of samples x <ref type="figure">Figure 2</ref>: Architecture for generating image-dependent perturbations. The generator outputs a perturbation, which is scaled to satisfy a norm constraint. It is then added to the original image, and clipped to produce the perturbed image. We use the ResNet Generator architecture for most of the image-dependent tasks. for which K(f (x)) = c x , while fooling ratio represents the ratio of images x for which K(f (x)) = K(x). Since we assume that the model achieves a high accuracy on natural images, these two metrics are highly correlated.</p><p>We consider two slightly different approaches for approximating f (·). The first approach is to parametrize it directly using a neural network f Θ (·). Hence, we seek Θ such that for most x ∈ N: K(f Θ (x)) = K(x). We also require that the perturbed image f Θ (x) look similar to the original image x. Hence, d(x, f Θ (x)) needs to be small for most x ∈ N, where d(·, ·) is a proper distance function. The second approach is to approximate the difference of natural and adversarial images with a neural network f Θ (·). We require that for most x ∈ N : K(x + f Θ (x)) = K(x) ≈ c x , and the L p norm of the additive perturbation f Θ (x) p needs to be small in order for it to be quasi-imperceptible. The second approach gives us better control over the perturbation magnitude. Hence, we will focus on this approach hereafter. <ref type="figure">Figure 2</ref> shows the architecture for generating imagedependent perturbations. Input image x is passed through the generator to create the perturbation f Θ (x). The perturbation is then scaled to constrain its norm. The result is the image-dependent perturbation which is added to the input image. We feed the clipped imagex to the network to obtain the output probabilities k(x). We use loss functions similar to the universal case as defined in equations 1-3. At inference time, we can discard the pre-trained model, and use only the generator to produce adversarial examples. This obviates the need for iterative gradient computations, and allows us to generate perturbations fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fooling Multiple Networks</head><p>Using generative models for creating adversarial perturbations enables us to train sophisticated models. For instance, we can consider training a single model for misleading multiple networks simultaneously. Suppose we have models K 1 , K 2 , . . . , K m trained on natural images. Let A K denote the space of adversarial examples for these target models, i.e.</p><formula xml:id="formula_9">A K = {a ∈ [0, 1] n \N | ∃ x ∈ N : d(x, a) &lt; ǫ , ∀i ∈ {1, . . . , m} : K i (a) = K i (x) ≈ c x }, in which d(·, ·)</formula><p>is a distance function, ǫ is a pre-specified threshold and c x is the ground-truth for x. We can consider both universal and image-dependent perturbations. In the case of universal perturbations, we seek a mapping</p><formula xml:id="formula_10">F : [0, 1]</formula><p>n → A K generating adversarial examples from input patterns. In practice, the function is approximated with a deep neural network F Θ . <ref type="figure" target="#fig_0">Figure 3</ref> depicts the corresponding architecture. It is similar to figure 1 other than that the resulting perturbed imagex is fed to each of the pre-trained models. The loss function for training the generator is a linear combination of fooling losses of pre-trained models as defined in equations 1-3. Hence, we have:</p><formula xml:id="formula_11">l multi−f ool = λ 1 · l f ool 1 + · · · + λ m · l f ool m<label>(4)</label></formula><p>in which {λ 1 , . . . , λ m } ⊂ IR is a set of weights chosen based on the difficulty of deceiving each target model. 79.5% 88.5%  <ref type="table">Table 2</ref>: Fooling rates of non-targeted universal perturbations using L ∞ norm as the metric.</p><p>The architecture for image-dependent perturbations is similar except that inputs to the generator are natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Classification</head><p>We generate adversarial examples for fooling classifiers pre-trained on the ImageNet dataset <ref type="bibr" target="#b13">[14]</ref>. For the Euclidean distance as the metric, we scale the output of the generator to have a fixed L 2 norm. We can also scale the generator's output to constrain its maximum value when dealing with the L ∞ norm. All results are reported on the 50,000 images of the ImageNet <ref type="bibr" target="#b13">[14]</ref> validation set. Note that the contrast of displayed perturbations is enhanced for better visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Universal Perturbations</head><p>Non-targeted Universal Perturbations. This setting corresponds to the architecture in <ref type="figure">figure 1</ref> with the loss functions defined in equations 1 and 2. Results are given in Tables 1 and 2 for L 2 and L ∞ norms respectively. For most cases our approach outperforms that of <ref type="bibr" target="#b34">[35]</ref>. Similar to <ref type="bibr" target="#b34">[35]</ref>, a value of 2000 is set as the L 2 -norm threshold of the universal perturbation, and a value of 10 is set for the L ∞ -norm when images are considered in [0, 255] range <ref type="bibr" target="#b6">7</ref> . We use UNet and ResNet Generator for L 2 and L ∞ norms respectively. We visualize the results in <ref type="figure">figure 4</ref>. Notice that the L 2 perturbation consists of a bird-like pattern in the top left. Intuitively, the network has learned that in this constrained problem it can successfully fool the classifier for the largest number of images by converging to a bird perturbation. On the other hand, when we optimize the model based on L ∞ norm, it distributes the perturbation to make use of the maximum permissible magnitude at each pixel.</p><p>Targeted Universal Perturbations. In this case we seek a single pattern which can be added to any image in the dataset to mislead the model into predicting a specified target label. We perform experiments with fixed L ∞ norm of 10, and use the ResNet generator for fooling the Inceptionv3 model. We use the loss function defined in equation 3 to train the generator. <ref type="figure" target="#fig_2">Figure 5</ref> depicts the perturbations for various targets. It also shows the top-1 target accuracy on the validation set, i.e. the ratio of perturbed samples classified as the desired target. We observe the the universal perturbation contains patterns resembling the target class. While this task is more difficult than the non-targeted one, our model achieves high target accuracies. To the best of our knowledge, we are the first to present effective targeted universal perturbations on the ImageNet dataset. To make sure that the model performs well for any target, we train it on 10 randomly sampled classes. The resulting average target accuracy for L ∞ = 10 is 52.0%, demonstrating generalizability of the model across different targets. <ref type="bibr" target="#b7">[8]</ref> proposes a strong method for creating targeted image-dependent perturbations. However, its iterative algorithm is very slow at inference time. It reports attacks that take several minutes to run for each image, making it infeasible in real-time scenarios in which the input image changes constantly. FGSM <ref type="bibr" target="#b17">[18]</ref> is a fast attack method but is not very accurate. In this work, we present adversarial attacks that are both fast and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image-dependent Perturbations</head><p>Non-targeted Image-dependent Perturbations. The corresponding architecture is given in <ref type="figure">figure 2</ref> with the loss function defined in equations 1 and 2. We use ResNet generator with 6 blocks for generating the perturbations. Similar to related works on image-dependent perturbations, we focus on L ∞ norm as the metric. Results are shown for various perturbation norms and pre-trained classifiers in <ref type="table" target="#tab_2">Table  3</ref>. <ref type="figure" target="#fig_3">Figure 6</ref> illustrates the perturbed images. In this case the model converges to simple patterns which can change the prediction for most images. As we observe, the perturbations contain features from the corresponding input images.</p><p>Targeted Image-dependent Perturbations. For this task we use the training scheme shown in <ref type="figure">figure 2</ref> with the loss function in equation 3. <ref type="figure" target="#fig_4">Figure 7</ref> shows samples of perturbed images for fooling the Inception-v3 model. The perturbations are barely perceptible, yet they can obtain high target accuracies. Moreover, the perturbation itself has features    </p><formula xml:id="formula_12">L ∞ = 7 L ∞ = 10 L ∞ = 13</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transferability and Fooling Multiple Networks</head><p>Several works have demonstrated that adversarial examples generated for one model may also be misclassified by other models. This property is referred to as transferability, and can be leveraged to perform black-box attacks <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>. We show that our generated perturbations can be transferred across different models. <ref type="table" target="#tab_4">Table  4</ref> shows the fooling ratio of a non-targeted universal attack trained on one network and evaluated on others. Each row corresponds to the pre-trained model based on which the attack model is learned. The last row of the table corresponds to a model trained to jointly mislead VGG-16 and VGG-19 models based on the architecture depicted in <ref type="figure" target="#fig_0">figure 3</ref>. We see that joint optimization results in better transferability  than training on a single target network. This is expected as the network has seen more models during training, so it generalizes better to unseen models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on Semantic Segmentation</head><p>Current methods for fooling semantic segmentation models such as <ref type="bibr" target="#b57">[58]</ref> and <ref type="bibr" target="#b33">[34]</ref> use iterative algorithms, which are hand-engineered for the specific task, and are slow at inference. We demonstrate that our proposed architectures are generalizable across different tasks. More specifically, we show that architectures similar to those used in the classification task yield strong results on fooling segmentation models. We leave extension to tasks other than classification and segmentation as future work. Experiments are performed on the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref>. It contains 2975 training and 500 validation images with a resolution of 2048 × 1024 pixels. Similar to <ref type="bibr" target="#b33">[34]</ref>, we downsample images and label maps to 1024 × 512 pixels using bilinear and nearest-neighbor interpolation respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Universal Perturbations</head><p>We first consider the more challenging case of targeted attacks in which a desired target label map is given. We use the same setting as in the classification task, i.e. the training architecture in <ref type="figure">figure 1</ref> with the fooling loss defined in equation 3. In order for our results to be comparable with <ref type="bibr" target="#b33">[34]</ref>, we consider FCN-8s <ref type="bibr" target="#b29">[30]</ref> as our segmentation model, and use L ∞ norm as the metric. Our setting corresponds to the static target segmentation in <ref type="bibr" target="#b33">[34]</ref>. We use the same target as the paper, and consider our performance metric to be success rate, i.e. the categorical accuracy between the prediction k(x) and the target t. <ref type="table" target="#tab_5">Table 5</ref> demonstrates our results. Our method outperforms the algorithm proposed in <ref type="bibr" target="#b33">[34]</ref> for most of the perturbation norms. We also visualize the results in <ref type="figure">figure 8</ref>. We observe that the generator fools the segmentation model by creating a universal perturbation which resembles the target label map. We also demonstrates the resulting mean IoU for non-targeted attacks in <ref type="table">Table 6</ref>. 79.5% 92.1% 97.2% UAP-Seg <ref type="bibr" target="#b33">[34]</ref> 80.3% 91.0% 96.3%  <ref type="table">Table 6</ref>: Mean IoU of non-targeted perturbations for fooling the FCN-8s segmentation model on the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image-dependent Perturbations</head><p>The targeted image-dependent task corresponds to the architecture in <ref type="figure">figure 2</ref> with the loss function in equation 3. We use the same target as the universal case. Results for various norms are given in <ref type="table">Table 7</ref>. As we expect, relaxing the constraint of universality leads to higher success rates. <ref type="figure">Figure 9</ref> illustrates the perturbations for L ∞ = 10. By closely inspecting the perturbations, we can observe patterns from both the target and the input image. As shown in <ref type="table">Table 6</ref>, image-dependent perturbations achieve smaller mean IoU by not having the universality constraint. The average inference time per image is 132.82 ms for the U-Net architecture and 335.73 ms for the ResNet generator <ref type="bibr" target="#b8">9</ref> . <ref type="bibr" target="#b8">9</ref> The time is measured on Titan Xp GPUs.</p><p>L ∞ = 5 L ∞ = 10 L ∞ = 20 GAP 87.0% 96.3% 98.2% <ref type="table">Table 7</ref>: Success rate of targeted image-dependent perturbations for fooling FCN-8s on the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Future Work</head><p>In this paper, we demonstrate the efficacy of generative models for creating adversarial examples. Four types of adversarial attacks are considered: targeted universal, non-targeted universal, targeted image-dependent and nontargeted image-dependent. We achieve high fooling rates on all tasks in the small perturbation norm regime. The perturbations can successfully transfer across different target models. Moreover, we demonstrate that similar architectures can be effectively used for fooling both classification and semantic segmentation models. This eliminates the need for designing task-specific attack methods, and paves the way for extending adversarial examples to other tasks. Future avenues of research include incorporating various properties such as transformation-invariance into the perturbations and extending the proposed framework to tasks other than classification and semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture for training a model to fool multiple target networks. The fooling loss for training the generator is a linear combination of fooling losses of target models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a ) 19 Figure 4 :</head><label>a194</label><figDesc>Figure 4: Non-targeted universal perturbations. Enhanced universal pattern is shown on the left, and two samples of perturbed images are given on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Targeted universal perturbations. Three different targets and the corresponding average target accuracy of perturbed images on Inception-v3 are given. Universal pattern is shown on the left and two sample perturbed images are depicted on the right. Perturbation norm is L ∞ = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Non-targeted image-dependent perturbations. From left to right: original image, enhanced perturbation and perturbed image. Three different thresholds are considered with Inception-v3 as the target model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Targeted image-dependent perturbations. Two different targets and the corresponding average target accuracy of perturbed images on Inception-v3 are shown. From left to right: original image, enhanced perturbation and perturbed image. Perturbation magnitude is set to L ∞ = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Targeted universal perturbations with L ∞ = 10 for fooling the FCN-8s semantic segmentation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Fooling rates of non-targeted universal perturba-</figDesc><table>tions for various classifiers pre-trained on ImageNet. Our 
method (GAP) is compared with Universal Adversarial Per-
turbations (UAP) [35] using L 2 norm as the metric. 

VGG16 VGG19 Inception 

5 

L ∞ = 10 
GAP 
UAP 

83.7% 
78.8% 

80.1% 
77.8% 

82.7% 

6 

78.9% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Fooling ratios (pre-trained models' accuracies) for 
non-targeted image-dependent perturbations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Transferability of non-targeted universal perturba- tions. The network is trained to fool the pre-trained model shown in each row, and is tested on the model shown in each column. Perturbation magnitude is set to L 2 = 2000. The last row indicates joint training on VGG-16 and VGG-19.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Success rate of targeted universal perturbations for fooling the FCN-8s segmentation model. Results are ob- tained on the validation set of the Cityscapes dataset.</figDesc><table>Task 
L ∞ = 5 L ∞ = 10 L ∞ = 20 
Universal 
12.8% 
4.0% 
2.1% 
Image-dependent 
6.9% 
2.1% 
0.4% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that x may or may not belong to the space of natural images. 2 For images of height h, width w and c channels: n = h × w × c. 3 We can also relax the constraint, and require that for most pixels the prediction is different from the ground-truth.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that K(x) = arg max k(x).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Since [35] does not report results on Inception-v3, we compare with their results on Inception-v1 (GoogLeNet). 6 This result uses the logit-based loss [7, 8] as opposed to the least-likely class loss (equation 2), which is used for other results in the table. 7 The average L 2 and L∞ norm of images in our validation set are consistent with those reported in [35].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The time is measured on Titan Xp GPUs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Target: Soccer Ball, Top-1 target accuracy: 91.3% (b) Target: Hamster, Top-1 target accuracy: 87.4%</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Seyed Ali Osia, Zekun Hao and Xun Huang for helpful discussions. This work is supported in part by a Google Focused Research Award and a Facebook equipment donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Defense against universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05929</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the robustness of semantic segmentation models to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09856</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07397</idno>
		<title level="m">Synthesizing robust adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial transformation networks: Learning to generate adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09387</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09491</idno>
		<title level="m">Exploring the space of black-box attacks on deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Houdini: Fooling deep structured prediction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05373</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Shield: Fast, practical defense and vaccination for deep learning using jpeg compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06816</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Countering adversarial images using input transformations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04357</idno>
		<title level="m">Stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">No need to worry about adversarial examples in object detection in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sibai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03501</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02613</idno>
		<title level="m">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations against semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05712</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08401</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Analysis of universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fast feature fool: A data independent approach to universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05572</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02697</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08926</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09344</idno>
		<title level="m">Certified defenses against adversarial examples</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Robust preprocessing: A robust defense method against adversary attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01549</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buckman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vijaykeerthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumaraguru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01448</idno>
		<title level="m">Hardening deep neural networks via adversarial model cascades</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10578</idno>
		<title level="m">Evaluating the robustness of neural networks: An extreme value theory approach</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01991</idno>
		<title level="m">Mitigating adversarial effects through randomization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08603</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
