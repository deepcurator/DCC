<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Top-down Visual Saliency Guided by Captions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasili</forename><surname>Ramanishka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
							<email>dasabir@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Top-down Visual Saliency Guided by Captions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural saliency methods have recently emerged as an effective mechanism for top-down task-driven visual search <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">31]</ref>. They can efficiently extract saliency heatmaps given a high-level semantic input, e.g., highlighting regions corresponding to an object category, without any per-pixel supervision at training time. They can also explain the internal representations learned by CNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">30]</ref>. However, suppose we wanted to search a visual scene for salient elements described by a natural language sentence ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>), or, given the description of an action, localize the most salient temporal and spatial regions corresponding to the subject, verb and other components ( <ref type="figure" target="#fig_1">Fig. 1(b)</ref>). Classification-based saliency methods are insufficient for such language-driven tasks as they are limited to isolated object labels and cannot handle textual queries.  Deep image and video captioning models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">28]</ref> excel at learning representations that translate visual input into language potentially discovering a mapping between visual concepts and words. However, despite the good captioning performance, they can be very hard to understand and are often criticized for being highly non-transparent "black boxes." They hardly provide any clear insight of the mapping learned internally between the image and the produced words. Consider for example, the video shown in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. Which region in the model is used to predict words like "woman" or "meat" in the generated caption? Is the word "woman" generated because the model recognized the woman in the video, or merely because the language model predicts that "A woman" is a likely way to start a sentence? Can the model learn to localize visual concepts corresponding to words while training only on weak annotations in the form of image or video-level captions? Can it localize words both in space and in time?</p><p>In this work, we address these questions by proposing a Caption-Guided Visual Saliency method that leverages deep captioning models to generate top-down saliency for both images and videos. Our approach is based on an encoderdecoder captioning model, and can produce spatial or spatiotemporal heatmaps for either a given input caption or a caption predicted by our model <ref type="figure" target="#fig_1">(Fig. 1)</ref>. In addition to facilitating visual search, this allows us to expose the inner workings of deep captioning models and provide much needed intuition of what these models are actually learning. This, in turn, can lead to improved model design in the future. Previous attempts at such model introspection have analyzed LSTMs trained on text generation <ref type="bibr" target="#b12">[13]</ref>, or CNNs trained on image-level classification <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32]</ref>. Recent "soft" attention models <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> produce heatmaps by learning an explicit attention layer that weighs the visual inputs prior to generating the next word, but require modification of the network and do not scale well. Thus, ours is the first attempt to analyze whether end-to-end visual captioning models can learn top-down saliency guided by linguistic descriptions without explicitly modeling saliency.</p><p>Our approach is inspired by the signal drop-out methods used to visualize convolutional activations in <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32]</ref>, however we study LSTM based encoder-decoder models and design a novel approach based on information gain. We estimate the saliency of each temporal frame and/or spatial region by computing the information gain it produces for generating the given word. This is done by replacing the input image or video by a single region and observing the effect on the word in terms of its generation probability given the single region only. We apply our approach to both still image and video description scenarios, adapting a popular encoder-decoder model for video captioning <ref type="bibr" target="#b21">[22]</ref> as our base model.</p><p>Our experiments show that LSTM-based encoderdecoder networks can indeed learn the relationship between pixels and caption words. To quantitatively evaluate how well the base model learns to localize words, we conduct experiments on the Flickr30kEntities image captioning dataset <ref type="bibr" target="#b16">[17]</ref>. We also use our approach to "explain" what the base video captioning model is learning on the publicly available large scale Microsoft Video-to-Text (MSR-VTT) video captioning dataset <ref type="bibr" target="#b24">[25]</ref>. We compare our approach to explicit "soft" attention models <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> and show that we can obtain similar text generation performance with less computational overhead, while also enabling more accurate localization of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Top-down neural saliency: Weak supervision in terms of class labels were used to compute the partial derivatives of CNN response with respect to input image regions to obtain class specific saliency map <ref type="bibr" target="#b18">[19]</ref>. The authors in <ref type="bibr" target="#b30">[30]</ref> used deconvolution with max-pooling layers that projects class activations back to the input pixels. While recent top-down saliency methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref> recover pixel importance for a given class using isolated object labels, we extend the idea to linguistic sentences. Soft Attention: "Soft" attention architectures, developed for machine translation <ref type="bibr" target="#b1">[2]</ref>, were recently extended to image captioning <ref type="bibr" target="#b27">[27]</ref>. Instead of treating all image regions equally, soft attention assigns different weights to different regions depending on the their content. Similarly, in video captioning, an LSTM with a soft attention layer attends to specific temporal segments of a video while generating the description <ref type="bibr" target="#b28">[28]</ref>. Compared to our top-down saliency model, one drawback of soft attention is that it requires an extra recurrent layer in addition to the LSTM decoder, requiring additional designing of this extra layer parameters. The size of this layer scales proportionally to the number of items being weighted, i.e., the number of frames or spatial regions. In contrast, our approach extracts the mapping between input pixels and output words from encoderdecoder models without requiring any explicit modeling of temporal or spatial attention and without modifying the network. Our intuition is that LSTMs can potentially capture the inter-dependencies between the input and the output sequences through the use of memory cells and gating mechanisms. Our framework visualizes both temporal and spatial attention without having to estimate additional weight parameters unlike explicit attention models, and can be used to analyse and provide explanations for a wide variety of encoder-decoder models. Captioning Models: Captioning models based on a combination of CNN and LSTM networks have shown impressive performance both for image and video captioning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">28]</ref>. Dense captioning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> proposed to both localize and describe salient image regions. Works on referring expression grounding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> localize input natural language phrases referring to objects or sceneparts in images. These methods use ground truth bounding boxes and phrases to learn a mapping between regions and phrases. We address the more difficult task of learning to relate regions to words and phrases without strong supervision of either, training only on images paired with their respective sentence captions. We also handle spatiotemporal grounding for videos in the same framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background: Encoder-Decoder Model</head><p>We start by briefly summarizing our base captioning model. We utilize the encoder-decoder video description framework <ref type="bibr" target="#b22">[23]</ref> which is based on sequence-to-sequence models proposed for neural translation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>. In Section 4 we will describe how our approach applies the same base model to caption still images.</p><p>Consider an input sequence of p video frames x = (x 1 , . . . , x p ) and a target sequence of n words y = (y 1 , . . . , y n ). The encoder first converts the video frames x into a sequence of m high-level feature descriptors:</p><formula xml:id="formula_0">V = (v 1 , . . . , v m ) = φ(x)<label>(1)</label></formula><p>where typically φ() is a CNN pre-trained for image classification. It then encodes the feature descriptors V into a fixed-length vector</p><formula xml:id="formula_1">z = E(v 1 , . . . , v m )</formula><p>, where E is some (potentially non-linear) function. In the S2VT <ref type="bibr" target="#b21">[22]</ref>, this is done by encoding V into a sequence of hidden state vectors h e i using an LSTM, where the state evolution equation is:</p><formula xml:id="formula_2">h e i = f (v i , h e i−1 ) for i ∈ {1, 2, . . . , m}<label>(2)</label></formula><p>and then taking z = h e m , the last LSTM state. Another approach is to take the average of all m feature descriptors <ref type="bibr" target="#b22">[23]</ref></p><formula xml:id="formula_3">, i.e., z = 1 m m i=1 v i .</formula><p>The decoder converts the encoded vector z into output sequence of words y t , t ∈ {1, . . . , n}. In particular, it sequentially generates conditional probability distribution for each element of the target sequence given encoded representation z and all the previously generated elements,</p><formula xml:id="formula_4">P (y t |y 1 , . . . , y t−1 , z) = D(y t−1 , h d t , z), h d t = g(y t−1 , h d t−1 , z) (3)</formula><p>where h d t is the hidden state of the decoding LSTM and g is again a nonlinear function. Soft Attention: Instead of using the last encoder LSTM state or averaging V , the authors in <ref type="bibr" target="#b28">[28]</ref> suggest keeping the entire sequence V and having the encoder compute a dynamic weighted sum:</p><formula xml:id="formula_5">z t = m i=1 α ti v i (4)</formula><p>Thus, instead of feeding an averaged feature vector into the decoder LSTM, at every timestep a weighted sum of the vectors is fed. The weights for every v i are computed depending on previous decoder state h d t−1 and encoded sequence V = (v 1 , . . . , v m ). In video captioning, this allows for a search of related visual concepts in the whole video depending on the previously generated words. As a result, one can think about attention in this model as a generalization of simple mean pooling across video frames. Weights α ti are obtained by normalizing e ti , as follows,</p><formula xml:id="formula_6">α ti = exp(e ti ) m k=1 exp(e tk ) e ti = w T tanh(W a h t−1 + U a v i + b a )<label>(5)</label></formula><p>where w, W a , U a and b a are attention parameters of the attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>We propose a top-down saliency approach called Caption-Guided Visual Saliency which produces spatial and/or temporal saliency values (attention) for still images or videos based on captions. The saliency map can be generated for a caption predicted by the base model, or for an arbitrary input sentence. Our approach can be used to understand the base captioning model, i.e. how well it is able to establish a correspondence between objects in the visual input and words in the sentence. We use the encoder-decoder captioning model as our base model (equations 1, 2, 3).</p><p>For each word in the sentence, we propose to compute the saliency value of each item in the input sequence by measuring the decrease in the probability of predicting that word based on observing just that single item. This approach is flexible, does not require augmenting the model with additional layers, and scales well with input size. In contrast, in the soft attention model, the decoder selects relevant items from the input with the help of trainable attention weights. This requires additional layers to predict the weights. Furthermore, it can only perform either temporal or spatial mapping, but not both. Our method estimates both a temporal and a spatial mapping between input and output using the base LSTM encoder-decoder model by recovering the implicit attention from the model. We describe the more general case of video in Section 4.1 and then show how this model can be applied to still images in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Saliency</head><p>In case of videos, we would like to compute the most salient spatiotemporal regions corresponding to words in the given sentence description of an event or activity. <ref type="figure">Figure 2</ref> shows an overview of the approach. The intuition is that, although the encoder discards temporal and spatial positions of visual concept activations by encoding them into a fixed-length vector, this information can still be extracted from the model. The encoded representation, containing activations of all visual concepts detected in the entire video, is passed on to the decoder LSTM at the start of the sentence generation process. The decoder then chooses parts of this state vector using LSTM output gates to predict the word at time t. As each word is generated, the presence of visual concepts in the decoder LSTM state continually evolves, and the evolved state vector in turn interacts with the output  Figure 2: Overview of our proposed top-down Caption-Guided Visual Saliency approach for temporal saliency in video. We use an encoder-decoder model to produce temporal saliency values for each frame i and each word t in a given input sentence. The values are computed by removing all but the ith descriptor from the input sequence, doing a forward pass, and comparing to the original word probability distribution. A similar idea can be applied to spatial image saliency. See text for details.</p><p>gates to generate the next word. As this interaction is complex and non-linear, we devise an indirect scheme to extract the evidence for the generation of each word. Our approach measures the amount of information lost when a single localized visual input is used to approximate the whole input sequence. The decoder predicts probability distributions p(y t ) of words from the vocabulary at every step of the decoding process. We assume that this probability distribution is our "true" distribution. Then we measure how much information the descriptor of item i carries for the word at timestep t. To do this, we remove all descriptors from the encoding stage except for the ith descriptor. After computing a forward pass through the encoder and decoder, this gives us a new probability distribution q i (y t ). We then compute the loss of information as the KL-divergence between the two distributions, p(y t ) = P (y t |y 1:t−1 , v 1:m )</p><formula xml:id="formula_7">q i (y t ) = P (y t |y 1:t−1 , v i ) (6) Loss(t, i) = D KL (p(y t ) q i (y t ))</formula><p>With the above formulation we can easily derive topdown saliency for word w predicted at time t. We assume that the query sentence S has "one-hot" "true" distributions on every timestep. With this assumption Eq. 6 reduces to:</p><formula xml:id="formula_8">Loss(t, i, w) = k∈W p(y t = k) log p(y t = k) q i (y t = k) = log 1 q i (y t = w)<label>(7)</label></formula><p>This process is not limited to produced word sequence only but can be used with any arbitrary query for a given video.</p><p>As the approximate receptive field of each descriptor can be estimated <ref type="bibr" target="#b0">1</ref> , we can define a saliency map for each word in the sentence by mapping Loss(t, i, w) to the center of the receptive field and and upsampling the resulting heatmap. It follows from Eq. 7 that Loss(t, i, w) ∈ [0; +∞), where values which are closer to zero correspond to higher saliency. To obtain a saliency value e ti , we negate the loss and linearly scale the resulting values to the [0, 1] interval, e ti = scale(−Loss(t, i, w))</p><p>It is important to discriminate between the values meant by Eq. 6 and. 7. The former can be used to evaluate the representativeness of individual descriptors compared to the full input sequence, while the latter induces top-down saliency maps for individual words at each time step. Finally, the saliency value for a group of words from the target sentence (e.g. a noun phrase "a small boy") is defined as sum of the corresponding saliency values for every word in the subsequence:</p><formula xml:id="formula_10">Loss({t 1 , ..., t q }, i) = q j=1</formula><p>Loss(t j , i).</p><p>Next we describe how this approach is applied to generate both temporal and spatial saliency in videos.</p><p>Temporal attention: For an input frame sequence V = (v 1 , . . . , v m ), the deterministic algorithm of sentence generation is given by the following recurrent relation:</p><formula xml:id="formula_12">w = argmax yt∈W p(y t |y 0:t−1 , v 1:m )<label>(10)</label></formula><p>where y 0 and y n are special "begin of sequence" and "end of sequence" tokens respectively. Given the word predicted at time t of the sentence, the relative saliency of the input frame v i can be computed as e ti (Eq. 8). In other words, we estimate the drop in probability of every word in the output sequence resulting from encoding only that input frame. Further, we normalize e t = (e t1 , . . . , e tm ) to obtain stochastic vectors as in Eq. 5 and interpret the resulting vectors α t = (α t1 , . . . , α tm ) as saliency over the input sequence V = (v 1 , . . . , v m ) for every word y t of the output sequence. This also induces a direct mapping between predicted words and the most salient frames for these words. Spatial attention: We can also estimate the attention on different frame patches as related to a particular word y t of a sentence. Although spatial pooling in the CNN discards the spatial location of detected visual concepts, the different gates of the LSTM enable it to focus on certain concepts depending on the LSTM hidden state. Let f k (a, b) be the activation of unit k (corresponding to some visual concept) at spatial location (a, b) in the last convolutional layer of the encoder <ref type="bibr" target="#b32">[32]</ref>. The CNN performs spatial average pooling to get a feature vector v i for the i th frame whose k th element is v ik = a,b f k (a, b). After that, the encoder embeds the descriptor into LSTM cell state according to the LSTM update rule. This process involves the LSTM input gate:</p><formula xml:id="formula_13">ρ i = σ(W vρ v i + W hρ h i−1 + b ρ )<label>(11)</label></formula><p>where the LSTM selects activations v ik by weighting them depending on the previous LSTM hidden state and v i itself (W vρ , W hρ and b ρ are trainable parameters). Note that,</p><formula xml:id="formula_14">Wvρvi= k w k v ik = k w k a,b f k (a, b) = a,b k w k f k (a, b) (12)</formula><p>where w k denotes the k th column of matrix W vρ . Since each unit activation f k (a, b) represents a certain visual concept <ref type="bibr" target="#b30">[30]</ref>, we see that the input gate learns to select input elements based on relevant concepts detected in the frame, regardless of their location. The explicit spatial location information of these concepts is lost after the spatial average pooling in the last convolutional layer, however, we can recover it from the actual activations f k (a, b). This is achieved by computing the information loss for different spatial regions in a frame in a similar way as was done for temporal attention extraction. The relative importance of region (a, b) in frame v i for word w predicted at time t can be estimated as:</p><formula xml:id="formula_15">e (a,b) ti = −Loss(t, i, w),</formula><p>where p(y t ) = P (y t |y 0:t−1 , v 1:m ),</p><formula xml:id="formula_16">q i (y t ) = P (y t |y 0:t−1 , v (a,b) i ),<label>(13)</label></formula><p>and where v</p><formula xml:id="formula_17">(a,b) ik = f k (a, b)</formula><p>. Assuming the number of spatial locations in a frame to be r, the prediction process (i.e. forward pass) is run m times to obtain temporal saliency maps and r × m times to obtain the spatial maps for the given video/sentence pair. This, in turn, involves n + 1 LSTM steps, so the total complexity is O(( r × m + m spatial and temporal</p><formula xml:id="formula_18">) × ( n + 1 LSTM steps ))<label>(14)</label></formula><p>Since all Loss(t, i, w) computations are performed independently, we can create a batch of size r × m + m and calculate all the saliency values efficiently in one pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Saliency</head><p>With minimal changes the above model can be applied to generate saliency for images. We accomplish this by rearranging the grid of descriptors produced by the last convolutional layer of the CNN into a "temporal" sequence V = (v 1 , . . . , v m ) by scanning the image in a sequential manner (row by row), starting from the upper left corner and ending at the bottom right corner. Our model uses the encoder LSTM to scan the image locations and encode the collected visual information into hidden states and then decodes those states into the word sequence. Generating a spatial saliency map can now be achieved by the same process as described for temporal saliency in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section shows examples of caption-driven saliency recovered by our method for the base S2VT model from videos and still images. We evaluate the quality of the recovered heatmaps on an image dataset annotated with ground truth object bounding boxes. We also evaluate the caption generation performance on both images and videos and compare it with the soft attention approach. Datasets We train and evaluate our model on two video description datasets, namely the Microsoft Video Description dataset (MSVD) <ref type="bibr" target="#b4">[5]</ref> and the Microsoft Research Video to Text (MSR-VTT) <ref type="bibr" target="#b24">[25]</ref> dataset. Both datasets have "in the wild" Youtube videos and natural language descriptions. MSVD contains 1970 clips of average length 10.2s with 80,827 natural language descriptions. MSR-VTT provides 41.2 hours of web videos as 10,000 clips of approx. 14.8s each and 200K natural language descriptions. In addition, we evaluated on one of the largest image captioning datasets, Flickr30kEntities <ref type="bibr" target="#b16">[17]</ref> which is an extension of the original Flick30k <ref type="bibr" target="#b29">[29]</ref> dataset with manual bounding box annotations for all noun phrases in all 158k image captions. Model details We implemented our model in TensorFlow <ref type="bibr" target="#b0">[1]</ref> using InceptionV3 <ref type="bibr" target="#b20">[21]</ref> pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> as CNN feature extractor. We use v 1 , . . . , v 26 , v i ∈ R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2048</head><p>for the video representation. v i were extracted from the average pooling layer pool 3 for 26 evenly spaced frames. For images we use feature outputs from the last convolutional layer mixed 10 as the input sequence to the encoder.</p><p>(a) A man in blue jeans and a white t-shirt is working on a window with rail guards.</p><p>(b) Two people are in a room , one man is putting on a shirt and tie.</p><p>(c) A man is adding steamed milk to a coffee beverage.   Thus, for video and image captioning the input sequences have length m = 26 and m = 64 respectively. The order of spatial descriptors for image captioning is described in Section 4.2. All images and video frames were scaled <ref type="table">Table 1</ref>: Evaluation of the proposed method on localizing all noun phrases from the ground truth captions in the Flickr30kEntities dataset using the pointing game protocol from <ref type="bibr" target="#b31">[31]</ref>. "Baseline random" samples the point of maximum saliency uniformly from the whole image and "Baseline center" corresponds to always pointing to the center.  <ref type="table">Table 2</ref>: Evaluation of the proposed method on Flickr30kEntities using the attention correctness metric and evaluation protocol from <ref type="bibr" target="#b13">[14]</ref> (including the frame cropping procedure). Soft attention performance is taken from <ref type="bibr" target="#b13">[14]</ref> as reported there. Baseline* shows our reevaluation of the uniform attention baseline.</p><p>bodyparts animals people instruments vehicles scene other clothing Avg per NP Baseline <ref type="bibr" target="#b13">[14]</ref> - shows the results and demonstrates that despite not using explicit attention layers, our model performs comparably to the soft attention method. The best model in terms of the METEOR metric on the validation split of Flickr30k was selected for the evaluation of saliency as presented below. Quantitative evaluation of saliency Given a pretrained model for image captioning, we test our method quantitatively using the pointing game strategy <ref type="bibr" target="#b31">[31]</ref> and attention correctness metric <ref type="bibr" target="#b13">[14]</ref>. To generate saliency maps, we feed ground truth captions from the test split of Flickr30k into our model. In pointing game evaluation, we obtain the maximum saliency point inside the image for each annotated noun phrase in each GT caption of Flickr30kEntities. We then test whether this point lies inside the bounding box or not. Accuracy is computed as Acc = #Hits #Hits+#M isses . To get a saliency map for noun phrases which are comprised of multiple tokens from the sentence, we sum loss values before their normalizing them to the [0, 1]. <ref type="table">Table 1</ref> shows the mean accuracy over all noun phrases (NPs) along with accuracies corresponding to categories (in different columns) from Flickr30kEntities. We compare to "Baseline random", where the maximum saliency point is sampled uniformly from the whole image and to a much stronger baseline denoted as "Baseline center". This baseline is designed to mimic the center bias present in consumer photos and assumes that the maximum saliency point is always at the center of the image. Compared to the random baseline, the accuracy of the proposed method is better on average (last column) as well as for all the individual categories (rest of the columns). While the average accuracy compared to the much stronger center baseline is only slightly better, the accuracy gain for some of the categories is significant. One possible reason may be that the objects in these categories, e.g., 'animals' or 'other' objects, tend to be away from the central region of an image, while people tend to be in the center of the photo. <ref type="table">Table 2</ref> provides a direct comparison of our method to the soft attention model <ref type="bibr" target="#b27">[27]</ref> in terms of the attention correctness metric proposed in <ref type="bibr" target="#b13">[14]</ref>. This metric measures average value for integral of attention function over bounding boxes. We directly report the results from <ref type="bibr" target="#b13">[14]</ref> for their implementation of uniform baseline, soft-attention model and its improved version where a captioning model was trained to focus on relevant objects in supervised manner. Our method outperforms all three of them.</p><formula xml:id="formula_19">- - - - - - - 0.321 Soft attention [27] - - - - - - - - 0.387 Soft attention - - - - - - - - 0</formula><p>We also provide the category specific values as we obtained from our own implementation of the uniform baseline (called "Baseline*"). "Baseline random" in <ref type="table">Table 1</ref> should roughly correspond to "Baseline" and "Baseline*" in <ref type="table">Table 2</ref>. Evidently, the exact values will be different as the evaluation protocols in the two tables are different. To compare the results fairly, we followed the same protocol as <ref type="bibr" target="#b13">[14]</ref> where the authors performed a central crop of both test and training images. Human-captured images or videos tend to put the objects of interest in the central region. Thus, any cropping operation which enhances this "central ten-a woman is skating on the snow a man is talking about a phone  dency" will, inherently, give a better measure of attention. This frame cropping strategy is another source of discrepancy in the baseline values in <ref type="table">Table 1</ref> and 2. <ref type="figure" target="#fig_0">Figures 3 and 4</ref> show example saliency maps on images from Flickr30kEntities for arbitrary query sentences and model-predicted captions, respectively. The arbitrary query comes from the ground truth descriptions. For each nounphrase, the saliency map is generated by summing the responses for each token in the phrase and then renormalizing them. The map is color coded where red shows the highest saliency while blue is the lowest. The maximum saliency point is marked with an asterisk, while the ground truth boxes for the noun-phrases are shown in white. It can be seen that our model almost always localizes humans correctly. For some other objects the model makes a few intuitive mistakes. For example, in <ref type="figure" target="#fig_0">Fig. 3a</ref>, though the saliency for "window" is not pointing to the groundtruth window, it focuses its highest attention (asterisk) on the gate which looks very similar to a window. <ref type="figure" target="#fig_5">In Fig. 4</ref>, the saliency map the predicted caption fof an image is shown. Some non-informative words (e.g., "a", "is" etc.) may appear to have concentrated saliency, however, this is merely a result of normalization. One surprising observation is that the model predicts 'a woman in a red and white outfit', however only the 'red' spatial attention is on the cyclist, while the 'white' attention is on other parts of the scene. <ref type="figure" target="#fig_6">Fig. 5</ref> shows examples of spatial and temporal saliency maps for videos from MSR-VTT dataset with model-predicted sentences. Most discriminative frames for each word are outlined in the same color as the word. Darker gray indicates higher magnitude of temporal saliency for the word. We omit visualization for uninformative words like articles, helper verbs and prepositions. An interesting observation about the top video is that the most salient visual inputs for "skating" are regions with snow, with little attention on the skier, which could explain the mistake. Additional results and source code are available at visionlearninggroup.github.io/caption-guided-saliency/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saliency visualizations in images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saliency visualizations in videos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a top-down saliency approach guided by captions and demonstrated that it can be used to understand the complex decision processes in image and video captioning without making modifications such as adding explicit attention layers. Our approach maintains good captioning performance while providing more accurate heatmaps than existing methods. The model is general and can be used to understand a wide variety of encoder-decoder architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Input: A man in a jacket is standing at the slot machine ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top-down Caption-Guided Visual Saliency approach that generates, for each word in a sentence, (a) spatial saliency in image and (b) spatiotemporal saliency in videos. For the video, we show temporally most important frames corresponding to the words at the bottom (arrows show positions of frames in the video) and spatial heatmaps indicating salient regions for these words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(d) A group of people are standing in a room filled with wooden furniture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Saliency maps (red to blue denotes high to low value) in Flickr30kentities generated for an arbitrary query sentence (shown below). Each row shows saliency map for different noun-phrases (shown at top-left corner) extracted from the query. Maximum saliency point is marked with asterisk and ground truth boxes are shown in white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Saliency maps generated for a caption (shown below the image) predicted by the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Spatial and temporal saliency maps in videos. For each word, darker grey indicates higher relative saliency of the frame. For better visualization, saliency values are not normalized but linearly mapped to the range [0, 1]. Most relevant frames for each word are shown at the bottom, highlighted with the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>. . .</head><label>.</label><figDesc>Output saliency map for word t</figDesc><table>. . . 

. . . 

. . . 
h m+1 
h m+t-1 
. . . 

y t-1 

h m+t 

p(y t ) 

d 0 
d k-1 
v i 
d T 

. . . 
. . . 
. . . 

q i (y t ) 

X 
X 
X 

Loss(p y t ,q i y t ) 

α 0 
α i-1 
α i 
α m 

normalize 

. . . 
. . . 

LSTM 

LSTM 

word t 

time i 

A 
. . . 
small 
boy 

Caption: 

h 0 
h i-1 
h i 
h m 
h m+1 
h m+t-1 
h m+t 

Video: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the captioning performance of our model and soft-attention on two video (MSVD, MSR-VTT) and one im- age (Flickr30k) datasets. Higher numbers are better.</figDesc><table>Model 
Dataset 
METEOR [9] 
Soft-Attn [28] MSVD 
30.0 
Our Model 
MSVD 
31.0 
Soft-Attn [26] MSR-VTT 
25.4 
Our Model 
MSR-VTT 
25.9 
Soft-Attn [27] Flickr30k 
18.5 
Our Model 
Flickr30k 
18.3 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">for our video descriptors, the receptive field is a single frame</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This research was supported in part by NSF IIS-1212928, DARPA, Adobe Research and a Google Faculty grant. We thank Subhashini Venugopalan for providing an implementation of S2VT <ref type="bibr" target="#b21">[22]</ref> and Stan Sclaroff for many useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics Workshop</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look and Think Twice: Capturing Top-Down Visual Attention With Feedback Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, December 2015</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collecting Highly Parallel Data for Paraphrase Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mind&apos;s Eye: A Recurrent Visual Representation for Image Caption Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural Language Object Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and Understanding Recurrent Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention Correctness in Neural Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding Deep Image Representations by Inverting Them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generation and Comprehension of Unambiguous Object Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grounding of Textual Phrases in Images by Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to Sequence -Video to Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Translating Videos to Natural Language Using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics -Human Language Technologies</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Supplementary Material</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Describing Videos by Exploiting Temporal Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<title level="m">From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<title level="m">Topdown Neural Attention by Excitation Backprop. European Conference on Computer vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Learning Deep Features for Discriminative Localization. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
