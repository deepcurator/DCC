<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EC-Net: an Edge-aware Point set Consolidation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
							<email>lqyu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
							<email>xzli@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<email>cwfu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
							<email>pheng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EC-Net: an Edge-aware Point set Consolidation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>point cloud</term>
					<term>learning</term>
					<term>neural network</term>
					<term>edge-aware</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Point clouds obtained from 3D scans are typically sparse, irregular, and noisy, and required to be consolidated. In this paper, we present the first deep learning based edge-aware technique to facilitate the consolidation of point clouds. We design our network to process points grouped in local patches, and train it to learn and help consolidate points, deliberately for edges. To achieve this, we formulate a regression component to simultaneously recover 3D point coordinates and pointto-edge distances from upsampled features, and an edge-aware joint loss function to directly minimize distances from output points to 3D meshes and to edges. Compared with previous neural network based works, our consolidation is edge-aware. During the synthesis, our network can attend to the detected sharp edges and enable more accurate 3D reconstructions. Also, we trained our network on virtual scanned point clouds, demonstrated the performance of our method on both synthetic and real point clouds, presented various surface reconstruction results, and showed how our method outperforms the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Point cloud consolidation is a process of "massaging" a point set into a surface <ref type="bibr" target="#b0">[1]</ref>, for enhancing the surface reconstruction quality. In the past two decades, a wide range of techniques have been developed to address this problem, including denoising, completion, resampling, and many more. However, these techniques are mostly based on priors, such as piecewise smoothness. Priors are typically over-simplified models of the actual geometry behavior, thus the prior-based techniques tend to work well for specific class of models rather than being general.</p><p>To implicitly model and characterize the geometry behavior, one common way is to take a data-driven approach and model the complex behavior using explicit examples. Data-driven surface reconstruction techniques <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> are based on matching local portions (often denoted as patches) to a set of examples. Particularly, the emergence of neural networks and their startling performance provide a new means for 3D reconstruction from point sets by data-driven learning <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. One of the main limitations of these neural network based methods is that they are oblivious to sharp features on 3D objects, where undersampling problems are typically more severe, making it challenging for an accurate object reconstruction.</p><p>In this paper, we present the first edge-aware consolidation network , namely EC-Net, for point cloud consolidation. The network is designed and trained, such that the output points admit to the surface characteristic of the 3D objects in the training set. More importantly, our method is edge-aware, in the sense that the network learns the geometry of edges from the training set, and during the test time, it identifies edge points and generates more points along the edges (and over the surface) to facilitate a 3D reconstruction that preserves sharp features.</p><p>Generally speaking, scanned point sets are irregular and non-uniform, and thus, do not lend themselves to be learned by common convolutional neural networks (CNN). Inspired by PointNet <ref type="bibr" target="#b8">[9]</ref>, we directly process 3D points by converting their coordinates into deep features and producing more points by feature expansion <ref type="bibr" target="#b6">[7]</ref>. Then, for efficient learning of the edges, we design our network to process points grouped as local patches in the point cloud. To do so, we develop a patch extraction scheme that solely works on points, so that we can extract patches of points for use consistently in both training and testing phases.</p><p>In addition, to train the network to be edge-aware, we associate edge and mesh triangle information with the training patches, and train the network to learn features from the patches by regressing point-to-edge distances and then the point coordinates. More importantly, we design a novel edge-ware joint loss function that can be efficiently computed for directly comparison between the output points and ground truth 3D meshes. Our loss function encourages the output points to be located close to the underlying surface and to the edges, as well as distributed more evenly on surface. Then in the inference phase, the network can generate and find output points close to the edges. Since it is difficult to annotate edges directly in real scanned point clouds, we train our network on synthesized virtual scanned point clouds, and show the performance of our method on both real and virtual scanned point clouds. By using our trained network, we show through various experiments that we can improve not only the point cloud consolidation results (see <ref type="figure" target="#fig_0">Figures 1(b) &amp; (c)</ref>), but also the surface reconstruction quality, compared to various state-of-the-art methods. All the code is available at the project webpage 1 .</p><p>Related works. Consolidating scanned data and imperfect point clouds has been an active research area since the early 90's <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. We briefly review some traditional geometric works and then discuss some recent related works that employ neural networks. For a more comprehensive survey, please refer to <ref type="bibr" target="#b12">[13]</ref>.</p><p>Point cloud consolidation. Early works in this area assumed smooth surface <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, the parameterization-free local projection operator (LOP) was devised to enhance the point set quality. However, these methods are oblivious to sharp edges and corners. To consolidate a point set in an edge-aware manner, some methods detected/sensed the sharp edges and arranged points deliberatively along edges to preserve their sharpness <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. Huang et al. <ref type="bibr" target="#b19">[20]</ref> developed the edge-aware resampling (EAR) algorithm; it computes reliable normals away from edges and then progressively upsamples points towards the surface singularities. Despite its promising results, EAR depends on the accuracy of the given/estimated normal. Preiner et al. <ref type="bibr" target="#b20">[21]</ref> developed CLOP, a continuous version of the LOP, for fast surface construction using the Gaussian mixture model to describe the point cloud density. To sum up, these geometric approaches either assume strong priors or rely on extra geometric attributes for upsampling point sets.</p><p>Neural networks for mesh and point cloud processing. Motivated by the promising results that deep learning methods have achieved for image and video problems, there has been increasing effort to leverage neural networks for geometry and 3D shape problems. To do so, early works extracted low-level geometric features as inputs to CNNs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Other works converted the input triangular meshes or point clouds to regular voxel grids <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> for CNN to process. However, pre-extracting low-level features may bring bias, while a volume representation demands a high computational cost and is constrained by its resolution.</p><p>Recently, point clouds have drawn more attention, and there are some works to utilize neural networks to directly process point clouds. Qi et al. <ref type="bibr" target="#b8">[9]</ref> firstly developed the PointNet, a network that takes a set of unordered points in 3D as inputs and learns features for object classification and segmentation. Later, they proposed the PointNet++ to enhance the network with a hierarchical feature learning technique <ref type="bibr" target="#b29">[30]</ref>. Subsequently, many other networks have been proposed for high-level analysis problems with point clouds <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. However, they all focus The pipeline of EC-Net. For each point in an input patch, we first encode its local geometry into a feature vector f (size: N × D) using PointNet++, and expand f into f ′ (size: rN × D 2 ) using a feature expansion mechanism. Then, we regress the residual point coordinates and also the point-to-edge distances (d) from the expanded features, and form the output point coordinates by adding the original point coordinates to the residual. Finally, the network identifies points on edges and yields output points. The network was trained with an edge-aware joint loss function that has four terms; see the yellow boxes on the right.</p><p>on analyzing global or mid-level attributes of point clouds. In some other aspects, Guerrero et al. <ref type="bibr" target="#b5">[6]</ref> proposed a network to estimate the local shape properties in point clouds, including normal and curvature. 3D reconstruction from 2D images has also been widely studied <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Our work is most related to PU-Net <ref type="bibr" target="#b6">[7]</ref>, which presented a network to upsample a point set. However, our method is edge-aware, and we extract local patches and train the network to learn edges in patches with a novel edge-aware joint loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we first present the training data preparation (Sec. 2.1) and the EC-Net architecture (Sec. 2.2). Then, we present the edge-aware joint loss function (Sec. 2.3) and the implementation details (Sec. 2.4). <ref type="figure" target="#fig_1">Figure 2</ref> shows the pipeline of EC-Net; see the supplemental material for the detailed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training data preparation</head><p>We train our network using point clouds synthesized from 3D objects, so that we can have ground truth surface and edge information. To start, we collect 3D meshes from ShapeNet <ref type="bibr" target="#b42">[43]</ref> and other online repositories, including simple 3D shapes, mechanical parts, and everyday objects such as chairs. Since we train the network with patches as inputs, we prepare a large amount of patches on the 3D meshes and do not require many meshes. Moreover, we manually sketch polylines on each 3D mesh to annotate sharp edges on the meshes; see <ref type="figure" target="#fig_2">Figure 3</ref>(a).  Virtual scanning. To obtain point clouds from the 3D mesh objects, we use the following virtual scanning procedure. First, we normalize the mesh to fit in [−1, +1] 3 , and evenly arrange a circle of 30 virtual cameras (50 • field of view) horizontally around (and to look at) the object. We then put the cameras two units from the object center and randomly perturb the camera positions slightly upward, downward or sideway. After that, we produce a point cloud for each camera by rendering a depth image of the object, adding quantization noise (see Sec. 3) to the depth values and pixel locations, and backprojecting each foreground pixel to obtain a 3D point sample. Then, we can compose the 3D point clouds from different cameras to obtain a virtual scanned data. Such sampling procedure mimics a real scanner with surface regions closer to the virtual camera receiving more point samples; see <ref type="figure" target="#fig_2">Figure 3</ref>(b) for two example results.</p><p>Patch extraction. From a point cloud (see <ref type="figure" target="#fig_3">Figure 4</ref>(c)), we aim to extract local groups of points (patches), such that the points in a patch are geodesically close to one another over the underlying surface. This is very important, since using Euclidean distances to select points could lead to points on opposite sides of a thin surface, e.g., see the thin plates in the chair shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b). Compared with <ref type="bibr" target="#b6">[7]</ref>, our patch extraction procedure directly operates on point clouds, not meshes, so we need a consistent extraction procedure for both network training and inference, where ground truth meshes are not available during the inference.</p><p>To this end, we first construct a weighted graph by considering each point as a node and creating an edge from each point to its k-nearest neighboring (k-nn) points, where k=10; see <ref type="figure" target="#fig_3">Figure 4</ref>(d). The edge weight is set as the Euclidean distance between the two points. Then, we randomly select m=100 points as the patch centroids; from each selected point, we use the Dijkstra algorithm to find the 2048 nearest points in terms of shortest path distances in the graph. Hence, we can find points that are approximately within a geodesic radius from the centroid. Further, we randomly selectN =1024 points out of the 2048 points to introduce randomness into the point distribution, and normalize the 3D coordinates of the points to have zero mean inside a unit ball. For patches used for training, we also find the associated mesh triangles and annotated edge segments near the patches as the ground truth information for training the network; see <ref type="figure" target="#fig_3">Figure 4</ref>(e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Edge-aware Point set Consolidation Network</head><p>In this subsection, we present the major components of EC-Net; see <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Feature embedding and expansion. This component first maps the neighboring information (raw 3D coordinates of nearby points) around each point into a feature vector using PointNet++ <ref type="bibr" target="#b29">[30]</ref> to account for the fact that the input points are irregular and unordered. The output is a D-dimensional multi-scale feature vector for each input point, where D is 256 in our experiments. In this step, we make the following adaptation for our problem. By design, PointNet++ processes a full point cloud of an object, while EC-Net processes local patches.</p><p>Since patches have open boundary, points near the boundary have neighbors mostly on one of its side only, so we found that the extracted features of these points are less accurate. Hence, out of theN feature vectors, we retain the N =N 2 feature vectors (denoted as f ) corresponding to points closer to the patch centroid. Next, the component synthesizes points by expanding features directly in feature space using the feature expansion module in <ref type="bibr" target="#b6">[7]</ref>, since points and features should be interchangeable. After this module, feature f (dimension:</p><formula xml:id="formula_0">N × D) are expanded to be f ′ (dimension: rN × D 2 )</formula><p>, where r is the upsampling rate and D 2 is the new feature dimension, which is set as 128; see again <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Edge distance regression. This component regresses a point-to-edge distance for each expanded feature (or point, equivalently) later for edge points identification. The regressed distance is an estimated shortest distance from the output point to the nearest annotated edge segment among all annotated edge segments associated with the patch. To do this, we extract a distance feature f dist from the expanded feature f ′ via a fully connected layer, and then regress the point-to-edge distance d from f dist via another fully connected layer. We do this in two steps, so that we can feed f dist also to the coordinate regression component.</p><p>Coordinate regression. This component reconstructs the 3D coordinates of the output points; see <ref type="figure" target="#fig_1">Figure 2</ref>. First, we concatenate the expanded feature f ′ with the distance feature f dist (from previous component) to form another feature, since f dist contains certain point-to-edge distance information. Then, we regress the point coordinates from the concatenated feature by applying two fully connected layers. Note that we only regress the residual 3D coordinates, and the network output 3D coordinates of the output points by adding the original 3D coordinates of the input points to the regressed residual 3D coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge points identification.</head><note type="other">Denoting d i as the regressed point-to-edge distance of output point x i , we next find a subset of output points, namely edge points (denoted as S ∆ d with threshold ∆ d ) that are near the edges:</note><formula xml:id="formula_1">S ∆ d = {x i } di&lt;∆ d .</formula><p>Note that this component is performed in both training and inference phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Edge-aware joint loss function</head><p>The loss function should encourage the output points to be (i) located close to the underlying object surface, (ii) edge-aware (located close to the annotated edges), and (iii) more evenly distributed on the object surface. To this end, we guide the network's behavior by designing an edge-aware joint loss function with the following four loss terms (see also the rightmost part of <ref type="figure" target="#fig_1">Figure 2</ref>):</p><p>Surface loss encourages the output points to be located close to the underlying surface. When extracting each training patch from the input point clouds, we find triangles and edge segments associated with the patch; see <ref type="figure" target="#fig_3">Figure 4</ref>. Hence, we can define surface loss using the minimum shortest distance from each output point x i to all the mesh triangles T associated with the patch: d T (x i , T ) = min t∈T d t (x i , t), where d t (x i , t) is the shortest distance from x i to triangle t ∈ T . It is worth noting that to compute d t in 3D, we need to consider seven cases, since the point on t that is the closest to x i may be located at triangle vertices, along triangle edges, or within the triangle face. Experimentally, we found that the algorithm [44] for calculating d t can be implemented using TensorFlow to automatically calculate the gradients when training the network. With d T computed for all the output points, we can sum them up to compute the surface loss:</p><formula xml:id="formula_2">L surf = 1 N 1≤i≤Ñ d 2 T (x i , T ) ,<label>(1)</label></formula><p>whereÑ = rN is the number of output points in each patch.</p><p>Edge loss encourages the output points to be edge-aware, i.e., located close to the edges. Denoting E as the set of annotated edge segments associated with a patch, we define edge loss using the minimum shortest distance from each edge point to all the edge segments in the patch: d E (x i , E) = min e∈E d e (x i , e), where d e (x i , e) is the shortest distance from edge point x i to any point on edge segment e ∈ E. Again, we implement the algorithm in <ref type="bibr" target="#b43">[45]</ref> to calculate d e for different shortest distance cases using TensorFlow to automatically calculate the gradients. Then, we sum up d E for all the edge points and obtain the edge loss:</p><formula xml:id="formula_3">L edge = xi∈S ∆ d d 2 E (x i , E) |S ∆ d |</formula><p>, where S ∆ d is the edge point set .</p><p>Repulsion loss encourages the output points to be more evenly distributed over the underlying surface. Given a set of output points x i , i = 1...Ñ , it is defined as</p><formula xml:id="formula_5">L repl = 1 N · K 1≤i≤Ñ i ′ ∈K(i) η( || x i ′ − x i || ) ,<label>(3)</label></formula><p>where K(i) is the set of indices for the K-nearest neighborhood of x i (we set K=4), || · || is the L2-norm, and η(r) = max(0, h 2 − r 2 ) is a function to penalize x i if it is too close to some other nearby points, where h is empirically set as 0.03 (which is the mean separation distance between points estimated from the number of points and bounding box diagonal length according to <ref type="bibr" target="#b19">[20]</ref>). It is worth noting that we only want to penalize x i when it is too close to some neighborhood points, so we only consider a few nearest neighboring points around x i ; moreover, we remove the repulsion effect when the point-to-point distance is above h.</p><p>Edge distance regression loss aims to guide the network to regress the pointto-edge distances d for the rN output points; see <ref type="figure" target="#fig_1">Figure 2</ref>. Considering that it is difficult for the network to regress d, since not all output points are actually close to the annotated edges. Hence, we design a truncated regression loss:</p><formula xml:id="formula_6">L regr = 1 N 1≤i≤Ñ T b (d E (x i , E)) − T b (d i ) 2 ,<label>(4)</label></formula><p>where T b (x) = max(0, min(x, b)) is a piecewise linear function with parameter b. Empirically, we found the network training not sensitive to b, and set it as 0.5.</p><p>End-to-end training. When training the network, we minimize the combined edge-aware joint loss function below with balancing weights α and β:</p><formula xml:id="formula_7">L = L surf + L repl + αL edge + βL regr .<label>(5)</label></formula><p>In our implementation, we set α and β as 0.1 and 0.01, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation Details</head><p>Network training. Before the training, each input patch is normalized to fit in [−1, 1] 3 . Then, we augment each patch on-the-fly in the network via a series of operators: a random rotation, a random translation in all dimensions by -0.2 to 0.2, a random scaling by 0.8 to 1.2, adding Gaussian noise to the patch points with σ set as 0.5% of the patch bounding box size, and randomly shuffling the ordering of points in the patch. We implement our method using TensorFlow, and train the network for 200 epochs using the Adam <ref type="bibr" target="#b44">[46]</ref> optimizer with a minibatch size of 12 and a learning rate of 0.001. In our experiments, the default upsampling rate r is set as 4. For threshold ∆ d , we empirically set it as 0.15, since it is not sensitive to slight variation in our experiments. Overall, it took around 5 hours to train the network on an NVidia TITAN Xp GPU.</p><p>Network inference. We apply a trained network to process point clouds also in a patch-wise manner. To do so, we first find a subset of points in a test point cloud, and take them as centroids to extract patches of points using the procedure in Sec. 2.1. For the patches to distribute more evenly over the point cloud (say with N pt points), we use farthest point sampling to randomly find M = β Npt N points in the test point cloud with parameter β, which is empirically set as three. Hence, each point in the point cloud should appear roughly in β different patches on average. After extracting the patches, we feed them into the network and apply the network to produce 3D coordinates and point-to-edge distances, as well as to identify edge points (see Sec. 2.3). Unlike the training phase, we set a smaller ∆ d , which is 0.05. We use a larger ∆ d in the training because training is an optimization process, where we want the network to consider more points to learn to identify the points near the edges.</p><p>Surface reconstruction. First, we build a k-nn graph for the output points from network. Then, we filter edge points by fitting line segments using RANSAC, and filter surface points (not near edges points) by finding small groups of nearby points in the k-nn graph in an edge-stopping manner and fitting planes using PCA. Edge stopping means we stop the breath-first growth at edge points; this avoids including irrelevant points beyond the edges. These steps are iterated several times. Lastly, we fill the tiny gap between edge and surface points by including some original points in the gap, and by applying dart throwing to add new points. To further reconstruct the surface, we follow the procedure in EAR <ref type="bibr" target="#b19">[20]</ref> to downsample the point set and compute normals, use ball pivoting <ref type="bibr" target="#b45">[47]</ref> or screened Poisson surface reconstruction <ref type="bibr" target="#b46">[48]</ref> to reconstruct the surface, and use a bilateral normal filtering <ref type="bibr" target="#b47">[49]</ref> to clean the resulting mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset overview. Since most models in <ref type="bibr" target="#b6">[7]</ref> are manifolds without sharp edges, we collected 24 CAD models and 12 everyday objects as our training data set, and manually annotate sharp edges on them; see supplemental material. Then, we randomly crop 2,400 patches from the models (see <ref type="figure" target="#fig_1">Figure 2</ref>) to train our network; see the procedure in Sec. 2.1. To perform the experiments presented in this section, we do not reuse the models in the training data set but download additional 3D models from ShapeNet <ref type="bibr" target="#b42">[43]</ref>. For each testing model, we also use the procedure in Sec. 2.1 to generate the virtual scanned point clouds as input. Surface reconstruction results. We demonstrate the quality of our method by applying it to consolidate point sets and reconstruct surfaces. <ref type="figure" target="#fig_4">Figure 5(a)</ref> shows the input point clouds generated from the testing models shown in <ref type="figure" target="#fig_4">Figure 5</ref> To reconstruct the surfaces, we follow the procedure in Sec. 2.4 and employ the ball pivoting algorithm (BPA) <ref type="bibr" target="#b45">[47]</ref> to reconstruct the surfaces. To show the effect of our network, we apply the same procedure with BPA to reconstruct surfaces directly from (i) the input point clouds (see <ref type="figure" target="#fig_4">Figure 5</ref>(d)), and from (ii) the input point clouds with PCA plane fitting for denoising (see <ref type="figure" target="#fig_4">Figure 5</ref>(e)), without using our network to process the point clouds.</p><p>Comparing the resulting reconstructed surfaces with the ground truth meshes shown in <ref type="figure" target="#fig_4">Figure 5</ref>(f), we can see that our method achieves the best visual quality, particularly on preserving the edges. In addition, we compute the mean Hausdorff distance between the reconstructed surfaces and their corresponding ground truth surfaces; see the mean errors shown in <ref type="figure" target="#fig_4">Figure 5</ref>(c), (d) &amp; (e); our method consistently has the lowest mean errors among all the testing models. These quantitative results, together with the visual comparison, give evidence that our method produces consolidated point sets and improves the surface reconstruction quality. Note that without the knowledge of edges learnt and recognized by the network, using PCA alone to denoise point clouds is not edge-aware; the sharp edges would be wiped away, if we directly apply PCA to the raw point cloud,  leading to the inferior surface reconstructions shown in <ref type="figure" target="#fig_4">Figure 5</ref>(e). To sum up, our consolidation facilitates high-quality reconstruction not only on sharp objects but also on usual smooth objects. It is worth to note that with our consolidation, the overall reconstruction quality also improves over the state-of-the-art surface reconstruction methods on the benchmark models in <ref type="bibr" target="#b48">[50]</ref>; due to page limit, please see the supplemental material for more details.</p><p>Comparing with other methods. In the experiment, we compare our method with state-of-the-art methods, EAR <ref type="bibr" target="#b19">[20]</ref>, CLOP <ref type="bibr" target="#b20">[21]</ref>, GPF <ref type="bibr" target="#b49">[51]</ref>, and PU-Net <ref type="bibr" target="#b6">[7]</ref>, by applying them to process and consolidate point clouds before the screened Poisson surface reconstruction <ref type="bibr" target="#b46">[48]</ref>. As for PU-Net, we train a new model using our training objects and code released by the author. For better comparison, we also apply patch-based manner in testing phase, as this can achieve better results. <ref type="figure" target="#fig_6">Figures 6(b)</ref>, (c) &amp; (d) present the comparison with EAR and PU-Net. We can see from the results that the sharp edges in the original mesh are mostly smoothed out if we take the point clouds from PU-Net for surface construction. EAR better preserves the sharp features on the reconstructed surfaces, but in case of severe noise or under-sampling, it may over-smooth the geometric details and sharp features, or over-sharpen the edges. It is because the method depends on the quality of the estimated normals; see the limitation paragraph in <ref type="bibr" target="#b19">[20]</ref>. Our method, which is based on an edge-aware neural network model, is able to learn and capture edge information with high learning generalization, our point cloud consolidation results can lead to surface reconstructions that are closer to the original meshes. <ref type="figure" target="#fig_7">Figure 7</ref> also demonstrates that our method helps enhance the Poisson reconstruction quality on real scans in terms of preserving sharp edges compared with CLOP <ref type="bibr" target="#b20">[21]</ref> and GPF <ref type="bibr" target="#b49">[51]</ref>.</p><p>We also quantitatively compare with EAR and PU-Net by calculating the minimum distances from output points to the associated original mesh (as ground truth) in the test dataset. <ref type="table" target="#tab_0">Table 1</ref> lists the mean and root mean square (RMS) values of different methods on the test models; see supplemental material for visual comparison. We can see from the table that points generated from our method are closer to the original meshes compared to others. The PU-Net uses the EMD loss to encourage output points to be located on the underlying surfaces, so this comparison shows that the results are sub-optimal compared with our method, which directly minimizes the distances from output points to surfaces.</p><p>Our results under varying quantization noise. In real scans, the acquired depth values are generally quantized nonlinearly depending on the distance from the scanner. In short, objects far from the scanner would have less precise depth values in fewer quantization levels. During the virtual scanning process, we mimic real scans by quantizing the depth values in N q levels. This means that there are only N q unique depth values over all the foreground pixels. In this subsection, we investigate the robustness of our method under different amount of quantization noise by producing three sets of point cloud data using virtual scan on a testing model with N q =120 (low noise), N q =80 (medium noise), and N q =50 (high noise) and then applying our method to consolidate the resulting point clouds. <ref type="figure" target="#fig_8">Figure 8</ref> presents the results, where the left, middle and right columns correspond to low, medium and high noise levels, respectively. From the blown-up views, we can see more points above the object surface for the point cloud with high noise. The statistics about the point-to-surface distances also reveal the noise level; see the blue plots in the 4th row. After the consolidation, the points are steered to the left (see the red plots in 4th row), meaning that the points are now all closer to the ground truth surface under different noise levels. Similar pattern can also be observed from the statistical results for the point-to-edge distances.</p><p>Results on real scans. We also apply our method to point clouds produced from real scans downloaded from Aim@Shape and obtained from the EAR project <ref type="bibr" target="#b19">[20]</ref>. <ref type="figure" target="#fig_7">Figure 7</ref> has shown some results on real scans, and <ref type="figure" target="#fig_9">Figure 9</ref> shows more consolidation and reconstruction results. As we can see, real scan point clouds are often noisy and have inhomogeneous point distribution. Comparing with the input point clouds, our method is still able to generate more points near the edges and on the surface, while better preserving the sharp features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and future works</head><p>We presented EC-Net, the first edge-aware network for consolidating point clouds. The network was trained on synthetic data and tested on both virtual and real data. To be edge-aware, we design a joint loss function to identify points along edges, and to encourage points to be located close to the underlying surface and edges, as well as being more evenly distributed over surface. We compared our method with various state-of-the-art methods for point cloud consolidation, showing improvement in the 3D reconstruction quality, especially at sharp features. Our method has a number of limitations. First, it is not designed for completion, so filling large holes and missing parts would be a separate and different problem. In future, we plan to investigate how to enhance the network and the training process for point cloud completion. For tiny structures that are severely undersampled, our network may not be able to reconstruct the sharp edges around them. With insufficient points, the patch could become too large compared to the tiny structure. Moreover, our current implementation considers patches with a fixed number of points, and it cannot adapt structures of varying scales. It would be an interesting problem to try to dynamically control the patch size and explore the use of larger context in the training and inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Given a point cloud (a) with noisy samples in inhomogeneous distribution, our method consolidates it and reconstructs a plausible surface (b). Compared with PU-Net (c), our method is edge-aware and can preserve sharp features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The pipeline of EC-Net. For each point in an input patch, we first encode its local geometry into a feature vector f (size: N × D) using PointNet++, and expand f into f ′ (size: rN × D 2 ) using a feature expansion mechanism. Then, we regress the residual point coordinates and also the point-to-edge distances (d) from the expanded features, and form the output point coordinates by adding the original point coordinates to the residual. Finally, the network identifies points on edges and yields output points. The network was trained with an edge-aware joint loss function that has four terms; see the yellow boxes on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Example annotated edges (in red) on some of our collected 3D meshes (a). Example point clouds produced from our virtual 3D scans (b). The point density varies and the zoom-in windows also reveal the synthetic noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Procedure to extract patches (a local group of points) from a point cloud; note that (a) and (b) are available only in the training (but not inference) phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Our method produces consolidated point clouds (b) that lead to higher quality surface reconstruction results (c). Predicted edge points are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(f), while Figure 5(b) &amp; (c) show our results (see supplemental material for additional reconstruction results).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Comparing surface reconstruction results produced by using PU-Net (b), EAR (c), and our method (d) to consolidate points with the original meshes (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Comparing reconstruction results on real scans produced by direct reconstruction (b) and with consolidation: CLOP, GPF, and our method (c-e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Results of using our method to consolidate point clouds with different amount of quantization noise. Top row: the testing Monitor model; 2nd and 3rd rows: blown-up views of the input point clouds with different noise level (low to high) and our consolidated results, respectively; 4th and 5th rows: statistics of point-to-surface and point-to-edge distances, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Three results from real scans (a). Note the diversity of the geometry, the sparseness of the inputs (a), and how well the network locates the edges (in red); see (b). The reconstruction results (c) are yet imperfect due to tiny regions that are severely undersampled (see the blown-up views in (a)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison: our method, PU-Net, and EAR.</figDesc><table>Model 
Mean (10 
−3 ) 
RMS (10 
−3 ) 
Our PU-Net EAR Our PU-Net EAR 
Camera 1.31 1.91 2.43 1.99 2.74 3.75 
Sofa 
1.72 3.20 1.56 2.40 4.68 2.87 
Chair 
0.88 1.70 1.93 1.27 2.50 3.54 
Fandisk 1.09 2.86 2.33 1.77 4.50 5.63 
Switch-pot 1.36 2.00 1.76 1.83 3.07 2.44 
Headphone 0.81 1.83 3.71 1.19 2.89 6.93 
Table 
1.15 2.14 2.74 1.62 3.12 5.34 
Monitor 0.61 2.01 2.58 0.97 2.71 5.73 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">⋆ indicates equal contributions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://yulequan.github.io/ec-net/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank anonymous reviewers for the comments and suggestions. The work is supported by the Research Grants Council of the Hong Kong Special Administrative Region (Project no. GRF 14225616), the Shenzhen Science and Technology Program (No. JCYJ20170413162617606 and No. JCYJ20160429190300857), and the CUHK strategic recruitment fund.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EC-Net</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computing and rendering point set surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. &amp; Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surface reconstruction using local shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno>EPFL- CONF-149318</idno>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data-driven structural priors for shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-driven shape analysis and processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="132" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Surface reconstruction with datadriven exemplar priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Remil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Design</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04954</idno>
		<title level="m">PCPNet: Learning local shape properties from raw point clouds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06761</idno>
		<title level="m">PU-Net: Point cloud upsampling network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AtlasNet: A papiermâché approach to learning 3D surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PointNet: deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Surface reconstruction from unorganized points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zippered polygon meshes from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new voronoi-based surface reconstruction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Amenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamvysselis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of surface reconstruction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Seversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="301" to="329" />
			<date type="published" when="2017" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameterization-free projection for geometry reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tal-Ezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Consolidation of unorganized point clouds for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="176" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape modeling with point-sampled geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kobbelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="641" to="650" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time point cloud refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Symposium on Point Based Graphics</title>
		<imprint>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust moving least-squares fitting with sharp features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="544" to="552" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature preserving point set surfaces based on non-linear kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="493" to="501" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Eurographics)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Edge-aware point set resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Continuous projection for fast l1 reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Preiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mattausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pajarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D mesh labeling via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning for robust normal estimation in unstructured point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (SGP)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shape completion using 3D-encoder-predictor CNNs and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">O-CNN: Octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6620" to="6629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3DCNN-DQN-RNN: a deep reinforcement learning framework for semantic parsing of large-scale 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5678" to="5687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PointNet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Point-wise convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Escape from cells: deep Kd-Networks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09869</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">PointFusion: Deep sensor fusion for 3D bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10871</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08588</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FoldingNet: Interpretable unsupervised learning on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
		<title level="m">Frustum PointNets for 3D object detection from RGB-D data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">PointCNN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph CNN for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning efficient point cloud generation for dense 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: an information-rich 3D model repository</title>
		<meeting><address><addrLine>Eberly, D</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Distance between point and line, ray, or line segment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eberly</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The ballpivoting algorithm for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mittleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. &amp; Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="359" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bilateral normal filtering for mesh denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K C</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. &amp; Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1521" to="1530" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A benchmark for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GPF: GMM-inspired feature-preserving point set filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. &amp; Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2315" to="2326" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
