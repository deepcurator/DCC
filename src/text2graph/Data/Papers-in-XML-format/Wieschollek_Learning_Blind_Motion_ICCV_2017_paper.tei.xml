<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Blind Motion Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Blind Motion Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>previous methods our result <ref type="figure">Figure 1</ref>. From a sequence of blurry inputs (lower row) our learning-based approach for blind burst-deblurring reconstructs fine details, which are not recovered by recent state-of-the-art methods like FBA <ref type="bibr" target="#b5">[6]</ref>. Both methods feature similar run-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>As </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Videos captured by handheld devices usually contain motion blur artifacts caused by a combination of camera shake (ego-motion) and dynamic scene content (object motion). With a fixed exposure time any movement during recording causes the sensor to observe an averaged signal from different points in the scene. A reconstruction of the sharp frame from a blurry observation is a highly ill-posed problem, denoted as blind or non-blind deconvolution depending on whether camera-shake information is known or not. In video and image burst deblurring the reconstruction process for a single frame can make use of additional data from neighboring frames. However, the problem is still challenging as each frame might encounter a different camera shake and the frames might not be aligned. For deconvolution of a static scene neural networks have been successfully applied using single frame <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> and multi-frame deblurring <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b4">5]</ref>. All recent network architectures for multi-frame and video deblurring <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref> require the input to match a fixed temporal and spatial size. Handling arbitrary spatial dimensions is theoretically possible by fully convolutional networks as done in <ref type="bibr" target="#b23">[24]</ref>, but they rely on a sliding window approach during inference due to limited memory on the GPU. For these approaches, the reconstruction of one frame is not possible by aggregating the information of longer sequences than the network was trained for. In contrast, our approach is a deblurring system that can deal with arbitrary lengths of sequences while featuring a fully convolutional network that can process full resolution video frames at once. Due to its small memory footprint it removes the need for sliding window approaches during inference, thus drastically accelerating the deblurring process. For processing arbitrary sequences we rely on a recurrent scheme. While convolutional LSTMs <ref type="bibr" target="#b17">[18]</ref> offer a straight-forward way to replace spatial convolutions in conventional architectures by recurrent units, we found them challenging and slow to train. Besides vanishing gradients effects they require a bag of tricks like carefully tuned gradient clipping parameters and a special variant of batch normalization. In order to circumvent these problems, we introduce a new recurrent encoder-decoder network. In the network we incorporate spatial residual connections and introduce novel temporal feature transfer between subsequent iterations. Besides the network architecture, we further create a novel training set for video deblurring as the success of datadriven approaches heavily depends on the amount and quality of available realistic training examples. As acquiring realistic ground-truth data is time-consuming, we successfully generate synthetic training data with literally no acquisition cost and demonstrate improved results and run time on various benchmark sets as for example demonstrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The problem of image deblurring can be formulated as a non-blind or a blind deconvolution version, depending on whether information about the blur is available or not. Blind image deblurring (BD) is quite common in real-world applications and has seen considerable progress in the last decade. A comprehensive review is provided in the recent overview article by Wang and Tao <ref type="bibr" target="#b28">[29]</ref>. Traditional stateof-the-art methods such as Sun et al. <ref type="bibr" target="#b25">[26]</ref> or Michaeli and Irani <ref type="bibr" target="#b15">[16]</ref> use carefully chosen patch-based priors for sharp image prediction. Data-driven methods based on neural networks have demonstrated success in non-blind restoration tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> as well as for the more challenging task of BD where the blur kernel is unknown <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>. Removing the blur from moving objects has been recently addressed in <ref type="bibr" target="#b16">[17]</ref>. To alleviate the ill-posedness of the problem <ref type="bibr" target="#b6">[7]</ref>, one might take multiple observations into account. Hereby, observations of a static scene, each of which is differently blurred, serve as inputs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9]</ref>. To incorporate video properties such as temporal consistency the methods of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref> use powerful and flexible generative models to explicitly estimate the unknown blur along with predicting the latent sharp image. However, this comes at the price of higher computation cost, which typically requires tens of minutes for the restoration process. To accomplish faster processing times Delbracio and Sapiro <ref type="bibr" target="#b5">[6]</ref> have presented a clever way to average a sequence of input frames based on Lucky Imaging methods. They propose to compute a weighted combination of all aligned input frames in the Fourier domain which favors stable Fourier coefficients in the burst containing sharp information. This yields much faster processing times and removes the requirement to compute the blur kernel explicitly.</p><p>Quite recently, Wieschollek et al. in <ref type="bibr" target="#b29">[30]</ref> introduce an endto-end trainable neural network architecture for multi-frame deblurring. Their approach directly computes a sharp image by processing the input burst in a patch-wise fashion, yielding state-of-the-art results. It has been shown, that this even enables treating spatially varying blur. The related task of deblurring of videos has been approached by Su et al. <ref type="bibr" target="#b23">[24]</ref>. Their approach uses the U-Net architecture <ref type="bibr" target="#b18">[19]</ref> with skip connection to directly regress the sharp image from an input burst. Their fully convolutional neural network learns an average of multiple inputs with reasonable performance. Unfortunately, both learning methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24]</ref> require to fix the temporal input size at training time and they are limited to a patch-based inference by the network layout <ref type="bibr" target="#b29">[30]</ref> and memory constraints <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Overview. In our approach a fully-convolutional neural network deblurs a frame I using information from previous frames I −1 , I −2 , . . . in an iterative, recurrent fashion. Incorporating a previous (blurry) observation improves the current prediction for I step by step. We will refer to these steps as deblur steps. Hence, the complete recurrent deblur network (RDN) consists of several deblur blocks (DB). We use weight-sharing between these to reduce the total amount of used parameters and introduce novel temporal skip connections between these deblur blocks to propagate latent features between the individual temporal steps. To effectively update the network parameters we unroll these steps during training. At inference time, the inputs can have arbitrary spatial dimensions as long as the processing of a minimum of two frames fits on the GPU. Moreover, the recurrent structure allows us to include an arbitrary number of frames helping to improve the output with each iteration. Hence, there is no need for padding burst sequences to match the network architecture as e.g. in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating realistic ground-truth data</head><p>Training a neural network to predict a sharp frame of a blurry input requires realistic training data featuring these two aligned versions for each video frame: a blurry version serving as the input and an associated sharp version serving as ground-truth. Obtaining this data is challenging as any recorded sequence might suffer from the described blur effects itself. Recent work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref> have built a training data set by recording videos captured at 240fps with a GoPro Hero camera to minimize the blur in the groundtruth. Frames from these high-fps videos are then processed and averaged to produce plausible motion blur synthetically. While they made significant effort to capture a broad range of different situations, this process is limited in the number of recorded samples, in the variety of scenes and in the used recording devices. For fast moving objects artifacts <ref type="figure">Figure 2</ref>. Snapshot of the training process. Each triplet shows the input with synthetic blur (left), the current network prediction (middle) and the associated ground-truth (right). All images are best viewed at higher resolution in the electronic version.</p><p>are likely to arise due to the finite framerate. We also tested this method for generating training data with a GoPro Hero camera but found it hard to produce a large enough dataset of sharp ground-truth videos of high quality. Rather than acquiring training data manually we propose to acquire and filter data from online media.</p><p>Training data. As people love to share and rate multimedia content, each year millions of video clips are uploaded to online platforms like YouTube. The video content ranges from short clips to professional videos of up to 8k resolution. From this source, we have collected videos with 4k-8k resolution and a frame rate of 60fps or 30fps. The video content ranges from movie trailers, sports events, advertisements to videos on everyday life. To remove compression artifacts and to obtain slightly sharper ground-truth we resized all collected videos by factor 1 /4 respectively 1 /8, finally obtaining full-HD resolution. Consider such a video with frames (f t ) t=1,2,...,T . For each frame pair (f t , f t+1 ) at time t we compute n additional synthetical subframes between the original frames f t , f t+1 resulting in a high frame rate video</p><formula xml:id="formula_0">(. . . , f (n−1) t−1 , f (n) t−1 , f t , f (1) t , f (2) t , . . . , f (n−1) t , f (n) t , f t+1 ).</formula><p>All subframes are computed by blending between the neighboring original frames f t and f t+1 warping both frames using the optical flow in both directions w ft→ft+1 and w ft+1→ft . Given both flow fields, we can generate an arbitrary number of subframes. For practical purposes, we set n = 40, thus implying an effective framerate of more than 1000fps without suffering from low signal-to-noise ratio (SNR) due to short exposure times. We want to stress that only parts of videos with reasonably sharp frames serve as ground-truth. For those the estimation of optical flow to approximate motion blur is possible and sufficient. The sub-frames are averaged to generate a plausible blurry version</p><formula xml:id="formula_1">b t = 1 1 + 2L f t + L ℓ=1 f (n−ℓ) t−1 + f (ℓ) t<label>(1)</label></formula><p>for each sharp frame f t . We use a mix of 20 and 40 for L to create different levels of motion blur. The entire computation can be done offline on a GPU. For all video parts that passed our sharpness test (5.43 hours in total) we produce a ground-truth video and blurry version both at 30 fps in full-HD. Besides the unlimited amount of training data another major advantage of this method is that it incorporates different capturing devices naturally. Further, the massive amount of available video content allows us to tweak all thresholds and parameters in a conservative way to reject video parts of bad quality (too dark, too static) without affecting the effective size of the training data. Though the recovered optical flow is not perfect we observed an acceptable quality of the synthetically motion blurred dataset. To add variety to the training data we crop random parts from the frames and resize them to 128×128px. <ref type="figure">Figure 2</ref> shows a few random examples from our training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Handling the time dimension</head><p>The typical input shape required by CNNs in computer vision tasks is [B, H, W, C] -batch size, height, width and number of channels. However, processing series of images includes a new dimension: time. To apply spatial convolution layers the additional dimension has to be "merged" either into the channel</p><formula xml:id="formula_2">[B, H, W, C · T ] or batch dimen- sion [B · T, H, W, C].</formula><p>Methods like <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> stack the time along the channel dimension rendering all information across the entire burst available without further modification. This comes at the price of removing information about the temporal order. Further, the number of input frames needs to be fixed before training, which limits their application. Longer sequences could only be processed with workarounds like padding and sliding window processing. On the other hand, merging the time-dimension into the batch dimension would give flexibility at processing different length of sequences. But the processing of each frame is then entirely decoupled from its adjacent frames -no information is propagated. Architectures using convLSTM <ref type="bibr" target="#b17">[18]</ref> or convGRU cells <ref type="bibr" target="#b3">[4]</ref> are designed to naturally handle time series but they would require several tricks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref> during training. We tried several architectures based on these recurrent cells but found them hard to train and observed hardly any improvement even after two days of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>Instead of including recurrent layers, we propose to formulate the entire network as a recurrent application of deblur blocks and successively process pairs of inputs (target frame and additional observation), which gives us the flexibility to handle arbitrary sequence lengths and enables information fusion inside the network. Consider a single deblur step with the current prediction I of shape [H, W, C] and blurry observation I −k . Inspired by the work of Ronneberger et al. <ref type="bibr" target="#b18">[19]</ref> and the recent success of residual connections <ref type="bibr" target="#b7">[8]</ref> we use an encoder-decoder architecture in each deblur block, see <ref type="figure" target="#fig_0">Figure 3</ref>. Hereby, the network only consists of convolution and transposeconvolution layers with batchnorm <ref type="bibr" target="#b10">[11]</ref>. We applied the ReLU activation to the input of the convolution layers C ·,· as proposed in <ref type="bibr" target="#b7">[8]</ref>.</p><p>The first trainable convolution layer expands the 6-channel input (two 128×128px RGB images during training) into 64 channels. In the encoder part, each residual block consists of a down-sampling convolution layer ( ) followed by three convolution layers ( ). The down-sampling layer halves the spatial dimension with stride 2 and doubles the effective number of channels [H,</p><formula xml:id="formula_3">W, C] → [H/2, W/2, C · 2].</formula><p>During the decoding step, the transposed-convolution layer ( ) inverts the effect of the downsampling [H,</p><formula xml:id="formula_4">W, C] → [2 · H, 2 · W, C/2].</formula><p>We use a filter size of 3 × 3 / 4 × 4 for all convolution/transposed-convolution layers. In the beginning an additional residual block without downsampling accounts for resolving larger blur by providing a larger receptive field.</p><p>To speed up the training process, we add skip-connections between the encoding and decoding part. Hereby, we add the extracted features from the encoder to the related decoder part. This enables the network to learn a residual between the blurry input and the sharp ground-truth rather than ultimately generating a sharp image from scratch. Hence, the network is fully-convolutional and therefore allows for arbitrary input sizes. Please refer to <ref type="table" target="#tab_0">Table 1</ref> for more details. </p><formula xml:id="formula_5">A 0,1 3 × 3 × 64 2 H /1 × H /1 × 64 C 1,1 3 × 3 × 64 2 H /2 × H /2 × 64 C 1,2 −C 1,4 3 × 3 × 64 1 H /2 × H /2 × 64 C 2,1 −C 2,4 3 × 3 × 64 1 H /2 × H /2 × 64 C 3,1 3 × 3 × 128 2 H /4 × H /4 × 128 C 3,1 −C 3,4 3 × 3 × 128 1 H /4 × H /4 × 128 C 4,1 3 × 3 × 256 2 H /8 × H /8 × 256* B 4,2 1 × 1 × 256 1 H /8 × H /8 × 256 C 4,3 −C 4,5 3 × 3 × 256 1 H /8 × H /8 × 256 C 5,1 4 × 4 × 128 1 /2 H /4 × H /4 × 128* B 5,2 1 × 1 × 128 1 H /4 × H /4 × 128 C 5,3 −C 5,5 3 × 3 × 128 1 H /4 × H /4 × 128 C 6,1 4 × 4 × 64 1 /2 H /2 × H /2 × 64* B 6,2 1 × 1 × 64 1 H /2 × H /2 × 64 C 6,3 −C 6,5 3 × 3 × 64 1 H /2 × H /2 × 64 C 7,1 4 × 4 × 64 1 /2 H /1 × H /1 × 64 C 7,2 4 × 4 × 6 1 H /1 × H /1 × 6 I (k) 3 × 3 × 3 1 H /1 × H /1 × 3</formula><p>Skip connections as temporal links. We also propose to propagate latent features between subsequent deblur blocks over time. For this, we concatenate specific layer activations from a previous iteration with some from the current deblur block. These skip connections are illustrated as green lines in <ref type="figure" target="#fig_0">Figure 3</ref>. Further, to reduce the channel dimension to match the required input shape for the next layer, we use a 1 × 1 convolution layer, denoted as blending layer B ·,· .</p><p>This way the network can learn a weighted sum by blending between the current features and propagated features from the previous iteration. This effectively halves the channel dimension. One advantage of such a construction is that we can disable these skip connections in the first deblur block and only apply these in subsequent iterations. Further, they can be applied to a pre-trained model without temporal skip connections.</p><p>Training details. Aligning inputs using homography matrices or estimated optical flow information can be errorprone and slows down the reconstruction preventing timecritical applications. Therefore, we trained the network directly on a sequence of unaligned frames featuring large camera shakes. To further challenge the network we add artificial camera shake to each blurry frame from synthetic PSF kernels on-the-fly. These PSF kernels of sizes 7 × 7, 11 × 11, 15 × 15 are generated by a Gaussian process simulating camera shake. To account for the effect of vanishing gradients, we force the output I (k) of each deblur block to match the sharp ground-truth I (gt) in the corresponding loss term L k (see <ref type="figure" target="#fig_0">Figure 3)</ref>. We use ADAM <ref type="bibr" target="#b13">[14]</ref> for minimzing the total loss L = 4 k=1 L k for sequences of 5 inputs. We leave the optimizer's default parameters (β 1 = 0.9, β 2 = 0.999) unchanged and use 5e-3 as the initial learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the performance of our proposed method in several experiments on challenging real-world examples. In addition, a comprehensive comparison to recent methods is given using the implementation provided by the respective authors. During inference we pass a pair of frames with resolution 720p into a deblur block iteratively. Each iteration takes approximately 0.57 seconds on an NVIDIA Titan X. For any larger frame sizes, we tile the input frames. The network was trained exclusively on our synthetically blurred dataset featuring both motion blur and camera shake. All provided results in the section are based on benchmark sets from previous methods. Our recurrent deblur network (RDN) generalizes to different kinds of unseen videos and recording devices. Please note, we include the fullresolution images and frames from videos in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Burst Deblurring</head><p>In burst deblurring the task is to restore a sharp frame from an entire sequence of aligned images. The sequence is usually taken by a single camera and only suffers from stationary blur caused by ego-motion. In our data-driven approach, we process each observation which finally produces significantly better results than previous proposed methods, e.g. consider the scene provided in <ref type="figure" target="#fig_2">Figure 5</ref>. Notably, ours is the first, which is able to restore the lettering below the license plate in <ref type="figure">Figure 8</ref>. Also for the wood scene in <ref type="figure">Figure 9</ref> sharper results are produced.</p><p>random shot FourierNet FBA RDN (ours) <ref type="figure">Figure 8</ref>. In contrast to previous state-of-the-art methods, our recurrent approach is able to even recover the subtle writing on the bottom of this number plate. It further reflects the original color tones from the random blurry shot.</p><p>Further, our network is applied to input images featuring spatially varying blur, which is quite common in real-world examples due to imperfect lenses or turbulences. <ref type="figure" target="#fig_1">Figure 4</ref> shows a comparison to the Efficient Filter Flow framework (EFF) <ref type="bibr" target="#b8">[9]</ref> which is explicitly designed to model this kind of blur. The results demonstrate two features of our approach: it is able to generalize to this kind of blur -no patch-wise processing as in <ref type="bibr" target="#b29">[30]</ref> was necessary -and due to its recurrent nature it can deal and exploit almost arbitrary many frames for deblurring one image. However, we observe in the top row of <ref type="figure" target="#fig_1">Figure 4</ref> that after adding more than 10 input images local contrast might saturate, potentially resulting also in a small color shift. A workaround might be some color-transfer method <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video Deblurring</head><p>In contrast to the previous task, videos are usually degraded by additional blur caused by object motion. Moreover, any deblurring approach has to solve the underlying frame alignment problem. Such an alignment step can be done offline, e.g. using a homography matrix or by estimating optical flow fields to warp the frames to the reference frame. While this kind of preprocessing delivers an easier task to the network, it might introduce artifacts which the network later has to account for. The approach of Su et al. <ref type="bibr" target="#b23">[24]</ref> (DBN) extensively use preprocessing for alignment and directly train their networks to solve both tasks: deblurring and removing artifacts. Our approach does not require any preprocessing and is therefore faster while producing comparable or better results. <ref type="figure">Figure 10</ref> shows a comparison between the DBN in <ref type="bibr" target="#b23">[24]</ref> and our network directly applied to the input. Significant improvement in sharpness by our method can be observed on the trousers, the hair of the woman or the hand of the baby, just to highlight a few. Artifacts due to the alignment procedure in the approach by Although the network has only seen training sequences of length 5, due to its recurrent structure it can handle longer sequences and further improve the prediction. However, too many input frames might introduce oversharpening. A random shot from the input is given on the left and the ground-truth on the right. We also compare against the EFF reconstructions from Hirsch et al. <ref type="bibr" target="#b8">[9]</ref>, which is dedicated to this task. Comparison to state-of-the-art multi-frame blind deconvolution algorithms FourierNet <ref type="bibr" target="#b29">[30]</ref>, FBA <ref type="bibr" target="#b5">[6]</ref> and ours (RDN) on realworld data for static scenes of low-light environments. RDN recovers significantly more detail. <ref type="figure">Figure 6</ref>. Visualization of features propagated along the temporal skip connections. The additional channels are projected to 2D and encoded in Hue colorspace. Apparently, the network learned to mark regions which might benefit from deblurring.</p><p>1 level 2 levels 3 levels 1 level 2 levels 3 levels 1 level 2 levels 3 levels <ref type="figure">Figure 7</ref>. Multi-scale input for large motion blur. We show the deblurred results with tradition single-scale input (1 level) or extending the input sequence with upscaled version of the deblurred results at half respectively quarter resolution.</p><p>where the white keys are distorted. While ours is competitive when removing small motion, their optical flow based methods produces slightly sharper results when the camera motion is severe as seen on the road markings in the in the "bicycle" scene. Due to the limited capacity of the trained networks neither their nor our approach is fully capable of recovering the strong motion blur of very fast motion.</p><p>Using multi-scale input. While our network has been trained on sequences of constant spatial resolution only, we experimented with feeding multi-scale input to recover strong object motion. In particular, we deblurred the entire input sequence at different levels n = 1, 2, 3 with 1 /2 n−1 resolution and then up-scaled the predicted result to obtain an additional new input frame for the sequence at the higher scale. While it partly helped to deal with larger motion blur which is not covered in the training data, the upsampling can produced artifacts which the network was not trained for. <ref type="figure">Figure 7</ref> shows such results. Although the bike became significant sharper, the static parts of the scene rendered a "comic style" appearance. Directly training such a multiscale network seems to be an interesting research direction.</p><p>Time-structure. Our network architecture consists of an "anti-causal" structure deblurring one frame by considering the original previous frames in a sequence-to-one mapping. I = DB(DB(I, I −1 ), . . .). We experimented with several sequence-to-sequence mapping approaches producing a sharp frame in an online wayÎ t = DB(I t ,Î t−1 ). We noticed no learning benefit which might be caused by the limited capability of propagating temporal information.  <ref type="figure">Figure 10</ref>. Video deblurring results. Applied to successive video frames with camera shake and motion blur our network produces favorable results compared to the approach by Su et al. <ref type="bibr" target="#b23">[24]</ref> (DBN). While their performance depends on the type of offline preprocessing to align the individual frames (plain, homography, optical flow) our method operates directly on the unaltered input frames. As the preprocessing might introduce artifacts such as blur, smearing etc. their network learned to partially correct those, which sometimes fails. The strong motion blur of the cyclist is not correctly deblurred by any existing method.  <ref type="figure">Figure 9</ref>. Performance of multi-frame blind deconvolution algorithms FourierNet <ref type="bibr" target="#b29">[30]</ref>, FBA <ref type="bibr" target="#b5">[6]</ref> and ours (RDN) on a forest scene. The structure of the leaves and the bark of the trees is significantly sharper.</p><p>Identifying valuable temporal information. One novel feature of our designed network architecture are the temporal skip connections <ref type="figure" target="#fig_0">(Figure 3</ref> in green) acting as information links between subsequent deblur blocks. As we do not add constraints to these links, we essentially allow the network to propagate whatever feature information seems to be beneficial for the next deblur block. To illustrate these temporal information, we visualized the respective layer activation in <ref type="figure">Figure 6</ref>. The illustration suggests that the network uses this opportunity to propagate image locations which might profit from further deblurring (yellowish parts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel recurrent network architecture -recurrent deblurring network (RDN) -for the efficient removal of blur caused by both ego and object motion from a sequence of unaligned blurry frames. Our proposed model enables fast processing of image sequences of arbitrary length and size. We introduce the concept of temporal skip connections between consecutive deblur blocks which allow for efficient information propagation across several time steps. Our proposed network iteratively improves the sharpness of a target frame given various blurry observations. Furthermore, we presented a novel method for the efficient generation of a vast number of blurry/sharp video sequence pairs, which is required to train learning based methods like the one we described. Using bidirectional optical flow between consecutive frames, our method creates synthetically intermediate frames to fake high-speed video recordings.</p><p>By averaging multiple consecutive frames we can emulate longer exposure times and thus motion blur in a realistic way. Hence, making use of the abundance of high-quality videos available on YouTube, we illustrated the generation process of an arbitrary amount of training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Given the current deblurred version of I each deblur block DB produces a sharper version of I using information contributed by another observation I −k . The deblur block follows the design of an encoder-decoder network with several residual blocks with skipconnections. To share learned features between various observations, we propagate some previous features into the current DB (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Recovery from image bursts with spatially varying blur. Reconstructions of a single image using 2 to 17 input frames are shown. Although the network has only seen training sequences of length 5, due to its recurrent structure it can handle longer sequences and further improve the prediction. However, too many input frames might introduce oversharpening. A random shot from the input is given on the left and the ground-truth on the right. We also compare against the EFF reconstructions from Hirsch et al. [9], which is dedicated to this task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison to state-of-the-art multi-frame blind deconvolution algorithms FourierNet [30], FBA [6] and ours (RDN) on realworld data for static scenes of low-light environments. RDN recovers significantly more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Network Specification. Outputs of layers marked with * are concatenated with features from previous deblur blocks except in the first step. This doubles the channel size of the output. The blending layers B·,· are only used after the first deblur step.</figDesc><table>layer 
filter size 
stride output shape 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP XX.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blind motion deblurring using multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational physics</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="5057" to="5071" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust dual motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Burst deblurring: Removing camera shake through fourier burst accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2385" to="2393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hand-held video deblurring via efficient fourier aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="270" to="283" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freeman. Time-constrained photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="333" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient filter flow for space-variant multiframe blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for direct text deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradiš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemcík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2015" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blurburst: Removing blur due to camera shake using multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using a locally adaptive linear blur model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalized recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Blind deblurring using internal patch recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="783" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01486</idno>
		<title level="m">Motion deblurring in the wild</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pȃtrȃucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<title level="m">Convolutional networks for biomedical image segmentation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The return of the gating network: Combining generative models and discriminative training in natural image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2665" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholköpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust multichannel blind deconvolution via fast alternating minimization. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1687" to="1700" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge-based blur kernel estimation using patch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference in Computational Photography (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marsik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemcik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07873</idno>
		<title level="m">Cnn for license plate motion deblurring</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6838</idno>
		<title level="m">Recent progress in image deblurring</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end learning for image burst deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-shot imaging: joint alignment, deblurring and resolution-enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2925" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-observation blind deconvolution with an adaptive sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1628" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intra-frame deblurring by leveraging inter-frame camera motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4036" to="4044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deconvolving psfs for a better motion deblurring using multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="636" to="647" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
