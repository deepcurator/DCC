<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Image Crowd Counting via Multi-Column Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
							<email>zhangyy2@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
							<email>zhouds@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
							<email>chensq@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
							<email>mayi@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghaitech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single-Image Crowd Counting via Multi-Column Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the new year eve of 2015, 35 people were killed in a massive stampede in Shanghai, China. Unfortunately, since then, many more massive stampedes have taken place around the world which have claimed many more victims. Accurately estimating crowds from images or videos has become an increasingly important application of computer vision technology for purposes of crowd control and public safety. In some scenarios, such as public rallies and sports events, the number or density of participating people is an essential piece of information for future event planning and space design. Good methods of crowd counting can also be extended to other domains, for instance, counting cells or bacteria from microscopic images, animal crowd estimates in wildlife sanctuaries, or estimating the number of vehicles at transportation hubs or traffic jams, etc.</p><p>Related work. Many algorithms have been proposed in the literature for crowd counting. Earlier methods <ref type="bibr" target="#b28">[29]</ref> adopt a detection-style framework that scans a detector over two consecutive frames of a video sequence to estimate the number of pedestrians, based on boosting appearance and motion features. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> have used a similar detectionbased framework for pedestrian counting. In detectionbased crowd counting methods, people typically assume a crowd is composed of individual entities which can be detected by some given detectors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>. The limitation of such detection-based methods is that occlusion among people in a clustered environment or in a very dense crowd significantly affects the performance of the detector hence the final estimation accuracy.</p><p>In counting crowds in videos, people have proposed to cluster trajectories of tracked visual features. For instance, <ref type="bibr" target="#b23">[24]</ref> has used highly parallelized version of the KLT tracker and agglomerative clustering to estimate the number of moving people. <ref type="bibr" target="#b2">[3]</ref> has tracked simple image features and probabilistically group them into clusters representing independently moving entities. However, such tracking-based methods do not work for estimating crowds from individual still images.</p><p>Arguably the most extensively used method for crowd counting is feature-based regression, see <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. The main steps of this kind of method are: 1) segmenting the foreground; 2) extracting various features from the foreground, such as area of crowd mask <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>, edge count <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref>, or texture features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref>; 3) utilizing a regression function to estimate the crowd count. Linear <ref type="bibr" target="#b22">[23]</ref> or piece-wise linear <ref type="bibr" target="#b24">[25]</ref> functions are relatively simple models and yield decent performance. Other more advanced/effective methods are ridge regression (RR) <ref type="bibr" target="#b6">[7]</ref>, Gaussian process regression (GPR) <ref type="bibr" target="#b3">[4]</ref>, and neural network (NN) <ref type="bibr" target="#b21">[22]</ref>.</p><p>There have also been some works focusing on crowd counting from still images. <ref type="bibr" target="#b11">[12]</ref> has proposed to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. In that work, a dataset of fifty crowd images containing 64K annotated humans (UCF CC 50) is introduced. <ref type="bibr" target="#b1">[2]</ref> has followed the work and estimated counts by fusing information from multiple sources, namely, interest points (SIFT), Fourier analysis, wavelet decomposition, GLCM features, and low confidence head detections. <ref type="bibr" target="#b27">[28]</ref> has utilized the features extracted from a pre-trained CNN to train a support vector machine (SVM) that subsequently generates counts for still images.</p><p>Recently Zhang et al. <ref type="bibr" target="#b32">[33]</ref> has proposed a CNN based method to count crowd in different scenes. They first pretrain a network for certain scenes. When a test image from a new scene is given, they choose similar training data to fine-tune the pretrained network based on the perspective information and similarity in density map. Their method demonstrates good performance on most existing datasets. But their method requires perspective maps both on training scenes and the test scene. Unfortunately, in many practical applications of crowd counting, the perspective maps are not readily available, which limits the applicability of such methods.</p><p>Contributions of this paper. In this paper, we aim to conduct accurate crowd counting from an arbitrary still image, with an arbitrary camera perspective and crowd density (see <ref type="figure" target="#fig_0">Figure 1</ref> for some typical examples). At first sight this seems to be a rather daunting task, since we obviously need to conquer series of challenges:</p><p>1. Foreground segmentation is indispensable in most existing work. However foreground segmentation is a challenging task all by itself and inaccurate segmentation will have irreversible bad effect on the final count. In our task, the viewpoint of an image can be arbitrary. Without information about scene geometry or motion, it is almost impossible to segment the crowd from its background accurately. Hence, we have to estimate the number of crowd without segmenting the foreground first.</p><p>2. The density and distribution of crowd vary significantly in our task (or datasets) and typically there are tremendous occlusions for most people in each image. Hence traditional detection-based methods do not work well on such images and situations.</p><p>3. As there might be significant variation in the scale of the people in the images, we need to utilize features at different scales all together in order to accurately estimate crowd counts for different images. Since we do not have tracked features and it is difficult to handcraft features for all different scales, we have to resort to methods that can automatically learn effective features.</p><p>To overcome above challenges, in this work, we propose a novel framework based on convolutional neural network (CNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> for crowd counting in an arbitrary still image. More specifically, we propose a multi-column convolutional neural network (MCNN) inspired by the work of <ref type="bibr" target="#b7">[8]</ref>, which has proposed multi-column deep neural networks for image classification. In their model, an arbitrary number of columns can be trained on inputs preprocessed in different ways. Then final predictions are obtained by averaging individual predictions of all deep neural networks. Our MCNN contains three columns of convolutional neural networks whose filters have different sizes. Input of the MCNN is the image, and its output is a crowd density map whose integral gives the overall crowd count. Contributions of this paper are summarized as follows:</p><p>1. The reason for us to adopt a multi-column architecture here is rather natural: the three columns correspond to filters with receptive fields of different sizes (large, medium, small) so that the features learned by each column CNN is adaptive to (hence the overall network is robust to) large variation in people/head size due to perspective effect or across different image resolutions.</p><p>2. In our MCNN, we replace the fully connected layer with a convolution layer whose filter size is 1 × 1. Therefore the input image of our model can be of arbitrary size to avoid distortion. The immediate output of the network is an estimate of the density of the crowd from which we derive the overall count.  <ref type="figure" target="#fig_0">Figure 1</ref> shows some representative samples of this dataset.</p><p>2. Multi-column CNN for Crowd Counting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Density map based crowd counting</head><p>To estimate the number of people in a given image via the Convolutional Neural Networks (CNNs), there are two natural configurations. One is a network whose input is the image and the output is the estimated head count. The other one is to output a density map of the crowd (say how many people per square meter), and then obtain the head count by integration. In this paper, we are in favor of the second choice for the following reasons:</p><p>1. Density map preserves more information. Compared to the total number of the crowd, density map gives the spatial distribution of the crowd in the given image, and such distribution information is useful in many applications. For example, if the density in a small region is much higher than that in other regions, it may indicate something abnormal happens there.</p><p>2. In learning the density map via a CNN, the learned filters are more adapted to heads of different sizes, hence more suitable for arbitrary inputs whose perspective effect varies significantly. Thus the filters are more semantic meaningful, and consequently improves the accuracy of crowd counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Density map via geometry-adaptive kernels</head><p>Since the CNN needs to be trained to estimate the crowd density map from an input image, the quality of density given in the training data very much determines the performance of our method. We first describe how to convert an image with labeled people heads to a map of crowd density.</p><p>If there is a head at pixel x i , we represent it as a delta function δ(x − x i ). Hence an image with N heads labeled can be represented as a function</p><formula xml:id="formula_0">H(x) = N ∑ i=1 δ(x − x i ).</formula><p>To convert this to a continuous density function, we may convolve this function with a Gaussian kernel <ref type="bibr" target="#b16">[17]</ref> G σ so that the density is F (x) = H(x) * G σ (x). However, such a density function assumes that these x i are independent samples in the image plane which is not the case here: In fact, each x i is a sample of the crowd density on the ground in the 3D scene and due to the perspective distortion, and the pixels associated with different samples x i correspond to areas of different sizes in the scene.</p><p>Therefore, to accurately estimate the crowd density F , we need to take into account the distortion caused by the homography between the ground plane and the image plane. Unfortunately, for the task (and datasets) at hand, we typically do not know the geometry of the scene. Nevertheless, if we assume around each head, the crowd is somewhat evenly distributed, then the average distance between the head and its nearest k neighbors (in the image) gives a reasonable estimate of the geometric distortion (caused by the perspective effect).</p><p>Therefore, we should determine the spread parameter σ based on the size of the head for each person within the image. However, in practice, it is almost impossible to accurately get the size of head due to the occlusion in many cases, and it is also difficult to find the underlying relationship between the head size the density map. Interesting we found that usually the head size is related to the distance between the centers of two neighboring persons in crowded scenes (please refer to <ref type="figure" target="#fig_1">Figure 2)</ref>. As a compromise, for the density maps of those crowded scenes, we propose to dataadaptively determine the spread parameter for each person based on its average distance to its neighbors. <ref type="bibr" target="#b0">1</ref> For each head x i in a given image, we denote the distances to its k nearest neighbors as {d</p><formula xml:id="formula_1">i 1 , d i 2 , . . . , d i m }. The average distance is therefored i = 1 m ∑ m j=1 d i j .</formula><p>Thus, the pixel associated with x i corresponds to an area on the ground in the scene roughly of a radius proportional tod i . Therefore, to estimate the crowd density around the pixel x i , we need to convolve δ(x − x i ) with a Gaussian kernel with variance σ i proportional tod i , More precisely, the density F should be</p><formula xml:id="formula_2">F (x) = N ∑ i=1 δ(x − x i ) * G σi (x), with σ i = βd i</formula><p>for some parameter β. In other words, we convolve the labels H with density kernels adaptive to the local geometry around each data point, referred to as geometry-adaptive kernels. In our experiment, we have found empirically β = 0.3 gives the best result. In <ref type="figure" target="#fig_1">Figure 2</ref>, we have shown so-obtained density maps of two exemplar images in our dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-column CNN for density map estimation</head><p>Due to perspective distortion, the images usually contain heads of very different sizes, hence filters with receptive fields of the same size are unlikely to capture characteristics of crowd density at different scales. Therefore, it is more natural to use filters with different sizes of local receptive field to learn the map from the raw pixels to the density maps. Motivated by the success of Multi-column Deep Neural Networks (MDNNs) <ref type="bibr" target="#b7">[8]</ref>, we propose to use a Multi-column CNN (MCNN) to learn the target density maps. In our MCNN, for each column, we use the filters of different sizes to model the density maps corresponding to heads of different scales. For instance, filters with larger receptive fields are more useful for modeling the density maps corresponding to larger heads.  <ref type="figure">Figure 3</ref>: The structure of the proposed multi-column convolutional neural network for crowd density map estimation.</p><p>The overall structure of our MCNN is illustrated in <ref type="figure">Figure 3</ref>. It contains three parallel CNNs whose filters are with local receptive fields of different sizes. For simplification, we use the same network structures for all columns (i.e., conv-pooling-conv-pooling) except for the sizes and numbers of filters. Max pooling is applied for each 2 × 2 region, and Rectified linear unit (ReLU) is adopted as the activation function because of its good performance for CNNs <ref type="bibr" target="#b31">[32]</ref>. To reduce the computational complexity (the number of parameters to be optimized), we use less number of filters for CNNs with larger filters. We stack the output feature maps of all CNNs and map them to a density map. To map the features maps to the density map, we adopt filters whose sizes are 1 × 1 <ref type="bibr" target="#b20">[21]</ref>. Then Euclidean distance is used to measure the difference between the estimated density map and ground truth. The loss function is defined as follows:</p><formula xml:id="formula_3">L(Θ) = 1 2N N ∑ i=1 ∥F (X i ; Θ) − F i ∥ 2 2 ,<label>(1)</label></formula><p>where Θ is a set of learnable parameters in the MCNN. N is the number of training image. X i is the input image and F i is the ground truth density map of image X i . F (X i ; Θ) stands for the estimated density map generated by MCNN which is parameterized with Θ for sample X i . L is the loss between estimated density map and the ground truth density map.</p><p>Remarks i) Since we use two layers of max pooling, the spatial resolution is reduced by Here we prefer the input images to be of their original sizes because resizing images to the same size will introduce additional distortion in the density map that is difficult to estimate. iii) Besides the fact that the filters have different sizes in our CNNs, another difference between our MCNN and conventional MDNNs is that we combine the outputs of all CNNs with learnable weights (i.e.,1×1 filters). In contrast, in MDNNs proposed by <ref type="bibr" target="#b7">[8]</ref>, the outputs are simply averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Optimization of MCNN</head><p>The loss function (1) can be optimized via batch-based stochastic gradient descent and backpropagation, typical for training neural networks. However, in reality, as the number of training samples are very limited, and the effect of gradient vanishing for deep neural networks, it is not easy to learn all the parameters simultaneously. Motivated by the success of pre-training of RBM <ref type="bibr" target="#b10">[11]</ref>, we pre-train CNN in each single column separately by directly mapping the outputs of the fourth convolutional layer to the density map. We then use these pre-trained CNNs to initialize CNNs in all columns and fine-tune all the parameters simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Transfer learning setting</head><p>One advantage of such a MCNN model for density estimation is that the filters are learned to model the density maps of heads with different sizes. Thus if the model is trained on a large dataset which contains heads of very different sizes, then the model can be easily adapted (or transferred) to another dataset whose crowd heads are of some particular sizes. If the target domain only contains a few training samples, we may simply fix the first several layers in each column in our MCNN, and only fine-tune the last few convolutional layers. There are two advantages for finetuning the last few layers in this case. Firstly, by fixing the first several layers, the knowledge learnt in the source domain can be preserved, and by fine-tuning the last few layers, the models can be adapted to the target domain. So the knowledge in both source domain and target domain can be integrated and help improve the accuracy. Secondly, comparing with fine-tuning the whole network, fine-tuning the last few layers greatly reduces the computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate our MCNN model on four different datasets -three existing datasets and our own dataset. Although comparing to most DNN based methods in the literature, the proposed MCNN model is not particularly deep nor sophisticated, it has nevertheless achieved competitive and often superior performance in all the datasets. In the end, we also demonstrate the generalizability of such a simple model in the transfer learning setting (as mentioned in section 2.5). Implementation of the proposed network and its training are based on the Caffe framework developed by <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation metric</head><p>By following the convention of existing works <ref type="bibr" target="#b27">[28]</ref> <ref type="bibr" target="#b32">[33]</ref> for crowd counting, we evaluate different methods with both the absolute error (MAE) and the mean squared error (MSE), which are defined as follows:</p><formula xml:id="formula_4">M AE = 1 N N ∑ 1 |z i −ẑ i |, M SE = 1 N N ∑ 1 (z i −ẑ i ) 2<label>(2)</label></formula><p>where N is the number of test images, z i is the actual number of people in the ith image, andẑ i is the estimated number of people in the ith image. Roughly speaking, M AE indicates the accuracy of the estimates, and M SE indicates the robustness of the estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Shanghaitech dataset</head><p>As exiting datasets are not entirely suitable for evaluation of the crowd count task considered in this work, we introduce a new large-scale crowd counting dataset named Shanghaitech which contains 1198 annotated images, with a total of 330,165 people with centers of their heads annotated. As far as we know, this dataset is the largest one in terms of the number of annotated people. This dataset consists of two parts: there are 482 images in Part A which are randomly crawled from the Internet, and 716 images in Part B which are taken from the busy streets of metropolitan areas in Shanghai. The crowd density varies significantly between the two subsets, making accurate estimation of the crowd more challenging than most existing datasets. Both Part A and Part B are divided into training and testing: 300 images of Part A are used for training and the remaining 182 images for testing;, and 400 images of Part B are for training and 316 for testing. Table1 gives the statistics of Shanghaitech dataset and its comparison with other datasets. We also give the crowd histograms of images in this dataset in <ref type="figure" target="#fig_3">Figure 4</ref>. If the work is accepted for publication, we will release the dataset, the annotations, as well as the training/testing protocol. To augment the training set for training the MCNN, we cropped 9 patches from each image at different locations, and each patch is 1/4 size of the original image. All the patches are used to train our MCNN model. For Part A, as the crowd density is usually very high, we use our geometry-adaptive kernels to generate the density maps, and the predicted density at overlapping region is calculated by averaging. For Part B, since the crowd is relatively sparse, we use the same spread in Gaussian kernel to generate the (ground truth) density maps. In our implementation, we first pre-train each column of MCNN independently. Then we fine-tune the whole network. <ref type="figure" target="#fig_4">Figure 5</ref> shows examples of ground truth density maps and estimated density maps of images in Part A.</p><p>We compare our method with the work of Zhang et al. <ref type="bibr" target="#b32">[33]</ref>, which also uses CNNs for crowd counting and achieved state-of-the-art accuracy at the time. Following the work of <ref type="bibr" target="#b32">[33]</ref>, we also compare our work with regression based method, which uses Local Binary Pattern (LBP) features extracted from the original image as input and uses ridge regression (RR) to predict the crowd number for each image. To extract LBP features, each image is uniformly   <ref type="table" target="#tab_3">Table 2</ref>.</p><p>The effect of pretraining in MCNN. We show the effect of our model without pretraining on Shanghaitech dataset Part A in <ref type="figure" target="#fig_5">Figure 6</ref>. We see that pretrained network outperforms the network without pretraining. The result verifies the necessity of pretraining for MCNN as optimization starting from random initialization tends to fall into local minima.</p><p>Single column CNNs vs MCNN. <ref type="figure" target="#fig_5">Figure 6</ref> shows the comparison of single column CNNs with MCNN on Shanghaitech dataset Part A. It can be seen that MCNNs significantly outperforms each single column CNN for both MAE and MSE. This verifies the effectiveness of the MCNN architecture. Comparison of different loss functions. We evaluate the performance of our framework with different loss functions. Other than mapping the images to their density maps, we can also map the images to the total head counts in the image directly. For the input image X i (i = 1, . . . , N ), its total head count is z i , and F (X i ; Θ) stands for the estimated density map and Θ is the parameters of MCNN. Then we arrive the following objective function:</p><formula xml:id="formula_5">L(Θ) = 1 2N N ∑ i=1 ∫ ∫ S F (X i ; Θ)dxdy − z i 2<label>(3)</label></formula><p>Here S stands for the spatial region of estimated density map, and ground truth of the density map is not used. For this loss, we also pretrain CNNs in each column separately. We call such a baseline as MCNN based crowd count regression (MCNN-CCR). Performance based on such loss function is listed in <ref type="table" target="#tab_3">Table 2</ref>, which is also compared with two existing methods as well as the method based on density map estimation (simply labeled as MCNN). We see that the results based on crowd count regression is rather poor. In a way, learning density map manages to preserve more information of the image, and subsequently helps improve the count accuracy.</p><p>In <ref type="figure" target="#fig_7">Figure 7</ref>, we compare the results of our method with those of Zhang et al. <ref type="bibr" target="#b32">[33]</ref> in more details. We group the test images in Part A and Part B into 10 groups according to crowd counts in an increasing order. We have 182+316 test images in Part A and Part B. Except for the 10th group which contains 20+37 images, other groups all have 18+31 images each. From the plots in the figure, we can see that our method is much more accurate and robust to large variation in crowd number/density. Absolute Counts   <ref type="bibr" target="#b32">[33]</ref> on Shanghaitech dataset: We evenly divided our test images into 10 groups according to increasing number of people. Absolute count in the vertical axis is the average crowd number of images in each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The UCF CC 50 dataset</head><p>The UCF CC 50 dataset is firstly introduced by H. Idrees et al. <ref type="bibr" target="#b11">[12]</ref>. This dataset contains 50 images from the Internet. It is a very challenging dataset, because of not only limited number of images, but also the crowd count of the image changes dramatically. The head counts range between 94 and 4543 with an average of 1280 individuals per image. The authors provided 63974 annotations in total for these fifty images. We perform 5-fold cross-validation by following the standard setting in <ref type="bibr" target="#b11">[12]</ref>. The same data augmentation approach as in that in Shanghaitech dataset.  <ref type="bibr" target="#b11">[12]</ref> 419.5 541.6 Zhang et al. <ref type="bibr" target="#b32">[33]</ref> 467.0 498.5 MCNN 377.6 509.1</p><p>We compare our method with four existing methods on UCF CC 50 dataset in <ref type="table" target="#tab_4">Table 3</ref>. Rodriguez et al. <ref type="bibr" target="#b25">[26]</ref> employs density map estimation to obtain better head detection results in crowd scenes. Lempitsky et al. <ref type="bibr" target="#b16">[17]</ref> adopts dense SIFT features on randomly selected patches and the MESA distance to learn a density regression model. The method presented in <ref type="bibr" target="#b11">[12]</ref> gets the crowd count estimation by using multi-source features. The work of Zhang et al. <ref type="bibr" target="#b32">[33]</ref> is based on crowd CNN model to estimate the crowd count of an image. Our method achieves the best MAE, and comparable MSE with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The UCSD dataset</head><p>We also evaluate our method on the UCSD dataset <ref type="bibr" target="#b3">[4]</ref>. This dataset contains 2000 frames chosen from one surveillance camera in the UCSD campus. The frame size is 158 × 238 and it is recoded at 10 fps. There are only about 25 persons on average in each frame (Please refer to <ref type="table" target="#tab_2">Table  1</ref>) The dataset provides the ROI for each video frame.</p><p>By following the same setting with <ref type="bibr" target="#b3">[4]</ref>, we use frames from 601 to 1400 as training data, and the remaining 1200 frames are used as test data. This dataset does not satisfy assumptions that the crowd is evenly distributed. So we fix the σ of the density map. The intensities of pixels out of ROI is set to zero, and we also use ROI to revise the last convolution layer. <ref type="table" target="#tab_5">Table 4</ref> shows the results of our method and other methods on this dataset. The proposed MCNN model outperforms both the foreground segmentation based methods and CNN based method <ref type="bibr" target="#b32">[33]</ref>. This indicates that our model can estimate not only images with extremely dense crowds but also images with relative sparse people. WorldExpo'10 crowd counting dataset was firstly introduced by Zhang et al. <ref type="bibr" target="#b32">[33]</ref>. This dataset contains 1132 annotated video sequences which are captured by 108 surveillance cameras, all from Shanghai 2010 WorldExpo. The authors of <ref type="bibr" target="#b32">[33]</ref> provided a total of 199,923 annotated pedestrians at the centers of their heads in 3980 frames. 3380 frames are used in training data. Testing dataset includes five different video sequences, and each video sequence contains 120 labeled frames. Five different regions of interest (ROI) are provided for the test scenes.</p><p>In this dataset, the perspective maps are given. For fair comparison, we followed the work of <ref type="bibr" target="#b32">[33]</ref>, generated the density map according to perspective map with the relation σ = 0.2 * M (x), M (x) denotes that the number of pixels in the image representing one square meter at that location.</p><p>To be consistent with <ref type="bibr" target="#b32">[33]</ref>, only ROI regions are considered in each test scene. So we modify the last convolution layer based on the ROI mask, namely, setting the neuron corresponding to the area out of ROI to zero. We use the same evaluation metric (MAE) suggested by the author of <ref type="bibr" target="#b32">[33]</ref>. <ref type="table" target="#tab_6">Table 5</ref> reports the results of different methods in the five test video sequences. Our method also achieves better performance than Fine-tuned Crowd CNN model <ref type="bibr" target="#b32">[33]</ref> in terms of average MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Evaluation on transfer learning</head><p>To demonstrate the generalizability of the learned model in our method, we test our method in the transfer learning setting by using the Part A of Shanghaitech dataset as the source domain and using the UCF CC 50 dataset as the target domain. Specifically, we train a MCNNs model with data in the source domain. For the crowd counting task in the target domain, we conduct two settings, i.e., (i) no training samples in the target domain, and (ii) There are only a few samples in the target domain. For case (i), we directly use our model trained on Part A of Shanghaitech dataset for evaluation. For case (ii), we use the training samples in the target domain to fine-tune the network. The performance of different settings is reported in <ref type="table" target="#tab_7">Table 6</ref>. The accuracy differences between models trained on UCF CC 50 and Part A are similar (377.7 vs 397.7), which means the model trained on Part A is already good enough for the task on UCF CC 50. By fine-tuning the last two layers of MCNN with training data on UCF CC 50, the accuracy can be greatly boosted (377.7 vs. 295.1). However, if the whole network is fine-tuned rather than only the last two layers, the performance drops significantly (295.1 vs 378.3), but still comparable (377.7 vs 378.31) with the M-CNN model trained with the training data of the target domain. The performance gap between fine-tuning the whole network and fine-tuning the last couple of layers is perhaps due to the reason that we have limited training samples in the UCF CC 50 dataset. Fine-tuning the last two layers ensures that the output of the model is adapted to the target domain, and keeping the first few layers of the model intact ensures that good features/filters learned from adequate data in the source domain will be preserved. But if the whole network is fine-tuned with inadequate data in the target domain, the learned model becomes similar to that learned with only the training data in the target domain. Hence the performance degrades to that of the model learned in the latter case. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we have proposed a Multi-column Convolution Neural Network which can estimate crowd number accurately in a single image from almost any perspective. To better evaluate performances of crowd counting methods under practical conditions, we have collected and labelled a new dataset named Shanghaitech which consists of two parts with a total of 330,165 people annotated. This is the largest dataset so far in terms of the annotated heads for crowd counting. Our model outperforms the state-of-art crowd counting methods on all datasets used for evaluation. Further, our model trained on a source domain can be easily transferred to a target domain by fine-tuning only the last few layers of the trained model, which demonstrates good generalizability of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgement</head><p>This work was supported by the Shanghai Pujiang Talent Program( No.15PJ1405700), and NSFC (No. 61502304).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Representative images of Part A in our new crowd dataset. (b) Representative images of Part B in our crowd dataset. All faces are blurred in (b) for privacy preservation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Original images and corresponding crowd density maps obtained by convolving geometry-adaptive Gaussian kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 4</head><label>1</label><figDesc>for each image. So in the training stage, we also down-sample each training sample by 1 4 before generating its density map. ii) Conventional C- NNs usually normalize their input images to the same size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histograms of crowd counts of our new dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The ground truth density map and estimated density map of our MCNN Model of two test images in part A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparing single column CNNs with MCNN and MCNN w/o pretraining on Part A. L, M, S stand for large kernel, medium kernel, small kernel respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of our method to Zhang et al. [33] on Shanghaitech dataset: We evenly divided our test images into 10 groups according to increasing number of people. Absolute count in the vertical axis is the average crowd number of images in each group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparation of Shanghaitech dataset with existing datasets: Num is the number of images; Max is the maximual crowd count; Min is the minimal crowd count; Ave is the average crowd count; Total is total number of labeled people.</figDesc><table>Dataset 
Resolution Num Max Min 
Ave 
Total 
UCSD 
158 × 238 
2000 
46 
11 
24.9 
49,885 
UCF CC 50 
different 
50 
4543 
94 
1279.5 63,974 
WorldExpo 
576 × 720 
3980 
253 
1 
50.2 
199,923 

Shanghaitech 
Part A 
different 
482 
3139 
33 
501.4 241,677 
Part B 768 × 1024 
716 
578 
9 
123.6 
88,488 

Test image 
Ground-truth 
Estimation 
Test image 
Ground-truth 
Estimation 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Comparing performances of different methods on Shanghaitech dataset.</figDesc><table>Part A 
Part B 
Method 
MAE MSE MAE MSE 
LBP+RR 
303.2 371.0 59.1 
81.7 
Zhang et al. [33] 181.8 277.7 32.0 
49.8 
MCNN-CCR 
245.0 336.1 70.9 
95.9 
MCNN 
110.2 173.2 26.4 
41.3 

Group ID 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparing results of different methods on the UCF CC 50 dataset.</figDesc><table>Method 
MAE MSE 
Rodriguez et al. [26] 655.7 697.8 
Lempitsky et al. [17] 493.4 487.1 
Idrees et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Comparing results of different methods on the UCSD dataset.</figDesc><table>Method 
MAE MSE 
Kernel Ridge Regression [1] 
2.16 
7.45 
Ridge Regression [7] 
2.25 
7.82 
Gaussian Process Regression [4] 
2.24 
7.97 
Cumulative Attribute Regression [6] 2.07 
6.86 
Zhang et al. [33] 
1.60 
3.31 
MCNN 
1.07 
1.35 

3.5. The WorldExpo'10 dataset 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Mean absolute errors of the WorldExpo'10 crowd counting dataset.</figDesc><table>Method 
Sence1 Sence2 Sence3 Sence4 Sence5 Average 
LBP + RR 
13.6 
59.8 
37.1 
21.8 
23.4 
31.0 
Zhang et al. [33] 
9.8 
14.1 
14.3 
22.2 
3.7 
12.9 
MCNN 
3.4 
20.6 
12.9 
13.0 
8.1 
11.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Transfer learning across datasets. "MCNN w/o transfer" means we train the MCNN using the training data in UCF CC 50 only, and data from the source domain are not used. "MCNN trained on Part A" means we do not use the training data in the target domain to fine-tune the MCNN trained in the source domain. Method MAE MSE MCNN w/o transfer 377.7 509.1 MCNN trained on Part A 397.7 624.1 Finetune the whole MCNN 378.3 594.6 Finetune the last two layers 295.1 490.23</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the images given the density or perspective maps, we directly use the given density maps in our experiments or use the density maps generated from perspective maps. For those data only contain very few persons and the sizes of heads are similar, we use the fixed spread parameter for all the persons.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition using kernel ridge regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08445</idno>
		<title level="m">People counting in high density crowds from still images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised bayesian detection of independent motion in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marked point processes for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2913" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NEURAL COMPUT</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Detecting humans in dense crowds using locally-consistent scale prior and global occlusion reasoning. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno>arX- iv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Counting pedestrians in crowds using viewpoint invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC. Citeseer</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Shape-based human detection and segmentation via hierarchical part-template matching. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="604" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian model adaptation for crowd counts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arX- iv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the efficacy of texture analysis for crowd monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Graphics, Image Processing, and Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A mrf-based approach for realtime subway monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counting crowded moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="705" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed data fusion for real-time crowding estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tesei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2423" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Counting in dense crowds using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On rectified linear units for speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3517" to="3521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Segmentation and tracking of multiple humans in crowded environments. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1198" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
