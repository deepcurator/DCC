<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attributes2Classname: A discriminative model for attribute-based unsupervised zero-shot learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkan</forename><surname>Demirel</surname></persName>
							<email>bdemirel@havelsan.com.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">HAVELSAN Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Hacettepe University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
							<email>gcinbis@cs.bilkent.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="institution">Bilkent University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Ikizler-Cinbis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hacettepe University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attributes2Classname: A discriminative model for attribute-based unsupervised zero-shot learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a novel approach for unsupervised zero-shot learning (ZSL)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Zero-shot learning (ZSL) enables identification of classes that are not seen before by means of transferring knowledge from seen classes to unseen classes. This knowledge transfer is usually done via utilizing prior information from various auxiliary sources, such as attributes (e.g. <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b4">4]</ref>), class hierarchies (e.g. <ref type="bibr" target="#b27">[27]</ref>), vectorspace embeddings of class names (e.g. <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b6">6]</ref>) and textual descriptions of classes (e.g. <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b10">10]</ref>). Among these, attributes stand out as an excellent source of prior information: (i) thanks to their visual distinctiveness, it is possible to build highly accurate visual recognition models of attributes; (ii) being linguistically descriptive, attributes can naturally be used to encode classes in terms of their visual appearances, functional affordances or other human- We propose a zero-shot recognition model based on attribute and class names. Unlike most other attributebased methods, our approach avoids the laborious attributeclass relations at test time, by discriminatively learning a word-embedding space for predicting the unseen class name, based on combinations of attribute names. understandable aspects.</p><p>Almost all attribute-based ZSL works, however, have an important disadvantage: attribute-class relations need to be precisely annotated not only for the seen (training) classes, but also for the unseen (zero-shot) classes (e.g. <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b5">5]</ref>). This usually involves collecting finegrained information about attributes and classes, which is a time-consuming and error-prone task limiting the scalability of the approaches to a great extent.</p><p>Several recent studies explore other sources of prior information to alleviate the need of collecting annotations at test time. These approaches rely on readily available sources like word embeddings and/or semantic class hierarchies, hence, do not require dedicated annotation efforts. We simply refer to these as unsupervised ZSL. Such approaches, however, exclude attributes at the cost of exhibiting a lower recognition performance <ref type="bibr" target="#b4">[4]</ref>.</p><p>Towards combining the practical merit of unsupervised ZSL with the recognition power of attribute-based meth-ods, we propose an attribute-based unsupervised ZSL approach. The main idea is to discriminatively learn a vectorspace representation of words in which the combination of attributes relating to a class and the corresponding class name are mapped to nearby points. In this manner, the model would map distinctive attributes in images to a semantic word vector space, using which we can predict unseen classes solely based on their names. This idea is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Our use of vector space word embeddings differs significantly from the way they are used in existing unsupervised ZSL methods: existing approaches (e.g. <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b4">4]</ref>) aim to build a comparison function directly between image features and class names. However, learning such a comparison function is difficult since word embeddings are likely to be dominated by non-visual semantics, due to lack of visual descriptions in the large-scale text corpora that is used in the estimation of the embedding vectors. Therefore, the resulting zero-shot models also tend to be dominated by nonvisual cues, which can degrade the zero-shot recognition accuracy. To address this issue, we propose to use the names of visual attributes as an intermediate layer that connects the image features and the class names in an unsupervised way for the unseen classes.</p><p>An additional interesting aspect of our approach is the capability of text-only training. Given pre-trained attribute models, the proposed ZSL model can be trained based on textual attribute-class associations, without the need for explicit image data even for training classes. This gives an extreme flexibility for scalability: the training set can be easily extended by enumerating class-attribute relationships, without the need for collecting accompanying image data. The resulting ZSL model can then be used for recognition of zero-shot classes for which no prior attribute information or visual training example is available.</p><p>We provide an extensive experimental evaluation on two ZSL object recognition and one ZSL action recognition benchmark datasets. The results indicate that the proposed method yields state-of-the-art unsupervised zero-shot recognition performance both for object and cross-domain action recognition. Our unsupervised ZSL model also provides competitive performance compared to the state-ofthe-art supervised ZSL methods. In addition, we experimentally demonstrate the success of our approach in the case of text-only training. Finally, the qualitative results suggest that the non-linear transformation of the proposed approach improves visual semantics of word embeddings, which can facilitate further research.</p><p>To sum up, our main contributions are as follows: (i) we propose a novel method for discriminatively learning a word vector space representation for relating class and attribute combinations purely based on their names. (ii) We show that the learned non-linear transformation improves the visual semantics of word vectors. (iii) Our method achieves the state-of-the-art performance among unsupervised ZSL approaches and (iv) we show that by augmenting the training dataset by additional class names and their attribute predicate matrices but no visual examples, a boost in performance can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Initial attempts towards zero-shot classification were supervised, in the sense that they require explicit attribute annotations of the test classes (e.g. <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref>). Lampert et al. <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b20">20]</ref> are among the first to use attributes in this setting. They propose direct (DAP) and indirect attribute prediction (IAP) where attribute and class relations are provided explicitly. Al-Halah et al. <ref type="bibr" target="#b5">[5]</ref> introduce hierarchy and apply attribute label propagation on object classes, to utilize attributes at different abstraction levels. Rohrbach et al. <ref type="bibr" target="#b27">[27]</ref> propose a similar hierarchical method, but they use only class taxonomies. Deng et al. <ref type="bibr" target="#b9">[9]</ref> introduce Hierarchy and Exclusion (HEX) graphs as a standalone layer to be used on top of any-feedforward architecture for classification. Jayaraman and Grauman <ref type="bibr" target="#b16">[16]</ref> propose a random forest approach to handle error tendencies of attributes. Romera et al. <ref type="bibr" target="#b29">[29]</ref> develop two linear layered network to handle relations between classes, attributes and features. Zhang and Saligrama <ref type="bibr" target="#b36">[36]</ref> propose a method to use semantic similarity embedding where target classes are represented with histograms of the source classes.</p><p>An important limitation of the aforementioned methods is their dependency on the attribute signatures of the test classes. To apply these approaches to additional unseen classes, the attribute signatures of those new classes need to be provided explicitly. Our method alleviates this need by learning a word representation that allows zero-shot classification by comparing class names and attribute combinations, with no explicit prior information about attribute relations of unseen classes.</p><p>Recently, unsupervised ZSL methods are gaining more attention, due to their increased scalability. Instead of using class-attribute relations at test time, various auxiliary sources of side information, such as textual information <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b10">10]</ref> or word embeddings <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8]</ref> are explored in such methods. Ba et al. <ref type="bibr" target="#b22">[22]</ref> propose to combine MLP and CNN networks handling text based information acquired from Wikipedia articles and visual information of images, respectively. Another interesting direction is explored by Elhoseiny et al. <ref type="bibr" target="#b10">[10]</ref>, where the classifiers are built directly on textual corpus that is accompanied with images.</p><p>Distributional word representations, or word embeddings, <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26]</ref> are becoming increasingly popular <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b14">14]</ref>, due to the powerful vector-space represen-tations where the distances can be meaningfully utilized. Akata et al. <ref type="bibr" target="#b3">[3]</ref> propose attribute label embedding (ALE) method that uses textual data as side information in the WSABIE <ref type="bibr" target="#b34">[34]</ref> formulation. Akata et al. <ref type="bibr" target="#b4">[4]</ref> improve ALE by using embedding vectors that were obtained from largescale text corpora. Frome et al. <ref type="bibr" target="#b14">[14]</ref> propose a similar model where a pre-trained CNN model is fine-tuned in an end-toend way to relate images with semantic class embeddings. Norouzi et al. <ref type="bibr" target="#b25">[25]</ref> proposes to use convex combinations of fixed class name embeddings, weighted by class posterior probabilities given by a pre-trained CNN model, to map images to the class name embedding space. In the recent approach of Akata et al. <ref type="bibr" target="#b1">[2]</ref> language representations are utilized jointly with the stronger supervision given by visual part annotations. Xian et al. <ref type="bibr" target="#b35">[35]</ref> use multiple visual embedding spaces to encode different visual characteristics of object classes. Jain et al. <ref type="bibr" target="#b15">[15]</ref> and Kordumova et al. <ref type="bibr" target="#b18">[18]</ref> leverage pre-trained object classifiers, and, actionobject similarities given by class embeddings to assign action labels to unseen videos.</p><p>The work closest to ours is Al-Halah et al. <ref type="bibr" target="#b6">[6]</ref>, which proposes an approach for using visual attributes in the unsupervised ZSL setting. In their approach, a model is learned to predict whether an individual attribute is related to a class name or not. For this purpose, they learn a separate bilinear compatibility function for each group of attributes, where similar attributes are grouped together to improve the performance. For unsupervised ZSL, this approach first estimates the association of attributes with the test class, and then employs an attribute-based ZSL method using the estimated class-attribute relations. Our approach differs in two major ways. First, instead of comparing classes with individual attribute names, we model the relationship between class names and combinations of attribute names. Second, as opposed to handling class-attribute relation estimation and zero-shot classification as two separate problems, we discriminatively train our attribute based ZSL model in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present the details of our approach. First, we explain our zero-shot learning model. Then, we describe how to train our ZSL model using discriminative image-based training and predicate-based training formulations. Finally, we briefly discuss our text-only training strategy for incorporating additional classes during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Zero-shot learning model</head><p>We define our ZSL model compatibility function f (x, y) : X × Y → R that measures the relevance of label y ∈ Y for a given image x ∈ X . Using this function, a test image x can be classified simply by choosing the class maximizing the compatibility score: arg max y f (x, y).</p><p>In order to enable zero-shot learning of classes based on class names only, we assume that an initial d 0 -dimensional vector space embedding ϕ y ∈ R d0 is available for each class y. These initial class name embeddings are obtained using general purpose corpora, due to lack of a large-scale text corpus dedicated for visual descriptions of objects. The representations obtained by the class embeddings, hence, are typically dominated by non-visual semantics. For instance, according to the GloVe vectors, the similarity between wolf and bear (both wild animals) is higher that the similarity between wolf and dog, though the latter pair is visually much more similar to each other.</p><p>These observations suggest that learning a compatibility function directly between the image features and class embeddings may not be easy due to non-visual components of word embeddings. To address this issue, we propose to leverage attributes, which are appealing for the dual representation they provide: each attribute corresponds to (i) a visual cue in the image domain, and, (ii) a named entity in the language domain, whose similarity with class names can be estimated using word embeddings. We define a function Φ(x) : X → R d for embedding each image based on the attribute combination associated with it:</p><formula xml:id="formula_0">Φ(x) = 1 a p(a|x) a p(a|x)T (ϕ a )<label>(1)</label></formula><p>where p(a|x) is the posterior probability of attribute a 1 , given by a pre-trained binary attribute classifier, ϕ a is the initial embedding vector of attribute a, and T :</p><formula xml:id="formula_1">R d0 → R d</formula><p>is the transformation that we aim to learn. Similarly, we define our class embedding function φ(y) : Y → R d as the transformation of the initial class name embeddings ϕ y :</p><formula xml:id="formula_2">φ(y) = T (ϕ y ).</formula><p>The purpose of the function T is to transform the initial word embeddings of attributes and classes such that each image, and its corresponding class are represented by nearby points in the d-dimensional vector embedding space. Consequently, we can define f (x, y) as a similarity measure between the image and class embeddings. In our approach, we opt for the cosine-similarity:</p><formula xml:id="formula_3">f (x, y) = Φ(x) T φ(y) Φ(x) φ(y)<label>(2)</label></formula><p>We emphasize that our approach requires only the name of an unseen class at test time, as the compatibility function relies solely on the learned attribute and class name embeddings, rather than attribute-class relations. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates our zero-shot classification approach. Given an image, we first apply the attribute predictors and compute a weighted average of the attribute name embeddings. The class assignment is done by comparing the resulting embedding of attribute combination with that of each (unseen) class name. The image is then assigned to the class with the highest cosine similarity.</p><p>As defined above, the embeddings of attribute combinations and class names are functions of the shared transformation T (ϕ). <ref type="bibr" target="#b1">2</ref> In our experiments, we define T (ϕ) as a two-layer feed-forward neural network. In the following sections, we describe techniques for discriminatively learning this transformation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image-based training (IBT)</head><p>In image-based training, we assume that there exists a supervised training set S of N examples. Each example forms an image and class label pair. By definition, no example in S belongs to one of the zero-shot test classes. Our goal is to discriminatively learn the function f (x, y) such that for each training example i, the compatibility score of the correct class y = y i is higher than any other class y j , by a margin of ∆(y i , y j ). More formally, the training constraint for the i-th training example is given by</p><formula xml:id="formula_4">f (x i , y i ) ≥ f (x i , y j ) + ∆(y i , y j ), ∀y j = y i (3)</formula><p>The margin function ∆ indicates a non-negative pairwise discrepancy value for each pair of the training classes.</p><p>As explained in the previous section, f (x, y) is a function of the transformation network T (ϕ). Let θ be the vector of all parameters in the transformation network. Inspired from the structural SVMs <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b28">28]</ref>, we formalize our ap- <ref type="bibr" target="#b1">2</ref> In principle, one can separately define a T (ϕ) for attribute names, and, another one for class names. We have explored this empirically, but did not observe a consistent improvement. Therefore, for the sake of simplicity, we use a shared transformation network in our experiments.</p><p>proach as a constrained optimization problem:</p><formula xml:id="formula_5">min θ,ξ λ||θ|| + N i=1 yj =yi ξ ij f (x i , y i ) ≥ f (x i , y j ) + ∆(y i , y j ) − ξ ij ∀y j = y i , ∀i<label>(4)</label></formula><p>where ξ is a vector of slack variables for soft-penalizing unsatisfied similarity constraints, and λ is the regularization weight. To avoid optimization over non-linear constraints, we can equivalently express this problem as an unconstrained optimization problem:</p><formula xml:id="formula_6">min θ λ θ 2 2 + N i=1 yj =yi max (0, f (x i , y j ) − f (x i , y i ) + ∆(y i , y j ))<label>(5)</label></formula><p>Using this formulation, the transformation T (ϕ) is learned in an discriminative and end-to-end manner, by ensuring that the correct class score is higher than the incorrect ones, for each image.</p><p>We empirically observe that cross-validating the number of iterations provides an effective regularization strategy, therefore, we fix λ = 0. We use average Hamming distance between the attribute indicator vectors, which denote the list of attributes associated with each class, to compute ∆ values. This is the only point where we utilize the class-attribute predicate matrix in our image-based training approach. In the absence of a predicate matrix, other types of ∆ functions, like word embedding similarities, may be explored, which we leave for future work. Other implementation details are provided in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Predicate-based training (PBT)</head><p>In this section, we propose an alternative training approach, which we call predicate-based training. In this approach, the goal is to learn the ZSL model solely based on the predicate matrix, which denotes the class-attribute relations. While image-based training is defined in terms of image-class similarities, we formulate predicate-based training in terms of class-class similarities, without directly using any visual examples during training.</p><p>The predicate matrix consists of per-class indicator vectors, where each element is one if the corresponding attribute is associated with the class, and zero, otherwise. We denote the indicator vector for class y by π y . Then, similar to image embedding function Φ(x), we define a predicateembedding function Ψ(π):</p><formula xml:id="formula_7">Ψ(π) = 1 a π(a) a π(a)T (ϕ a ).<label>(6)</label></formula><p>This embedding function is obtained by replacing posterior probabilities in Eq. (1) by binary attribute-class relations. Then, we define a new compatibility function g(π, y), as the cosine similarity between the vector Ψ(π) and vector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>φ(y). This function is basically similar to Eq. (2), where the image embedding Φ(x) is replaced by the attribute indicator embedding Ψ(π).</head><p>Finally, we define the learning problem as optimizing the function g(x, y) such that for each class, the compatibility score for its ideal set of attributes π y is higher than the attribute combination π y ′ of another class y ′ , by a margin of ∆(y, y ′ ). This constraint aims to ensure that the similarity between the name embedding of a set of attributes and the embedding of a class name reliably indicates the visual similarity indicated by the predicate matrix.</p><p>This definition leads us to an unconstrained optimization problem analogous to Eq. <ref type="formula" target="#formula_6">(5)</ref>:</p><formula xml:id="formula_8">min θ λ θ 2 2 + K y=1 y ′ =yi max (0, g(π y ′ , y i ) − g(π yi , y i ) + ∆(y i , y ′ )) (7)</formula><p>where K indicates the number of training classes in the predicate matrix. As in image-based training, we define ∆(y, y ′ ) as the average Hamming distance between π y and π y ′ , and use λ = 0. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the predicate-based training approach. As shown in this figure, the main idea is to project the ϕ word representations into a new space, where the similarity between a class and an attribute combination in terms of their name vectors is indicative of their visual similarity. At test time, we use the learned transformation network in zero-shot classification via the compatibility function f (x, y) in Eq. (2). This compatibility function uses only attribute classifier outputs and the transformed word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Text-only training</head><p>Predicate-based training, as explained in the previous section, is completely based on a class-attribute predicate matrix for the training classes, and training images are used only for pre-training attribute classifiers that will be used at test time. In contrast, image-based training, directly learns the ZSL model based on attribute classification probabilities in training images, therefore in principle, we expect imagebased training to perform better. This is, in fact, verified in our experimental results: while predicate-based training shows competitive accuracy, we obtain our state-of-the-art results using image-based training.</p><p>Despite the relatively lower performance of predicatebased training, it has one interesting property: we can expand the training set by simply adding textual information for additional novel classes into the predicate matrix. This allows improving the ZSL model by using classes with no visual examples. We call incorporation of additional training classes in this manner as text-based training. In Section 4, we empirically show that it is possible to improve the predicate-based training using text-based training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the effectiveness of the proposed approach, we consider two different ZSL applications: zero-shot object classification and zero-shot action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Zero Shot Object Classification</head><p>In this part, we explain our zero-shot object classification experiments on two common datasets namely AwA <ref type="bibr" target="#b20">[20]</ref>, aPaY <ref type="bibr" target="#b13">[13]</ref>. AwA dataset <ref type="bibr" target="#b20">[20]</ref> contains 30,475 images of 50 different animal classes. 85 per-class attribute labels are provided in the dataset. In the predefined split for zero-shot learning, 40 animal classes are marked for training and 10 classes for testing. aPaY dataset <ref type="bibr" target="#b13">[13]</ref> is formed of images obtained from two different sources. aPascal (aP) part of this dataset is obtained from PASCAL VOC 2008 <ref type="bibr" target="#b11">[11]</ref>. This part contains 12,695 images of 20 different classes. The second part, aYahoo (aY), is collected using Yahoo search engine and contains 2,644 images of 12 object classes completely different from aPascal classes. Images are annotated with 64 binary per-image attribute labels. In zeroshot learning settings on this dataset, aPascal part is used for training and aYahoo part is used for testing. We follow the same experimental setup as in <ref type="bibr" target="#b5">[5]</ref> and only use training split of aPascal part to learn attribute classifiers.</p><p>Attribute Classifiers. We use CNN-M2K features <ref type="bibr" target="#b5">[5]</ref> to encode images and train attribute classifiers. We resize each image to 256x256 and then subtract the mean image. Data , we obtain the attribute classifiers by training ℓ 2 -regularized squared-hinge-loss linear SVMs. Parameter selection is done using 10-fold cross validation over the training set and Platt scaling is applied to map the attribute prediction scores to posterior probabilities. For image-based training, crossvalidation outputs are used as the classification scores in training images.</p><p>Word Embeddings. For each class and attribute name, we generate a 300-dimensional word embedding vector using GloVe <ref type="bibr" target="#b26">[26]</ref> based on Common Crawl Data 3 . These word vectors are publicly available <ref type="bibr" target="#b4">4</ref> . For those names that consist of multiple words, we use the average of the word vectors.</p><p>Word Representation Learning. We define the transformation function as a two layer feed-forward network. We use 2-fold cross-validation over the training set to select number of hidden units and number of iterations. tanh function is used as the activation function in the first hidden layer and sigmoid function is used in the second hidden layer. Adam <ref type="bibr" target="#b17">[17]</ref> is used for stochastic optimization, and learning rate value is set to 1e-4. Implementation is done using TensorFlow <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr" target="#b5">5</ref> Results. In our experiments, we first evaluate the performance of attribute classifiers, since this is likely to have a significant influence on zero-shot classification. The attribute classifiers yield 80.56% mean AUC on the AwA dataset, 84.91% mean AUC on the aPaY dataset. These results suggest that our attribute classifiers are relatively accurate, if not perfect. Further improvements in attribute classification are likely to have a positive impact on the final ZSL performance. <ref type="table" target="#tab_0">Table 1</ref> presents the experimental results for our approach. In this table, baseline represents the case where the transformation T (ϕ) is defined as an identity mapping. PBT (predicate-based training) represents our proposed approach that learns a transformation using the attribute predi- cate matrix, whereas IBT (image-based training) represents learning transformation using training images. The results in <ref type="table" target="#tab_0">Table 1</ref> shows the importance and success of our learning formulations, compared to the baseline. In addition, we observe that image-based training outperforms predicatebased training on average, which is in accordance with our expectations. Class-wise accuracy comparison of PBT and IBT methods is given in <ref type="figure" target="#fig_3">Figure 4</ref>. We observe that some of the classes respond particularly well to the image-based training. <ref type="table" target="#tab_1">Table 2</ref> presents a comparison of our results against a number of supervised and unsupervised ZSL methods. In this table, the supervision corresponds to the information needed during test time for zero-shot learning: the supervised methods require additional data about the unseen classes such as attribute-class predicate matrices, whereas unsupervised methods do not require any explicit inputs about test classes. Hence, supervised methods have a very major advantage in this comparison, as they employ external attribute signatures of test classes. In contrast, unsupervised methods carry out zero-shot classification among the test classes without using data additional to the training set. Finally, we note that, we exclude ZSL methods that operate on low-level visual image features, as their results are not directly comparable. Instead, for the sake of fair comparison, we only compare to those methods that use similar convolutional neural network based image representations.</p><p>From <ref type="table" target="#tab_1">Table 2</ref> we see that on AwA and aPaY datasets, our unsupervised ZSL method yields state-of-the-art classification performance compared to other unsupervised ZSL methods. In addition, our method performs on par with some of the supervised ZSL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Zero Shot Action Recognition</head><p>For zero-shot action recognition, we evaluate our approach on UCF-Sports Action Recognition Dataset <ref type="bibr" target="#b30">[30]</ref>. The dataset is formed of videos from various sport actions which are featured from television channels such as the BBC and ESPN, and contains a total of 150 videos of 10 different sport action classes.  <ref type="bibr" target="#b27">[27]</ref> 57.4 31.7 HAT <ref type="bibr" target="#b5">[5]</ref> 63.1 38.3 ALE-attr <ref type="bibr" target="#b4">[4]</ref> 66.7 -SSE-INT <ref type="bibr" target="#b36">[36]</ref> 71.5 44.2 SSE-ReLU <ref type="bibr" target="#b36">[36]</ref> 76.3 46.2 SynC-attr <ref type="bibr" target="#b8">[8]</ref> 76.3 -SDL <ref type="bibr" target="#b38">[38]</ref> 79.1 50.4 JFA <ref type="bibr" target="#b37">[37]</ref> 81.0 52.0</p><p>Word Embeddings. Following <ref type="bibr" target="#b15">[15]</ref>, we utilize 500-dimensional word embedding vectors generated with the skip-gram model of word2vec <ref type="bibr" target="#b23">[23]</ref> learned over YFCC100M <ref type="bibr" target="#b32">[32]</ref> dataset. YFCC100M dataset contains metadata tags of about 100M Flickr images and the word vectors obtained from YFCC100M are publicly available <ref type="bibr" target="#b6">6</ref> .</p><p>Object Classifiers. Since there is no explicit definition of attributes for actions, the object cues can be leveraged instead of attributes, as suggested by <ref type="bibr" target="#b15">[15]</ref>. To this end, we obtain predicate matrices from the textual data by measuring the cosine similarity between actions and object classification scores. We operate on the object classification responses made available by <ref type="bibr" target="#b15">[15]</ref> 6 . These are obtained by AlexNet <ref type="bibr" target="#b19">[19]</ref>, where every 10th frame is sampled for each video and each sampled frame is represented with the total of 15,293 ImageNet object categories. Average pooling is applied afterwards, so that each video is represented with 15,293 dimensional vectors. To have a fair comparison, we also apply the sparsification step of <ref type="bibr" target="#b15">[15]</ref> using the same parameters. This sparsification is done for eliminating noisy object classification responses.</p><p>Word Representation Learning. Model learning settings are the same with those of ZSL object classification experiments, with the exception that only image-based loss is used, because predicate matrices are not available during training. Since we do not have any training data for target datasets, we train our transformation function with a different dataset (i.e. UCF-101 <ref type="bibr" target="#b31">[31]</ref>). To avoid any overlap be-6 staff.fnwi.uva.nl/m.jain/projects/Objects2action.html <ref type="table">Table 3</ref>: Zero-shot action recognition accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>UCF-Sport DAP <ref type="bibr" target="#b20">[20]</ref> 11.7 objects2action <ref type="bibr" target="#b15">[15]</ref> 26.4 Our method 28.3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>We compare our approach with Objects2Action <ref type="bibr" target="#b15">[15]</ref> and DAP <ref type="bibr" target="#b20">[20]</ref> methods. The normalized accuracy results are shown in <ref type="table">Table 3</ref>. From these results we see that our approach for relating action names and object cues in the transformed word vector space yields promising results in UCF-Sport dataset. These results show that our embedding transformation function carries substantial semantic information not only between training and test sets, but also across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training on Textual Data</head><p>As stated before, one of the interesting aspects of our formulation is the ability to train over only textual data (i.e. names of attributes, objects and classes), without having any visual examples of training classes. In this case, using our model, we can use the pre-trained attribute classifiers, together with the learned semantic word vector representation and predict the class of a newly seen example.</p><p>To demonstrate the effect, we select 20 classes outside the AwA dataset from Wikipedia Animal List <ref type="bibr" target="#b7">7</ref> , and build an attribute-class predicate matrix. We then learn the corresponding semantic vector space using only these classes that have no image data. The results are shown in  From left-to-right, the columns show the query class (first column), and the most similar classes according to raw word embeddings (second column), those using the transformation learned by PBT (third column), and those using the transformation learned by IBT (fourth column), respectively.</p><p>additional class names and their predicate matrix, the accuracy improves from 60.7% to 63.0%. These results suggest that the performance of the proposed model can be improved by just enumerating additional class names and their corresponding attribute lists, without necessarily collecting additional image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visual Similarities of Word Vectors</head><p>One of the favorable aspects of our method is that it can lead to visually more consistent word embeddings of visual entities. To demonstrate this, <ref type="figure" target="#fig_4">Figure 5</ref> shows the similarities across the classes according to the original and transformed word embeddings in the AwA dataset. In the first row, we see that while one of the most similar classes to the killer whale is elephant using the original embeddings, this changes to the dolphin class after using the transformation learned by IBT. We observe similar improvements for other classes, such as mole (second row) and wolf (third row), for which the word embeddings transformed by PBT or IBT training lead to visually more sensible word similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Randomly Sampled Vectors</head><p>To quantify the importance of initial word embeddings, we evaluate our approach on the AwA dataset by using vectors sampled from a uniform distribution, instead of pretrained GloVe vectors. In this case, PBT yields 28.6%, and IBT yields 13.6% top-1 classification accuracy, which are significantly lower than our actual results (PBT 69.9% and IBT 60.7%). This observation highlights the importance of leveraging prior knowledge derived from unsupervised text corpora through pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>An important limitation of the existing attribute-based methods for zero-shot learning is their dependency on the attribute signatures of the unseen classes. To eliminate this dependency, in this work, we leverage attributes as an intermediate representation, in an unsupervised way for the unseen classes. To this end, we learn a discriminative word representation such that the similarities between class and attribute names follow the visual similarity, and use this learned representation to transfer knowledge from seen to unseen classes. Our proposed zero-shot learning method is easily scalable to work with any unseen class without requiring manually defined attribute-class annotations or any type of auxiliary data.</p><p>Experimental results on several benchmark datasets demonstrate the efficiency of our approach, establishing the state-of-the-art among the unsupervised zero-shot learning methods. The qualitative results show that the non-linear transformation using the proposed approach improves distributed word vectors in terms of visual semantics. In addition, we show that by adding just text-based class names and their attribute signatures, the training set can be easily extended, which can further boost the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We propose a zero-shot recognition model based on attribute and class names. Unlike most other attributebased methods, our approach avoids the laborious attributeclass relations at test time, by discriminatively learning a word-embedding space for predicting the unseen class name, based on combinations of attribute names.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our unsupervised zero-shot recognition model. Prediction depends on the similarity between discriminatively learned representations of attribute combinations and class names. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of our predicate-based training approach, which uses only the predicate matrix of class and attribute relations as the source of supervision. The goal is to represent class and attribute combinations, based on their names, in a space where each class is closest to its ideal attribute combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Class-wise prediction accuracies on AwA Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top-3 most similar classes for some example classes from the AwA dataset. The similarities of the class word vectors are measured by cosine similarity. The images shown depict class representatives. From left-to-right, the columns show the query class (first column), and the most similar classes according to raw word embeddings (second column), those using the transformation learned by PBT (third column), and those using the transformation learned by IBT (fourth column), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Zero-shot classification performance of proposed predicate-based (PBT) and image-based (IBT) methods on AwA and aPaY datasets. We report normalized accuracy.</figDesc><table>Method AwA aPaY 
Baseline 10.2 
16.0 
PBT 
60.7 
29.4 
IBT 
69.9 
38.2 

augmentation is applied via using five different crops and 
their flipped versions. Outputs of fc7 layer are used, result-
ing in 2,048 dimensional feature vectors. Following [13]</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Comparison to state-of-the-art ZSL methods (un- supervised and supervised).</figDesc><table>Test supervision Method 
AwA aPaY 

unsupervised 

DeViSE[14] 
44.5 
25.5 
ConSE[25] 
46.1 
22.0 
Text2Visual[10, 7] 55.3 
30.2 
SynC[8] 
57.5 
-
ALE[4] 
58.8 
33.3 
LatEm[35] 
62.9 
-
CAAP[6] 
67.5 
37.0 
Our method 
69.9 
38.2 

supervised 

DAP[20] 
54.0 
28.5 
ENS</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot learning using external training class names and their predicate matrices. These EXT classes con- sist of class names outside AwA dataset and do not include image data. The method is trained only on class names and their predicate matrices. We report normalized accuracy.tween datasets, we exclude the common action classes from the training set for an accurate zero-shot setting. Some of such common classes that are excluded from training are Diving and Horse Riding.</figDesc><table>Method Train Classes Accuracy 
PBT 

EXT 

44.0 
PBT 
AWA 
60.7 
PBT 
AWA+EXT 
63.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Note that, here, we only train the PBT model, because IBT is based on image data. Training our model using only addi- tional textual class names and their corresponding attribute predicate matrices gives an impressive accuracy of 44.0%. Moreover, when we augment the AwA train set with these 7 en.wikipedia.org/wiki/List_of_animal_names</figDesc><table>K. Whale 

B. Whale Elephant 
Walrus 
B. Whale Walrus 
P. Bear 
B. Whale Dolphin 
Walrus 

Mole 
Weasel 
S. Cat 
B. Whale 
Squirrel 
Beaver 
Mouse 
Mouse 
Hamster 
Bat 

Wolf 
P. Bear 
G. Bear 
Fox 
G. Bear Shepherd 
Fox 
Fox 
Bobcat Shepherd 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The normalization in the denominator aims to make the embeddings comparable across images with varying number of observed attributes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">commoncrawl.org/the-data/ 4 nlp.stanford.edu/projects/glove/ 5 github.com/berkandemirel/attributes2classname</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multicue zero-shot learning with strong supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How to transfer? zero-shot object recognition via hierarchical transfer of semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="837" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5975" to="5984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision</title>
		<meeting>European Conf. on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Write a classifier: Zero-shot learning using purely textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision</title>
		<meeting>IEEE Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2584" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Results</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Objects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision</title>
		<meeting>IEEE Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4588" to="4596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3464" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pooling objects for recognizing scenes without examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kordumova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia Retrieval</title>
		<meeting>ACM Int. Conf. Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting deep zeroshot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision</title>
		<meeting>IEEE Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4247" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Empiricial Methods in Natural Language Processing</title>
		<meeting>of the Empiricial Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T C G D</forename><surname>Roller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition in realistic sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision in Sports</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="181" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012-11" />
		</imprint>
		<respStmt>
			<orgName>University of Central Florida</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The new data and new challenges in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision</title>
		<meeting>IEEE Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4166" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning joint feature adaptation for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6034" to="6042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision</title>
		<meeting>European Conf. on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="533" to="548" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
