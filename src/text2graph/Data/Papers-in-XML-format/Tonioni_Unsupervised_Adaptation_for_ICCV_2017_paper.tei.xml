<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Adaptation for Deep Stereo</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
							<email>alessio.tonioni@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Viale del Risorgimento 2</addrLine>
									<settlement>Bologna</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
							<email>matteo.poggi8@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Viale del Risorgimento 2</addrLine>
									<settlement>Bologna</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
							<email>stefano.mattoccia@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Viale del Risorgimento 2</addrLine>
									<settlement>Bologna</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
							<email>luigi.distefano@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Viale del Risorgimento 2</addrLine>
									<settlement>Bologna</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Adaptation for Deep Stereo</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recent ground-breaking works have shown that deep neural networks can be trained end-to-end to regress dense disparity maps directly from image pairs. Computer generated imagery is deployed to gather the large data corpus required to train such networks, an additional fine-tuning allowing to adapt the model to work well also on real and possibly diverse environments. Yet, besides a few public datasets such as Kitti, the ground-truth needed to adapt the network to a new scenario is hardly available in practice. In this paper we propose a novel unsupervised adaptation approach that enables to fine-tune a deep learning stereo model without any ground-truth information. We rely on off-the-shelf stereo algorithms together with state-of-the-art confidence measures, the latter able to ascertain upon correctness of the measurements yielded by former. Thus, we train the network based on a novel loss-function that penalizes predictions disagreeing with the highly confident disparities provided by the algorithm and enforces a smoothness constraint. Experiments on popular datasets (KITTI  2012, KITTI 2015 and Middlebury 2014 and other challenging test images demonstrate the effectiveness of our proposal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Availability of accurate 3D data is key to a large variety of high-level computer vision tasks, such as autonomous driving, 3D reconstruction and many others. Thus, several depth estimation techniques exhibiting different degrees of effectiveness and deployability have been proposed throughout the years. Among them, stereo vision proved to be one of the most promising methodologies to infer accurate depth information in both indoor and outdoor settings. However, recent datasets, such as KITTI <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> and Middlebury 2014 <ref type="bibr" target="#b18">[19]</ref>, emphasized major shortcomings of stereo in the challenging environmental conditions found in most practical applications <ref type="bibr" target="#b10">[11]</ref>. The widespread diffusion of deep learning in computer vision has also affected stereo vision. In particular, Convolutional Neural Networks (CNNs) proved very effective to compute matching costs between the patches of a stereo pair <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>, although these novel approaches still requires to be plugged into well established disparity optimization and refinement pipelines (e.g., <ref type="bibr" target="#b24">[25]</ref>) to achieve state-of-the-art accuracy. A ground-breaking forward step is DispNet , <ref type="bibr" target="#b9">[10]</ref>, a deep architecture trained from scratch to regress dense disparity measurements end-to-end from image pairs, thereby dismissing all the machinery traditionally deployed to optimize/refine disparities and speeding up the computation considerably. However, due to the high capacity of the model as well as the input consisting in image pairs rather than patch pairs, this approach mandates a huge amount of supervised training data not available in existing datasets (i.e. tens of thousands of stereo pairs with ground-truth). Therefore, the network is trained leveraging on large synthetic datasets generated by computer graphics <ref type="bibr" target="#b9">[10]</ref> and then fine-tuned on fewer available real data with ground truth <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> in order to improve effectiveness in the addressed scenario <ref type="bibr" target="#b9">[10]</ref>. Yet, the performance of a deep stereo model may deteriorate substantially when the supervised data needed to perform adaptation to a new environment are not available. For example, <ref type="figure" target="#fig_0">Figure 1</ref> (c) shows how DispNet <ref type="bibr" target="#b9">[10]</ref> yields gross errors on a stereo pair of a dataset <ref type="bibr" target="#b10">[11]</ref> lacking the ground-truth information to finetune the network. Unfortunately, besides a few research datasets, stereo pairs with ground-truth disparities are quite rarely available as well as cumbersome and expensive to create in any practical settings. This state of affairs may limit deployability of deep stereo architectures significantly.</p><p>To tackle the above mentioned issue, in this paper we propose a novel unsupervised adaptation approach that enables to fine-tune a deep stereo network without any ground-truth information. The first key observation to our approach is that computer vision researchers have pursued for decades the development of general-purpose stereo correspondence algorithms that do not require any adaptation to be deployed in different scenarios. The second is that, although traditional stereo algorithms exhibit wellknown shortcomings in specific conditions (e.g., occlusions, texture-less areas, photometric distortions ..), recent state-of-the-art confidence measures, more often than not relying on machine learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, can effectively highlight uncertain disparity assignments. Thus, we propose to leverage on traditional stereo algorithms and state-of-the-art confidence measures in order to fine-tune a deep stereo model based on disparities provided by standard stereo algorithms that are deemed as highly reliable by the confidence measure. <ref type="figure" target="#fig_0">Figure 1 (d)</ref> shows that our unsupervised adaptation approach can improve dramatically the output provided by DispNet <ref type="bibr" target="#b9">[10]</ref> on a dataset lacking the ground-truth to fine-tune the network with supervision. Our approach deploys a loss function that, taking as target variables the disparity measurements provided by the stereo algorithm, weighs the error contribution associated with each prediction according to the estimated confidence in the corresponding target value. Moreover, we introduce a smoothing term in the loss that penalize dissimilar predictions at nearby spatial locations, based on the conjecture that as high confidence target disparities may turn out sparse, enforcing smoothness helps propagating the predictions from high confidence locations towards low confidence ones. The effectiveness of our unsupervised technique is demonstrated by experimental evaluation on KITTI datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> and Middlebury 2014 <ref type="bibr" target="#b18">[19]</ref>, assessing both adaptation ability and generalization to new data. We also report qualitative results on challenging images <ref type="bibr" target="#b10">[11]</ref>, so to highlight the need for an effective unsupervised adaptation methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the past decades several algorithms have been proposed to tackle the stereo correspondence problem and, according to <ref type="bibr" target="#b19">[20]</ref>, they can be categorized into two broad classes: local and global methods. Both perform a subset of the following four steps: 1) matching cost computation 2) cost aggregation 3) disparity computation/optimization 4) disparity refinement. Although local methods can be very fast, global approaches are in general more effective. Among the latter, a good trade-off between accuracy and execution time is represented by the Semi Global Matching (SGM) algorithm <ref type="bibr" target="#b5">[6]</ref>. This method, also implemented on different embedded architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, is a very popular solution to disparity optimization adopted by most top-performing algorithms on challenging datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>, such as e.g. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref>. A further boost to stereo accuracy in challenging environments has been achieved deploying deep learning techniques within a conventional stereo pipeline based on SGM. In this field <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref> inferred matching costs by training a CNN to compare image patches. In particular, Zbontar and LeCun <ref type="bibr" target="#b24">[25]</ref> established a common baseline for any other attempt to push forward the state-of-the-art. A different strategy proposed in <ref type="bibr" target="#b14">[15]</ref> deploys deep learning to merge disparity maps of multiple algorithms so as to obtain a more accurate estimation. Nevertheless, such deep learning approaches also showed that well-established optimization methodologies such as SGM are still required to achieve very accurate results (e.g., <ref type="bibr" target="#b24">[25]</ref>).</p><p>A major departure from this line of research has been proposed by Mayer et al. <ref type="bibr" target="#b9">[10]</ref>, who tackle the disparity estimation problem without leveraging on any conventional stereo technique. They achieved very accurate results on the KITTI datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> by training end-to-end a deep architecture, DispNet, so to infer dense disparity maps directly from a pair of input images. As there exist no dataset with ground-truth large enough to train such a network, they deployed a synthetic, yet somehow realistic, dataset specifically created for this purpose. A subsequent fine-tuning on real datasets, however, is key to substantially improve accuracy.</p><p>Recent trends concerning confidence measures for stereo, reviewed and evaluated by Hu and Mordohai <ref type="bibr" target="#b6">[7]</ref> and more recently by Poggi et al. <ref type="bibr" target="#b17">[18]</ref>, are also relevant to our work, in particular state-of-the-art approaches leveraging on machine-learning to pursue confidence prediction. Hausler et al. <ref type="bibr" target="#b4">[5]</ref> proposed to combine multiple confidence measures and features, as orthogonal as possible, within a random forest framework. The same strategy was adopted by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, though deploying more effective confidence measures and features. Confidence pre-diction has also been tackled recently by deep learning approaches. Poggi and Mattoccia <ref type="bibr" target="#b16">[17]</ref> and Seki and Pollefeys <ref type="bibr" target="#b20">[21]</ref> propose two different strategies to train a CNN to predict confidence measures directly from disparity maps. Regardless of the adopted strategy, confidence measures have been deployed to improve the overall accuracy of conventional stereo vision pipelines as shown in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. Finally, Mostegel et al. <ref type="bibr" target="#b12">[13]</ref>, propose unsupervised training of confidence measures leveraging on contradictions between multiple depth maps from different viewpoints.</p><p>Thus, though both machine/deep learning and confidence measures are becoming more and more relevant to the stereo literature, we are not aware of any previous work concerned with deploying confidence measure to help training unsupervisedly a machine learning algorithm pursuing disparity estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unsupervised Adaptation</head><p>As vouched by the experimental findings reported in Sec. 4.2, 4.3, the main issue with large networks aimed at dense disparity estimation from image pairs is robustness to different deployment scenarios. In fact, when dealing with environments quite different from those employed to train the network, the accuracy may quickly drop and the model would need to be adapted to the new settings in order to achieve comparable performance. This step requires a dataset with ground truth that is seldom available in practical applications.</p><p>Our proposal tackles this issue by enabling adaptation of the network in an unsupervised fashion by leveraging on a conventional stereo algorithm and a reliable confidence measure. Starting from a pre-trained model, we fine tune it to minimize a novel loss function (L) made out of two terms: a Confidence Guided Loss (C L ) and a Smoothing Term (S), with hyper-parameter λ weighing the contribution of the latter:</p><formula xml:id="formula_0">L = C L + λ * S (1)</formula><p>Such a loss function enables to adapt the pre-trained model to deal with any new environment by simply processing a pool of stereo pairs and without requiring any groundtruth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Confidence Guided Loss</head><p>Once trained on very large datasets with ground truth, end-to-end stereo networks like DispNet can predict a disparity map directly from the input stereo pair. As reported in <ref type="bibr" target="#b9">[10]</ref>, the authors firstly trained the network on a huge synthetic generated dataset of 25000 image pairs with valid disparity label for each pixel, then adapted it to a different environment through a much smaller amount of image pairs endowed with even sparse ground truth labels (i.e. the nearly 200 training images of KITTI2012 <ref type="bibr" target="#b3">[4]</ref> where only a subset of pixels have meaningful disparity values). To account for the missing values within the images used to fine-tune the network they simply set the loss function to 0 at such locations, given that, even if only a small portion of output receives meaningful gradients, the system is still able to adapt fairly well to the new scenario and hence to ameliorate its overall accuracy.</p><p>However, despite the elegance and effectiveness of such methodology, for most real world scenarios the adaptation would be impossible because we can not expect availability of enough ground truth data, even at sparse locations. On the other hand, what we could reasonably expect is availability of stereo pairs acquired in the field. Hence, the first contribution of our work is to fill this gap by providing a methodology to obtain disparity labels for the adaptation phase using conventional stereo algorithms (e.g., AD-CENSUS <ref type="bibr" target="#b23">[24]</ref> or SGM <ref type="bibr" target="#b5">[6]</ref>). Unfortunately a network like DispNet trained on the raw output of AD-CENSUS or SGM would, at best, learn to imitate the overall behavior of the chosen stereo algorithm, including its intrinsic shortcomings, thus leading to unsatisfactory results. However, by taking advantage of effective confidence measures recently proposed, like <ref type="bibr" target="#b16">[17]</ref>, we can discriminate between reliable and unreliable disparity measurements, to select the former and fine tune the model using such smaller and sparse set of points as if they were ground truth labels.</p><p>Given an input stereo pair I L and I R , we denote asD the disparity map predicted by the stereo network, D the disparity map computed by a conventional stereo algorithm and C a confidence map measuring the reliability of each element in D, with C(p) ∈ [0, 1]∀p ∈ P , with P the set of all spatial locations. We define the Confidence Guided Loss (C L ) as:</p><formula xml:id="formula_1">C L = 1 |P | p∈P E (p)<label>(2)</label></formula><formula xml:id="formula_2">E (p) = C (p) · |D (p) − D (p) | if C (p) ≥ τ 0 if C (p) &lt; τ<label>(3)</label></formula><p>τ ∈ [0, 1] being a hyper-parameter of our method that controls the sparseness and reliability of the disparity measurements provided by the stereo algorithm that act as target variables in our learning process. Higher values of τ let fewer measurements contribute to the loss but with a lower probability of injecting wrong disparities into the process. It is worth pointing out that should the confidence measure behave perfectly, minimizing such loss function with an appropriate τ might be taught of as to fine-tuning on sparse ground truth data with the same amount of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Smoothness Term</head><p>Although fine-tuning on sparse ground truth data, as proposed in <ref type="bibr" target="#b9">[10]</ref>, does improve the disparities predicted in unseen scenarios, it may still be regarded as an approximation of the ideal optimization process that would leverage on dense labels. Therefore, to compensate for the sparsity of target measurements, we introduce in the loss function an additional smoothness term S that tends to penalize diverse predictions at nearby spatial locations.</p><p>Given a distance function D (p, q) between two spatial locations p, q, we denote as N p the set of neighbours of spatial location p: N p = {q|D (p, q) &lt; δ}. We compute the average absolute difference between the disparity predicted at p and those predicted at each q ∈ N p :</p><formula xml:id="formula_3">E (p) = 1 |N p | q∈Np |D(q) −D(p)|<label>(4)</label></formula><p>The smoothing term is obtained by averaging E (p) across all spatial locations:</p><formula xml:id="formula_4">S = 1 |P | p∈P E (p)<label>(5)</label></formula><p>The distance function, D, as well as the radius of the neighborhood, δ, are hyper-parameters of the proposed smoothing term. It is worth observing that, optimized alone, such term would produce a uniform disparity map as output. However, when carefully weighted in conjunction with C L , it helps spreading the information associated with sparse target measurements towards the other spatial locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To validate our proposal we choose DispNet-Corr1D <ref type="bibr" target="#b9">[10]</ref>, from now on referred to as DispNet, as network architecture for end-to-end disparity regression, AD-CENSUS <ref type="bibr" target="#b23">[24]</ref> and SGM <ref type="bibr" target="#b5">[6]</ref> as off-the-shelf stereo algorithms and CCCN <ref type="bibr" target="#b16">[17]</ref> as confidence estimator. The choice of the confidence estimator has been driven by its top performance and broad applicability, the latter due to the method requiring only the disparity map to estimate the confidence. As for Dispnet, we modified the original authors code to incorporate our novel loss formulation and fine tuned the network starting from the publicly available weights obtained after training on synthetic data only. For CCCN we used the original implementation as well as the provided weights without any retraining or fine tuning. Lastly, we used a custom implementation of SGM and AD-CENSUS based on the original papers. We will firstly introduce the procedure used to properly tune the hyper-parameters of our learning process, then we will show that our method not only allows to effectively fine-tune the chosen disparity regression network without any labeled data but also does improve the generalization capability of the model across similar domains. images using AD-CENSUS as stereo algorithm and CCCN as confidence measure. The blue curve shows that the higher is τ the lower is the number of points used in our learning process. The orange curve reports the percentage of correct points between those selected by the confidence measure that belong also to the available sparse ground truth (less than 30% of the total points, black horizontal line), which is obtained by comparing the disparities estimated at the selected points to the ground truth disparities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning Process</head><p>To find optimal values for the hyper-parameters of our learning machinery, we choose to rely on the commonly used KITTI datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. In particular, to get insights on the training and generalization performance of our method, we have used the images from KITTI 2012 as training set and those from KITTI 2015 as test set. For all our experiments we initialize DispNet according to the weights obtained after 1200000 training steps on synthetic data and publicly released by the authors. In the experiments dealing with hyper-parameters tuning, we have used AD-CENSUS <ref type="bibr" target="#b23">[24]</ref> as stereo algorithm to compute the disparity maps that are then validated by the chosen confidence measure <ref type="bibr" target="#b16">[17]</ref> in order to sift-out the actual target variables.</p><p>For these experiments, to obtain useful insights in an acceptable training time, we carried out just 10000 fine tuning steps for each test configuration with batch size equal to 4 on the 194 KITTI 2012 images(∼200 epochs) and feeding the network with random crops of the original images of size 768 × 384. To increase the variety of the training set, we perform random data augmentation (color, brightness and contrast transformations) as done by the authors of <ref type="bibr" target="#b9">[10]</ref>. We use ADAM <ref type="bibr" target="#b7">[8]</ref> as optimizer with an initial learning rate equal to 0.0001 and an exponential decay every 2000 step with γ = 0.5.</p><p>The first parameter that needs to be carefully tuned is τ , which allows for filtering out wrong disparity assignments according to the scores provided by confidence measure. <ref type="figure" target="#fig_1">Figure 2</ref> shows that even for high values of τ we can get disparity maps denser than the available ground truth data for KITTI 2012. Moreover, cross comparing such points with the available sparse ground truth, we can ob- serve that, for quite high τ values (i.e. &gt; 0.9), nearly 100% of the points selected by our method that appear at available ground truth locations carry correct disparities. Although we cannot assess upon the correctness of the points selected by our method that do not coincide with available ground truth locations, there seems to be no reason to believe that the confidence measure would behave much differently therein. Therefore, <ref type="figure" target="#fig_1">Figure 2</ref> seems to support the intuition that high confidence disparities are very likely correct and hence may effectively act as "surrogate" ground truth data within our unsupervised learning process. Moreover, compared to the sparse ground truth data available in the KITTI datasets, a favourable property of our selected disparities is the larger spread across the whole image. This enables our method to look at portions of the scene seldom included in ground truth data. From <ref type="figure" target="#fig_2">Figure 3</ref> we can notice that for high values of τ , even though the density of our disparity map is similar (or slightly lower) with respect to the ground truth data, we gather samples more spread across all the image. For example, even with τ = 0.99, the top of the trees on the left and one of the farthest car in the scene are always visible in our unsupervised disparity map but not included in the available ground truth data. We will show in section 4.3 that this property leads to better generalization performance.</p><p>Given this preliminary observations, we tried different values for τ and report the training and generalization error in <ref type="figure" target="#fig_3">Figure 4</ref>. We observe a perfectly smooth descending behavior of the Training and Generalization error (percentage of wrongly predicted pixel) with increasing value of τ . Given this outcome we can conclude that the higher the value of τ the better the performance of the network. Thus, we set τ = 0.99. Such value selects, on this training set, 22.07% of available pixels (slightly less than the available ground truth points) with an accuracy of the pixels for which we have a ground truth disparity annotation equal to 99.65%. Once set τ , we evaluate how a proper tuning of the smoothing term of our loss function enables to improve the overall performance. For these experiments we choose as distance function D (p, q) the L1 distance and δ = 1. Keeping the same set-up as used to tune τ <ref type="figure" target="#fig_3">(Figure 4)</ref>, we perform experiments on the KITTI 2012 dataset with different values of λ ∈ [0, 1], the results reported in <ref type="figure" target="#fig_4">Figure 5</ref>. Looking at the training error it is clear how our regularization term can improve the performance of the network. However the value of λ must be kept &lt; 0.6 in order to not over-smooth predictions. More importantly, even the generalization performance of the network is influenced by the magnitude of λ, with the lowest generalization error obtained using λ = 0.1. WE believe that the explanation for this behavior is that the network compensates for the missing target measurements by creating a useful training signal thanks to the smoothing factor that propagates information from existing target measurements to nearby locations. However, the value of λ must be kept low so to not overcome the contribution of the confidence guided loss. From the careful tuning outlined so far, we found that the best configuration for our unsupervised framework is τ = 0.99 and λ = 0.1 using D (p, q) and δ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adaptation</head><p>Given the best configuration of hyper-parameters, we evaluate the effectiveness of our unsupervised adaptation   <ref type="table">Table 1</ref>. Adaptation results on the KITTI 2015 training dataset. DispNet: no fine-tuning; DispNet K12-GT: supervised fine-tuning on an annotated and quite similar dataset (KITTI 2012); DispNet CENSUS: unsupervised adaptation using the AD-CENSUS stereo algorithm; DispNet SGM: unsupervised adaptation using the SGM stereo algorithm. methodology when dealing with environments never seen before. To assess performance, on one hand we assume the KITTI 2012 training dataset as a known scenario on which ground-truth data to fine-tune DispNet are available . On the other hand, we assume KITTI 2015 and Middlebury 2014 as novel environments with no ground-truth available for fine-tuning. Thus, we perform unsupervised adaptation on KITTI 2015 and Middlebury 2014 and compare accuracy with respect to both the original DispNet architecture (i.e., trained on synthetic data only) as well as to DispNet finetuned on KITTI 2012 by the available ground truth. Following this protocol, we can prove that our unsupervised adaptation improves significantly the accuracy of the original network. i.e. that unsupervised fine-tuning is feasible and works well, and that, in absence of ground-truth data, unsupervised fine-tuning on the addressed scenario is more effective than transferring a supervised fine-tuning from another annotated (and quite similar) environment <ref type="bibr" target="#b0">1</ref> . To assess the performance of our proposal with different stereo algorithms, in these experiments we use AD-CENSUS and Semi-Global Matching (SGM), the latter leveraging as data term the final cost computed by AD-CENSUS and with smoothing penalties P 1 = 0.2 and P 2 = 0.5, being the matching costs between 0 and 1. <ref type="table">Table 1</ref> reports the error rate (i.e., the percentage of pixels having an error larger than θ) and the average disparity error on the entire KITTI 2015 (θ = 3) and Middlebury 2014 (θ = 1) training sets. For both datasets we use the standard evaluation protocol; for Middlebury we resized the stereo pairs to quarter resolution to have a disparity range similar to the KITTI datasets. We highlight how, regardless of the chosen off-the-shelf stereo algorithm being either AD-CENSUS or SGM, our unsupervised adaptation approach achieves higher accuracy with respect to the original DispNet architecture as well as to DispNet fine-tuned supervisedly on KITTI 2012 on both datasets and according to both metrics. <ref type="table">Table 1</ref> reports also on the first two rows the accuracy of the two stereo algorithms deployed for adaptation: their very high error rates demonstrate how the proposed confidence guided loss and smoothness term can handle effectively the high number of wrong assignments within the disparity maps yielded by the stereo algorithms that provide the "raw" target variables to the learning process.</p><p>As for the results on KITTI 2015, it is worth highlighting that our approach is able to outperform DispNet fine-tuned through the ground-truth data of a very similar dataset (i.e., KITTI 2012). Thus, despite the high similarity between the two datasets in terms of image content, which renders finetuning on KITTI 2012 beneficial to DispNet, as vouched by the nearly 3% decrease of the error rate and the reduced average disparity error, our proposed unsupervised adaptation turns out more effective obtaining an even higher accuracy. Moreover, we point out how our unsupervised adaptation method is effective with both the considered off-the-shelf stereo algorithms, which are characterized by quite different error rates and behaviors. This is particularly relevant to AD-CENSUS, whose average error rate is quite high (i.e., on average, more than 35% of wrong pixels in each map).</p><p>This experiment shows that our methodology can be deployed to effectively fine-tune a deep stereo network without the need of ground truth disparities. Moreover our confidence guided loss proves to be able to drastically improve the performance of a deep stereo system even if the raw target values used for the unsupervised tuning are very noisy, such as it the case of the disparity map computed by AD-CENSUS. Interestingly, DispNet adapted from such noisy data yields more accurate disparity maps with respect to undergoing a fine tuned based on ground truth data from a different though similar scenario. In a further experiment we included in our usupervised fine-tuning of DispNet based on AD-CENSUS only the stereo pairs of the KITTI 2015 training dataset with available ground-truth, i.e. given the scene labeled as "000000", we process unspervisedly only the "000000 10" stereo pairs rather than also those labeled as "000000 11", so to deploy a similar number of images   <ref type="table">Table 2</ref>. Results on the KITTI 2012 and KITTI 2015 training datasets. DispNet: no fine-tuning; DispNet K12-GT: supervised fine-tuning on the ground-truth from KITTI 2012; DispNet CEN-SUS: unsupervised adaptation on KITTI 2012 using the AD-CENSUS stereo algorithm; DispNet SGM: unsupervised adaptation on KITTI 2012 using the SGM stereo algorithm .</p><p>as DispNet fine-tuned on Kitti 2012. In these settings we observe only a modest increase of the error rate and average disparity error of about 0.09% and 0.04% respectively.</p><p>As for the evaluation on Middlebury 2014, we first highlight how fine-tuning DispNet on Kitti 2012 yields a large increase of the error rate with respect to the model trained on synthetic data only and does not significantly ameliorates the average disparity error (somehow similarly to Kitti 2015). This shows that, when fine-tuned on samples depicting very different environments (such as KITTI 2012 in this case), the network can reduce the magnitude of mismatching disparities but cannot increase the overall number of correct pixels (indeed, on Middlebury such amount is vastly decreased). Conversely, adapting unsupervisedly DispNet with our technique yields a substantial reduction of both the average disparity error as well as of the error rate, in particular by more than 11% when deploying SGM as the stereo algorithm. Overall, these results support the effectiveness of the proposed unsupervised adaptation approach even on a challenging and very varied environment such as the Middlebury dataset. In <ref type="figure" target="#fig_5">Figure 6</ref> we show qualitative results on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalization</head><p>Once assessed the superiority of unsupervised adaptation with respect to fine-tuning by ground-truth data from different datasets, we also inquire about the generalization capability of our technique when dealing with the same data as deployed by traditional fine-tuning based on groundtruth. In particular, we perform both traditional fine-tuning and unsupervised adaptation on the KITTI 2012 training  <ref type="table">Table 3</ref>. Intersection between confident points and ground-truth data as function of the threshold value τ and its error rate, for both AD-Census <ref type="bibr" target="#b23">[24]</ref> and SGM <ref type="bibr" target="#b5">[6]</ref> algorithms.</p><formula xml:id="formula_5">AD-CENSUS SGM τ gt ∩ τ (%) bad 3 (%) gt ∩ τ (%) bad 3 (%) 0</formula><p>dataset, then we evaluate the performance of the networks also on the KITTI 2015 training dataset in order to assess generalization performance 2 . We perform unsupervised adaptation on the frames with available ground-truth only (i.e., given 000000 scene and its stereo pairs labeled as " 10" and " 11", we obtain disparity and confidence only for the first pair), in order to make use of the same number of stereo pairs in the different tuning procedures for a fair comparison. <ref type="table">Table 2</ref> reports error rates (i.e., the percentage of pixels having a disparity error larger than 3) and average disparity error on both KITTI 2012 and KITTI 2015 training datasets. As we could expect, the network fine-tuned on ground-truth data (DispNet K12-GT) achieves a lower error rate with respect to the networks adapted unspervisedly. On the other hand, the unsupervised technique yields a lower average disparity error. To test the generalization property, we focus on results obtained on the KITTI 2015 dataset. Our unsupervised adaptation enables the network to outperform that fine-tuned supervisedly regarding both the error rate and the average disparity error, whatever stereo algorithm is deployed during the training phase.</p><p>These results can be explained by recalling the consideration already discussed in Section 4. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the pixels with a confidence higher than τ are more widely spread throughout the image than the available ground-truth pixels. <ref type="table">Table 3</ref> reports the intersection between confident (i.e., having a confidence value higher than the threshold τ ) and ground-truth pixels as percentage of the total amount of available ground-truth samples; as expected, increasing τ such intersection gets smaller. In particular, with a threshold value of 0.99 and the AD-Census algorithm the subset of pixels processed during adaptation contains only 32% of the ground-truth data used by the common fine-tuning technique, while with the same threshold and the SGM algorithm this percentage rises to 68%. This means that all the remaining samples contributing to adaptation (i.e. 68 and 32% for, respectively, AD-CENSUS and SGM) encode patterns unseen using a traditional fine-tuning procedure. Thus, the network can learn from more varied and generic samples with respect to ground-truth which is, among other things, all contained in the lower part of the images. Moreover, the Table also reports the average error rate (bad 3) on the intersection, about 1% for both algorithms, stressing how the disparities computed on this subset of pixel are almost equivalent to ground-truth data. Assuming this property to be true for the rest of the pixels having confidence higher than τ , the unsupervised adaptation can learn many behaviors not encoded by the pixels providing the groundtruth, which is conducive to better generalization.</p><formula xml:id="formula_6">(a) (b) (c) (d)<label>(e)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results on Challenging Sequences</head><p>To further test the effectiveness of the proposed approach, we adapt unsupervisedly DispNet on a set of challenging stereo sequences acquired in bad weather conditions <ref type="bibr" target="#b10">[11]</ref>. Peculiar to these sequences is the unavailability of ground-truth data, making them a well-fitting case study for our proposal. <ref type="figure" target="#fig_6">Figure 7</ref> reports some notable examples, on which the adaptation technique prove to solve most of the issues related to illumination and weather conditions. Additional examples are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We have demonstrated that it is possible to adapt a deep learning stereo network to a brand new environment without using ground-truth disparity labels. The implementation code will be made available <ref type="bibr" target="#b2">3</ref> . The experimental evaluation proved that our proposal can better generalize when moving to similar contexts with respect to fine-tuning techniques based on sparse ground-truth data. Based on these findings, we plan to investigate on whether and how our approach may be deployed to train from scratch in a completely unsupervised manner a deep stereo network. Purposely, we may leverage jointly on different and somehow complementary stereo algorithms <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15]</ref> as raw target disparities to be validated by the confidence estimator. Another line of further research concerns the development of a realtime self-adaptive stereo system, which would be able to adapt autonomously and on-line to an unseen environment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Effectiveness of unsupervised adaptation. (a),(b): Left and right images belonging to a challenging stereo pair of the dataset without ground-truth proposed in [11]. (c): Output provided by Dispnet-Corr1D [10]. (d): Output achieved after unsupervised adaptation of Dispnet-Corr1D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Percentage of points with confidence&gt; τ on KITTI 2012 images using AD-CENSUS as stereo algorithm and CCCN as confidence measure. The blue curve shows that the higher is τ the lower is the number of points used in our learning process. The orange curve reports the percentage of correct points between those selected by the confidence measure that belong also to the available sparse ground truth (less than 30% of the total points, black horizontal line), which is obtained by comparing the disparities estimated at the selected points to the ground truth disparities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Spatial distribution of training samples on stereo pair 000073 from KITTI 2015. Top row: reference image, disparity map yielded by the AD-CENSUS algorithm and corresponding confidence map obtained by CCNN [17]. Bottom row, from left to right: three colormaps obtained by thresholding the confidence map with τ equal to 0, 0.5 and 0.99, respectively. The colormaps depict in green the points above threshold and in blue their intersection with the available ground-truth points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Performance of the network after 10000 steps of finetuning for different values of τ . We report as Training Error the percentage of pixel with disparity mismatch &gt; 3 on the training set (KITTI 2012) and as Generalization Error the same metric computed on unseen data from KITTI 2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Performance of the network after 10000 steps of finetuning for different values of λ and with τ = 0.99.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative result on the PianoL image from the Middlebury 14 dataset with average error reported between bracket. From left to right, ground truth disparity map (white points are undefined) and disparity maps obtained with different stereo algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Unsupervised adaptation on action. (a) reference image, (b) disparity map according to census algorithm [24], (c) disparity map filtered by CCNN [17], (d) outcome of DispNet before adaptation, (e) final disparity map, by adapted DispNet.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This protocol is also compliant to the KITTI submission rules, which forbid to process the test data in any manner before submitting results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We follow this protocol to avoid multiple submission to the KITTI benchmark.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/CVLAB-Unibo/ Unsupervised-Adaptation-for-Deep-Stereo</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time stereo vision system using semi-global matching disparity estimation: Architecture and fpga-implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Banz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hesselbarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Flatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSAMOS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="93" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A deep visual correspondence embedding model for stereo matching costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="972" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A real-time low-power stereo vision engine using semi-global matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eberli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Rob. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensemble learning for confidence measures in stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeusler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. Proceedings</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="328" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A quantitative evaluation of confidence measures for stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2121" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference for Learning Representations</title>
		<meeting>the 3rd International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient Deep Learning for Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Outdoor stereo camera system for the generation of real-world benchmark data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jähne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page">21107</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using self-contradiction to learn confidence measures in stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mostegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rumpler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leveraging stereo matching with learning-based confidence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep stereo fusion: combining multiple disparity hypotheses with deep-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on 3D Vision</title>
		<meeting>the 4th International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a general-purpose confidence measure based on o(1) features and a smarter aggregation strategy for semi global matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on 3D Vision</title>
		<meeting>the 4th International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from scratch a confidence measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th British Conference on Machine Vision, BMVC</title>
		<meeting>the 27th British Conference on Machine Vision, BMVC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of confidence measures in a machine learning world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV), ICCV&apos;17</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV), ICCV&apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nešić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Patch based confidence prediction for dense disparity map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to detect ground control points for improving the accuracy of stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble classifier for combining stereo matching algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on 3D Vision, 3DV &apos;15</title>
		<meeting>the 2015 International Conference on 3D Vision, 3DV &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third European Conference on Computer Vision</title>
		<meeting>the Third European Conference on Computer Vision<address><addrLine>Secaucus, NJ, USA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
	<note>ECCV &apos;94. Inc. 3, 4, 6</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
