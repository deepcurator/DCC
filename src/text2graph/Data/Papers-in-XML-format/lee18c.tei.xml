<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Path Planning Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
						</author>
						<title level="a" type="main">Gated Path Planning Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A common type of sub-task that arises in various reinforcement learning domains is path finding: finding a shortest set of actions to reach a subgoal from some starting state. Path finding is a fundamental part of any application which requires navigating in an environment, such as robotics <ref type="bibr" target="#b0">(Ackerman &amp; Guizzo, 2015)</ref> and video game AI <ref type="bibr" target="#b17">(Silver, 2005)</ref>. Due to its ubiquity in these important applications, recent work <ref type="bibr" target="#b20">(Tamar et al., 2017)</ref> has designed a differentiable submodule that performs path-finding as directed by the agent in some inner loop. These Value Iteration Network (VIN) * Equal contribution 1 Carnegie Mellon University, Machine Learning Department. Correspondence to: Lisa Lee &lt;lslee@cs.cmu.edu&gt;, Emilio Parisotto &lt;eparisot@cs.cmu.edu&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s). modules mimic the application of Value Iteration on a 2D grid world, but without a pre-specified model or reward function. VINs were shown to be capable of computing near-optimal paths in 2D mazes and 3D landscapes where the transition model P (s 0 |s, a) was not provided a priori and had to be learned.</p><p>In this paper, we show that VINs are often plagued by training instability, oscillating between high and low performance between epochs; random seed sensitivity, often converging to different performances depending on the random seed that was used; and hyperparameter sensitivity, where relatively small changes in hyperparameters can cause diverging behaviour. Owing to these optimization difficulties, we reframe the VIN as a recurrent-convolutional network, which enables us to replace the unconventional recurrent VIN update (convolution &amp; max-pooling) with well-established gated recurrent operators such as the LSTM update <ref type="bibr" target="#b6">(Hochreiter &amp; Schmidhuber, 1997)</ref>. These Gated Path Planning Networks (GPPNs) are a more general model that relaxes the architectural inductive bias of VINs that was designed to perform a computation resembling valueiteration.</p><p>We then establish empirically that GPPNs perform better or equal to the performance of VINs on a wide variety of 2D maze experiments, including different transition models, maze sizes and different training dataset sizes. We further demonstrate that GPNNs exhibit fewer optimization issues than VINs, including reducing random seed and hyperparameter sensitivity and increasing training stability. GPPNs are also shown to work with larger kernel sizes, often outperforming VINs with significantly fewer recurrent iterations, and also learn faster on average and generalize better given less training samples. Finally, we present results for both VIN and GPPN on challenging 3D ViZDoom environments <ref type="bibr" target="#b8">(Kempka et al., 2016)</ref>, where the planner is only provided with first-person RGB images instead of the top-down 2D maze design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In reinforcement learning, the environment is formulated as a Markov decision process (MDP) consisting of states s, actions a, a reward function R, and state transition kernels P (s 0 | s, a). Value iteration is a method of computing an optimal policy ⇡ and its value </p><formula xml:id="formula_0">V ⇡ (s) = E ⇡ [ P 1 t=0 t R(s t , a t , s t+1 ) | s 0 = s],</formula><formula xml:id="formula_1">Q (k) (s, a) = X s 0 P (s 0 | s, a) ⇣ R(s, a, s 0 ) + V (k 1) (s 0 ) ⌘ , V (k) (s) = max a Q (k) (s, a).</formula><p>The value function V (k) converges to V ⇤ in the limit as k ! 1, and the optimal policy can be recovered as ⇡ ⇤ (s) := arg max a Q (1) (s, a) <ref type="bibr" target="#b19">(Sutton &amp; Barto, 2018)</ref>.</p><p>Despite the theoretical guarantees, value iteration requires a pre-specified environment model. <ref type="bibr" target="#b20">Tamar et al. (2017)</ref> introduced the Value Iteration Network (VIN), which is capable of learning these MDP parameters from data automatically. The VIN reformulates value iteration as a recursive process of applying convolutions and max-pooling over the feature channels:</p><formula xml:id="formula_2">Q (k) a,i 0 ,j 0 = X i,j ⇣ W R a,i,jRi 0 i,j 0 j + W V a,i,jV (k 1) i 0 i,j 0 j ⌘ , V (k) i,j = max aQ (k) a,i,j ,<label>(1)</label></formula><p>where the indices i, j 2 [m] correspond to cells in the m⇥m maze,R,Q,V is the VIN estimated reward, action-value and value functions, respectively,ā is the action index of thē Q feature map, and W R , W V are the convolutional weights for the reward function and value function, respectively. In the following iteration, the previous valueV is stacked with R for the convolution step. <ref type="bibr" target="#b20">Tamar et al. (2017)</ref> showed that VINs have much greater success at path planning than baseline CNN and feedforward architectures in a variety of 2D and graph-based navigation tasks. The demonstrated success of VIN has made it an important component of models designed to solve downstream tasks where navigation is crucial <ref type="bibr" target="#b7">(Karkus et al., 2017;</ref><ref type="bibr" target="#b2">Gupta et al., 2017a;</ref>. For example, <ref type="bibr" target="#b2">Gupta et al. (2017a;</ref> designed a Deep RL agent to perform navigation within partially observable and noisy environments by combining a VIN module with a 2D-structured memory map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, we explore whether the inductive biases provided by the VIN are even necessary: is it possible that using alternative, more general architectures might work better than those of the VIN? We can view the VIN update (1) within the perspective of a convolutional-recurrent network, updating a recurrent state V (k) i 0 ,j 0 at every spatial position</p><formula xml:id="formula_3">(i 0 , j 0 ) in each iteration: V (k) i 0 ,j 0 = max a 0 @ X i,j W R a,i,jRi 0 i,j 0 j + W V a,i,jV (k 1) i 0 i,j 0 j 1 A = max a ⇣ W R aR[i 0 ,j 0 ,3] + W V aV (k 1) [i 0 ,j 0 ,3] ⌘ ,<label>(2)</label></formula><p>where X [i 0 ,j 0 ,F ] denotes the image patch centered at position (i 0 , j 0 ) with kernel size F . From (2), it can be seen that VIN follows the standard recurrent neural network (RNN) update where the recurrent state is updated by taking a linear combination of the inputR and the previous recurrent stateV (k 1) , and passing their sum through a nonlinearity maxā. The main differences from a standard RNN are the following: the non-conventional nonlinearity (channelwise max-pooling) used in VIN; the hidden dimension of the recurrent network, which is essentially one; the sparse weight matrices, where the non-zero values of the weight matrices represent neighboring inputs and units which are local in space; and the restriction of kernel sizes to 3.</p><p>Under this perspective, it is easy to question whether the adherence to these strict architectural biases is even necessary, given the long history of demonstrations that standard non-gated recurrent operators are difficult to optimize due to effects such as vanishing and exploding gradients <ref type="bibr" target="#b15">(Pascanu et al., 2013)</ref>.</p><p>We can easily replace the recurrent VIN update in (2) with the well-established LSTM update <ref type="bibr" target="#b6">(Hochreiter &amp; Schmidhuber, 1997)</ref>, whose gated update alleviates many of the problems with standard recurrent networks:</p><formula xml:id="formula_4">h (k) i 0 ,j 0 , c (k) i 0 ,j 0 = LSTM X a ⇣ W R aR[i 0 ,j 0 ,F ] + W h a h (k 1) [i 0 ,j 0 ,F ] ⌘ , c (k 1) i 0 ,j 0 ! ,<label>(3)</label></formula><p>where F is the convolution kernel size. This recurrent update (3) still maintains the convolutional properties of the input and recurrent weight matrix as in VIN. It involves taking as input the F ⇥ F convolution of the input vectorR and previous hidden states h <ref type="bibr">(k 1)</ref> , and the previous cell state</p><formula xml:id="formula_5">c (k 1)</formula><p>i 0 ,j 0 of the LSTM at the central position (i 0 , j 0 ). We call path planning modules which use these gated updates Gated Path Planning Networks (GPPNs). The GPPN is an LSTM which uses convolution of previous spatially-contiguous hidden states for its input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Environments and Maze Transition Types</head><p>We test VIN and GPPN on 2D maze environments and 3D ViZDoom environments <ref type="figure" target="#fig_0">(Figure 1</ref>) on a variety of settings such as training dataset size, maze size and maze transition kernel.</p><p>We used three different maze transition kernels: In NEWS, the agent can move North, East, West, or South; in Differential Drive, the agent can move forward along its current orientation, or turn left/right by 90 degrees; in Moore, the agent can move to any of the eight cells in its Moore neighborhood. In the NEWS and Moore transition types, the target is an x-y coordinate, while in Differential Drive the target contains an orientation along with the x-y coordinate. Consequently, the dimension of the goal map given as input to the models is 1 ⇥ m ⇥ m for NEWS and Moore, and 4 ⇥ m ⇥ m for Differential Drive, where m is the maze size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">2D Maze Environment</head><p>The 2D maze environment is created with a maze generation process that uses Depth-First Search with the Recursive Backtracker algorithm (Maze Generation Algorithms, 2018) to construct the maze tree, resulting in a fully connected maze (see <ref type="figure" target="#fig_0">Figure 1a)</ref>. For each maze, we sample a probability d uniformly from <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Then for each wall, we delete the wall with probability d.</p><p>For our experiments on the 2D mazes, the state vector consists of the maze and the goal location, each of which are represented by a binary m ⇥ m matrix, where m ⇥ m is the maze size. We use early stopping based on validation set metrics to choose the final models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D ViZDoom Environment</head><p>We use the Doom Game Engine and the ViZDoom API <ref type="bibr" target="#b8">(Kempka et al., 2016)</ref> to create mazes in a simulated 3D environment (see <ref type="figure" target="#fig_0">Figure 1b)</ref>. The maze design for the 3D mazes are generated in exactly the same manner as the 2D mazes, using Depth-First Search with the Recursive Backtracker algorithm followed by wall pruning with a uniformly sampled probability d. In the 3D ViZDoom experiments, these map images are given as input to the model (instead of the 2D map design). This setup is similar to the one used for localization experiments by <ref type="bibr" target="#b1">Chaplot et al. (2018)</ref> who argue that these images are easier to obtain as compared to constructing an accurate map design of an environment in the real world. The model needs to learn to infer the map design from these images along with learning to plan, which makes the task more challenging in 3D environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments &amp; Discussion</head><p>In this section, we empirically compare VIN and GPPN using two metrics: %Optimal (%Opt) is the percentage of states whose predicted paths under the policy estimated by the model has optimal length, and %Success (%Suc) is the percentage of states whose predicted paths under the policy estimated by the model reach the goal state. The reported performance is on a held-out test split. In contrast with the metrics reported in <ref type="bibr" target="#b20">(Tamar et al., 2017)</ref>, we do not stochastically sample rollouts but instead evaluate and train the output policy of the models directly on all states simultaneously. This reduces optimization noise and makes it easier to tell whether difficulties with training are due to sampling noise or model architecture/capacity.</p><p>All analyses are based on 2D maze results, except in Section 5.8 where we discuss 3D ViZDoom results. In order to make comparison fair, we utilized a hidden dimension of 150 for GPPN and 600 for VIN, owing to the approximately 4⇥ increase in parameters a GPPN contains due to the 4 gates it computes. Unless otherwise noted, the results were obtained by doing a hyperparameter sweep of (K, F ) over K 2 {5, 10, 15, 20, 30} and F 2 {3, 5, 7, 9, 11}, and using a 25k/5k/5k train-val-test split. Other experimental details are deferred to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Varying Kernel Size F</head><p>One question that can be asked of the architectural choices of the VIN is whether the kernel size needs to be the same dimension as the true underlying transition model. The kernel size used in VIN was set to 3 ⇥ 3 with a stride of 1, which is sufficient to represent the true transition model when the agent can move anywhere in the Moore neighborhood, but it limits the rate at which information propagates spatially with each iteration. With a kernel size of 3 ⇥ 3 and stride of 1, the receptive field of a unit in the last iteration's feature map increases with rate (3 + 2K) ⇥ (3 + 2K) where K is the iteration count, meaning that the maximum path length information travels scales directly with iteration count k. Therefore for long-term planning in larger environments, <ref type="bibr" target="#b20">Tamar et al. (2017)</ref> designed a multi-scale variant called the Hierarchical VIN. Hierarchical VINs rely on downsampling the maps into multi-scale hierarchies, and then doing VIN planning and up-scaling, progressively growing the map until it regains its original, un-downsampled size.</p><p>Another potential method to do long-range planning without requiring a multi-scale hierarchy is to instead increase the kernel size. An increased kernel size would cause the receptive field to grow more rapidly, potentially allowing the models to require fewer iterations K before reaching well-performing policies. In this section, we sought to test out the feasibility of increasing the kernel size of VINs and GPPNs. These results are summarized in <ref type="table" target="#tab_1">Table 1</ref>. All the models were trained with the best K setting for each F and transition kernel. From the results, we can clearly see that GPPN can handle training with larger F values, and moreover, GPPN often performs better than VIN with larger values of F . In contrast, we can observe that VIN's performance drops significantly after its kernel size is increased more than 5, with its best performing settings being either 3 or 5 depending on the true transition model. These results show that GPPN can learn planning approximations that work with F &gt; 3 much more stably than VIN, and could further suggest that GPPN can work as well as VIN with less iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Varying Iteration Count K</head><p>Following the above results showing that GPPN benefits from increased F , we further evaluated the effect of varying both iteration count K and kernel size F on the VIN and GPPN models. <ref type="table">Table 2</ref> shows %Optimal and %Success results of VIN and GPPN on 15⇥15 2D mazes for different values of F and K. We can see from NEWS column in the table that GPPN with F &gt; 7 can get results on par with the best VIN model with only K = 5 iterations. This shows that GPPN can learn to more effectively propagate information spatially in a smaller number of iterations than VIN can, and outperforms VIN even when VIN is given a much larger number of iterations. Additionally, we can see that VIN has significant trouble learning when both K and F are large in the differential drive mazes and to a lesser extent in the NEWS mazes. <ref type="table">Table 3</ref> shows the results of VIN and GPPN with varying iteration counts K and the best F setting for each K. Owing to the larger kernel size, GPPN with smaller number of iterations K  10 can get results on par with the best VIN model. Generally, both models benefit from a larger K (assuming the best F setting is used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Different Maze Transition Kernels</head><p>From <ref type="table" target="#tab_1">Tables 1 and 3</ref>, we can observe the performance of VIN and GPPN across a variety of different underlying groundtruth transition kernels (NEWS, Moore, and Differential Drive). From these results, we can see that GPPN consistently outperforms VIN on all the transition kernel types. An interesting observation is that VIN does very well at Differential Drive, consistently obtaining high results, although GPPN still does better than or on par with VIN. The reasons why VIN is so well suited to Differential Drive are not clear, and a preliminary analysis of VIN's feature weights and reward vectors did not reveal any intuition on why this is the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effect of Dataset Size</head><p>A potential benefit of the stronger architectural biases of VIN might be that they can enable better generalization given less training data. In this section, we designed experiments that set out to test this hypothesis. We trained VINs and GPPNs on datasets with varying number of training samples for all three maze transition kernels, and the results are given in <ref type="table" target="#tab_3">Table 4</ref>. We can see that GPPN consistently outperforms VIN across all dataset sizes and maze models. Interestingly, we can observe that the performance gap between VIN and GPPN is larger the less data there is, demonstrating the opposite effect to our hypothesis. This could suggest that the architectural biases do not in fact aid generalization performance, or that there is another problem, such as perhaps the difficulty of optimizing VIN, that overshadows the benefit that the inductive bias could potentially provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Random Seed and Hyperparameter Sensitivity</head><p>The hypothesis this section sought to verify was whether the particular recurrent-convolutional form of the VIN did indeed negatively affect its optimization, as many ungated <ref type="table">Table 2</ref>. Test performance on 2D mazes of size 15 ⇥ 15 with varying kernel sizes F and iteration counts K. "-" indicates the training diverged. GPPN outperforms VIN under best settings of (K, F ), indicated in bold. By utilizing a larger F , GPPN can learn to more effectively propagate information spatially in a smaller number of iterations (K  10) than VIN can. %Opt for NEWS %Opt for Moore %Opt for Differential Drive Model K F = 3 F = 5 F = 7 F = 9 F = 11 F = 3 F = 5 F = 7 F = 9 F = 11 F = 3 F = 5 F = 7 F = 9 F = 11 recurrent updates suffer from optimization problems including training instability and higher sensitivity to weight initialization and hyperparameters due to gradient scaling problems <ref type="bibr" target="#b15">(Pascanu et al., 2013)</ref>.</p><p>We test each architecture's sensitivity to random seeds by running several experiments with the same hyperparameters but different random seeds, and measuring the variance in their final performance. These results are reported in Table 5. The results show that GPPN gets consistently lower variance than VIN over different random seed initializations, supporting the hypothesis that the LSTM update enables more training stability and easier optimization than the ungated recurrent update in VIN.</p><p>We additionally test hyperparameter sensitivity in <ref type="figure">Figure 2</ref>. We take all the results obtained on a hyperparameter sweep over settings (K, F ) where K was varied over K 2 {5, 10, 15, 20, 30} and F was varied over F 2 {3, 5, 7, 9, 11}. We then rank these results, and the x-axis is the top-x ranked hyperparameter settings and the corresponding y-axis is the average %Opt/%Suc of those x settings. This plot thus measures how stable the performance of the architecture is to hyperparameter changes as the number of hyperparameter settings we consider grows. Therefore, architectures whose average top-x ranked performance remains high and relatively flat demonstrates that good performance with the architecture can be obtained with many different hyperparameter settings. This suggests that these models are both easier to optimize and consistently better than alternatives, and higher performance was not due to a single lucky hyperparameter setting. We can see from the figures that the performance of GPPN is clearly both higher and more stable over hyperparameter settings than VIN.</p><p>In <ref type="figure">Figure 3</ref>, we plot the learning curves for VIN and GPPN on 2D mazes with varying K and F . These plots show that VIN's performance often oscillates between epochs (especially for larger kernel sizes F &gt; 3), while GPPN is much more stable. Learning curves for other experiments showing a similar result are included in the Appendix. The <ref type="table">Table 5</ref>. Mean and standard deviation %Opt after 30 epochs, taken over 3 runs, on 2D mazes of size 15⇥15. Bold indicates best result across all K for each model and transition kernel. The results were obtained using the best setting of F for each K and dataset size 100k. GPPN exhibits lower variance between runs.  <ref type="figure">Figure 2</ref>. The y-axis is the average Test %Opt (or %Suc) of the topn hyperparameter settings (K, F ) over K 2 {5, 10, 15, 20, 30} and F 2 {3, 5, 7, 9, 11}. The results are on 2D mazes of size 15 ⇥ 15. These plots measure how stable the performance of each model is to hyperparameter changes as we increase the number of hyperparameter settings considered. GPPN exhibits less hyperparameter sentivitiy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEWS %Opt</head><p>training stability of GPPN provides more evidence to the hypothesis that GPPNs are simpler to optimize than VINs and consistently outperform them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Learning Speed</head><p>In this section, we examine whether VINs or GPPNs learn faster. To do this, we measure the number of training epochs (passes over the entire dataset) that it takes for each model to reach a specific %Opt for the first time. These results are reported in <ref type="table">Table 6</ref>. We can see from this table that GPPN learns significantly faster, often reaching 95% within 5-6 epochs. Comparatively, VIN sometimes never reaches 95%, as is the case for the NEWS mazes, or it takes 2-5 times as many epochs. This is the case even on the Differential Drive mazes, where VIN takes 2-3 times longer to train despite also getting high final performance. <ref type="table">Table 6</ref>. The number of epochs it takes for each model to attain a certain %Opt (50%, 75%, 90%, 95%) on the validation set under best settings of (K, F ). The results are on 2D mazes of size 15 ⇥ 15. GPPN learns faster.  <ref type="table">Table 7</ref>. Test performance on 2D mazes with varying maze sizes m ⇥ m under best settings of (K, F ) for each model. For the larger 28 ⇥ 28 maze, we train for 100 epochs and sweep over K 2 {14, 28, 56} to account for longer trajectories required to solve some mazes. GPPN performs better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEWS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEWS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Larger Maze Size</head><p>To test whether the improved performance GPPN persists even on larger, more challenging mazes, we evaluated the models on a dataset of mazes of size 28 ⇥ 28, and varied K 2 {14, 28, 56} <ref type="table">(Table 7)</ref>. We used a training dataset size of 25k. GPPN outperformed VIN by a significant margin (3-6% for %Opt and %Suc) for all cases except Diff. Drive 15 ⇥ 15, where the gap was closer (GPPN 99.3 vs. VIN 98.3 for %Opt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">3D ViZDoom Experiments</head><p>In the 3D ViZDoom experiments, the state vector consists of RGB images showing the first-person view of the environment at each position and orientation, instead of the top-down 2D maze design (represented by a binary m ⇥ m matrix) as in the 2D maze experiments. To process the map images, we use a Convolutional Neural Network <ref type="bibr" target="#b10">(LeCun et al., 1989)</ref> consisting of two convolutional layers: first layer with 32 filters of size 8 ⇥ 8 and a stride of 4, and second layer with 64 filters of size 4 ⇥ 4 with a stride 2 ⇥ 2, followed by a linear layer of size 256.</p><p>1 The 256-dimensional representation for all the 4 orientations at each location is concatenated to create a 1024-dimensional representation. These representations of each location are then stacked at the corresponding x-y coordinate to create a map representation of size 1024 ⇥ m ⇥ m. The map representation is then <ref type="figure">Figure 3</ref>. Performance on 2D mazes of size 15 ⇥ 15 with varying iteration counts K and kernel sizes F . All models are trained using dataset size 25k. VIN exhibits higher training instability, its performance often oscillating between epochs.</p><p>passed through two more convolutional layers (first layer with 64 filters and the second layer with 1 filter, both of size 3 ⇥ 3 and a stride of 1) to predict a maze design matrix of size 1 ⇥ m ⇥ m, which is trained using an auxillary binary cross-entropy loss. The predicted maze design is then stacked with the goal map and passed to the VIN or GPPN module in the same way as the 2D experiments.</p><p>The 3D ViZDoom results are summarized in <ref type="table">Table 8</ref>. %Acc is the accuracy for predicting the top-down 2D maze design from first-person RGB images. Learning to plan in the 3D environments is more challenging due to the difficulty of simultaneously optimizing both the original planner loss and the auxiliary maze prediction loss. We can see that when %Acc is low, i.e., the planner module must rely on a noisy maze design, then the planner metrics %Opt and %Suc also suffer. We observe that VIN is more prone to overfitting on the training dataset: its validation %Acc is low (&lt; 91%) for all three transition kernels, whereas GPPN achieves higher validation %Acc on NEWS and Moore. However, GPPN also overfits on the Differential Drive. <ref type="bibr" target="#b7">Karkus et al. (2017)</ref> looked at extending differentiable planning towards being able to plan in partially observable environments. In their setting, the agent is not provided a-priori with its position within the environment and thus needs to maintain a belief state over where it actually is. Similar to VIN's differentiable extension of VI, the QMDP-Net archi- <ref type="table">Table 8</ref>. Performance on 3D ViZDoom mazes. %Acc is accuracy for predicting the top-down 2D maze design from first-person RGB maze images. When %Acc is low, then the model must use a noisy maze design from which to plan, so %Opt and %Suc suffer as well. The results were obtained using K = 30, the best setting of F for each transition kernel, a smaller dataset size 10k (due to memory and time constraints), a smaller learning rate 5e-4, and 100 training epochs. VIN is more prone to overfitting: its validation %Acc is low for all three transition kernels, while GPPN achieves higher validation %Acc on NEWS and Moore. tecture was based on creating a differentiable analogue of the QMDP algorithm <ref type="bibr" target="#b12">(Littman et al., 1995)</ref>, an algorithm designed to approximate belief space planning in POMDPs. The architecture itself consisted of a filter module, which maintained the beliefs over which states the agent currently was in, and a planning module, which determined what action to take next. The planning module was essentially using a VIN to enable it to make more informed decisions on which parts of the environment to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Works</head><p>In recent work there has been a variety of deep reinforcement learning models that have examined combining an internal planning process with model-free methods. The Predictron <ref type="bibr" target="#b18">(Silver et al., 2016</ref>) was a value function approximator which predicted a policy's value by internally rolling out an LSTM forward predictive model of the agent's future rewards, discounts and values. These future rewards, values and discounts were then accumulated together, with the idea that this would predict a more accurate value by forcing the architecture to model a multi-step rollout. A later extension, Value Predictive Networks <ref type="bibr" target="#b14">(Oh et al., 2017)</ref>, learnt a forward model that is used to predict the future rewards and values of executing a multi-step rollout. Although similar to the Predictron, they considered the control setting, where not only a value function had to be learnt but a policy as well. They demonstrated that their model, trained using modelfree methods, was able to outperform existing methods on a 2D goal navigation task and outperformed DQN on Atari games.</p><p>Convolutional-recurrent networks similar to the VIN and GPPN have had a recent history of use within computer vision, particularly for applications which have both a spatial and temporal aspect. Convolutional LSTMs (ConvLSTMs) were first used in the application of precipitation nowcasting, where the goal was to predict rainfall intensity within a region using past data <ref type="bibr" target="#b16">(Shi et al., 2015)</ref>. Recurrentconvolutional networks have also been used within computer vision applications where there is no explicit temporal aspect, such as object recognition. Feedback Networks <ref type="bibr" target="#b21">(Zamir et al., 2017</ref>) utilized a ConvLSTM in order to allow information to feedback from higher layers to lower layers by unrolling the ConvLSTM over time. This enabled the Feedback Network to attain performance better than or on par with Residual Networks (ResNets) <ref type="bibr" target="#b5">(He et al., 2016)</ref>, one of the most commonly used feedforward architectures for object recognition.</p><p>A deeper connection has also been explored between residual and convolutional-recurrent networks. <ref type="bibr" target="#b11">(Liao &amp; Poggio, 2016)</ref> tested whether weight tying between layers in a ResNet significantly affects performance, finding that although performance slightly degrades, the change is not drastic. They provide some hypotheses on these results, suggesting that deep feedforward networks like ResNets are approximating recurrent networks in some capacity. While the GPPN can be seen as an instance of ConvLSTMs, our paper is the first to apply it to the domain of differentiable path planning and to show that, in general, structuring differentiable path planning within the context of convolutionalrecurrent networks enables use of previous well-established recurrent architectures such as LSTM and GRUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we re-formulated VIN as a convolutionalrecurrent network and designed a new planning module called the Gated Path Planning Network (GPPN) which replaced the unconventional recurrent update in VIN with a well-established gated LSTM recurrent operator. We presented experimental results comparing VIN and GPPN on 2D path-planning maze tasks and a 3D navigation task in the video game Doom, showing that the GPPN achieves results no worse and often better than VIN. The LSTM update alleviates many of the optimization issues including training instability and sensitivity to random seeds and hyperparameter settings. The GPPN is also able to utilize a larger kernel size, which the VIN is largely unable to do due to training instability, allowing the GPPN to work as well as VIN with fewer iterations. The GPPN also learns significantly faster, attaining high performance after only a few epochs, whereas the VIN takes longer to train. Finally, the relative performance improvement of GPPN over VIN increases with less training data. In conclusion, our analyses suggest that the inductive biases of VIN are not necessary in the design of a well-performing differentiable path planning module, and that the use of more general, gated recurrent architectures provides significant benefits over VINs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) A sample 2D maze. (b) A sample 3D Doom maze and examples of screenshots showing the first-person view of the environment at three locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For each Doom maze, we take RGB screenshots showing the first-person view of the environment at each position and orientation. A sample 3D Doom maze and example screenshot images are shown in Figure 1. For an m ⇥ m maze with 4 orientations, this results in a total of 4m 2 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>where 2 [0, 1] is a discount factor and R(s t , a t , s t+1 ) is a reward function. More specifically, value iteration starts with an arbitrary function V (0) and iteratively computes:</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Test performance on 2D mazes of size 15 ⇥ 15 with vary- ing kernel sizes F and best K setting for each F . Bold indicates best result across all F for each model and transition kernel. VIN performs worse with larger F , while GPPN is more robust when F is varied and actually works better with larger F .</figDesc><table>NEWS 
Moore 
Diff. Drive 
Model F %Opt %Suc %Opt %Suc %Opt %Suc 

VIN 3 93.4 93.5 90.5 91.3 98.4 99.1 
VIN 5 93.9 94.1 96.3 96.6 96.4 98.6 
VIN 7 92.7 93.0 95.1 95.6 92.2 96.2 
VIN 9 86.8 87.8 92.0 93.0 91.2 95.2 
VIN 11 87.6 88.3 92.7 93.8 87.9 93.8 

GPPN 3 97.6 98.3 96.8 97.6 96.4 98.1 
GPPN 5 98.6 99.0 98.4 99.1 98.7 99.5 
GPPN 7 99.0 99.3 98.8 99.3 99.1 99.7 
GPPN 9 99.0 99.4 98.8 99.3 99.3 99.7 
GPPN 11 99.2 99.5 98.6 99.2 99.2 99.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 3. Test performance on 2D mazes of size 15 ⇥ 15 with vary- ing iteration counts K and best F setting for each K. Bold indicates best result across all K for each model and transition kernel. Generally, increasing K improves performance.</figDesc><table>VIN 5 55.6 87.7 84.6 86.3 
86.6 
75.0 86.7 88.9 92.0 
92.3 
74.8 91.9 91.5 91.2 
87.9 
VIN 10 79.0 83.3 92.2 86.8 
86.7 
90.5 91.4 95.1 89.4 
92.7 
92.4 96.1 92.2 84.0 
64.4 
VIN 15 91.3 92.9 92.7 85.4 
87.6 
88.7 89.6 92.4 90.0 
91.0 
96.7 96.4 90.1 65.2 
23.0 
VIN 20 93.4 93.9 91.4 86.3 
85.5 
80.9 92.8 90.7 89.1 
90.4 
97.7 94.8 89.0 40.0 
22.3 
VIN 30 71.2 92.8 84.5 86.5 
86.4 
80.5 96.3 92.5 91.7 
89.1 
98.4 95.9 89.5 
-
-

GPPN 5 66.2 86.5 90.8 92.4 
93.0 
75.9 90.4 93.4 93.9 
94.1 
62.4 82.3 88.6 90.1 
91.2 
GPPN 10 91.2 96.1 97.1 97.6 
97.7 
93.3 96.5 97.4 97.6 
97.4 
87.7 95.4 96.1 97.0 
97.4 
GPPN 15 95.3 98.1 98.5 98.3 
98.8 
96.1 97.7 98.1 98.1 
98.3 
93.5 97.1 97.8 97.7 
99.0 
GPPN 20 97.4 98.4 99.0 99.0 
99.2 
96.8 98.4 98.5 98.7 
98.6 
95.8 97.9 98.4 98.4 
98.9 
GPPN 30 97.6 98.6 99.0 98.6 
98.8 
98.0 98.4 98.8 98.8 
98.4 
96.4 98.7 99.1 99.3 
99.2 

NEWS 
Moore 
Diff. Drive 
Model K %Opt %Suc %Opt %Suc %Opt %Suc 

VIN 5 87.7 88.4 92.3 93.3 91.9 95.8 
VIN 10 92.2 92.5 95.1 95.6 96.1 97.9 
VIN 15 92.9 93.0 92.4 93.9 96.7 98.3 
VIN 20 93.9 94.1 92.8 94.0 97.7 98.8 
VIN 30 92.8 93.2 96.3 96.6 98.4 99.1 

GPPN 5 93.0 94.3 94.1 96.1 91.2 95.6 
GPPN 10 97.7 98.4 97.6 98.4 97.4 98.8 
GPPN 15 98.8 99.2 98.3 98.9 99.0 99.6 
GPPN 20 99.2 99.5 98.7 99.2 98.9 99.5 
GPPN 30 99.0 99.3 98.8 99.3 99.3 99.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Test performance on 2D mazes of size 15 ⇥ 15 with vary- ing dataset sizes N under best settings of (K, F ) for each model. Both models improve with more training data (larger N ). GPPN performs relatively better than VIN with less data, suggesting that the VIN architectural biases do not help generalization perfor- mance.</figDesc><table>NEWS 
Moore 
Diff. Drive 
N Model %Opt %Suc %Opt %Suc %Opt %Suc 

10k VIN 90.3 90.6 88.1 90.5 97.5 98.4 
10k GPPN 97.8 98.6 97.6 98.4 98.0 99.4 

25k VIN 93.9 94.1 96.3 96.6 98.4 99.1 
25k GPPN 99.2 99.5 98.8 99.3 99.3 99.7 

100k VIN 97.3 97.3 97.1 97.5 98.9 99.4 
100k GPPN 99.9 99.9 99.7 99.8 99.9 99.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Train Val Test Kernel Model %Acc %Opt %Suc %Acc %Opt %Suc %Opt %SucDiff. Drive VIN 100.0 99.4 99.7 90.5 89.0 90.5 96.9 97.9 Diff. Drive GPPN 99.8 99.5 100.0 85.0 81.0 85.0 91.4 96.0</figDesc><table>NEWS 
VIN 99.9 82.3 83.0 81.5 80.8 81.5 79.0 79.7 
NEWS GPPN 99.9 99.4 99.7 94.9 93.2 94.9 94.1 95.9 

Moore 
VIN 99.6 86.5 88.9 89.1 86.7 89.1 84.6 87.6 
Moore GPPN 99.6 98.1 99.4 97.4 95.3 97.4 94.5 97.2 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This architecture was adapted from a previous work which is shown to perform well at playing deathmatches in Doom (Lample &amp; Chaplot, 2017).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>LL is supported by a NSF GRFP Fellowship and by the CMU SEI under Contract FA8702-15-D-0002, Section H Clause, AFLCMC (H)-H001: 6-18014. EP, DC, and RS are supported in part by Apple, Nvidia NVAIL, DARPA D17AP00001, IARPA DIVA award D17PC00340, and ONR award N000141512791. The authors would also like to thank Nvidia for providing GPU support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">irobot brings visual mapping and navigation to the roomba 980</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guizzo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active neural localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR 2018)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cognitive mapping and planning for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7272" to="7281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unifying map and landmark based representations for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>abs/1712.08125</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning for planning under partial observability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qmdp-Net</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2017)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4697" to="4707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ViZDoom: A Doom-based AI research platform for visual reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaśkowski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computational Intelligence and Games</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="341" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Playing fps games with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2140" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bridging the gaps between residual learning, recurrent neural networks and visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03640</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning policies for partially observable environments: Scaling up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Maze generation algorithms -Wikipedia, the free encyclopedia</title>
		<ptr target="https://en.wikipedia.org/wiki/Maze_generation_algorithm#Recursive_backtracker" />
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note>Maze Generation Algorithms. Online; accessed 9</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Value prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6120" to="6130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML 2013)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chun Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2015)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cooperative</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pathfinding</surname></persName>
		</author>
		<title level="m">AIIDE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08810</idno>
		<title level="m">The predictron: End-to-end learning and planning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Value Iteration Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbeel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentySixth International Joint Conference on Artificial Intelligence (IJCAI 2017)</title>
		<meeting>the TwentySixth International Joint Conference on Artificial Intelligence (IJCAI 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4949" to="4953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feedback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
