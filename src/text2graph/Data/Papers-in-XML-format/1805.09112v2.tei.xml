<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperbolic Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
							<email>octavian.ganea@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
							<email>gary.becigneul@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
							<email>thomas.hofmann@inf.ethz.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ETH Zürich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hyperbolic Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However, the representational power of hyperbolic geometry is not yet on par with Euclidean geometry, mostly because of the absence of corresponding hyperbolic neural network layers. This makes it hard to use hyperbolic embeddings in downstream tasks. Here, we bridge this gap in a principled manner by combining the formalism of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep learning tools: multinomial logistic regression, feed-forward and recurrent neural networks such as gated recurrent units. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically, we show that, even if hyperbolic optimization tools are limited, hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is common in machine learning to represent data as being embedded in the Euclidean space R n . The main reason for such a choice is simply convenience, as this space has a vectorial structure, closedform formulas of distance and inner-product, and is the natural generalization of our intuition-friendly, visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to feed them as input to neural networks, which has led to unprecedented performance on a broad range of problems, including sentiment detection <ref type="bibr" target="#b14">[15]</ref>, machine translation <ref type="bibr" target="#b2">[3]</ref>, textual entailment <ref type="bibr" target="#b21">[22]</ref> or knowledge base link prediction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Despite the success of Euclidean embeddings, recent research has proven that many types of complex data (e.g. graph data) from a multitude of fields (e.g. Biology, Network Science, Computer Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy <ref type="bibr" target="#b7">[8]</ref>. In such cases, the Euclidean space does not provide the most powerful or meaningful geometrical representations. For example, <ref type="bibr" target="#b9">[10]</ref> shows that arbitrary tree structures cannot be embedded with arbitrary low distortion (i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions, but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the exponential growth of distances matches the exponential growth of nodes with the tree depth.</p><p>The adoption of neural networks and deep learning in these non-Euclidean settings has been rather limited until very recently, the main reason being the non-trivial or impossible principled generalizations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation, vector inner product) as well as, in more complex geometries, the lack of closed form expressions for basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a correspondence in these geometries.</p><p>How should one generalize deep neural models to non-Euclidean domains ? In this paper we address this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> and used to visualize large taxonomies <ref type="bibr" target="#b17">[18]</ref> or to embed heterogeneous complex networks <ref type="bibr" target="#b16">[17]</ref>. In machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings for hierarchical, taxonomic or entailment data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Disjoint subtrees from the latent hierarchical structure surprisingly disentangle and cluster in the embedding space as a simple reflection of the space's negative curvature. However, appropriate deep learning tools are needed to embed feature data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data (e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images) would benefit from suitable hyperbolic RNNs.</p><p>The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry in the context of neural networks and deep learning by generalizing in a principled manner both the basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry. We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations introduced by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref> with the Riemannian geometry properties of the manifold. We smoothly parametrize basic operations and objects in all spaces of constant negative curvature using a unified framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic spaces can be continuously deformed into each other. On a series of experiments and datasets we showcase the effectiveness of our hyperbolic neural network layers compared to their "classic" Euclidean variants on textual entailment and noisy-prefix recognition tasks. We hope that this paper will open exciting future directions in the nascent field of Geometric Deep Learning.</p><p>2 The Geometry of the Poincaré Ball</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basics of Riemannian geometry</head><p>We briefly introduce basic concepts of differential geometry largely needed for a principled generalization of Euclidean neural networks. For more rigorous and in-depth expositions, see <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>An n-dimensional manifold M is a space that can locally be approximated by R n : it is a generalization to higher dimensions of the notion of a 2D surface. For x ∈ M, one can define the tangent space T x M of M at x as the first order linear approximation of M around x. A Riemannian metric g = (g x ) x∈M on M is a collection of inner-products g x : T x M × T x M → R varying smoothly with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g. Although a choice of a Riemannian metric g on M only seems to define the geometry locally on M, it induces global distances by integrating the length (of the speed vector living in the tangent space) of a shortest path between two points:</p><formula xml:id="formula_0">d(x, y) = inf γ 1 0 g γ(t) (γ(t),γ(t))dt,<label>(1)</label></formula><p>where γ ∈ C ∞ ([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line in Euclidean space. The parallel transport P x→y : T x M → T y M is a linear isometry between tangent spaces which corresponds to moving tangent vectors along geodesics and defines a canonical way to connect tangent spaces. The exponential map exp x at x, when well-defined, gives a way to project back a vector v of the tangent space T x M at x, to a point exp x (v) ∈ M on the manifold. This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm directionγ(0) := v ∈ T x M as t → exp x (tv). For geodesically complete manifolds, such as the Poincaré ball considered in this work, exp x is well-defined on the full tangent space T x M. Finally, a metricg is said to be conformal to another metric g if it defines the same angles, i.e.</p><formula xml:id="formula_1">g x (u, v) g x (u, u) g x (v, v) = g x (u, v) g x (u, u) g x (v, v) ,<label>(2)</label></formula><p>for all x ∈ M, u, v ∈ T x M \ {0}. This is equivalent to the existence of a smooth function λ : M → R, called the conformal factor, such thatg x = λ 2</p><p>x g x for all x ∈ M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hyperbolic space: the Poincaré ball</head><p>The hyperbolic space has five isometric models that one can work with <ref type="bibr" target="#b8">[9]</ref>. Similarly as in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b10">[11]</ref>, we choose to work in the Poincaré ball. The Poincaré ball model (D n , g D ) is defined by the manifold D n = {x ∈ R n : x &lt; 1} equipped with the following Riemannian metric:</p><formula xml:id="formula_2">g D x = λ 2 x g E , where λ x := 2 1 − x 2 ,<label>(3)</label></formula><p>g E = I n being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to the Euclidean one. The induced distance between two points x, y ∈ D n is known to be given by</p><formula xml:id="formula_3">d D (x, y) = cosh −1 1 + 2 x − y 2 (1 − x 2 )(1 − y 2 ) .<label>(4)</label></formula><p>Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈ T x D n \ {0} is given by</p><formula xml:id="formula_4">cos(∠(u, v)) = g D x (u, v) g D x (u, u) g D x (v, v) = u, v u v .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gyrovector spaces</head><p>In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition, subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide the algebraic setting for Euclidean geometry <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>In particular, these operations are used in special relativity, allowing to add speed vectors belonging to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball, hence not exceeding the speed of light.</p><p>We will make extensive use of these operations in our definitions of hyperbolic neural networks. Möbius addition. The Möbius addition of x and y in D n c is defined as</p><formula xml:id="formula_5">For c ≥ 0, denote 2 by D n c := {x ∈ R n | c x 2 &lt; 1}. Note that if c = 0, then D n c = R n ; if c &gt; 0, then D</formula><formula xml:id="formula_6">x ⊕ c y := (1 + 2c x, y + c y 2 )x + (1 − c x 2 )y 1 + 2c x, y + c 2 x 2 y 2 .<label>(6)</label></formula><p>In particular, when c = 0, one recovers the Euclidean addition of two vectors in R n . Note that without loss of generality, the case c &gt; 0 can be reduced to c = 1. Unless stated otherwise, we will use ⊕ as ⊕ 1 to simplify notations. For general c &gt; 0, this operation is not commutative nor associative. However, it satisfies x ⊕ c 0 = 0 ⊕ c x = 0. Moreover, for any x, y ∈ D </p><formula xml:id="formula_7">x ∈ D n c \ {0} by r ∈ R is defined as r ⊗ c x := (1/ √ c) tanh(r tanh −1 ( √ c x )) x x ,<label>(7)</label></formula><p>and r ⊗ c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar multiplication when c goes to zero: lim c→0 r ⊗ c x = rx. This operation satisfies desirable properties such as n ⊗ c x = x ⊕ c · · · ⊕ c x (n additions), (r + r ) ⊗ c x = r ⊗ c x ⊕ c r ⊗ c x (scalar distributivity 3 ), (rr ) ⊗ c x = r ⊗ c (r ⊗ c x) (scalar associativity) and |r| ⊗ c x/ r ⊗ c x = x/ x (scaling property).</p><p>Distance. If one defines the generalized hyperbolic metric tensor g c as the metric conformal to the Euclidean one, with conformal factor λ</p><formula xml:id="formula_8">c x := 2/(1 − c x 2 ), then the induced distance function on (D n c , g c ) is given by 4 d c (x, y) = (2/ √ c) tanh −1 √ c − x ⊕ c y .<label>(8)</label></formula><p>Again, observe that lim c→0 d c (x, y) = 2 x − y , i.e. we recover Euclidean geometry in the limit <ref type="bibr" target="#b4">5</ref> . Moreover, for c = 1 we recover d D of Eq. (4).</p><p>Hyperbolic trigonometry. Similarly as in the Euclidean space, one can define the notions of hyperbolic angles or gyroangles (when using the ⊕ c ), as well as hyperbolic law of sines in the generalized Poincaré ball (D n c , g c ).</p><p>We make use of these notions in our proofs. See Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball</head><p>In this subsection, we present how geodesics in the Poincaré ball model are usually described with Möbius operations, and push one step further the existing connection between gyrovector spaces and the Poincaré ball by finding new identities involving the exponential map, and parallel transport.</p><p>In particular, these findings provide us with a simpler formulation of Möbius scalar multiplication, yielding a natural definition of matrix-vector multiplication in the Poincaré ball.</p><p>Riemannian gyroline element. The Riemannian gyroline element is defined for an infinitesimal dx as ds := (x + dx) c x, and its size is given by <ref type="bibr">[26, section 3.7]</ref>:</p><formula xml:id="formula_9">ds = (x + dx) c x = dx /(1 − c x 2 ).<label>(9)</label></formula><p>What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line  </p><formula xml:id="formula_10">exp c x (v) = x ⊕ c tanh √ c λ c x v 2 v √ c v , log c x (y) = 2 √ cλ c x tanh −1 ( √ c − x ⊕ c y ) −x ⊕ c y − x ⊕ c y .<label>(12</label></formula><formula xml:id="formula_11">exp c 0 (v) = tanh( √ c v ) v √ c v , log c 0 (y) = tanh −1 ( √ c y ) y √ c y .<label>(13)</label></formula><p>Moreover, we still recover Euclidean geometry in the limit c → 0, as lim c→0 exp c x (v) = x + v is the Euclidean exponential map, and lim c→0 log c x (y) = y − x is the Euclidean logarithmic map.</p><p>Möbius scalar multiplication using exponential and logarithmic maps. We studied the exponential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplication (Eq. <ref type="formula" target="#formula_7">(7)</ref>). We found the following: Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0 with the logarithmic map, multiplying this projection by the scalar r in T 0 D n c , and then projecting it back on the manifold with the exponential map:</p><formula xml:id="formula_12">r ⊗ c x = exp c 0 (r log c 0 (x)), ∀r ∈ R, x ∈ D n c .<label>(14)</label></formula><p>In addition, we recover the well-known relation between geodesics connecting two points and the exponential map:</p><formula xml:id="formula_13">γ x→y (t) = x ⊕ c (−x ⊕ c y) ⊗ c t = exp c x (t log c x (y)), t ∈ [0, 1].<label>(15)</label></formula><p>This last result enables us to generalize scalar multiplication in order to define matrix-vector multiplication between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.</p><p>Parallel transport. Finally, we connect parallel transport (from T 0 D n c ) to gyrovector spaces with the following theorem, which we prove in appendix B. </p><formula xml:id="formula_14">P c 0→x (v) = log c x (x ⊕ c exp c 0 (v)) = λ c 0 λ c x v.<label>(16)</label></formula><p>As we'll see later, this result is crucial in order to define and optimize parameters shared between different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hyperbolic Neural Networks</head><p>Neural networks can be seen as being made of compositions of basic operations, such as linear maps, bias translations, pointwise non-linearities and a final sigmoid or softmax layer. We first explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present possible adaptations of various recurrent neural networks to the hyperbolic domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hyperbolic multiclass logistic regression</head><p>In order to perform multi-class classification on the Poincaré ball, one needs to generalize multinomial logistic regression (MLR) − also called softmax regression − to the Poincaré ball.</p><p>Reformulating Euclidean MLR. Let's first reformulate Euclidean MLR from the perspective of distances to margin hyperplanes, as in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">Section 5]</ref>. This will allow us to easily generalize it.</p><p>Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:</p><formula xml:id="formula_15">∀k ∈ {1, ..., K}, p(y = k|x) ∝ exp (( a k , x − b k )) , where b k ∈ R, x, a k ∈ R n .<label>(17)</label></formula><p>Note that any affine hyperplane in R n can be written with a normal vector a and a scalar shift b:</p><formula xml:id="formula_16">H a,b = {x ∈ R n : a, x − b = 0}</formula><p>, where a ∈ R n \ {0}, and b ∈ R.</p><p>As in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">Section 5]</ref>, we note that a,</p><formula xml:id="formula_18">x − b = sign( a, x − b) a d(x, H a,b )</formula><p>. Using Eq. <ref type="formula" target="#formula_0">(17)</ref>:</p><formula xml:id="formula_19">p(y = k|x) ∝ exp(sign( a k , x − b k ) a k d(x, H a k ,b k )), b k ∈ R, x, a k ∈ R n .<label>(19)</label></formula><p>As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. <ref type="formula" target="#formula_0">(18)</ref> to other spaces such as the Poincaré ball, we reformulate it as follows:</p><formula xml:id="formula_20">H a,p = {x ∈ R n : −p + x, a = 0} = p + {a} ⊥ , where p ∈ R n , a ∈ R n \ {0}.<label>(20)</label></formula><p>This new definition relates to the previous one asH a,p = H a, a,p . Rewriting Eq. <ref type="formula" target="#formula_0">(19)</ref> with b = a, p :</p><formula xml:id="formula_21">p(y = k|x) ∝ exp(sign( −p k + x, a k ) a k d(x,H a k ,p k )), with p k , x, a k ∈ R n .<label>(21)</label></formula><p>It is now natural to adapt the previous definition to the hyperbolic setting by replacing + by ⊕ c :</p><formula xml:id="formula_22">Definition 3.1 (Poincaré hyperplanes). For p ∈ D n c , a ∈ T p D n c \ {0}, let {a} ⊥ := {z ∈ T p D n c : g c p (z, a) = 0} = {z ∈ T p D n c : z, a = 0}. Then, we define Poincaré hyperplanes as H c a,p := {x ∈ D n c : log c p (x), a p = 0} = exp c p ({a} ⊥ ) = {x ∈ D n c : −p ⊕ c x, a = 0}. (22)</formula><p>The last equality is shown appendix C.H c a,p can also be described as the union of images of all geodesics in D n c orthogonal to a and containing p. Notice that our definition matches that of hypergyroplanes, see <ref type="bibr">[27, definition 5.8]</ref>. A 3D hyperplane example is depicted in <ref type="figure" target="#fig_5">Fig. 1</ref>.</p><p>Next, we need the following theorem, proved in appendix D: Theorem 5.</p><formula xml:id="formula_23">d c (x,H c a,p ) := inf w∈H c a,p d c (x, w) = 1 √ c sinh −1 2 √ c| −p ⊕ c x, a | (1 − c − p ⊕ c x 2 ) a .<label>(23)</label></formula><p>Final formula for MLR in the Poincaré ball. Putting together Eq. <ref type="formula" target="#formula_0">(21)</ref> and Thm. 5, we get the hyperbolic MLR formulation. Given K classes and</p><formula xml:id="formula_24">k ∈ {1, . . . , K}, p k ∈ D n c , a k ∈ T p k D n c \ {0}: p(y = k|x) ∝ exp(sign( −p k ⊕ c x, a k ) g c p k (a k , a k )d c (x,H c a k ,p k )), ∀x ∈ D n c ,<label>(24)</label></formula><p>or, equivalently</p><formula xml:id="formula_25">p(y = k|x) ∝ exp λ c p k a k √ c sinh −1 2 √ c −p k ⊕ c x, a k (1 − c − p k ⊕ c x 2 ) a k , ∀x ∈ D n c .<label>(25)</label></formula><p>Notice that when c goes to zero, this goes to</p><formula xml:id="formula_26">p(y = k|x) ∝ exp(4 −p k + x, a k ) = exp((λ 0 p k ) 2 −p k + x, a k ) = exp( −p k + x, a k 0 )</formula><p>, recovering the usual Euclidean softmax.</p><p>However, at this point it is unclear how to perform optimization over a k , since it lives in T p k D n c and hence depends on p k . The solution is that one should write</p><formula xml:id="formula_27">a k = P c 0→p k (a k ) = (λ c 0 /λ c p k )a k , where a k ∈ T 0 D n c = R</formula><p>n , and optimize a k as a Euclidean parameter. In order to define hyperbolic neural networks, it is crucial to define a canonically simple parametric family of transformations, playing the role of linear mappings in usual Euclidean neural networks, and to know how to apply pointwise non-linearities. Inspiring ourselves from our reformulation of Möbius scalar multiplication in Eq. <ref type="formula" target="#formula_0">(14)</ref>, we define: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperbolic feed-forward layers</head><formula xml:id="formula_28">f ⊗c (x) := exp c 0 (f (log c 0 (x))),<label>(26)</label></formula><p>where exp Note that similarly as for other Möbius operations, we recover the Euclidean mapping in the limit c → 0 if f is continuous, as lim c→0 f ⊗c (x) = f (x). This definition satisfies a few desirable properties too, such as:</p><formula xml:id="formula_29">(f • g) ⊗c = f ⊗c • g ⊗c for f : R m → R l and g : R n → R m (morphism property), and f ⊗c (x)/ f ⊗c (x) = f (x)/ f (x) for f (x) = 0 (direction preserving).</formula><p>It is then straight-forward to prove the following result:</p><p>Lemma 6 (Möbius matrix-vector multiplication). If M : R n → R m is a linear map, which we identify with its matrix representation, then ∀x ∈ D n c , if M x = 0 we have</p><formula xml:id="formula_30">M ⊗c (x) = (1/ √ c) tanh M x x tanh −1 ( √ c x ) M x M x ,<label>(27)</label></formula><p>and M ⊗c (x) = 0 if M x = 0. Moreover, if we define the Möbius matrix-vector multiplication of</p><formula xml:id="formula_31">M ∈ M m,n (R) and x ∈ D n c by M ⊗ c x := M ⊗c (x), then we have (M M ) ⊗ c x = M ⊗ c (M ⊗ c x) for M ∈ M l,m (R) and M ∈ M m,n (R) (matrix associativity), (rM ) ⊗ c x = r ⊗ c (M ⊗ c x)</formula><p>for r ∈ R and M ∈ M m,n (R) (scalar-matrix associativity) and M ⊗ c x = M x for all M ∈ O n (R) (rotations are preserved).</p><p>Pointwise non-linearity. If ϕ : R n → R n is a pointwise non-linearity, then its Möbius version ϕ ⊗c can be applied to elements of the Poincaré ball. </p><p>We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular role in this model. Indeed, consider multiple layers of the form</p><formula xml:id="formula_33">f k (x) = ϕ k (M k x), each of which having Möbius version f ⊗c k (x) = ϕ ⊗c k (M k ⊗ c x). Then their composition can be re-written f ⊗c k • · · · • f ⊗c 1 = exp c 0 •f k • · · · • f 1 • log c</formula><p>0 . This means that these operations can essentially be performed in Euclidean space. Therefore, it is the interposition between those with the bias translation of Eq. (28) which differentiates this model from its Euclidean counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation of multiple input vectors. If a vector x ∈ R</head><p>n+p is the (vertical) concatenation of two vectors x 1 ∈ R n , x 2 ∈ R p , and M ∈ M m,n+p (R) can be written as the (horizontal) concatenation of two matrices M 1 ∈ M m,n (R) and</p><formula xml:id="formula_34">M 2 ∈ M m,p (R), then M x = M 1 x 1 + M 2 x 2 .</formula><p>We generalize this to hyperbolic spaces: if we are given</p><formula xml:id="formula_35">x 1 ∈ D n c , x 2 ∈ D p c , x = (x 1 x 2 ) T ∈ D n c ×D p c , and M, M 1 , M 2 as before, then we define M ⊗ c x := M 1 ⊗ c x 1 ⊕ c M 2 ⊗ c x 2 .</formula><p>Note that when c goes to zero, we recover the Euclidean formulation, as  </p><formula xml:id="formula_36">lim c→0 M ⊗ c x = lim c→0 M 1 ⊗ c x 1 ⊕ c M 2 ⊗ c x 2 = M 1 x 1 + M 2 x 2 = M x. Moreover, hyperbolic vectors x ∈ D</formula><formula xml:id="formula_37">h t+1 = ϕ ⊗c (W ⊗ c h t ⊕ c U ⊗ c x t ⊕ c b), h t ∈ D n c , x t ∈ D d c .<label>(29)</label></formula><p>Note that if inputs x t 's are Euclidean, one can writex t := exp c 0 (x t ) and use the above formula, since exp</p><formula xml:id="formula_38">c W ⊗cht (P c 0→W ⊗cht (U x t )) = W ⊗ c h t ⊕ c exp c 0 (U x t ) = W ⊗ c h t ⊕ c U ⊗ cxt .</formula><p>GRU architecture. One can also adapt the GRU architecture:</p><formula xml:id="formula_39">r t = σ(W r h t−1 + U r x t + b r ), z t = σ(W z h t−1 + U z x t + b z ), h t = ϕ(W (r t h t−1 ) + U x t + b), h t = (1 − z t ) h t−1 + z t h t ,<label>(30)</label></formula><p>where denotes pointwise product. First, how should we adapt the pointwise multiplication by a scaling gate? Note that the definition of the Möbius version (see Eq. <ref type="formula" target="#formula_1">(26)</ref>) can be naturally extended to maps f :</p><formula xml:id="formula_40">R n × R p → R m as f ⊗c : (h, h ) ∈ D n c × D p c → exp c 0 (f (log c 0 (h), log c 0 (h ))). In particular, choosing f (h, h ) := σ(h) h yields 6 f ⊗c (h, h ) = exp c 0 (σ(log c 0 (h)) log c 0 (h )) = diag(σ(log c 0 (h))) ⊗ c h .</formula><p>Hence we adapt r t h t−1 to diag(r t ) ⊗ c h t−1 and the reset gate r t to:</p><formula xml:id="formula_41">r t = σ log c 0 (W r ⊗ c h t−1 ⊕ c U r ⊗ c x t ⊕ c b r ),<label>(31)</label></formula><p>and similarly for the update gate z t . Note that as the argument of σ in the above is unbounded, r t and z t can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:</p><formula xml:id="formula_42">h t = ϕ ⊗c ((W diag(r t )) ⊗ c h t−1 ⊕ c U ⊗ c x t ⊕ b),<label>(32)</label></formula><p>where Möbius matrix associativity simplifies</p><formula xml:id="formula_43">W ⊗ c (diag(r t ) ⊗ c h t−1 ) into (W diag(r t )) ⊗ c h t−1 .</formula><p>Finally, we propose to adapt the update-gate equation as</p><formula xml:id="formula_44">h t = h t−1 ⊕ c diag(z t ) ⊗ c (−h t−1 ⊕ cht ).<label>(33)</label></formula><p>Note that when c goes to zero, one recovers the usual GRU. Moreover, if z t = 0 or z t = 1, then h t becomes h t−1 orh t respectively, similarly as in the usual GRU. This adaptation was obtained by adapting <ref type="bibr" target="#b23">[24]</ref>: in this work, the authors re-derive the update-gate mechanism from a first principle called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the notion of gyroderivative <ref type="bibr" target="#b3">[4]</ref> and proving a gyro-chain-rule (see appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>SNLI task and dataset. We evaluate our method on two tasks. The first is natural language inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classification task is to predict whether the second sentence can be inferred from the first one. This defines a partial order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task, SNLI <ref type="bibr" target="#b6">[7]</ref>. It consists of 570K training, 10K validation and 10K test sentence pairs. Following <ref type="bibr" target="#b27">[28]</ref>, we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while the "entailment" class gives the positive pairs.</p><p>PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks are more significant when the underlying data structure is closer to a tree. To test this, we design a proof-of-concept task of detection of noisy prefixes, i.e. given two sentences, one has to decide if the second sentence is a noisy prefix of the first, or a random sentence. We thus build synthetic datasets PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random first sentence of random length at most 20 and one random prefix of it, a second positive sentence is generated by randomly replacing Z% of the words of the prefix, and a second negative sentence of same length is randomly generated. Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.</p><p>Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which are just rescaled Euclidean gradients when working in the conformal Poincaré model <ref type="bibr" target="#b20">[21]</ref>. Thus, back-propagation can be applied in the standard way.</p><p>In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean, depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes (entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which case one has to use log 0 and exp 0 functions to move between the two manifolds in a correct manner as explained for Eq. 26.</p><p>Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for the hyperbolic layers). We optimize the Euclidean parameters with Adam <ref type="bibr" target="#b15">[16]</ref> (learning rate 0.001). Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian stochastic gradient descent (RSGD) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. We also experiment with projected RSGD <ref type="bibr" target="#b20">[21]</ref>, but optimization was sometimes less stable. We use a different constant learning rate for word embeddings (0.1) and other hyperbolic weights (0.01) because words are updated less frequently. Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state embedding dimension of 5 (we focus on the low dimensional setting that was shown to already be effective <ref type="bibr" target="#b20">[21]</ref>), batch size of 64. We ran all methods for a fixed number of 30 epochs. For all models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence embeddings, but the hyperbolic specific geometry is more pronounced at the ball border, i.e. at the hyperbolic "infinity", compared to the center of the ball.</p><p>For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the test result corresponding to the best validation result from these 3 runs. We do this because the highly non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local minima, suggesting that initialization is very important.</p><p>Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have an advantage over hyperbolic baselines because more sophisticated optimization algorithms such as Adam do not have a hyperbolic analogue at the moment. We first observe that all GRU models overpass their RNN variants. Hyperbolic RNNs and GRUs have the most significant improvement over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-10% − for which the tree relation between sentences and their prefixes is more prominent − we reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the representational power of hyperbolic networks. Also, note that on SNLI our methods perform similarly as with their Euclidean variants.  <ref type="table">Table 1</ref>: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if the difference is less than 0.5%).</p><p>used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation is needed for this direction (see below).</p><p>We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model should first adjust the angular layout in order to disentangle the representations, before increasing their norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior was observed in the context of embedding trees by <ref type="bibr" target="#b20">[21]</ref>. Details in appendix F. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLR classification experiments.</head><p>For the sentence entailment classification task we do not see a clear advantage of hyperbolic MLR compared to its Euclidean variant. A possible reason is that, when trained endto-end, the model might decide to place positive and negative embeddings in a manner that is already well separated with a classic MLR. As a consequence, we further investigate MLR for the task of subtree classification. Using an open source implementation <ref type="bibr" target="#b6">7</ref> of <ref type="bibr" target="#b20">[21]</ref>, we pre-trained Poincaré embeddings of the WordNet noun hierarchy (82,115 nodes). We then choose one node in this tree (see <ref type="table" target="#tab_1">Table 2</ref>) and classify all other nodes (solely based on their embeddings) as being part of the subtree rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes that are divided into a negative training and negative test set respectively. Three variants of MLR are then trained on top of pre-trained Poincaré embeddings <ref type="bibr" target="#b20">[21]</ref> to solve this binary classification task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after mapping all embeddings in the tangent space at 0 using the log 0 map. We use different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the same learning rate. During training, we always sample the same number of negative and positive nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are trained for 30 epochs and the final F1 score is reported (no hyperparameters to validate are used, thus we do not require a validation set). This procedure is repeated for four subtrees of different sizes.</p><p>Quantitative results are presented in  <ref type="table" target="#tab_1">Table 2</ref>: Test F1 classification scores for four different subtrees of WordNet noun tree. All nodes in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%); these counts are shown below each subtree root. The same splitting procedure is applied for the remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on top of pre-trained Poincaré embeddings <ref type="bibr" target="#b20">[21]</ref> to solve this binary classification task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after mapping all embeddings in the tangent space at 0 using the log 0 map. 95% confidence intervals for 3 different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).</p><p>further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes (geodesics in this case) in <ref type="figure" target="#fig_6">Figure 2</ref>. We can see that respecting the hyperbolic geometry is very important for a quality classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be generalized in a principled manner to all spaces of constant negative curvature combining Riemannian geometry with the elegant theory of gyrovector spaces. Empirically we found that our models outperform or are on par with corresponding Euclidean architectures on sequential data with implicit hierarchical structure. We hope to trigger exciting future research related to better understanding of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning methods.</p><p>Our data and Tensorflow <ref type="bibr" target="#b0">[1]</ref> code are publicly available <ref type="bibr" target="#b7">8</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperbolic Trigonometry</head><p>Hyperbolic angles. For A, B, C ∈ D n c , we denote by ∠A := ∠BAC the angle between the two geodesics starting from A and ending at B and C respectively. This angle can be defined in two equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given by Eq. 5, or ii) using the formula</p><formula xml:id="formula_45">cos(∠A) = (−A) ⊕ c B (−A) ⊕ c B , (−A) ⊕ c C (−A) ⊕ c C ,<label>(34)</label></formula><p>In this case, ∠A is also called a gyroangle in the work of <ref type="bibr">[26, section 4]</ref>.</p><p>Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ D n c , we denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and C respectively, and byc = d c (B, A) the length of the hyperbolic segment BA (and similarly for others), then we have:</p><formula xml:id="formula_46">sin(∠A) sinh( √ cã) = sin(∠B) sinh( √ cb) = sin(∠C) sinh( √ cc) .<label>(35)</label></formula><p>Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 4 Theorem 4.</head><p>In the manifold (D </p><formula xml:id="formula_47">X : t ∈ [0, 1] → log c γ(t) (γ(t) ⊕ c exp c 0 (v)) ∈ T γ(t) D n c .<label>(37)</label></formula><p>Clearly, X is a vector field along γ such that X(0) = v. Now define</p><formula xml:id="formula_48">P c 0→x : v ∈ T 0 D n c → log c x (x ⊕ c exp c 0 (v)) ∈ T x D n c .<label>(38)</label></formula><p>From Eq. <ref type="formula" target="#formula_0">(12)</ref>, it is easily seen that</p><formula xml:id="formula_49">P c 0→x (v) = λ c 0 λ c x v, hence P c 0→x is a linear isometry from T 0 D n c to T x D n c . Since P c 0→x (v) = X(1)</formula><p>, it is enough to prove that X is parallel in order to guarantee that P c 0→x is the parallel transport from</p><formula xml:id="formula_50">T 0 D n c to T x D n c .</formula><p>Since X is a vector field along γ, its covariant derivative can be expressed with the Levi-Civita connection ∇ c associated to g c :</p><formula xml:id="formula_51">DX ∂t = ∇ ċ γ(t) X.<label>(39)</label></formula><p>Let's compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system, they can be written as</p><formula xml:id="formula_52">Γ i jk = 1 2 (g c ) il (∂ j g c lk + ∂ k g c lj − ∂ l g c jk ),<label>(40)</label></formula><p>where superscripts denote the inverse metric tensor and using Einstein's notations. As g</p><formula xml:id="formula_53">c ij = (λ c ) 2 δ ij , at γ(t) ∈ D n c this yields: Γ i jk = cλ c γ(t) (δ ik γ(t) j + δ ij γ(t) k − δ jk γ(t) i ).<label>(41)</label></formula><p>9 i.e. that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Theorem 5</head><p>Theorem 5.</p><formula xml:id="formula_54">d c (x,H c a,p ) := inf w∈H c a,p d c (x, w) = 1 √ c sinh −1 2 √ c| −p ⊕ c x, a | (1 − c − p ⊕ c x 2 ) a .<label>(55)</label></formula><p>Proof. We first need to prove the following lemma, trivial in the Euclidean space, but not in the Poincaré ball: Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique orthogonal projection on any given geodesic that does not pass through the point. Formally, for all y ∈ D n c and for all geodesics γ x→z (·) s.t. y / ∈ Im γ x→z , there exists an unique w ∈ Im γ x→z s.t. ∠(γ w→y , γ x→z ) = π/2.</p><p>Proof. We first note that any geodesic in D n c has the form γ(t) = u ⊕ c v ⊗ c t as given by Eq. 11, and has two "points at infinity" lying on the ball border (v = 0):</p><formula xml:id="formula_55">γ(±∞) = u ⊕ c ±v √ c v ∈ ∂D n c .<label>(56)</label></formula><p>Using the notations in the lemma statement, the closed-form of γ x→z is given by Eq. <ref type="formula" target="#formula_0">(10)</ref>:</p><formula xml:id="formula_56">γ x→z (t) = x ⊕ c (−x ⊕ c z) ⊗ c t</formula><p>We denote by x , z ∈ ∂D n c its points at infinity as described by Eq. (56). Then, the hyperbolic angle ∠ywx is well defined from Eq. (34):</p><formula xml:id="formula_57">cos(∠(γ w→y , γ x→z )) = cos(∠ywz ) = −w ⊕ c y, −w ⊕ c z − w ⊕ c y · − w ⊕ c z .<label>(57)</label></formula><p>We now perform 2 steps for this proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i) Existence of w:</head><p>The angle function from Eq. (57) is continuous w.r.t t when w = γ x→z (t). So we first prove existence of an angle of π/2 by continuously moving w from x to z when t goes from −∞ to ∞, and observing that cos(∠ywz ) goes from −1 to 1 as follows:</p><formula xml:id="formula_58">cos(∠yx z ) = 1 &amp; lim w→z cos(∠ywz ) = −1.<label>(58)</label></formula><p>The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the definition of ⊕ c ) that a ⊕ c b = a, when a = 1/ √ c (which is the case of x ). The right part of Eq. (58) follows from the fact that ∠ywz = π − ∠ywx (from the conformal property, or from Eq. (34)) and cos(∠yz x ) = 1 (proved as above).</p><p>Hence cos(∠ywz ) has to pass through 0 when going from −1 to 1, which achieves the proof of existence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ii) Uniqueness of w:</head><p>Assume by contradiction that there are two w and w on γ x→z that form angles ∠ywx and ∠yw x of π/2. Since w, w , x are on the same geodesic, we have</p><formula xml:id="formula_59">π/2 = ∠yw x = ∠yw w = ∠ywx = ∠yw w<label>(59)</label></formula><p>So ∆yww has two right angles, but in the Poincaré ball this is impossible. Now, we need two more lemmas: Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to a geodesic (not passing through the point) is minimizing the distance between the point and the geodesic.</p><p>Proof. The proof is similar with the Euclidean case and it's based on hyperbolic sine law and the fact that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Derivation of the Hyperbolic GRU Update-gate</head><p>In <ref type="bibr" target="#b23">[24]</ref>, the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is the following.</p><p>Recovering the update-gate from time-warping. A naive RNN is given by the equation h(t + 1) = ϕ(W h(t) + U x(t) + b)</p><p>Let's drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a first-order Taylor development gives h(t + δt) ≈ h(t) + δt dh dt (t) for small δt. Combining this for δt = 1 with the naive RNN equation, one gets dh dt (t) = ϕ(W h(t) + U x(t)) − h(t). </p><p>= dα dt (t) ⊗ c dh dt (α(t)) (Möbius scalar associativity) <ref type="formula" target="#formula_1">(82)</ref> where we set u = δt(α (t) + O(δt)), with u → 0 when δt → 0, which concludes.</p><p>Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have</p><formula xml:id="formula_63">dh dt (t) = dα dt (t) ⊗ c (−h(t) ⊕ c ϕ ⊗c (W ⊗ ch (t) ⊕ c U ⊗ cx (t))).<label>(83)</label></formula><p>Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields</p><formula xml:id="formula_64">h(t + 1) = h(t) ⊕ c dα dt (t) ⊗ c (−h(t) ⊕ c ϕ ⊗c (W ⊗ c h(t) ⊕ c U ⊗ c x(t))).<label>(84)</label></formula><p>Since α is a time-warping, by definition its derivative is positive and one can choose to parametrize it with an update-gate z t (a scalar) defined with a sigmoid. Generalizing this scalar scaling by the Möbius version of the pointwise scaling yields the Möbius matrix scaling diag(z t ) ⊗ c ·, leading to our proposed Eq. (33) for the hyperbolic GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Experimental Investigations</head><p>The following empirical facts were observed for both hyperbolic RNNs and GRUs.</p><p>We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings can go close to the border (hyperbolic "infinity"), hence exploiting the hyperbolic nature of the space. Moreover, the faster the two sentence norms go to 1, the more it's likely that a good local minima was reached. See figures 3 and 5.</p><p>We often observe that test accuracy starts increasing exactly when sentence embedding norms do. However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few epochs, which does not happen in the Euclidean case. See figures 3, 5 and 4. This mysterious fact was also exhibited in a similar way by <ref type="bibr" target="#b20">[21]</ref> which suggests that the model first has to adjust the angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting hyperbolic geometry.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>If c = 1 then we recover the usual ball D n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>have (−x) ⊕ c x = x ⊕ c (−x) = 0 and (−x) ⊕ c (x ⊕ c y) = y (left-cancellation law). The Möbius substraction is then defined by the use of the following notation: x c y := x ⊕ c (−y). See [29, section 2.1] for a geometric interpretation of the Möbius addition. Möbius scalar multiplication. For c &gt; 0, the Möbius scalar multiplication of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>element 2 dx /(1 − c x 2 ) of the Riemannian manifold (D Exponential and logarithmic maps. The following lemma gives the closed-form derivation of exponential and logarithmic maps. Lemma 2. For any point xgiven for v = 0 and y = x by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>maps have more appealing forms when x = 0, namely for v ∈ T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 4 .</head><label>4</label><figDesc>In the manifold (D n c , g c ), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T 0 D n c to another tangent space T x D n c is given by the following isometry:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of a hyperbolic hyperplane in D 3 1 plotted using sampling. The red point is p. The shown normal axis to the hyperplane through p is parallel to a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Definition 3. 2 (</head><label>2</label><figDesc>Möbius version). For f : R n → R m , we define the Möbius version of f as the map from D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Bias translation. The generalization of a translation in the Poincaré ball is naturally given by moving along geodesics. But should we use the Möbius sum x ⊕ c b with a hyperbolic bias b or the exponential map exp c x (b ) with a Euclidean bias b ? These views are unified with parallel transport (see Thm 4). Möbius translation of a point x ∈ D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>n c can also be "concatenated" with real features y ∈ R by doing: M ⊗ c x ⊕ c y ⊗ c b with learnable b ∈ D m c and M ∈ M m,n (R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>. A simple RNN can be defined by h t+1 = ϕ(W h t + U x t + b) where ϕ is a pointwise non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the hyperbolic space as follows. For parameters W ∈ M m,n (R), U ∈ M m,d (R), b ∈ D m c , we define:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Numerical errors. Gradients of the basic operations defined above (e.g. ⊕ c , exponential map) are not defined when the hyperbolic argument vectors are on the ball border, i.e. √ c x = 1. Thus, we always project results of these operations in the ball of radius 1 − , where = 10 −5 . Numerical errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an = 10 −15 before they are used in any of the above operations. Finally, arguments of the tanh function are clipped between ±15 to avoid numerical errors, while arguments of tanh −1 are clipped to at most 1 − 10 −5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary MLR used to classify nodes as being part in the GROUP.N.01 subtree of the WordNet noun hierarchy solely based on their Poincaré embeddings. The positive points (from the subtree) are in blue, the negative points (the rest) are in red and the trained positive separation hyperplane is depicted in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>c ), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ TThe geodesic in D n c from 0 to x is given in Eq. (10) by γ(t) = x ⊗ c t, for t ∈ [0, 1]. Let v ∈ T 0 D n c . Then it is of common knowledge that there exists a unique parallel 9 vector field X along γ (i.e. X(t) ∈ T γ(t) D n c , ∀t ∈ [0, 1]) such that X(0) = v. Let's define:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>for t ∈ [0, 1], where D ∂t denotes the covariant derivative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>αα</head><label></label><figDesc>(t) + O(δt) δt(α (t) + O(δt)) ⊗ c [−h(α(t)) ⊕ c h(α(t) + δt(α (t) + O(δt)))](t) δt(α (t) + O(δt)) ⊗ c [−h(α(t)) ⊕ c h(α(t) + δt(α (t) + O(δt)))]c [−h(α(t)) ⊕ c h(α(t) + u)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>of the first sentence. Averaged over all sentences in the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PREFIX-30% accuracy and first (premise) sentence norm plots for different runs of the same architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half) MLR. The X axis shows millions of training examples processed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PREFIX-30% accuracy and first (premise) sentence norm plots for different runs of the same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis shows millions of training examples processed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: PREFIX-30% accuracy and first (premise) sentence norm plots for different runs of the same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis shows millions of training examples processed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Moreover, hyperbolic and Euclidean MLR are on par when</figDesc><table>SNLI 

PREFIX-10% PREFIX-30% PREFIX-50% 

FULLY EUCLIDEAN RNN 
79.34 % 
89.62 % 
81.71 % 
72.10 % 
HYPERBOLIC RNN+FFNN, EUCL MLR 
79.18 % 
96.36 % 
87.83 % 
76.50 % 
FULLY HYPERBOLIC RNN 
78.21 % 
96.91 % 
87.25 % 
62.94 % 
FULLY EUCLIDEAN GRU 
81.52 % 
95.96 % 
86.47 % 
75.04 % 
HYPERBOLIC GRU+FFNN, EUCL MLR 
79.76 % 
97.36 % 
88.47 % 
76.87 % 
FULLY HYPERBOLIC GRU 
81.19 % 
97.14 % 
88.26 % 
76.44 % 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>We can see that the hyperbolic MLR overpasses its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide</figDesc><table>WORDNET 

SUBTREE 

MODEL 
D = 2 
D = 3 
D = 5 
D = 10 

ANIMAL.N.01 

3218 / 798 

HYPERBOLIC 
DIRECT EUCL 
log 0 + EUCL 

47.43 ± 1.07% 
41.69 ± 0.19% 
38.89 ± 0.01% 

91.92 ± 0.61% 
68.43 ± 3.90% 
62.57 ± 0.61% 

98.07 ± 0.55% 
95.59 ± 1.18% 
89.21 ± 1.34% 

99.26 ± 0.59% 
99.36 ± 0.18% 
98.27 ± 0.70% 

GROUP.N.01 

6649 / 1727 

HYPERBOLIC 
DIRECT EUCL 
log 0 + EUCL 

81.72 ± 0.17% 
61.13 ± 0.42% 
60.75 ± 0.24% 

89.87 ± 2.73% 
63.56 ± 1.22% 
61.98 ± 0.57% 

87.89 ± 0.80% 
67.82 ± 0.81% 
67.92 ± 0.74% 

91.91 ± 3.07% 
91.38 ± 1.19% 
91.41 ± 0.18% 

WORKER.N.01 

861 / 254 

HYPERBOLIC 
DIRECT EUCL 
log 0 + EUCL 

12.68 ± 0.82% 
10.86 ± 0.01% 
9.04 ± 0.06% 

24.09 ± 1.49% 
22.39 ± 0.04% 
22.57 ± 0.20% 

55.46 ± 5.49% 
35.23 ± 3.16% 
26.47 ± 0.78% 

66.83 ± 11.38% 
47.29 ± 3.93% 
36.66 ± 2.74% 

MAMMAL.N.01 

953 / 228 

HYPERBOLIC 
DIRECT EUCL 
log 0 + EUCL 

32.01 ± 17.14% 
15.58 ± 0.04% 
13.10 ± 0.13% 

87.54 ± 4.55% 
44.68 ± 1.87% 
44.89 ± 1.18% 

88.73 ± 3.22% 
59.35 ± 1.31% 
52.51 ± 0.85% 

91.37 ± 6.09% 
77.76 ± 5.08% 
56.11 ± 2.21% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We take different notations as in [25] where the author uses s = 1/ √ c.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c). 4 The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y). 5 The factor 2 comes from the conformal factor λx = 2/(1 − x 2 ), which is a convention setting the curvature to −1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi's on its diagonal.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/dalab/hyperbolic_cones</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/dalab/hyperbolic_nn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Test accuracy (b) Norm of the first sentence. Averaged over all sentences in the test set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Igor Petrovski for useful pointers regarding the implementation. This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning Systems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since γ(t) = (1/ √ c) tanh(t tanh −1 ( √ c x )) x x , it is easily seen thatγ(t) is colinear to γ(t). Hence there exists K x t ∈ R such thatγ(t) = K </p><p>Combining these yields</p><p>Replacing with the Christoffel symbols of ∇ c at γ(t) gives</p><p>Moreover,</p><p>Putting together everything, we obtain</p><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Eq. (22)</head><p>Proof. Two steps proof:</p><p>, we have that:</p><p>This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that</p><p>which is what we wanted.</p><p>Then, using Eq. <ref type="formula">(12)</ref>, we derive that: log</p><p>which is orthogonal to a, by assumption. This implies log Proof. γ p→w (t) = p ⊕ c (−p ⊕ c w) ⊗ c t. Then, it is easy to check the condition in Eq. <ref type="formula">(22)</ref>:</p><p>We now turn back to our proof. Let x ∈ D n c be an arbitrary point andH c a,p a Poincaré hyperplane. We prove that there is at least one point w * ∈H c a,p that achieves the infimum distance</p><p>and, moreover, that this distance is the same as the one in the theorem's statement.</p><p>We first note that for any point w ∈H c a,p , if ∠xwp = π/2, then w = w * . Indeed, using Lemma 8 and Lemma 9, it is obvious that the projection of x to γ p→w will give a strictly lower distance.</p><p>Thus, we only consider w ∈H c a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right triangle ∆xwp, one gets:</p><p>One of the above quantities does not depend on w:</p><p>The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (because ∠xpw &lt; π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is maximized. But, we already have from Eq. (34) that:</p><p>To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw) depends only on the geodesic γ p→w and not on w itself, and because there is always an orthogonal projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to find the maximum of Eq. (64) when w ∈H </p><p>Using that fact that log </p><p>and we are left with a well known Euclidean problem which is equivalent to finding the minimum angle between the vector −p ⊕ c x (viewed as Euclidean) and the hyperplane {a} ⊥ . This angle is given by the Euclidean orthogonal projection whose sin value is the distance from the vector's endpoint to the hyperplane divided by the vector's length:</p><p>It follows that a point w * ∈H c a,p satisfying Eq. (67) exists (but might not be unique). Combining Eqs. (61),(62),(63) and (67) concludes the proof.</p><p>As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function of t called the time-warping. Denoting byh(t) := h(α(t)) andx(t) := x(α(t)), using the chain rule</p><p>Removing the tildas to simplify notations, discretizing back with</p><p>Requiring that our class of neural networks be invariant to time-warpings means that this class should contain RNNs defined by Eq. (71), i.e. that dα dt (t) can be learned. As this is a positive quantity, we can parametrize it as z(t) = σ(W z h(t) + U z x(t)), recovering the forget-gate equation:</p><p>Adapting this idea to hyperbolic RNNs. The gyroderivative <ref type="bibr" target="#b3">[4]</ref> of a map h :</p><p>Using Möbius scalar associativity and the left-cancellation law leads us to</p><p>for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. <ref type="formula">(29)</ref> with δt = 1, one gets dh dt (t) = −h(t) ⊕ c ϕ ⊗c (W ⊗ c h(t) ⊕ c U ⊗ c x(t)).</p><p>For the next step, we need the following lemma:</p><p>Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → D n c with a well-defined gyro-derivative, ifh := h • α, then we have</p><p>where dα dt (t) denotes the usual derivative.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<title level="m">A system for large-scale machine learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analytic hyperbolic geometry and Albert Einstein&apos;s special theory of relativity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Ungar Abraham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>World scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The hyperbolic derivative in the poincaré ball model of hyperbolic geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Graciela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><forename type="middle">A</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical analysis and applications</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="333" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James W Cannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parry</surname></persName>
		</author>
		<title level="m">Hyperbolic geometry. Flavors of geometry</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Christopher De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03329</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperbolic entailment cones for learning hierarchical embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-fifth international conference on machine learning (ICML)</title>
		<meeting>the thirty-fifth international conference on machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hyperbolic groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhael</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in group theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="75" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the tree-likeness of hyperbolic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hamann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Ricci flow in Riemannian geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Andrews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marián</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A focus+ context technique based on hyperbolic geometry for visualizing large hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lamping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramana</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pirolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hyperplane margin classifiers on the multinomial manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on machine learning (ICML)</title>
		<meeting>the international conference on machine learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">66</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on machine learning (ICML)</title>
		<meeting>the international conference on machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6341" to="6350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A comprehensive introduction to differential geometry. Publish or perish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Spivak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Can recurrent neural networks warp time?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hyperbolic trigonometry and its application in the poincaré ball model of hyperbolic geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A gyrovector space approach to hyperbolic geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><forename type="middle">Albert</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="194" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Analytic hyperbolic geometry in n dimensions: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><forename type="middle">Albert</forename><surname>Ungar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A geometric interpretation of ungar&apos;s addition and of gyration in the hyperbolic plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topology and its Applications</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="242" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>On the other hand, since X(t) = (λ</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
