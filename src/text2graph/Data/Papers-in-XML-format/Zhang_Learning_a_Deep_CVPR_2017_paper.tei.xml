<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a Deep Embedding Model for Zero-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a Deep Embedding Model for Zero-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Zero-shot learning (ZSL)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A recent trend in developing visual recognition models is to scale up the number of object categories. However, most existing recognition models are based on supervised learning and require a large amount (at least 100s) of training samples to be collected and annotated for each object class to capture its intra-class appearance variations <ref type="bibr" target="#b5">[6]</ref>. This severely limits their scalability -collecting daily objects such as chair is easier, but many other categories are rare (e.g., a newly identified specie of beetle on a remote pacific island). None of these models can work with few or even no training samples for a given class. In contrast, humans are very good at recognising objects without seeing any visual samples, i.e., zero-shot learning (ZSL). For example, a child would have no problem recognising a zebra if she has seen horses before and also read elsewhere that a zebra is a horse but with black-and-white stripes on it. Inspired by humans' ZSL ability, recently there is a surge of interest in machine ZSL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>A zero-shot learning method relies on the existence of a labelled training set of seen classes and the knowledge about how an unseen class is semantically related to the seen classes. Seen and unseen classes are usually related in a high dimensional vector space, called semantic space, where the knowledge from seen classes can be transferred to unseen classes. The semantic spaces used by most early works are based on semantic attributes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>. Given a defined attribute ontology, each class name can be represented by an attribute vector and termed as a class prototype. More recently, semantic word vector space <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10]</ref> and sentence descriptions/captions <ref type="bibr" target="#b33">[34]</ref> have started to gain popularity. With the former, the class names are projected into a word vector space so that different classes can be compared, whilst with the latter, a neural language model is required to provide a vector representation of the description.</p><p>With the semantic space and a visual feature representation of image content, ZSL is typically solved in two steps: (1) A joint embedding space is learned where both the semantic vectors (prototypes) and the visual feature vectors can be projected to; and (2) nearest neighbour (NN) search is performed in this embedding space to match the projection of an image feature vector against that of an unseen class prototype. Most state-of-the-arts ZSL models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22]</ref> use deep CNN features for visual feature representation; the features are extracted with pretrained CNN models. They differ mainly in how to learn the embedding space given the features. They are thus not end-to-end deep learning models.</p><p>In this paper, we focus on end-to-end learning of a deep embedding based ZSL model which offers a number of advantages. First, end-to-end optimisation can potentially lead to learning a better embedding space. For example, if sentence descriptions are used as the input to a neural language model such as recurrent neural networks (RNNs) for computing a semantic space, both the neural language model and the CNN visual feature representation learning model can be jointly optimised in an end-to-end fashion. Second, a neural network based joint embedding model offers the flexibility for addressing various transfer learning problems such as multi-task learning and multi-domain learning <ref type="bibr" target="#b45">[46]</ref>. Third, when multiple semantic spaces are available, this model can provide a natural mechanism for fusing the multiple modalities. However, despite all these intrinsic advantages, in practice, the few existing end-to-end deep models for ZSL in the literature <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34]</ref> fail to demonstrate these advantages and yield only weaker or merely comparable performances on benchmarks when compared to non-deep learning alternatives.</p><p>We argue that the key to the success of a deep embedding model for ZSL is the choice of the embedding space. Existing models, regardless whether they are deep or nondeep, choose either the semantic space <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b9">10]</ref> or an intermediate embedding space <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b10">11]</ref> as the embedding space. However, since the embedding space is of high dimension and NN search is to be performed there, the hubness problem is inevitable <ref type="bibr" target="#b32">[33]</ref>, that is, a few unseen class prototypes will become the NNs of many data points, i.e., hubs. Using the semantic space as the embedding space means that the visual feature vectors need to be projected into the semantic space which will shrink the variance of the projected data points and thus aggravate the hubness problem <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>In this work, we propose a novel deep neural network based embedding model for ZSL which differs from existing models in that: (1) To alleviate the hubness problem, we use the output visual feature space of a CNN subnet as the embedding space. The resulting projection direction is from a semantic space, e.g., attribute or word vector, to a visual feature space. Such a direction is opposite to the one adopted by most existing models. We provide a theoretical analysis and some intuitive visualisations to explain why this would help us counter the hubness problem. <ref type="formula" target="#formula_3">(2)</ref> A simple yet effective multi-modality fusion method is developed in our neural network model which is flexible and importantly enables end-to-end learning of the semantic space representation.</p><p>The contributions of this work are as follows: (i) A novel deep embedding model for ZSL has been formulated which differs from existing models in the selection of embedding space. (ii) A multi-modality fusion method is further developed to combine different semantic representations and to enable end-to-end learning of the representations. Extensive experiments carried out on four benchmarks including AwA <ref type="bibr" target="#b21">[22]</ref>, CUB <ref type="bibr" target="#b44">[45]</ref> and large scale ILSVRC 2010 and ILSVRC 2012 <ref type="bibr" target="#b5">[6]</ref> show that our model beats all the stateof-the-art models presented to date, often by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic space Existing ZSL methods differ in what semantic spaces are used: typically either attribute <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>, word vector <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10]</ref>, or text description <ref type="bibr" target="#b33">[34]</ref>. It has been shown that an attribute space is often more effective than a word vector space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. This is hardly surprising as additional attribute annotations are required for each class. Similarly, state-of-the-art results on fine-grained recognition tasks have been achieved in <ref type="bibr" target="#b33">[34]</ref> using image sentence descriptions to construct the semantic space. Again, the good performance is obtained at the price of more manual annotation: 10 sentence descriptions need to be collected for each image, which is even more expensive than attribute annotation. This is why the word vector semantic space is still attractive: it is 'free' and is the only choice for large scale recognition with many unseen classes <ref type="bibr" target="#b12">[13]</ref>. In this work, all three semantic spaces are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusing multiple semantic spaces</head><p>Multiple semantic spaces are often complementary to each other; fusing them thus can potentially lead to improvements in recognition performance. Score-level fusion is perhaps the simplest strategy <ref type="bibr" target="#b13">[14]</ref>. More sophisticated multi-view embedding models have been proposed. Akata et al. <ref type="bibr" target="#b1">[2]</ref> learn a joint embedding semantic space between attribute, text and hierarchical relationship which relies heavily on hyperparameter search. Multi-view canonical correlation analysis (CCA) has also been employed <ref type="bibr" target="#b10">[11]</ref> to explore different modalities of testing data in a transductive way. Differing from these models, our neural network based model has an embedding layer to fuse different semantic spaces and connect the fused representation with the rest of the visual-semantic embedding network for end-to-end learning. Unlike <ref type="bibr" target="#b10">[11]</ref>, it is inductive and does not require to access the whole test set at once. Embedding model Existing methods also differ in the visual-semantic embedding model used. They can be categorised into two groups: (1) The first group learns a mapping function by regression from the visual feature space to the semantic space with pre-computed features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13]</ref> or deep neural network regression <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10]</ref>. For these embedding models, the semantic space is the embedding space. (2) The second group of models implicitly learn the relationship between the visual and semantic space through a common intermediate space, again either with a neural network formulation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref> or without <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b10">11]</ref>. The embedding space is thus neither the visual feature space, nor the semantic space. We show in this work that using the visual feature space as the embedding space is intrinsically advantageous due to its ability to alleviate the hubness problem. Deep ZSL model All recent ZSL models use deep CNN features as inputs to their embedding model. However, few are deep end-to-end models. Existing deep neural network based ZSL works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34]</ref> differ in whether they use the semantic space or an intermediate space as the embedding space, as mentioned above. They also use different losses. Some of them use margin-based losses <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34]</ref>. Socher et al <ref type="bibr" target="#b42">[43]</ref> choose a euclidean distance loss. Ba et al <ref type="bibr" target="#b23">[24]</ref> takes a dot product between the embedded visual feature and semantic vectors and consider three training losses, including a binary cross entropy loss, hinge loss and Euclidean distance loss. In our model, we find that the least square loss between the two embedded vectors is very effective and offers an easy theoretical justification as for why it copes with the hubness problem better. The work in <ref type="bibr" target="#b33">[34]</ref> differs from the other models in that it integrates a neural language model into its neural network for end-to-end learning of the embedding space as well as the language model. In additional to the ability of jointly learning the neural language model and embedding model, our model is capable of fusing text description with other semantic spaces and achieves better performance than <ref type="bibr" target="#b33">[34]</ref>. The hubness problem The phenomenon of the presence of 'universal' neighbours, or hubs, in a high-dimensional space for nearest neighbour search was first studied by Radovanovic et al. <ref type="bibr" target="#b25">[26]</ref>. They show that hubness is an inherent property of data distributions in a high-dimensional vector space, and a specific aspect of the curse of dimensionality. A couple of recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> noted that regression based zero-shot learning methods suffer from the hubness problem and proposed solutions to mitigate the hubness problem. Among them, the method in <ref type="bibr" target="#b6">[7]</ref> relies on the modelling of the global distribution of test unseen data ranks w.r.t. each class prototypes to ease the hubness problem. It is thus transductive. In contrast, the method in <ref type="bibr" target="#b40">[41]</ref> is inductive: It argued that least square regularised projection functions make the hubness problem worse and proposed to perform reverse regression, i.e., embedding class prototypes into the visual feature space. Our model also uses the visual feature space as the embedding space but achieve so by using an end-to-end deep neural network which yields far superior performance on ZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition</head><p>Assume a labelled training set of N training samples is given as</p><formula xml:id="formula_0">D tr = {(I i , y u i , t u i ), i = 1, .</formula><p>. . , N }, with associated class label set T tr , where I i is the i-th training image, y</p><formula xml:id="formula_1">u i ∈ R</formula><p>L×1 is its corresponding L-dimensional semantic representation vector, t u i ∈ T tr is the u-th training class label for the i-th training image. Given a new test image I j , the goal of ZSL is to predict a class label t v j ∈ T te , where t v j is the v-th test class label for the j-th test instance. We have T tr ∩ T te = ∅, i.e., the training (seen) classes and test (unseen) classes are disjoint. Note that each class label t u or t v is associated with a pre-defined semantic space representation y u or y v (e.g. attribute vector), referred to as semantic class prototypes. For the training set, y u i is given because each training image I i is labelled by a semantic representation vector representing its corresponding class label t u j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model architecture</head><p>The architecture of our model is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. It has two branches. One branch is the visual encoding branch, which consists of a CNN subnet that takes an image I i as input and outputs a D-dimensional feature vector φ(I i ) ∈ R D×1 . This D-dimensional visual feature space will be used as the embedding space where both the image content and the semantic representation of the class that the image belongs to will be embedded. The semantic embedding is achieved by the other branch which is a semantic encoding subnet. Specifically, it takes a L-dimensional semantic representation vector of the corresponding class y u i as input, and after going through two fully connected (FC) linear + Rectified Linear Unit (ReLU) layers outputs a D-dimensional semantic embedding vector. Each of the FC layer has a l 2 parameter regularisation loss. The two branches are linked together by a least square embedding loss which aims to minimise the discrepancy between the visual feature φ(I i ) and its class representation embedding vector in the visual feature space. With the three losses, our objective function is as follows:</p><formula xml:id="formula_2">L(W 1 , W 2 ) = 1 N N i=1 ||φ(I i ) − f 1 (W 2 f 1 (W 1 y u i ))|| 2 +λ(||W 1 || 2 + ||W 2 || 2 ) (1)</formula><p>where W 1 ∈ R L×M are the weights to be learned in the first FC layer and W 2 ∈ R M ×D for the second FC layer. λ is the hyperparameter weighting the strengths of the two parameter regularisation losses against the embedding loss. We set f 1 ( ) to be the Rectified Linear Unit (ReLU) which introduces nonlinearity in the encoding subnet <ref type="bibr" target="#b20">[21]</ref>.</p><p>After that, the classification of the test image I j in the visual feature space can be achieved by simply calculating its distance to the embed prototypes:</p><formula xml:id="formula_3">v = arg min v D(φ(I j ), f 1 (W 2 f 1 (W 1 y v )))<label>(2)</label></formula><p>where D is a distance function, and y v is the semantic space vector of the v-th test class prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multiple semantic space fusion</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we can consider the semantic representation and the first FC and ReLU layer together as a semantic representation unit. When there is only one semantic space considered, it is illustrated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. However, when more than one semantic spaces are used, e.g., we want to fuse attribute vector with word vector for semantic representation of classes, the structure of the semantic representation unit is changed slightly, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. More specifically, we map different semantic representation vectors to a multi-modal fusion layer/space where they are added. The output of the semantic representation unit thus becomes:</p><formula xml:id="formula_4">f 2 (W (1) 1 · y u1 i + W (2) 1 · y u2 i ),<label>(3)</label></formula><p>where y u1 i ∈ R L1×1 and y u2 i ∈ R L2×1 denote two different semantic space representations (e.g., attribute and word vector), "+" denotes element-wise sum, W</p><formula xml:id="formula_5">(1) 1 ∈ R L1×M</formula><p>and W <ref type="bibr" target="#b1">(2)</ref> 1 ∈ R L2×M are the weights which will be learned. f 2 ( ) is the element-wise scaled hyperbolic tangent function <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_6">f 2 (x) = 1.7159 · tanh( 2 3 x).<label>(4)</label></formula><p>This activation function forces the gradient into the most non-linear value range and leads to a faster training process than the basic hyperbolic tangent function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Bidirectional LSTM encoder for description</head><p>The structure of the semantic representation unit needs to be changed again, when text description is avalialbe for each training image (see <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). In this work, we use a recurrent neural network (RNN) to encode the content of a text description (a variable length sentence) into a fixedlength semantic vector. Specifically, given a text description of T words, x = (x 1 , . . . , x T ) we use a Bidirectional RNN model <ref type="bibr" target="#b38">[39]</ref> to encode them. For the RNN cell, the Long-Shot Term Memory (LSTM) <ref type="bibr" target="#b16">[17]</ref> units are used as the recurrent units. The LSTM is a special kind of RNN, which introduces the concept of gating to control the message passing between different times steps. In this way, it could potentially model long term dependencies. Following <ref type="bibr" target="#b15">[16]</ref>, the model has two types of states to keep track of the historical records: a cell state c and a hidden state h. For a particular time step t, they are computed by integrating the current inputs x t and previous state (c t−1 , h t−1 ). During the integrating, three types of gates are used to control the messaging passing: an input gate i t , a forget gate f t and an output gate o t .</p><p>We omit the formulation of the bidirectional LSTM here and refer the readers to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> for details. With the bidirectional LSTM model, we use the final output as our encoded semantic feature vector to represent the text description:</p><formula xml:id="formula_7">f (W− → h · − → h + W← − h · ← − h ),<label>(5)</label></formula><p>where − → h denote the forward final hidden state, ← − h denote the backward final hidden state. f ( ) = f 1 ( ) if text description is used only for semantic space unit, and f ( ) = f 2 ( ) if other semantic space need to be fused (Sec. 3.3). W− → h and W← − h are the weights which will be learned. In the testing stage, we first extract text encoding from test descriptions and then average them per-class to form the test prototypes as in <ref type="bibr" target="#b33">[34]</ref>. Note that since our ZSL model is a neural network, it is possible now to learn the RNN encoding subnet using the training data together with the rest of the network in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">The hubness problem</head><p>How does our model deal with the hubness problem? First we show that our objective function is closely related to that of the ridge regression formulation. In particular, if we use the matrix form and write the outputs of the semantic representation unit as A and the outputs of the CNN visual feature encoder as B, and ignore the ReLU unit for now, our training objective becomes</p><formula xml:id="formula_8">L(W) = ||B − WA|| 2 F + λ||W|| 2 F ,<label>(6)</label></formula><p>which is basically ridge regression. It is well known that ridge regression has a closed-form solution W = BA ⊤ (AA ⊤ + λI) −1 . Thus we have:</p><formula xml:id="formula_9">||WA|| 2 = ||BA ⊤ (AA ⊤ + λI) −1 A|| 2 ≤ ||B|| 2 ||A ⊤ (AA ⊤ + λI) −1 A|| 2<label>(7)</label></formula><p>It can be further shown that</p><formula xml:id="formula_10">||A ⊤ (AA ⊤ + λI) −1 A|| 2 = σ 2 σ 2 + λ ≤ 1.<label>(8)</label></formula><p>Where σ is the largest singular value of A. So we have ||WA|| 2 ≤ ||B|| 2 . This means the mapped source data ||WA|| 2 are likely to be closer to the origin of the space than the target data ||B|| 2 , with a smaller variance. Why does this matter in the context of ZSL? <ref type="figure" target="#fig_1">Figure 2</ref> gives an intuitive explanation. Specifically, assuming the feature distribution is uniform in the visual feature space, <ref type="figure" target="#fig_1">Fig. 2(a)</ref> shows that if the projected class prototypes are slightly shrunk towards the origin, it would not change how hubness problem arises -in other words, it at least does not make the hubness issue worse. However, if the mapping direction were to be reversed, that is, we use the semantic vector space as the embedding space and project the visual feature vectors φ(I) into the space, the training objective is still ridge regression-like, so the projected visual feature representation vectors will be shrunk towards the origin as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. Then there is an adverse effect: the semantic vectors which are closer to the origin are more likely to become hubs, i.e. nearest neighbours to many projected visual feature representation vectors. This is confirmed by our experiments (see Sec. 4) which show that using which space as the embedding space makes a big difference in terms of the degree/seriousness of the resultant hubness problem and therefore the ZSL performance. Measure of hubness To measure the degree of hubness in a nearest neighbour search problem, the skewness of the (empirical) N k distribution is used, following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>. The N k distribution is the distribution of the number N k (i) of times each prototype i is found in the top k of the ranking for test samples (i.e. their k-nearest neighbour), and its skewness is defined as follows:</p><formula xml:id="formula_11">(N k skewness) = l i=1 (N k (i) − E[N k ]) 3 /l V ar[N k ] 3 2</formula><p>, <ref type="formula">(9)</ref> where l is the total number of test prototypes. A large skewness value indicates the emergence of more hubs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Relationship to other deep ZSL models</head><p>Let's now compare the proposed model with the related end-to-end neural network based models: DeViSE <ref type="bibr" target="#b9">[10]</ref>, Socher et al. <ref type="bibr" target="#b42">[43]</ref>, MTMDL <ref type="bibr" target="#b45">[46]</ref>, and Ba et al. <ref type="bibr" target="#b23">[24]</ref>. Their model structures fall into two groups. In the first group (see <ref type="figure" target="#fig_3">Fig. 3(a)</ref>), DeViSE <ref type="bibr" target="#b9">[10]</ref> and Socher et al. <ref type="bibr" target="#b42">[43]</ref> map the CNN visual feature vector to a semantic space by a hinge ranking loss or least square loss. In contrast, MTMDL <ref type="bibr" target="#b45">[46]</ref> and Ba et al. <ref type="bibr" target="#b23">[24]</ref> fuse visual space and semantic space to a common intermediate space and then use a hinge ranking loss or a binary cross entropy loss (see <ref type="figure" target="#fig_3">Fig. 3(b)</ref>). For both groups, the learned embedding model will make the variance of WA to be smaller than that of B, which would thus make the hubness problem worse. In summary, the hubness will persist regardless what embedding model is adopted, as long as NN search is conducted in a high dimensional space. Our model does not worsen it, whist other deep models do, which leads to the performance difference as demonstrated in our experiments.    <ref type="bibr" target="#b44">[45]</ref> contains 11,788 images of 200 bird species. We use the same split as in <ref type="bibr" target="#b1">[2]</ref> with 150 classes for training and 50 disjoint classes for testing. ImageNet (ILSVRC) 2010 1K <ref type="bibr" target="#b37">[38]</ref> consists of 1,000 categories and more than 1.2 million images. We use the same training/test split as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10]</ref> which gives 800 classes for training and 200 classes for testing. ImageNet (ILSVRC) 2012/2010: for this dataset, we use the same setting as <ref type="bibr" target="#b12">[13]</ref>, that is, ILSVRC 2012 1K is used as the training seen classes, while 360 classes in ILSVRC 2010 which do not appear in ILSVRC 2012 are used as the test unseen classes. Semantic space For AwA, we use the continuous 85-dimension class-level attributes provided in <ref type="bibr" target="#b21">[22]</ref>, which have been used by all recent works. For the word vector space, we use the 1,000 dimension word vectors provided in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. For CUB, continuous 312-dimension class-level attributes and 10 descriptions per image provided in <ref type="bibr" target="#b33">[34]</ref> are used. For ILSVRC 2010 and ILSVRC 2012, we trained a skip-gram language model <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> on a corpus of 4.6M Wikipedia documents to extract 1,000 word vectors for each class. Model setting and training Unless otherwise specified, We use the Inception-V2 <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b18">19]</ref> as the CNN subnet of our model in all our experiments, the top pooling units are used for visual feature space with dimension D = 1, 024. The CNN subnet is pre-trained on ILSVRC 2012 1K classification without fine-tuning, same as the recent deep ZSL works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>. For fair comparison with DeViSE <ref type="bibr" target="#b9">[10]</ref>, ConSE <ref type="bibr" target="#b30">[31]</ref> and AMP <ref type="bibr" target="#b13">[14]</ref> on ILSVRC 2010, we also use the Alexnet <ref type="bibr" target="#b20">[21]</ref> architecture and pretrain it from scratch using the 800 training classes. All input images are resized to 224 × 224. Fully connected layers of our model are initialised with random weights for all of our experiments. Adam <ref type="bibr" target="#b19">[20]</ref> is used to optimise our model with a learning rate of 0.0001 and a minibatch size of 64. The model is implemented based on Tensorflow. Parameter setting In the semantic encoding branch of our network, the output size of the first FC layer M is set to 300 and 700 for AwA and CUB respectively when a single semantic space is used (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). Specifically, we use one FC layer for ImageNet in our experiments. For multiple semantic space fusion, the multi-modal fusion layer output size is set to 900 (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). When the semantic representation was encoded from descriptions for the CUB dataset, a bidirectional LSTM encoding subnet is employed (see <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). We use the BasicLSTMCell in Tensorflow as our RNN cell and employ ReLU as activation function. We set the input sequence length to 30; longer text inputs are cut off at this point and shorter ones are zero-padded. The word embedding size and the number of LSTM unit are both 512. Note that with this LSTM subnet, RMSprop is used in the place of Adam to optimise the whole network with a learning rate of 0.0001, a minibatch size of 64 and gradient clipped at 5. The loss weighting factor λ in Eq. <ref type="formula">(1)</ref> is searched by five-fold cross-validation. Specifically, 20% of the seen classes in the training set are used to form a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on AwA and CUB</head><p>Competitors Numerous existing works reported results on these two relatively small-scale datasets. Among them, only the most competitive ones are selected for comparison due to space constraint. The selected 13 can be categorised into the non-deep model group and the deep model group. All the non-deep models use ImageNet pretrained CNN to extract visual features. They differ in which CNN model is used: F O indicates that overfeat <ref type="bibr" target="#b39">[40]</ref> is used; F G for GoogLeNet <ref type="bibr" target="#b43">[44]</ref>; and F V for VGG net <ref type="bibr" target="#b41">[42]</ref>. The second group are all neural network based with a CNN subnet. For fair comparison, we implement the models in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b23">24]</ref> on AwA and CUB with Inception-V2 as the CNN subnet as in our model and <ref type="bibr" target="#b33">[34]</ref>. The compared methods also differ in the semantic spaces used. Attributes (A) are used by all methods; some also use word vector (W) either as an alternative to attributes, or in conjunction with attributes (A+W). For CUB, recently the instance-level sentence descriptions (D) are used <ref type="bibr" target="#b33">[34]</ref>. Note that only inductive methods are considered. Some recent methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> are tranductive in that they use all test data at once for model training, which gives them a big unfair advantage.</p><p>Comparative results on AwA From <ref type="table">Table 1</ref> we can make the following observations: (1) Our model achieves the best results either with attribute or word vector. When both semantic spaces are used, our result is further improved to 88.1%, which is 7.6% higher than the best result reported so far <ref type="bibr" target="#b47">[48]</ref>. <ref type="formula" target="#formula_3">(2)</ref> The performance gap between our model to the existing neural network based models are particular striking. In fact, the four models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b23">24]</ref> achieve weaker results than most of the compared non-deep models that use deep features only and do not perform endto-end training. The verify our claim that selecting the appropriate visual-semantic embedding space is critical for the deep embedding models to work. (3) As expected, the word vector space is less informative than the attribute space (86.7% vs. 78.8%) even though our word vector space alone result already beats all published results except for one <ref type="bibr" target="#b47">[48]</ref>. Nevertheless, fusing the two spaces still brings some improvement (1.4%).</p><p>Comparative results on CUB <ref type="table">Table 1</ref> shows that on the fine-grained dataset CUB, our model also achieves the best result. In particular, with attribute only, our result of 58.3% is 3.8% higher than the strongest competitor <ref type="bibr" target="#b3">[4]</ref>. The best result reported so far, however, was obtained by the neural network based DS-SJE <ref type="bibr" target="#b33">[34]</ref> at 56.8% using sentence descriptions. It is worth pointing out that this result was obtained using a word-CNN-RNN neural language model, whilst our model uses a bidirectional LSTM subnet, which is easier to train end-to-end with the rest of the network. When the same LSTM based neural language model is used, DS-SJE reports a lower accuracy of 53.0%. Further more, with attribute only, the result of DS-SJE (50.4%) is much lower than ours. This is significant because annotating attributes for fine-grained classes is probably just about manageable; but annotating 10 descriptions for each images is unlikely to scale to large number of classes. It is also evident that fusing attribute with descriptions leads to further improvement.  <ref type="bibr" target="#b43">[44]</ref>; and FV for VGG net <ref type="bibr" target="#b41">[42]</ref>. For neural network based methods, all use Inception-V2 (GoogLeNet with batch normalisation) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b18">19]</ref> as the CNN subnet, indicated as NG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on ImageNet</head><p>Comparative results on ILSVRC 2010 Compared to AwA and CUB, far fewer works report results on the largescale ImageNet ZSL tasks. We compare our model against 8 alternatives on ILSVRC 2010 in <ref type="table" target="#tab_2">Table 2</ref>, where we use hit@5 rather than hit@1 accuracy as in the small dataset experiments. Note that existing works follow two settings. Some of them <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref>   Comparative results on ILSVRC 2012/2010 Even fewer published results on this dataset are available. <ref type="table" target="#tab_4">Table 3</ref> shows that our model clearly outperform the state-of-the-art alternatives by a large margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further analysis</head><p>Importance of embedding space selection We argued that the key for an effective deep embedding model is the use of the CNN output visual feature space rather than the semantic space as the embedding space. In this experiment, we modify our model in <ref type="figure" target="#fig_0">Fig. 1</ref> by moving the two FC layers from the semantic embedding branch to the CNN feature extraction branch so that the embedding space now becomes the semantic space (attributes are used). <ref type="table">Table 4</ref> shows that by mapping the visual features to the semantic embedding space, the performance on AwA drops by 26.1% on AwA, highlighting the importance of selecting the right embedding space. We also hypothesised that using the CNN visual feature space as the embedding layer would lead to less hub-chimpanzee giant panda leopard persian cat pig hippopotamus humpback whale raccoon rat seal <ref type="figure">Figure 4</ref>. Visualisation of the distribution of the 10 unseen class images in the two embedding spaces on AwA using t-SNE <ref type="bibr" target="#b24">[25]</ref>. Different classes as well as their corresponding class prototypes (in squares) are shown in different colours. Better viewed in colour.</p><formula xml:id="formula_12">(a) S → V (b) V → S</formula><p>ness problem. To verify that we measure the hubness using the skewness score (see Sec. 3.5). <ref type="table">Table 5</ref> shows clearly that the hubness problem is much more severe when the wrong embedding space is selected. We also plot the data distribution of the 10 unseen classes of AwA together with the prototypes. <ref type="figure">Figure 4</ref> suggests that with the visual feature space as the embedding space, the 10 classes form compact clusters and are near to their corresponding prototypes, whilst in the semantic space, the data distributions of different classes are much less separated and a few prototypes are clearly hubs causing miss-classification.  <ref type="table">Table 5</ref>. N1 skewness score on AwA and CUB with different embedding space.</p><p>Neural network formulation Can we apply the idea of using visual feature space as embedding space to other models? To answer this, we consider a very simple model based on linear ridge regression which maps from the CNN feature space to the attribute semantic space or vice versa. In <ref type="table">Table 6</ref>, we can see that even for such a simple model, very impressive results are obtained with the right choice of embedding space. The results also show that with our neural network based model, much better performance can be obtained due to the introduced nonlinearity and its ability to learn end-to-end.  <ref type="table">Table 6</ref>. Zero-shot classification accuracy (%) comparison with linear regression on AwA and CUB.</p><p>Choices of the loss function As reviewed in Sec. 2, most existing ZSL models use either margin based losses or binary cross entropy loss to learn the embedding model. In this work, least square loss is used. <ref type="table">Table 4</ref> shows that when the semantic space is used as the embedding space, a slightly inferior result is obtained using a hinge ranking loss in place of least square loss in our model. However, least square loss is clearly better when the visual feature space is the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel deep embedding model for zero-shot learning. The model differs from existing ZSL model in that it uses the CNN output feature space as the embedding space. We hypothesise that this embedding space would lead to less hubness problem compared to the alternative selections of embedding space. Further more, the proposed model offers the flexible of utilising multiple semantic spaces and is capable of end-to-end learning when the semantic space itself is computed using a neural network. Extensive experiments show that our model achieves state-of-the-art performance on a number of benchmark datasets and validate the hypothesis that selecting the correct embedding space is the key for achieving the excellent performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the network architecture of our deep embedding model. The detailed architecture of the semantic representation unit in the left branch (semantic encoding subnet) is given in (a), (b) and (c) which correspond to the single modality (semantic space) case, the multiple (two) modality case, and the case where one of the modalities is text description. For the case in (c), the semantic representation itself is a neural network (RNN) which is learned end-to-end with the rest of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of the effects of different embedding directions on the hubness problem. S: semantic space, and V: visual feature space. Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The existing deep ZSL models' architectures fall into two groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>benchmarks are selected: AwA (Ani- mals with Attributes) [22] consists of 30,745 images of 50 classes. It has a fixed split for evaluation with 40 training classes and 10 test classes. CUB (CUB-200-2011)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>FO if overfeat [40] is used; FG for GoogLeNet</figDesc><table>Model 
F 
SS 
AwA 
CUB 

AMP [14] 
F O 
A+W 
66.0 
-
SJE [2] 
F G 
A 
66.7 
50.1 
SJE [2] 
F G 
A+W 
73.9 
51.7 
ESZSL [37] 
F G 
A 
76.3 
47.2 
SSE-ReLU [47] 
F V 
A 
76.3 
30.4 
JLSE [48] 
F V 
A 
80.5 
42.1 
SS-Voc [13] 
F O 
A/W 
78.3/68.9 
-
SynC-struct [4] 
F G 
A 
72.9 
54.5 
SEC-ML [3] 
F V 
A 
77.3 
43.3 

DeViSE [10] 
N G 
A/W 
56.7/50.4 
33.5 
Socher et al. [43] 
N G 
A/W 
60.8/50.3 
39.6 
MTMDL [46] 
N G 
A/W 
63.7/55.3 
32.3 
Ba et al. [24] 
N G 
A/W 
69.3/58.7 
34.0 
DS-SJE [34] 
N G 
A/D 
-
50.4/56.8 

Ours 
N G 
A/W(D) 
86.7/78.8 
58.3/53.5 

Ours 
N G 
A+W(D) 
88.1 
59.0 

Table 1. Zero-shot classification accuracy (%) comparison on 
AwA and CUB. SS: semantic space; A: attribute space; W: se-
mantic word vector space; D: sentence description (only available 
for CUB). F: how the visual feature space is computed; For non-
deep models: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>use existing CNN model (e.g. VGG/GoogLeNet) pretrained from ILSVRC 2012 1K classes to initialise their model or extract deep visual fea- ture. Comparing to these two methods under the same set- ting, our model gives 60.7%, which beats the nearest rival PDDM [18] by over 12%. For comparing with the other 6 methods, we follow their setting and pretrain our CNN sub- net from scratch with Alexnet [21] architecture using the 800 training classes for fair comparison. The results show that again, significant improvement has been obtained with our model.</figDesc><table>Model 
hit@5 

ConSE [31] 
28.5 
DeViSE [10] 
31.8 
Mensink et al. [27] 
35.7 
Rohrbach [36] 
34.8 
PST [35] 
34.0 
AMP [14] 
41.0 

Ours 
46.7 

Gaussian Embedding [30] 
45.7 
PDDM [18] 
48.2 

Ours 
60.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Comparative results (%) on ILSVRC 2010</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Comparative results (%) on ILSVRC 2012/2010.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Table 4. Effects of selecting different embedding space and dif- ferent loss functions on zero-shot classification accuracy (%) on AwA.</figDesc><table>Loss 
Visual → Semantic 
Semantic → Visual 

Least square loss 
60.6 
86.7 
Hinge loss 
57.7 
72.8 

N 1 skewness 
AwA 
CUB 

Visual → Semantic 
0.4162 
8.2697 
Semantic → Visual 
-0.4834 
2.2594 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was funded in part by the European FP7 Project SUNNY (grant agreement no. 313243).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised vocabulary-informed learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local similarity-aware deep feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attributebased classification for zero-shot visual object categorization. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visualizing data using t-SNE. JMLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Angeliki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Georgiana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gaussian visual-linguistic embedding for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radovanović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ivanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiclass recognition and part localization with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A unified perspective on multi-domain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
