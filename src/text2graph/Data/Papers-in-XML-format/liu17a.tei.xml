<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Inflated Exponential Family Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Ping</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
						</author>
						<title level="a" type="main">Zero-Inflated Exponential Family Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Word embeddings are a widely-used tool to analyze language, and exponential family embeddings <ref type="bibr" target="#b21">(Rudolph et al., 2016)</ref> generalize the technique to other types of data. One challenge to fitting embedding methods is sparse data, such as a document/term matrix that contains many zeros. To address this issue, practitioners typically downweight or subsample the zeros, thus focusing learning on the non-zero entries. In this paper, we develop zero-inflated embeddings, a new embedding method that is designed to learn from sparse observations. In a zero-inflated embedding (ZIE), a zero in the data can come from an interaction to other data (i.e., an embedding) or from a separate process by which many observations are equal to zero (i.e. a probability mass at zero). Fitting a ZIE naturally downweights the zeros and dampens their influence on the model. Across many types of datalanguage, movie ratings, shopping histories, and bird watching logs-we found that zero-inflated embeddings provide improved predictive performance over standard approaches and find better vector representation of items.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Word embeddings use distributed representations to capture usage patterns in language data <ref type="bibr" target="#b6">(Harris, 1954;</ref><ref type="bibr" target="#b22">Rumelhart et al., 1988;</ref><ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b18">Pennington et al., 2014)</ref>. The main idea is to fit the conditional distribution of words by using vector representations, called embeddings. The learned parametersthe embedding vectors-are useful as features about the meanings of words. Word embeddings have become a widely used method for unsupervised analysis of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, <ref type="bibr">Australia, PMLR 70, 2017</ref><ref type="bibr">. Copyright 2017</ref> by the author(s).</p><p>In a recent paper, <ref type="bibr" target="#b21">Rudolph et al. (2016)</ref> developed exponential family embeddings. Their work casts embeddings in a probabilistic framework and generalizes them to model various types of high-dimensional data.</p><p>Exponential family embeddings give a recipe for creating new types of embeddings. There are three ingredients. First is a notion of a context, e.g., a neighborhood of surrounding words around a word in a document. Second is a conditional distribution of data given its context, e.g., a categorical distribution for a word. Third is an "embedding structure" that captures parameter sharing, e.g., that the embedding vector for PHILOSOPHY is the same wherever it appears in the data. <ref type="bibr" target="#b21">Rudolph et al. (2016)</ref> show that exponential family embeddings embody many existing methods for word embeddings. Further, they easily extend to other scenarios, such as movie ratings, shopping basket purchases, and neuroscience data.</p><p>Many applications of embeddings-both the classical application to language and the others types of data-involve sparse observations, data that contain many zero entries. As examples, shoppers do not purchase most of the items in the store, authors do not use most of the words in a vocabulary, and movie-watchers do not view most of the movies in an online collection. Sparse observations are challenging for many machine learning methods because the zeros dominate the data. Most methods will focus on capturing and predicting them, and embeddings are no exception.</p><p>Folk wisdom says that zeros in sparse data contain less information than the non-zeros. Consequently, practitioners use various methods to downweight them <ref type="bibr" target="#b9">(Hu et al., 2008)</ref> or downsample them, as is often the case in word embeddings <ref type="bibr" target="#b14">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b21">Rudolph et al., 2016)</ref>. Empirically, methods that downweight and downsample the zeros far outperform their counterparts.</p><p>What this folk wisdom suggests is that zeros often occur for one of two reasons-either they are part of the underlying process that we are trying to model, such as capturing the meanings of words, or they part of a different process, such as that only a particular part of speech belongs in a particular location in a sentence. As other examples, a film enthusiast might not watch a movie either because she does not think she will like it or because she has never heard of it; a shopper might not buy a brand of cookies either be-cause he doesn't like them or because he didn't see them. Motivated by this intuition, we develop zero-inflated embeddings, a probabilistic embedding model that captures the special status of the zeros in the data matrix.</p><p>The main idea is as follows. Exponential family embeddings model the conditional distribution of each data point given its context, where the parameter to that distribution relates to the embedding vectors. In a zero-inflated embedding, the conditional distribution places extra probability mass on zero, capturing conditions other than the embedding under which an item might not appear. If an observed zero is explained by this extra probability, then the corresponding item is not exposed to its relation with other items.</p><p>The probability of seeing a zero that is not from the embedding model can be a fixed quantity or can depend on other properties, such as the popularity of an item, demographics about the shopper, or the parts of speech of the context words. While zero-inflated embeddings sometimes fall into the class of an exponential family embedding (as in the Bernoulli case), they sometimes reflect a more complex distribution.</p><p>The practical effect is that zero-inflated embeddings intelligently downweight the zeros of the data-the embeddings no longer need to explain all of the zeros-and empirically improve the learned representations of the words or other type of data. We will demonstrate zero-inflated embeddings on language, movie ratings, and shopping baskets, as described above. We also study zero-inflated embeddings on bird watching logs <ref type="bibr" target="#b17">(Munson et al., 2015)</ref>, where features like time and location can influence which birds are possible to see.</p><p>Below, we develop zero-inflated embeddings and show how we can flexibly define the "exposure model" alongside the exponential family embedding model. We derive two algorithms to fit them and study them on a variety of data sets. Zero-inflated embeddings improve performance in language, shopping histories, recommendation system data, and bird watching logs.</p><p>Related work. The main thread of work on downweighting zeros comes from recommendation systems, where absent user-item interaction can be misconstrued as a user disliking an item. <ref type="bibr" target="#b9">Hu et al. (2008)</ref> and <ref type="bibr" target="#b20">Rendle et al. (2009)</ref> proposed to manually down-weight zeros and showed excellent empirical performance. <ref type="bibr" target="#b12">Liang et al. (2016)</ref> builds on this work and introduces an exposure model to explain zero entries. The exposure model captures whether a user does not see an item or intentionally chooses not to consume it. In a sense, zero-inflated embeddings build on <ref type="bibr" target="#b12">Liang et al. (2016)</ref>, using a type of "exposure model" in the context of embeddings and capturing item-item interactions. They also share similarities with the spike and slab model <ref type="bibr" target="#b15">(Mitchell &amp; Beauchamp, 1988)</ref> and zero-inflated regression models <ref type="bibr" target="#b11">(Lambert, 1992)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Zero-Inflated Embeddings</head><p>We first review exponential family embeddings. We then develop zero-inflated embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Exponential Family Embedding</head><p>An Exponential Family Embedding (EFE) generalizes word embedding to other types of data. It uses vector representations to capture conditional probabilities of data given a context. Specifically, an EFE aims to learn vector representation of items, that is, to represent each item j ∈ {1, . . . , J} with an embedding vector ρ j ∈ R K and a context vector α j ∈ R K . Vectors ρ j and α j for all j are denoted as ρ and α, respectively.</p><p>An EFE model learns from observation-context pairs. In each pair i ∈ {1, . . . , N }, the observation x i = {x ij : j ∈ s i } contains values observed from one or multiple items in s i , and the context y i = {y ij : j ∈ c i } contains the values of related items in a context set c i .</p><p>An EFE is defined by three elements: the context, the conditional distribution, and the embedding structure. We have defined the context. The conditional distribution of x i given its context y i is from the exponential family,</p><formula xml:id="formula_0">x i ∼ ExpFam η(y i , s i ), T (x i ) ,<label>(1)</label></formula><p>where the context y i provides the natural parameter through η(y i , s i ), and T (x i ) is the sufficient statistics.</p><p>The embeddings come into play in the natural parameter,</p><formula xml:id="formula_1">η(y i , s i ) = f ρ si j∈ci y ij α j ,<label>(2)</label></formula><p>where the columns of ρ si are embedding vectors of items in s i and f (·) is a link function. The EFE conditional probability p(x i |y i ; ρ si , α ci ) specifies the distribution of the values of items in s i in their context. When there is no ambiguity, we write the probability as p(x i |y i ). By fitting the conditional probability, the embedding model captures the interaction between items in s i and items in c i .</p><p>The embedding structure of EFE decides how the vectors of items are shared by different observation-context pairs. It is essentially the definition of c i and s i , which indicate how to attach item indices j-s to pair indices i-s.</p><p>Finally, an EFE puts Gaussian prior over α and ρ. For all j,</p><formula xml:id="formula_2">α j ∼ Gaussian(0, σ 2 α I) (3) ρ j ∼ Gaussian(0, σ 2 ρ I)<label>(4)</label></formula><p>where I is a K-identity matrix and σ 2 α and σ 2 ρ are hyperparameters controlling the variance of the context and embedding vectors.</p><p>An EFE infers α and ρ by maximizing the conditional log likelihood of observations given their contexts. The learned vectors α and ρ are able to capture the correlation between items and their contexts.</p><p>We give two examples. In movie ratings, we can extract observation-context pairs from a person's ratings: the observation x i is the rating of a single movie in s i , |s i | = 1, and the context is the ratings y i of all other movies rated by the same person. In language, the observation at a text position is a one-hot vector x i indicating which word is there and the context y i is the vector representation of words in the context. In this case s i and c i are both the entire vocabulary, {1, . . . , J}. The original word embedding model <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Mnih &amp; Hinton, 2007)</ref> assumes the conditional distribution to be the multinomial distribution with 1 trial. The word2vec model optimized by negative sampling (NEG) <ref type="bibr" target="#b14">(Mikolov et al., 2013b</ref>) uses a product of Bernoulli distributions as the conditional distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Exposure modeling with zero-inflation</head><p>An EFE explains every observation x i by an item's interaction with its context. However, as we described in Section 1, we may not want the embeddings to explain every observation, especially when they are dominated by zeros.</p><p>A Zero-Inflated Embedding (ZIE) places extra probability mass at zero in the embedding distribution. This mass can be thought of as the probability that the corresponding item is not "exposed" to its interaction with other items. In a ZIE, the embeddings vectors need not capture the (zero) data that are explained by the extra mass.</p><p>In more detail, for each observed value x ij we explicitly define an exposure indicator b ij to indicate whether the corresponding item j is exposed to the interaction with context items (b ij = 1) or not (b ij = 0). Each b ij is a random variable from Bernoulli distribution with probability u ij ,</p><formula xml:id="formula_3">b ij ∼ Bernoulli(u ij ).<label>(5)</label></formula><p>The exposure indicators and exposure probabilities of the items in the observation x i are collectively denoted as b i = {b ij : j ∈ s i } and u i = {u ij : j ∈ s i } respectively.</p><p>In many applications, we have the information about the exposure probability u ij . Suppose we have a set of covariates v i ∈ R d for each i related to the exposure probability. We fit u ij with a logistic regression,</p><formula xml:id="formula_4">u ij = logistic(w j v i + w 0 j ),<label>(6)</label></formula><p>where w j are the coefficients and w 0 j is the intercept. (If there is no such covariates, then only the intercept term is used, which means that the exposure probabilities of items are shared by observation-context pairs.) Next, we incorporate the exposure indicator into the embedding model. When x i = {x ij } is an observation with only one item j, the indicator b ij decides whether x ij is zero or from the embedding distribution,</p><formula xml:id="formula_5">x ij ∼ δ 0 if b ij = 0 ExpFam η(y i , s i ), T (x ij ) if b ij = 1 .<label>(7)</label></formula><p>The distribution δ 0 has probability mass 1 at 0.</p><p>When x i has multiple entries, the indicator vector b i decides the exposure of each item separately. The items not exposed have zero values,</p><formula xml:id="formula_6">x ij ∼ δ 0 , ∀j ∈ s i , b ij = 0.<label>(8)</label></formula><p>Let the items exposed be s</p><formula xml:id="formula_7">+ i = {j : j ∈ s i , b ij = 1}</formula><p>, and their values are x</p><formula xml:id="formula_8">+ i = {x ij : j ∈ s i , b ij = 1}. Then x + i</formula><p>is from a smaller embedding model restricted to s</p><formula xml:id="formula_9">+ i . x + i ∼ ExpFam η(y i , s + i ), T (x + i ) .<label>(9)</label></formula><p>We have p(</p><formula xml:id="formula_10">x i |y i , b i ) = p(x + i |y i , b i ) when s i has either single or multiple items.</formula><p>Finally, if the exposure probabilities are fit by covariates then each weight vector w j is also given a Gaussian prior,</p><formula xml:id="formula_11">w j ∼ Gaussian(0, σ 2 w I), j = {1, . . . , J}.<label>(10)</label></formula><p>The identity matrix I has size d here, and σ 2 w is the hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Inference</head><p>In this subsection, we derive the method of inferring α and ρ from ZIE. The approach is to maximize the loglikelihood i log p(x i |y i , u i ) with all hidden variables b i marginalized.</p><p>In this subsection, we derive the solution with u i -s instead of (w j , w 0 j )-s for notational simplicity. Once we can calculate the gradient of each u i , the gradient calculation of each (w j , w 0 j ) is straightforward. The probability p(x</p><formula xml:id="formula_12">+ i |y i , b i )</formula><p>is indeed from the basic embedding model, and we denote it asp(x + i |y i ) to explicitly show how the basic embedding model is positioned in the solution.</p><p>The inference is easy when the observation x i is about one item. We can either use EM to maximize its exact variational lower bound or directly marginalize out the hidden variable. We briefly give both solutions here, as they have different indications of zero inflation.</p><p>We first give the EM solution. In E step, we calculate the posterior distribution p(b ij |x ij , y i , u ij ) of each exposure indicator b ij . This distribution is a Bernoulli distribution, and its parameter is denoted as µ ij . Applying the Bayesian rule, we have</p><formula xml:id="formula_13">µ ij = uijp(xij =0|yi) 1−uij +uijp(xij =0|yi) if x ij = 0 1 if x ij = 0 .<label>(11)</label></formula><p>The lower bound of the log-likelihood of pair i is</p><formula xml:id="formula_14">E log p(x ij |y i , b ij ) − KL(µ ij , u ij ) = µ ij logp(x ij |y i ) − KL(µ ij , u ij ) if x ij = 0 logp(x ij |y i ) + log u ij if x ij = 0 .<label>(12)</label></formula><p>The expectation is taken with respect to p(b ij |x ij , y i , u ij ), and KL(µ ij , u ij ) is the KL-divergence from the prior to posterior of b ij . In M step, the lower bound is maximized with respect to α and ρ. Note that, there is no need to take derivative with respect to µ ij when taking gradient steps even though it is a function of α and ρ, because µ ij already maximizes the lower bound <ref type="bibr" target="#b8">(Hoffman et al., 2013)</ref>.</p><p>Eq. <ref type="formula" target="#formula_0">(12)</ref> shows that zero-inflation downweights zero entries by µ ij when learning α and ρ. This method of downweighting zeros is derived in a systematic way instead of a hack.</p><p>We can also marginalize b ij directly.</p><formula xml:id="formula_15">p(x ij |y i , u ij ) = u ijp (x ij = 0|y i ) + (1 − u ij ) if x ij = 0 u ijp (x ij |y i ) if x ij = 0 .<label>(13)</label></formula><p>This equation makes the zero-inflation clearer. Now let's work on the inference problem when x i has values of multiple items. In this case, we need to consider an embedding modelp(x + i |y i ) for each configuration of b i , so there are potentially exponential number of embedding models to consider. We have to exploit the structure of the embedding model to give a tractable MLE problem. In this paper, we consider two special cases, that the embedding models are independent for items in s i , and that the observation x i is from a multinomial distribution.</p><p>In the first case, the embedding model is the product of the embedding models with the same context. Word2vec with NEG training is a special case with single models as Bernoulli embedding <ref type="bibr" target="#b14">(Mikolov et al., 2013b)</ref>.</p><formula xml:id="formula_16">p(x i |y i , b i ; ρ si , α ci ) = j∈si p(x ij |y i , b ij ; ρ j , α ci ). (14)</formula><p>The exposure indicators are independent of each other, so the entire model can be decomposed over items in s i and solved as single models. We omit the detail here. Now we consider the case when the embedding distribution is multinomial with 1 trial, which is the model assumption of word embedding prior to the proposal of NEG training <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013a)</ref>. The link function f (·) is the identity function, so the vector products in η(·) directly give logits of the multinomial distribution. Denote the natural parameter as η i = ρ si j∈ci y ij α j . The probability vector of the multinomial distribution is π i = softmax(η i ).</p><p>The logarithm of the joint probability of the model for one pair is</p><formula xml:id="formula_17">log p(x i , b i |y i ) = logp(x + i |y i ) + log p(b i ).<label>(15)</label></formula><p>Note that the probabilityp(x + i |y i ) is from the embedding model decided by b i . To learn the model, we need to maximize the log-likelihood of the data with the hidden variable b i marginalized.</p><p>To avoid considering exponentially large number of models, we use the following relation between the full embedding model and the one with items exposed only in s</p><formula xml:id="formula_18">+ i . logp(x + i |y i ) = logp(x i |y i ) − log(b i π i ).<label>(16)</label></formula><p>Here the last term re-normalizes the probability ofp(x i |y i ) to get the probability of picking one from these items that are exposed.</p><p>Expandp(x i |y i ) and cancel the normalizer, then we have</p><formula xml:id="formula_19">logp(x + i |y i ) = η ij * − log(b i exp(η i )).<label>(17)</label></formula><p>Here j * is the index such that x ij * = 1.</p><p>Now we consider the problem of marginalizing b i via variational inference. Let q(b i ) be the variational distribution. Combine Eq. (15) and <ref type="formula" target="#formula_0">(17)</ref>, then the variational lower bound is,</p><formula xml:id="formula_20">L q = E q(bi) logp(x + i |y i ) − KL(q(b i ), p(b i )) = η ij * − E q log(b i exp(η i )) − KL(q(b i ), p(b i )).<label>(18)</label></formula><formula xml:id="formula_21">KL(q(b i ), p(b i )) is the KL-divergence of p(b i ) from the posterior q(b i ).</formula><p>This lower bound often needs to be maximized in the online manner due the large quantity of data, but the expectation of the logarithm is challenging to estimate even with moderate size of s i . In this work, we find a data-related lower bound of the expectation term.</p><p>Let γ be any subset of s i such that j * ∈ γ and |γ| = r, and b iγ be the sub-vector of b i indexed by γ. If</p><formula xml:id="formula_22">exp(η ij * ) j∈si exp(η ij ) ≥ r |s i | ,<label>(19)</label></formula><formula xml:id="formula_23">then b i exp(η i ) ≤ |si| r b iγ exp(η iγ )</formula><p>for any b i , and then we have another lower bound</p><formula xml:id="formula_24">max q L q ≥ max q(biγ ) η ij * − E q(biγ ) log(b iγ exp(η iγ )) + log r |s i | − KL(q(b iγ ), p(b iγ )). (20)</formula><p>Here q(b iγ ) is the marginal of some q(b i ).</p><p>We maximize the objective on the r.h.s. of Eq. <ref type="formula" target="#formula_1">(20)</ref> with respect to q(b iγ ) and model parameters. In each iteration of calculation, we randomly sample a subset γ, maximize the lower bound with respect to q(b iγ ), and calculate the gradient of model parameters. The maximization with respect to q(b iγ ) is tractable for small r. For larger r, we restrict the form of q(b iγ ). In our experiment, we set r = 5, and let q(b iγ ) assign zero probability to any b i that has more than one zero entry. Then we only need to consider r configurations of b i : the case that all items in γ are exposed and the cases that only one item, j ∈ γ, j = j * , is not exposed, so the maximization with respect to q(b iγ ) can be calculated efficiently.</p><p>The lower bound in Eq. (20) essentially uses r − 1 "negative" items in the random set γ to contrast the item j * in a smaller multinomial distribution. Since the set γ is randomly selected, every j = j * has the chance to be used as a negative sample. The maximization procedure encourages the model to get larger value of η ij * , and empirically the condition (19) often holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Computation with Subgradient</head><p>The data for embedding is often in large amount, so optimization with stochastic gradients is critically important. In problems where s i has only one item, a pair with nonzero observation often provides more informative gradients than a pair with zero observation. In our optimization, we keep all pairs with non-zero observations and sub-sample zero observations to estimate an unbiased stochastic gradient. The resultant optimization is much faster than that with full gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Empirical Study</head><p>In this section, we empirically evaluate the Zero-Inflated Embeddings. We compare four models, two baselines and two variants of our model, in the following subsections: 1) EFE is the basic exponential family embedding model; 2) EFE-dz assigns weight 0.1 to zero entries in the training data (same as <ref type="bibr" target="#b21">(Rudolph et al., 2016)</ref>); 3) ZIE-0 is the zero-inflated embedding model and fits the exposure probabilities with the intercept term only; and 4) ZIE-cov fits exposure probabilities with covariates.</p><p>All models are evluated with four datasets, eBird-PA, MovieLens-100K, Market, and Wiki-S, which will be introduced in detail in the following subsections. Their general information is tabulated in <ref type="table" target="#tab_0">Table 1</ref>. The last column lists the number of exposure covariates, which is used by ZIE-cov to fit the exposure probability.</p><p>All four models are optimized by AdaGrad <ref type="bibr" target="#b3">(Duchi et al., 2011)</ref> implemented in TensorFlow 1 , and the AdaGrad parameter η for step length is set to 0.1. One tenth of the training set is separated out as the validation set, whose log-likelihood is used to check whether the optimization procedure converges. The variance parameters of α, ρ, and w are set to 1 for all experiments.</p><p>We report two types of predictive log-likelihood on the test set, the log-likelihood of all observations (denoted as "all") and that of non-zero entries only (denoted as "pos"). For non-zero entries, the predictive log-likelihood is calculated as log p(x i |y i , x i &gt; 0) = log p(x i |y i ) − log(1 − p(x i = 0|y i )). The predictive log-likelihood is also estimated through sub-sampling in the same way as in training. We use α vectors as embeddings of items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bird embedding from bird observations</head><p>In this experiment, we embed bird species into the vector space by studying their co-occurance pattern in bird observations <ref type="bibr" target="#b17">(Munson et al., 2015)</ref>. The data subset eBird-PA consists of bird observations from a rectangular area that mostly overlaps Pennsylvania and the period from day 180 to day 210 of years from 2002 to 2014. Each datum in the subset is a checklist of counts of 213 bird species reported from one observation event. The values of these counts range from zero to hundreds. Some extraordinarily large counts are treated as outliers and set to the mean of positive counts of that species. Associated with each checklist there are 13 observation covariates, such as effort time, effort distance, and observation time of the day. The dataset is randomly split into two thirds as the training set and one third as the test set.</p><p>The embedding model use Poisson distribution to fit the count of each species j given the counts of all other species for each checklist, so s i = {j} and c i = {1, . . . J}\j. The link function is log softplus(·), which means the Poisson parameter λ is the softplus function of the linear product. The embedding dimension K iterates over the set Performance comparison: <ref type="table" target="#tab_1">Table 2</ref> shows the predictive log-likelihoods of the four models with different values of K. The two models with zero-inflation get much better predictive log-likelihood on the entire test set. On positive observations, ZIE models get similar results with the model downweighting zero entries. ZIE-cov performs slightly better than ZIE-0 on all observations and has similar performance with ZIE-0 on positive observations. We have also tried other negative weights (0.05, 0.2) for EFE-dz and found a similar trend: smaller weight gives slightly better predictive log-likelihood on positive observations but worse overall predictive log-likelihood.</p><p>Sensitiveness to data sparsity: We down-sample positive observations of one common species, American Robin <ref type="figure" target="#fig_0">(Figure 1, left)</ref>, and test how the embedded vectors changes. Specifically, we randomly set positive counts of American Robin to zero and keep only r = {0.005, 0.01, 0.05, 0.1} of positive counts. For each r, we compared the embedded vectors learned from downsampled data with those learned from the original data.</p><p>The vectors are compared by their respectively induced Poisson parameter λ. Let j * be the index of American Robin, then for each species j = j * , the distribution of the count of j given one American Robin has parameter λ j = softplus(ρ j α j * ).</p><p>From the original and down-sampled data, we get two embeded vectors and then have two Poisson parameters λ orig j and λ down j . The first measure of difference is the symmetric KL divergence (sKL) of the predictive probabilities of presence calculated from the two λ values with Poisson distribution,</p><formula xml:id="formula_25">∆ p = sKL p(x j &gt; 0; λ orig j ), p(x j &gt; 0; λ down j ) .</formula><p>The second measure is the absolute difference of λ values,</p><formula xml:id="formula_26">∆ λ = |λ orig j − λ down j</formula><p>|. These two measures are averaged over all species. <ref type="table" target="#tab_2">Table 3</ref> shows these measures calculated from different models with K = 64. We can see that the embedding model with exposure explanation is less sensitive to missing observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exposure explanation:</head><p>We also explore what the exposure  probability captures about zero observations. We check the learned coefficients w of the species Bald Eagle <ref type="figure" target="#fig_0">(Figure 1</ref>, middle). Its coefficient corresponding to the effort hours of the observation is large, at the percentage of 0.83 of all birds. It agrees with bird watching, since eagles are usually rare and need long time to be spotted. Another example is Eastern Screech-Owl <ref type="figure" target="#fig_0">(Figure 1, right)</ref>, which is only active at night. Its two coefficients corresponding to the observation time at hours 7-12 and hours 12-18 of the day are the smallest among all species. With the learned coefficients, the exposure probabilites are able distinguish the generative process of zeros according to their observation conditions and thus downweight zeros more correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Movie embedding from movie ratings</head><p>In this experiment, we study movie ratings and embed movies as vectors. The MovieLens-100K dataset <ref type="bibr" target="#b5">(Harper &amp; Konstan, 2015)</ref> consists of movie ratings from different users. It is preprocessed in the same way as <ref type="bibr" target="#b21">Rudolph et al. (2016)</ref>: translating ratings above 2 to the range 1-3 and treating absent ratings and ratings no greater than 2 as zeros. We remove all movies with less than 20 ratings. Six covariates, the age, the gender, and four profession categories, are extracted from each user and used as exposure covariates.</p><p>For the ratings from the same person, the embedding model fit the rating of one movie given the ratings of all other movies. The embedding distribution is the binomial distribution with 3 trials. The parameter K ranges over {8, 16, 32}. We run 10 fold cross validation on this dataset.</p><p>Performance comparison: In this result, ZIE-0 and ZIEcov performs much better than the two baselines in predictive performance on all entries. With zero-entries downweighted, EFE-dz is able to predict non-zeros better than  Figure 2. Exposure probability versus rating frequency. Movies with high rating frequency and movies with low ratings tend to get high exposure probability.</p><p>ZIE-0 and ZIE-cov, but it is much less capable to predict which entries are zero. ZIE-cov slightly improves over ZIE-0 in predictive log-likelihood of both types.</p><p>Exposure probability versus rating frequency: We investigate the exposure probability learned without covariates. We plot the exposure probability of movie versus its frequency of ratings in <ref type="figure">Fig.2</ref>. Each dot represents a movie, its position indicating its exposure probability and rating frequency, and its color being the average of its positive ratings. The exposure probability is from the vector u.</p><p>This figure shows that the exposure probability generally correlates with the popularity of the movie. However, some movies with low rating frequency are given high exposure probabilities. These movies have low ratings, which can and should be explained by the embedding component. For example, the movie "Money Train", whose average rating is at the percentage of 0.08 among all movies, has the largest ratio of exposure probability over rating frequency. The movies with high rating frequency get high exposure probabilities no matter their average rating is high or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Product embedding from market baskets</head><p>In this experiment, we embed products in grocery stores into the vector space. The Market dataset <ref type="bibr" target="#b2">(Bronnenberg et al., 2008)</ref> consists of purchase records of anonymous households in grocery stores. Each record is for a single shopping trip and contains the costumer ID, respective counts of items purchased, and other information. From the dataset, we take 20 related covariates, such as income range, working hours, age group, marital status, and smok- ing status, to describe each user.</p><p>The embedding model fits the count of one item given counts of other items in the same shopping trip. All embedding models use Poisson distribution as the conditional distribution and log softplus(·) as the link function. The covariates of each item-context pair are the covariates of the customer making the shopping trip. This dataset is randomly split into a training set and a test set by 2:1. The number K of embedding dimensions ranges over {16, 32, 64}.</p><p>Performance comparison: On this dataset, we compare the performances by the predictive log-likelihood on the test set. <ref type="table" target="#tab_4">Table 5</ref> shows the predictive log-likelihood of different models. ZIE-cov gives the largest values of predictive log-likelihood of both type. The exposure covariates of this dataset is informative, so the improvement of ZIE-cov is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Word embedding</head><p>In this subsection, we test word embedding with Wikipedia documents. We take the first million documents of the Wikipedia corpus prepared by <ref type="bibr" target="#b19">Reese et al. (2010)</ref>. The words in these documents have been tagged by FreeLing with part-of-speech (POS) tags. We keep the top 10,000 frequent words as our vocabulary and remove all words not in the vocabulary. The covariates of each word is from the POS tag of the preceding word no matter the word is removed or not. The original FreeLing POS tags has 64 types, and we combine them into 11 larger types, such as noun, verb, and adverb. The covariates are indicators of these 11 types. The subset is further split into a training set and a test set by 2:1.</p><p>On this dataset, we test embedding models with Bernoulli distribution (ZIE) and our relaxed multinomial distribution ( ZIE-m). ZIE-cov and ZIE-m-cov use the tag type as the exposure covariates. For Bernoulli distributions, zero entries are downweighted by weight 0.0005 (the target word versus 5 negative words). For multinomial distribution, we randomly sample 5 words as the set r for each word-context pair. We use a context window size of 1, so the context of a word position is the two words before and after the position. Note that word2vec with CBOW and NEG training    <ref type="bibr" target="#b14">(Mikolov et al., 2013b</ref>) is generally the same as EFE when it takes some settings related to implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance comparison:</head><p>Tab. 6 shows the average predictive log-likelihood per document on the test set by Bernoulli embedding models. The embedding model gets the best predictive performance when it uses POS tags to fit the exposure probability.</p><p>We also compare all models on three benchmark datasets of word similarities, MEN <ref type="bibr" target="#b0">(Baroni et al., 2014)</ref>, WS353 <ref type="bibr" target="#b4">(Finkelstein et al., 2002)</ref>, and SIMLEX999 <ref type="bibr" target="#b7">(Hill et al., 2014)</ref>, and show the results in <ref type="table" target="#tab_6">Table 7</ref>, with each entry being Spearman's correlation of embedding similarities and human-rated similarities. The code is from the repository 2 constructed by <ref type="bibr" target="#b10">Jastrzebski et al. (2017)</ref>. The last column as a reference shows the performance of GloVe <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref> word vectors with dimension 300 trained on 6 billion documents. The results on the three tasks shows that ZIE models perform slightly better than the baseline model.</p><p>Word relation learned with exposure explanation: To better understand how the exposure model affects the embedded vectors of words, we check cosine similarities of word vectors α learned by different models. <ref type="table">Table 8</ref> shows examples of similar words discovered by Bernoulli embedding models.</p><p>We have two interesting observations. First, word embedding with zero-inflation often gives lower similarity scores than the one without. This means the embedded word vectors of ZIE models are more spread out in the space. Second, the distance between a noun word and its plural form often have shorter distance with the vectors learned by ZIE-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary</head><p>In this work, we have proposed zero-inflated exponential family embedding. With the exposure indicator explaining unrelated zero observations, the real embedding component is able to focus more on observations from item interactions, so the embedded vectors can better represent items in terms of item relations. We have investigated ZIE for two types of embedding models: the observation being from one and multiple items. We have also developed the inference algorithms for different embedding models with zero inflation. Experiment results indicate that ZIE improves the predictive log-likelihood of the data. Qualitative analysis shows that the embedded vectors from the ZIE model have better quality than the vectors learned with the basic embedding model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Three bird species in study. Left: American Robin, middle: Bald Eagle, right: Eastern Screech-Owl.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>6 .</head><label>6</label><figDesc>Predictive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Information about datasets.</figDesc><table>dataset 
# item # nonzero sparsity range # covar 
eBird-PA 
213 
410k 
0.08 
N 
13 

MovieLens-100K 

811 
78.5k 
0.1 
0-3 
6 
Market 
7903 
737k 
10 

−3 

N 
20 
Wiki-S 
10000 
365m 
10 

−4 

0/1 
11 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>ZIE models improve predictive log-likelihood on bird data.</figDesc><table>K 
EFE 
EFE-dz 
ZIE-0 
ZIE-cov 

32 
all −0.416(.002) 
−0.555(.001) 
−0.324(.001) 
−0.314(.001) 
pos −2.407(.017) −1.855(.012) −1.844(.011) −1.840(.011) 

64 
all −0.374(.002) 
−0.490(.001) 
−0.308(.001) 
−0.298(.001) 
pos −2.140(.016) −1.727(.013) −1.736(.012) −1.739(.012) 

128 
all −0.348(.002) 
−0.459(.001) 
−0.300(.001) 
−0.291(.001) 
pos −1.992(.019) −1.681(.015) −1.705(.015) −1.708(.015) 

{32, 64, 128}. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>The measures (∆p / ∆ λ ) of embedded vectors calculated at 4 levels of downsampling (smaller is better). Embedded vectors learned by ZIE models are more resistant to down-sampling.</figDesc><table>d.s. ratio 
EFE 
EFE-dz 
ZIE-0 
ZIE-cov 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>ZIE models improves predictive log-likelihood values on movie rating data.</figDesc><table>K 
EFE 
EFE-dz 
ZIE-0 
ZIE-cov 

8 
all −0.461(.001) 
−0.740(.001) 
−0.350(.001) −0.349(.001) 
pos −1.870(.007) −1.145(.003) 
−1.170(.004) 
−1.163(.004) 

16 
all −0.450(.001) 
−0.706(.001) 
−0.348(.001) −0.348(.001) 
pos −1.795(.007) −1.146(.003) 
−1.214(.004) 
−1.207(.004) 

32 
all −0.450(.001) 
−0.669(.001) 
−0.348(.001) −0.349(.001) 
pos −1.758(.007) −1.152(.004) 
−1.267(.005) 
−1.265(.005) 

0.000 
0.002 
0.004 
0.006 
0.008 
Rating frequency 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 .</head><label>5</label><figDesc>ZIE models improves the predictive log-likelihood on Market data</figDesc><table>K 
EFE 
EFE-dz 
ZIE-0 
ZIE-cov 

32 
all −0.014(.000) −0.024(.000) −0.009(.000) −0.007(.000) 
pos −4.719(.024) −2.169(.012) −2.195(.012) −1.060(.004) 

64 
all −0.014(.000) −0.022(.000) −0.009(.000) −0.008(.000) 
pos −4.581(.023) −2.030(.011) −2.190(.012) −1.127(.005) 

128 
all −0.015(.000) −0.023(.000) −0.009(.000) −0.008(.000) 
pos −4.746(.023) −2.041(.011) −2.290(.012) −1.295(.005) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 7 .</head><label>7</label><figDesc>Performance of different models on word similarity tasks.</figDesc><table>EFE 
ZIE-0 
ZIE-cov ZIE-m-0 ZIE-m-cov GloVe 

MEN 
0.610 0.616 
0.592 
0.608 
0.611 
0.737 
WS353 
0.537 
0.533 
0.543 
0.557 
0.562 
0.522 
SIMLEX999 0.290 0.281 
0.276 
0.268 
0.264 
0.371 

Table 8. Nearest words find by three embedding models. 
battle 
philosophy 
novel 
class 
english 
bible 

EFE 

combat (0.75) sociology (0.81) 
book (0.85) 
division (0.67) 
welsh (0.88) 
hebrew (0.75) 
defeat (0.73) 
theology (0.79) 
novels (0.80) 
k (0.66) 
french (0.87) 
study (0.73) 
invasion (0.73) tradition (0.77) 
poem (0.79) 
field (0.63) 
spanish (0.86) 
biblical (0.73) 

ZIE-0 

fire (0.62) 
religion (0.69) 
story (0.72) 
division (0.56) 
french (0.85) 
poetry (0.63) 
battles (0.62) 
society (0.68) 
book (0.71) 
family (0.55) swedish (0.81) 
hebrew (0.61) 
assault (0.61) 
theology (0.67) 
novels (0.70) 
classes (0.54) 
irish (0.79) 
dictionary (0.60) 

ZIE-cov 

battles (0.61) 
principles (0.69) novels (0.76) 
classes (0.56) 
french (0.84) 
biblical (0.64) 
assault (0.60) 
religion (0.68) 
story (0.74) 
rank (0.54) 
spanish (0.83) 
hebrew (0.63) 
attack (0.59) 
theology (0.67) fantasy (0.71) 
grade (0.53) 
swedish (0.75) 
texts (0.61) 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Columbia University, 500 W 120th St., New York, NY 10027 2 Tufts University, 161 College Ave., Medford, MA 02155. Correspondence to: Li-Ping Liu &lt;ll3105@columbia.edu&gt;, David M. Blei &lt;david.blei@columbia.edu&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.tensorflow.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/kudkudak/word-embeddings-benchmarks cov. The difference of a word and its plural form are more in grammar than semantics. The POS tags can partially explain the grammatical difference. For example, numbers (tagged as number) often go before a plural instead of a singular word. In such cases, the singular word as a negative example will be downweighted, and thus the embedding component is more likely to treat the singular word and its plural as similar words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C-4032, and the Alfred P. Sloan Foundation. Thank all reviewers of the paper for their feedback. Thank Francisco J. R. Ruiz, Maja R. Rudolph, Kriste Krstovski, and Aonan Zhang for helpful comments and discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janvin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Database paper-the IRI marketing data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Bronnenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mela</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="745" to="748" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">21212159</biblScope>
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The MovieLens datasets: history and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<idno>19:1-19:19</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z. Distributional structure. Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SimLex-999: evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leśniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02170</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-inflated poisson regression, with an application to defects in manufacturing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling user exposure in recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="951" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781[cs.CL]</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian variable selection in linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Beauchamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">404</biblScope>
			<biblScope unit="page" from="1023" to="1032" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The eBird reference dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hochachka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sorokina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GloVe: global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wikicorpus: A word-sense disambiguated multilingual wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Language Resources and Evaluation Conference</title>
		<meeting>the 7th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1418" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidtthieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exponential family embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputing: Foundations of Research</title>
		<editor>Anderson, J. A. and Rosenfeld, E.</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="696" to="699" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
