<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentially Private Database Release via Kernel Mean Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Balog</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
						</author>
						<title level="a" type="main">Differentially Private Database Release via Kernel Mean Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We lay theoretical foundations for new database release mechanisms that allow third-parties to construct consistent estimators of population statistics, while ensuring that the privacy of each individual contributing to the database is protected. The proposed framework rests on two main ideas. First, releasing (an estimate of) the kernel mean embedding of the data generating random variable instead of the database itself still allows thirdparties to construct consistent estimators of a wide class of population statistics. Second, the algorithm can satisfy the definition of differential privacy by basing the released kernel mean embedding on entirely synthetic data points, while controlling accuracy through the metric available in a Reproducing Kernel Hilbert Space. We describe two instantiations of the proposed framework, suitable under different scenarios, and prove theoretical results guaranteeing differential privacy of the resulting algorithms and the consistency of estimators constructed from their outputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We aim to contribute to the body of research on the trade-off between releasing datasets from which publicly beneficial statistical inferences can be drawn, and between protecting the privacy of individuals who contribute to such datasets. Currently the most successful formalisation of protecting user privacy is provided by differential privacy <ref type="bibr" target="#b6">(Dwork &amp; Roth, 2014)</ref>, which is a definition that any algorithm operating on a database may or may not satisfy. An algorithm that does satisfy the definition ensures that a particular individual does not lose too much privacy by deciding to contribute to the database on which the algorithm operates.</p><p>While differentially private algorithms for releasing entire 1 MPI-IS, Tübingen, Germany 2 University of Cambridge, UK. Correspondence to: Matej Balog &lt;first.surname@gmail.com&gt;. Code: https://github.com/matejbalog/RKHS-private-database/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s).</p><p>databases have been studied previously <ref type="bibr" target="#b1">(Blum et al., 2008;</ref><ref type="bibr" target="#b27">Wasserman &amp; Zhou, 2010;</ref><ref type="bibr" target="#b28">Zhou et al., 2009)</ref>, most algorithms focus on releasing a privacy-protected version of a particular summary statistic, or of a statistical model trained on the private dataset. In this work we revisit the more difficult non-interactive, or offline setting, where the database owner aims to release a privacy-protected version of the entire database without knowing what statistics third-parties may wish to compute in the future.</p><p>In our new framework we propose to use the kernel mean embedding <ref type="bibr" target="#b23">(Smola et al., 2007)</ref> as an intermediate representation of a database. It is (1) sufficiently rich in the sense that it captures a wide class of statistical properties of the data, while at the same time (2) it lives in a Reproducing Kernel Hilbert Space (RKHS), where it can be handled mathematically in a principled way and privacy-protected in a unified manner, independently of the type of data appearing in the database. Although kernel mean embeddings are functions in an abstract Hilbert space, in practice they can be (at least approximately) represented using a possibly weighted set of data points in input space (i.e. a set of database rows). The privacy-protected kernel mean embedding is released to the public in this representation, however, using synthetic datapoints instead of the private ones. As a result, our framework can be seen as leading to synthetic database algorithms. We validate our approach by instantiating two concrete algorithms and proving that they output consistent estimators of the true kernel mean embedding of the data generating process, while satisfying the definition of differential privacy. The consistency results ensure that third-parties can carry out a wide variety of statistically founded computation on the released data, such as constructing consistent estimators of population statistics, estimating the Maximum Mean Discrepancy (MMD) between distributions, and two-sample testing <ref type="bibr" target="#b9">(Gretton et al., 2012)</ref>, or using the data in the kernel probabilistic programming framework for random variable arithmetics <ref type="bibr" target="#b22">Simon-Gabriel et al., 2016</ref>, Section 3), repeatedly and unlimitedly without being able to, or having to worry about, violating user privacy.</p><p>One of our algorithms is especially suited to the interesting scenario where a (small) subset of a database has already been published. This situation can arise in a wide variety of settings, for example, due to weaker privacy protections in the past, due to a leak, or due to the presence of an incentive, financial or otherwise, for users to publish their data. In such a situation our algorithm provides a principled approach for reweighting the public data in such a way that the accuracy of statistical inferences on this dataset benefits from the larger sample size (including the private data), while maintaining differential privacy for the undisclosed data.</p><p>In summary, the contributions of this paper are: 1. A new framework for designing database release algorithms with the guarantee of differential privacy. The framework uses kernel mean embeddings as intermediate database representations, so that the RKHS metric can be used to control accuracy of the released synthetic database in a principled manner (Section 3).</p><p>2. Two instantiations of our framework in the form of two synthetic database algorithms, with proofs of their consistency, convergence rates and differential privacy, as well as basic empirical illustrations of their performance on synthetic datasets (Sections 4 and 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Differential Privacy</head><p>Definition 1 <ref type="bibr" target="#b5">(Dwork, 2006)</ref>. For ε &gt; 0, δ ≥ 0, algorithm A is said to be (ε, δ)-differentially private if for all neighbouring databases D ∼ D (differing in at most one element) and all measurable subsets S of the co-domain of A,</p><formula xml:id="formula_0">P (A(D) ∈ S) ≤ e ε P (A(D ) ∈ S) + δ.<label>(1)</label></formula><p>The parameter ε controls the amount of information the algorithm can leak about an individual, while a positive δ allows the algorithm to produce an unlikely output that leaks more information, but only with probability up to δ. This notion is sometimes called approximate differential privacy; an algorithm that is (ε, 0)-differentially private is simply said to be ε-differentially private. Note that any non-trivial differentially private algorithm must be randomised; the definition asserts that the distribution of algorithm outputs is not too sensitive to changing one row in the database.</p><p>When the algorithm's output is a finite vector A(D) ∈ R J , two standard random perturbation mechanisms for making this output differentially private are the Laplace and Gaussian mechanisms. As the perturbation needs to mask the contribution of each individual entry of the database D, the scale of the added noise is closely linked to the notion of sensitivity, measuring how much the algorithm's output can change due to changing a single data point:</p><formula xml:id="formula_1">∆ 1 := sup D∼D A(D) − A(D ) 1 ,<label>(2)</label></formula><formula xml:id="formula_2">∆ 2 := sup D∼D A(D) − A(D ) 2 .<label>(3)</label></formula><p>The Laplace mechanism adds i.i.d. Lap(∆ 1 /ε) noise to each of the J coordinates of the output vector and ensures pure ε-differential privacy, while the Gaussian mechanism adds i.i.d. N (0, σ 2 ) noise to each coordinate, where σ 2 &gt; 2∆ Differential privacy is preserved under post-processing: if an algorithm A is (ε, δ)-differentially private, then so is its sequential composition B(A(·)) with any other algorithm B that does not have direct or indirect access to the private database D <ref type="bibr" target="#b6">(Dwork &amp; Roth, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Kernels, RKHS, and Kernel Mean Embeddings</head><p>A kernel on a non-empty set (data type) X is a binary positive-definite function k(·, ·) : X × X → R. Intuitively it can be thought of as expressing the similarity between any two elements in X . The literature on kernels is vast and their properties are well studied <ref type="bibr" target="#b20">(Schölkopf &amp; Smola, 2002)</ref>; many kernels are known for a large variety of data types such as vectors, strings, time series, graphs, etc, and kernels can be composed to yield valid kernels for composite data types (e.g. the type of a database row containing both numerical and string data).</p><p>The kernel mean embedding (KME) of an X -valued random variable X in the RKHS is the function µ <ref type="bibr" target="#b23">Smola et al., 2007)</ref>. Several popular kernels have been proved to be characteristic <ref type="bibr" target="#b7">(Fukumizu et al., 2008)</ref>, in which case the map p X → µ k X , where p X is the distribution of X, is injective. This means that no information about the distribution of X is lost when passing to its KME µ k X . In practice, the KME of a random variable X is approximated using a sample x 1 , . . . , x N drawn from X, which can be used to construct an empirical KMEμ k X of X in the RKHS: a function given by y → 1 N N n=1 k(x n , y). When the x n 's are i.i.d., under a boundedness conditionμ k X converges to the true KME µ <ref type="bibr" target="#b13">Lopez-Paz et al., 2015)</ref> 1 . Our approach relies on the metric of the RKHS in which these KMEs live. The RKHS H k is a space of functions, endowed with an inner product ·, · H k that satisfies the reproducing property k(x, ·), h = h(x) for all x ∈ X and h ∈ H k . The inner product induces a norm · H k , which can be used to measure distances µ k X − µ k Y H k between distributions of X and Y . This can be exploited for various purposes such as two-sample tests <ref type="bibr" target="#b9">(Gretton et al., 2012)</ref>, independence testing <ref type="bibr" target="#b8">(Gretton et al., 2005)</ref>, or one can attempt to minimise this distance in order to match one distribution to another.</p><formula xml:id="formula_3">k X : X → R, y → E X [k(X, y)], defined whenever E X [ k(X, X)] &lt; ∞ (</formula><formula xml:id="formula_4">k X at rate O p (N −1/2 ), inde- pendently of the dimension of X (</formula><p>An example of such minimisation are reduced set methods <ref type="bibr" target="#b2">(Burges, 1996;</ref><ref type="bibr">Schölkopf &amp; Smola, 2002, Chap. 18)</ref>, which replace a set of points S = {x 1 , . . . , x N } ⊆ X with a weighted set R = {(z 1 , w 1 ), . . . , (z M , w M )} ⊆ X × R (of potentially smaller size), where the new points z m can, but need not equal any of the x n s, such that the KME computed using the reduced set R is close to the KME computed using the original set S, as measured by the RKHS norm:</p><formula xml:id="formula_5">µ k S − µ k R H k = 1 N N n=1 k(x n , ·) − M m=1 w m k(z m , ·) H k .</formula><p>Reduced set methods are usually motivated by the computational savings arising when |R| &lt; |S|; we will invoke them mainly to replace a collection S of private data points with a (possibly weighted) set R of synthetic data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Throughout this work, we assume the following setup. A database curator wishes to publicly release a database D = {x 1 , . . . x N } ∈ X N containing private data about N individuals, with each data point (database row) x n taking values in a non-empty set X . The set X can be arbitrarily rich, for example, it could be a product of Euclidean spaces, integer spaces, sets of strings, etc.; we only require availability of a kernel function k : X × X → R on X . We assume that the N rows x 1 , . . . , x N in the database D can be thought of as i.i.d. observations from some X -valued data-generating random variable X (but see Section 7 for a discussion about relaxing this assumption). The database curator, wishing to protect the privacy of individuals in the database, seeks a database release mechanism that satisfies the definition of (ε, δ)-differential privacy, with ε &gt; 0 and δ ≥ 0 given. The main purpose of releasing the database is to allow third parties to construct estimators of population statistics (i.e. properties of the distribution of X), but it is not known at the time of release what statistics the third-parties will be interested in.</p><p>To lighten notation, henceforth we drop the superscript k from KMEs (such as µ k X ) and the subscript k from the RKHS H k , whenever k is the kernel on X chosen by the database curator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Algorithm Template</head><p>We propose the following general algorithm template for differentially private database release:</p><p>1. Construct a consistent estimatorμ X of the KME µ X of X using the private database.</p><p>2. Obtain a perturbed versionμ X of the constructed estimateμ X to ensure differential privacy.</p><p>3. Release a (potentially approximate) representation ofμ X in terms of a (possibly weighted) dataset</p><formula xml:id="formula_6">{(z 1 , w 1 ), . . . , (z M , w M )} ⊆ X × R.</formula><p>The released representation should be such that</p><formula xml:id="formula_7">M m=1 w m k(z m , ·)</formula><p>is a consistent estimator of the true KME µ X , i.e. such that the RKHS distance between the two converges to 0 in probability as the private database size N , and together with it the synthetic database size M , go to infinity.</p><p>Each step of this template admits several possibilities. For the first step we have discussed the standard empirical KME</p><formula xml:id="formula_8">1 N N n=1 k(x n , ·) with x 1 , . . . , x N i.i.d. observations of X,</formula><p>but the framework remains valid with improved estimators such as kernel-based quadrature <ref type="bibr" target="#b4">(Chen et al., 2010)</ref> or the shrinkage kernel mean estimators of <ref type="bibr" target="#b16">(Muandet et al., 2016)</ref>.</p><p>As the KMEsμ X and µ X live in the RKHS H of the kernel k, a natural mechanism for privatisingμ X in the second step would be to follow <ref type="bibr" target="#b10">(Hall et al., 2013)</ref> and pointwise add toμ X a suitably scaled sample path g of a Gaussian process with covariance function k(·, ·). This does ensure (ε, δ)-differential privacy of the resulting functionμ X = µ X + g, but unfortunatelyμ X ∈ H, because the RKHS norm g H of a Gaussian process sample path with the same kernel k is infinite almost surely <ref type="bibr" target="#b18">(Rasmussen &amp; Williams, 2005)</ref>. While our framework allows pursuing this direction by, for example, moving to a larger function space that does contain the Gaussian process sample path, in this work we will present algorithms that achieve differential privacy by mappingμ X into a finite-dimensional Hilbert space and then employing the standard Laplace or Gaussian mechanisms to the finite coordinate vector.</p><p>Differential privacy is preserved under post-processing, but the third step does require some care to ensure that private data is not leaked. Specifically, when several possible</p><formula xml:id="formula_9">(approximate) representationsμ X ≈ M m=1 w m k(z m , ·)</formula><p>in terms of a weighted dataset (w 1 , z 1 ), . . . , (w M , z M ) are possible, committing to a particular one reveals more information than just the functionμ X (consider, for example, the extreme case where the representation would be in terms of the private points x 1 , . . . , x N ). One thus needs to either control the privacy leak due to choosing a representation in a way that depends on the private data, or, as we do in our concrete algorithms below, choose a representation independently of the private data (but still minimising its RKHS distance to the privacy-protectedμ X ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Versatility</head><p>Algorithms in our framework release a possibly weighted</p><formula xml:id="formula_10">synthetic dataset {(z 1 , w 1 ), . . . , (z M , w M )} ⊆ X × R such that M m=1 w m k(z m , ·)</formula><p>is a consistent estimator of the true KME µ X of the data generating random variable X. This allows third-parties to perform a wide spectrum of statistical computation, all without having to worry about violating differential privacy:</p><p>1. Kernel probabilistic programming : The versatility of our approach is greatly expanded thanks to the result of <ref type="bibr" target="#b22">(Simon-Gabriel et al., 2016)</ref>, who showed that under technical conditions, applying a continuous function f to all points z m in the synthetic dataset yields a consistent estimator</p><formula xml:id="formula_11">M m=1 w m k f (f (z m ), ·)</formula><p>of the KME µ f (X) of the transformed random variable f (X), even when the points z 1 , . . . , z M are not i.i.d. (as they may not be, depending on the particular synthetic database release algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Consistent estimation of population statistics:</head><formula xml:id="formula_12">For any RKHS function h ∈ H, we have µ X , h H = E[h(X)]</formula><p>, so a consistent estimator of µ X yields a consistent estimator of the expectation of h(X). It can be evaluated using the reproducing kernel property:</p><formula xml:id="formula_13">E[h(X)] = µ X , h H ≈ M m=1 w m k(z m , ·), h H = M m=1 w m h(z m ).<label>(4)</label></formula><p>For example, approximating the indicator function 1 S of a set S ⊆ X with functions in the RKHS allows estimating probabilities: E[1 S (X)] = P[X ∈ S] (note that 1 S itself may not be an element of the RKHS).</p><p>3. MMD estimation and two-sample testing <ref type="bibr" target="#b9">(Gretton et al., 2012)</ref>: Given another random variable Y on X , one can consistently estimate the Maximum Mean Discrepancy (MMD) distance µ X − µ Y H between the distributions of X and Y , and in particular to construct a two-sample test based on this distance. Given a sample</p><formula xml:id="formula_14">y 1 , . . . , y L ∼ Y : µ X − µ Y H ≈ M m=1 w m k(z m , ·) − 1 L L l=1 k(y l , ·) H ,</formula><p>which can again be evaluated using the reproducing property.</p><p>4. Subsequent use of synthetic data: Since the output of the algorithm is a (possibly weighted) database, thirdparties are free to use this data for arbitrary purposes, such as training any machine learning model on this data. Models trained purely on this data can be released with differential privacy guaranteed; however, the accuracy of such models on real data remains an empirical question that is beyond the scope of this work.</p><p>An orthogonal spectrum of versatility arises from the fact that the third step in the algorithm template can constrain the released dataset (z 1 , w 1 ), . . . , (z M , w M ) to be more convenient or more computationally efficient for further processing. For example, one could fix the weights to uniform w m = 1 M to obtain an unweighted dataset, or to replace an expensive data type with a cheaper subset, such as requesting floats instead of doubles in the z m 's. All this can be performed while an RKHS distance is available to control accuracy betweenμ X and its released representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Concrete Algorithms</head><p>As a first illustrative example, we describe how a particular case of an existing, but inefficient synthetic database algorithm already fits into our framework. The exponential mechanism <ref type="bibr" target="#b15">(McSherry &amp; Talwar, 2007</ref>) is a general mechanism for ensuring ε-differential privacy, and in our setting it operates as follows: given a similarity measure s : X N × X M → R between (private) databases of size N and (synthetic) databases of size M , output a random (synthetic) database R with probability proportional to exp( To fit this into our framework, we can take s(D, R) = − µ D − µ R H to be the negative RKHS distance between the KMEs computed using D and R, and achieve ε-differential privacy by releasing R with probability proportional to exp(− ε 2∆1 µ D − µ R H ). This solves steps 2 and 3 of our general algorithm template simultaneously, as it directly samples a concrete representation of a "perturbed" KME µ R . The algorithm essentially corresponds to the SmallDB algorithm of <ref type="bibr" target="#b1">Blum et al. (2008)</ref>, except for choosing the RKHS distance as a well-studied similarity measure between two databases.</p><p>The principal issue with this algorithm is its computational infeasibility except in trivial cases, as it requires sampling from a probability distribution supported on all potential synthetic databases, and employing an approximate sampling scheme can break the differential privacy guarantee of the exponential mechanism. In Sections 4 and 5 respectively, we describe two concrete synthetic database release algorithms that may possess failure modes where they become inefficient, but employing approximations in those cases can only affect their statistical accuracy, not the promise of differential privacy.</p><p>Algorithm 1 Differentially private database release via a synthetic data subspace of the RKHS Input: database D = {x 1 , . . . , x N } ⊆ X , kernel k on X , privacy parameters ε &gt; 0 and δ &gt; 0 Output: (ε, δ)-differentially private, weighted synthetic database (representing an estimate of µ X in the RKHS H of k)</p><formula xml:id="formula_15">1: M ← M (N ) ∈ ω(1) ∩ o(N 2 )</formula><p>, number of synthetic data points to use 2: z 1 , . . . , z M ← initialised deterministically or randomly from some distribution q on X 3: H M ← Span <ref type="figure" target="#fig_1">({k(z 1 , ·)</ref>, . . . , k(z M , ·)}) ≤ H 4: b 1 , . . . , b F ← orthonormal basis of H M (obtained using, e.g. Gram-Schmidt) 5:</p><formula xml:id="formula_16">μ X ← 1 N N n=1 k(x n , ·), empirical KME of X in H 6: µ X ← F f =1 b f ,μ X H b f =: F f =1 α f b f , projection ofμ X onto H M 7: β ← α + N (0, 8 ln(1.25/δ) N 2 ε 2 I F )</formula><p>, an (ε, δ)-differentially private version of the coordinate vector α (Gaussian mechanism)</p><formula xml:id="formula_17">8:μ X ← F f =1 β f b f = M m=1 w m k(z m , ·), re-expressed in terms of k(z m , ·)'s 9: return (z 1 , w 1 ), . . . , (z M , w M )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Perturbation in Synthetic-Data Subspace</head><p>In this section we describe an instantiation of the framework proposed in Section 3 that achieves differential privacy of the KME by projecting it onto a finite-dimensional subspace of the RKHS spanned by feature maps k(z m , ·) of synthetic data points z 1 , . . . , z M , and perturbing the resulting finite coordinate vector. To ensure differential privacy, the synthetic data points are chosen independently of the private database. As a result, statistical efficiency of this approach will depend on the choice of synthetic data points, with efficiency increasing if there are enough synthetic data points to capture the patterns in the private data. Therefore this algorithm is especially suited to the scenario discussed in Section 1, where a part of the database (or of a similar one) has already been published, as this can serve as a good starting set for the synthetic data points.</p><p>The setting where some observations from X have already been released highlights the fact that differential privacy only protects against additional privacy violation due to an individual deciding to contribute to the private database; if a particular user's data has already been published, differential privacy does not protect against privacy violations based on exploiting this previously published data.</p><p>The algorithm is formalised as Algorithm 1 above. Lines 1-2 choose synthetic data points z 1 , . . . , z M independently of the private data (only using the database size N ). Lines 3-4 construct the linear subspace H M of H spanned by feature maps of the chosen synthetic data points, and compute a (finite) basis for it. Only then the private data is accessed: the empirical KMEμ X is computed (line 5), projected onto the subspace H M and expressed in terms of the precomputed basis (line 6). The basis coefficients of the projection are then perturbed to achieve differential privacy (line 7), and the perturbed elementμ X ∈ H M is then re-expressed in terms of the spanning set containing feature maps of synthetic data points (line 8). This expansion is finally released to the public (line 9).</p><p>Line 1 stipulates that the number of synthetic data points M → ∞ as N → ∞, but asymptotically slower than N 2 . This is to ensure that the privatisation noise added in the subspace H M to each coordinate is small enough overall to preserve consistency, as stated in the following Theorem 2. This theorem assures us that Algorithm 1 produces a consistent estimator of the true KME µ X , if the synthetic data points are sampled from a distribution with sufficiently large support. Due to space constraints, all proofs appear in Appendix A.</p><p>Theorem 2. Let X be a compact metric space and k a continuous kernel on X . If the synthetic data points z 1 , z 2 , . . . are sampled i.i.d. from a distribution q on X such that the support of X is included in the support of q, then Algorithm 1 outputs a consistent estimator of the KME µ X :</p><formula xml:id="formula_18">M m=1 w m k(z m , ·) P → µ X as N → ∞.</formula><p>As discussed by <ref type="bibr" target="#b22">Simon-Gabriel et al. (2016)</ref>, these assumptions are usually satisfied: X can be taken to be compact whenever the data comes from measurements with any bounded range, and many kernels are continuous, including all kernels on discrete spaces (w.r.t. to the discrete topology).</p><p>In order to use the output of Algorithm 1 in the very general kernel probabilistic programming framework and obtain a consistent estimator of the KME µ f (X) of f (X) for any continuous function f , there is a technical condition that the L 1 norm M m=1 |w m | of the released weights may need to remain bounded by a constant as N → ∞ (Simon- <ref type="bibr" target="#b22">Gabriel et al., 2016)</ref>. This is not enforced by Algorithm 1, but Theorem 11 in Appendix A.1 shows how a simple regularisation in the final stage of the algorithm achieves this without breaking consistency (or privacy).</p><p>The next result about Algorithm 1 shows that it is differentially private whenever k(x, x) ≤ 1 for all x ∈ X . This is a weak assumption that holds for all normalised kernels, and can be achieved by simple rescaling for any bounded kernel (such that sup x∈X k(x, x) &lt; ∞). When X is a compact domain, all continuous kernels are bounded. Horizontally we varied M , the number of publicly releasable data points. Stricter privacy requirements (lower ε) naturally lead to lower accuracy. Increasing M does not always necessarily improve accuracy, since a new public data point always increases the total amount of privatising noise that needs to be added, but this might not be outweighed by its positive contribution towards covering relevant parts of the input space. In all cases, for sufficiently small M Algorithm 1 provided a significantly more accurate estimate than µ baseline .</p><p>Proposition 3. If k(x, x) ≤ 1 for all x ∈ X , then Algorithm 1 is (ε, δ)-differentially private. For a finite private database, actual performance will heavily depend on how the synthetic data points are chosen. We consider the following two extreme scenarios:</p><p>1. No publishable subset: No rows of the private database are, or can be made public unmodified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Publishable subset:</head><p>A small proportion of the private database is already public, or can be made public.</p><p>Proposition 5 (Algorithm 1, No publishable subset). Say X is a bounded subset of R D , the kernel k is Lipschitz, and the synthetic data points z 1 , z 2 , . . . are sampled i.i.d. from a distribution q with density bounded away from 0 on any bounded subset of R D . Then M = M (N ) can be chosen so that the output of Algorithm 1 converges to the true KME µ X in RKHS norm at a rate O p (N −1/(D+1+c) ), where c is any fixed positive number c &gt; 0.</p><p>Unsurprisingly, the convergence rate deteriorates with input dimension D, since without prior information about the private data manifold it is increasingly difficult for randomly sampled synthetic points to capture patterns in the private data. One of the main strengths of KMEs is that the empirical estimator converges to the true embedding at a rate O p (N −1/2 ) independently of the input dimension D, so we see that in this unfavourable scenario Algorithm 1 incurs a substantial privacy cost in high dimensions. On the other hand, if a small, but fixed proportion of the private database is publishable, then Algorithm 1 incurs no privacy cost in terms of the convergence rate:</p><p>Proposition 6 (Algorithm 1, Publishable subset). Say that a fixed proportion η of the private database can be published unmodified. Using this part of the database as the synthetic data points, Algorithm 1 outputs a consistent estimator of µ X that converges in RKHS norm at a rate O p (N −1/2 ).</p><p>Note that in this scenario the rate O p (N −1/2 ) can be also achieved by uniform weighting of the synthetic data points,</p><formula xml:id="formula_19">sinceμ baseline := 1 M M m=1 k(z m , ·)</formula><p>with z m = x m is already a consistent estimator of µ X (although based on a much smaller sample size M = ηN N ). The purpose of Algorithm 1 is to find (generally non-uniform) w 1 , . . . , w M that reweight the public data points using the information in the large private dataset, but respecting differential privacy. Proposition 6 confirmed theoretically that this does not hurt the convergence rate, while <ref type="figure" target="#fig_1">Figure 1</ref> shows empirically on two synthetic datasets of dimensions D = 2 and D = 5 that Algorithm 1 can in fact yield more accurate estimates of the KME thanμ baseline , especially when the proportion of public data points is small. This is encouraging, since obtaining permission to publish a larger subset of the private data unchanged will usually come at an increased cost. The ability to instead reweight a smaller public dataset in a differentially private manner using Algorithm 1 is therefore useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Differentially private database release via a random features RKHS</head><p>Input: database D = {x 1 , . . . , x N } ⊆ X , kernel k on X , privacy parameters ε &gt; 0 and δ &gt; 0 Output: (ε, δ)-differentially private, weighted synthetic database (representing an estimate of µ X in the RKHS H of k)</p><formula xml:id="formula_20">1: J ← J(N ) ∈ ω(1) ∩ o(N 2 )</formula><p>, number of random features to use 2: φ ← random feature map X → R J for kernel k with J features  <ref type="figure" target="#fig_1">w 1 )</ref>, . . . , (z M , w M ) ← approximateμ φ X in the RKHS H φ using a Reduced set method:</p><formula xml:id="formula_21">3:μ φ X ← 1 N N n=1 φ(x n ) ∈ R J , empirical KME of X in</formula><formula xml:id="formula_22">(z 1 , w 1 ), . . . , (z M , w M ) ≈ argmin (z 1 ,w 1 ),...,(z M ,w M ) s.t. m |w m |≤1 M m=1 w m φ(z m ) −μ φ X H φ (5) 7: return (z 1 , w 1 ), . . . , (z M , w M )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Perturbation in Random-Features RKHS</head><p>Another approach to ensuring differential privacy is to map the potentially infinite dimensional RKHS H of k into a different, finite-dimensional RKHS H φ using random features <ref type="bibr" target="#b17">(Rahimi &amp; Recht, 2007)</ref>, privacy-protect the finite coordinate vector in this space <ref type="bibr" target="#b3">(Chaudhuri et al., 2011)</ref>, and then employ a reduced set method to find an expansion of the resulting RKHS element in terms of synthetic data points. In contrast to Algorithm 1, both the weights and locations of synthetic data points can be optimised here.</p><p>The algorithm is formalised as Algorithm 2 above. Lines 1-2 pick the number J = J(N ) of random features to use, and construct a random feature map φ with that many features. Lines 3-4 compute the empirical KME of X in the RKHS H φ corresponding to the kernel induced by the random features, and then privacy-protect the resulting finite, realvalued vector. Lines 5-6 run a blindly initialised Reduced set method to find a weighted synthetic dataset whose KME in H φ is close to the privacy-protected KME of the private database. Line 7 releases this weighted dataset to the public.</p><p>The following theorem confirms that Algorithm 2 outputs a consistent estimator of the true KME µ X , provided that the optimisation problem <ref type="formula">(5)</ref> is solved exactly, and the random features converge to the kernel k uniformly on X . On compact sets X this requirement is satisfied by general schemes such as random Fourier features and random binning for shift-invariant kernels <ref type="bibr" target="#b17">(Rahimi &amp; Recht, 2007)</ref>, or by random features for dot product kernels <ref type="bibr" target="#b11">(Kar &amp; Karnick, 2012)</ref>.</p><formula xml:id="formula_23">Theorem 7. If φ(·) T φ(·) → k(·, ·)</formula><p>converges uniformly in X × X as J → ∞, then the output of Algorithm 2 is a consistent estimator of the true KME µ X as N → ∞.</p><p>Moreover, a uniform convergence rate for the random features, such as the one for random Fourier features by <ref type="bibr" target="#b24">Sriperumbudur &amp; Szabo (2015)</ref>, can be used to derive a convergence rate for the output of Algorithm 2:</p><formula xml:id="formula_24">Proposition 8. If φ(·) T φ(·) → k(·, ·) converges uniformly in X × X at a rate O p (J −1/2 ) as J → ∞, then J = J(N )</formula><p>can be chosen so that the output of Algorithm 2 converges to the true KME µ X at a rate O p (N −1/3 ).</p><p>The empirical KME of the private databaseμ X converges at a rate O p (N −1/2 ), so we see that under perfect optimisation, the privacy cost incurred by Algorithm 2 is a factor of N 1/6 . In practice performance will also depend on the Reduced set method used, and the computational budget allocated to it. <ref type="figure">Figure 2</ref> shows how the incurred error (in terms of RKHS distance) varies with the number of synthetic data points M . The additional ability of Algorithm 2 to optimise the locations of the synthetic data points (rather than just the weights, as in Algorithm 1) seems to be more helpful in the higher-dimensional case D = 5, where the randomly sampled synthetic data points are less likely to land close to private data points. Proposition 9. If φ(x) 2 ≤ 1 for all x ∈ X , then Algorithm 2 is (ε, δ)-differentially private.</p><p>This L 2 -boundedness requirement on the random feature vectors φ(x) is reasonable under the weak assumption k(x, x) ≤ 1 for all x ∈ X discussed in Section 4, as in that case φ(x)</p><formula xml:id="formula_25">2 2 = φ(x) T φ(x) ≈ k(x, x) ≤ 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Synthetic database release algorithms with a differential privacy guarantee have been studied in the literature before. <ref type="bibr" target="#b14">Machanavajjhala et al. (2008)</ref> analyzed such a procedure for count data, ensuring privacy by sampling a distribution and then synthetic counts from a Dirichlet-Multinomial posterior. <ref type="bibr" target="#b1">Blum et al. (2008)</ref> studied the exponential mechanism applied to synthetic database generation, which leads to a very general, but unfortunately inefficient algorithm (see also Section 3.4). <ref type="bibr" target="#b27">Wasserman &amp; Zhou (2010)</ref> provided a theoretical comparison of this algorithm to sampling synthetic Figure 2: RKHS distance (lower is better) to the (private) empirical KMEμX computed using the same databases as in <ref type="figure" target="#fig_1">Figure 1</ref>, of dimensions D = 2 (left) and D = 5 (right), but this time without a publishable subset. The synthetic data points for Algorithm 1 were therefore sampled from a wide Gaussian distribution; please see Appendix B for further details. Algorithm 2 is capable of outperforming Algorithm 1 thanks to its ability to optimise the synthetic data point locations, but this depends on the precise optimisation procedure used and the optimisation problem becomes harder in higher dimensions.</p><p>databases from deterministically smoothed, or randomly perturbed histograms. Unlike our approach, these algorithms achieve differential privacy by sampling synthetic data points from a specific distribution, where resorting to approximate sampling can break the privacy guarantee. In our framework we propose to arrive at the synthetic database using a reduced set method, where poor performance could affect statistical usefulness of the synthetic database, but cannot break its differential privacy. <ref type="bibr" target="#b28">Zhou et al. (2009)</ref> and <ref type="bibr" target="#b12">Kenthapadi et al. (2012)</ref> proposed randomised database compression schemes that yield synthetic databases useful for particular types of algorithms, while guaranteeing differential privacy. The former compresses the number of data points using a random linear or affine transformation of the entire database, and the result can be used by procedures that rely on the empirical covariance of the original data. The latter compresses the number of data point dimensions while approximately preserving distances between original, private data points.</p><p>Differentially private learning in a RKHS has also been studied, with <ref type="bibr" target="#b3">Chaudhuri et al. (2011)</ref> and <ref type="bibr" target="#b19">Rubinstein et al. (2012)</ref> having independently presented release mechanisms for the result of an empirical risk minimisation procedure (such as a SVM). Similarly to our Algorithm 2, they map data points into a finite-dimensional space defined by random features and carry out the privacy-protecting perturbation there. However, they do not require the final stage of invoking a Reduced set method to construct a synthetic database, because the output (such as a trained SVM) is only used for evaluation on test points, for which it suffices to additionally release the used random feature map φ.</p><p>As our framework stipulates privacy-protecting an empirical KME, which is a function X → R, the work on differential privacy for functional data is of relevance. <ref type="bibr" target="#b10">Hall et al. (2013)</ref> showed how an RKHS element can be made differentially private via pointwise addition of a Gaussian process sample path, but as discussed in Section 3.2, the resulting function is no longer an element of the RKHS. Recently, <ref type="bibr" target="#b0">Aldà &amp; Rubinstein (2017)</ref> proposed a general Bernstein mechanism for ε-differentially private function release. The released function can be evaluated pointwise arbitrarily many times, but again, the geometry of the RKHS to which the unperturbed function belonged cannot be easily exploited anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>We proposed a framework for constructing differentially private synthetic database release algorithms, based on the idea of using KMEs in RKHS as intermediate database representations. To justify our framework, we presented two concrete algorithms and proved theoretical results guaranteeing their consistency and differential privacy. We also studied their finite-sample convergence rates, and provided empirical illustrations of their performance on synthetic datasets. We believe that exploring other instantiations of this framework, and comparing them theoretically and empirically, can be a fruitful direction for future research.</p><p>The i.i.d. assumption on database rows can be relaxed. For example, if they are identically distributed (as a random variable X), but not necessarily independent, the framework remains valid as long as a consistent estimator of the KME µ X can be constructed from the database rows. A common situation where this arises is, for example, duplication of database rows due to user error.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ε</head><label></label><figDesc>2∆1 s(D, R)), where D is the actual private database and ∆ 1 is the L 1 sensitivity of s w.r.t. D. This ensures ε-differential privacy (McSherry &amp; Talwar, 2007).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RKHS distance (lower is better) to the (private) empirical KMEμX computed using the entire private database of size N = 100, 000. The dimension of the database was D = 2 (left) or D = 5 (right); please see Appendix B for further details of the setup. Horizontally we varied M , the number of publicly releasable data points. Stricter privacy requirements (lower ε) naturally lead to lower accuracy. Increasing M does not always necessarily improve accuracy, since a new public data point always increases the total amount of privatising noise that needs to be added, but this might not be outweighed by its positive contribution towards covering relevant parts of the input space. In all cases, for sufficiently small M Algorithm 1 provided a significantly more accurate estimate than µ baseline .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Remark 4 .</head><label>4</label><figDesc>One usually requires that δ decreases faster than polynomially with the database size N (Dwork &amp; Roth, 2014). The proof of Theorem 2 remains valid whenever M (N ) ∈ o(N 2 / ln(1.25/δ(N ))), so for example we could have δ(N ) = e − √ N and M (N ) ∈ o(N 3/2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>I</head><label></label><figDesc>RKHS H φ of the random features kernel k φ (·, ·) := φ(·)J ), an (ε, δ)-differentially private version of the vectorμ φ X (Gaussian mechanism) 5: M ← M (N ) ≥ N , number of synthetic expansion points to use for representingμ φ X 6: (z 1 ,</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">2 ln(1.25/δ)/ε 2 , and ensures (ε, δ)-differential privacy. Applying these mechanisms thus requires computing (an upper bound on) the relevant sensitivity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The KME can be viewed as a smoothed version of the density, which is easier to estimate than the density itself; rates of nonparametric density estimation or statistical powers of two-sample or independence tests involving pX are known to necessarily degrade with growing dimension (Tolstikhin et al., 2017, Section 4.3).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Bharath Sriperumbudur and the anonymous reviewers for helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Bernstein mechanism: Function release under differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aldà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A learning theory approach to non-interactive database privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simplified support vector decision rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-samples from kernel herding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Conference on Automata, Languages and Programming (ICALP)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Algorithmic Foundations of Differential Privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel measures of conditional dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Measuring statistical dependence with Hilbert-Schmidt norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALT</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differential Privacy for Functions and Functional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random feature maps for dot product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1204.2606</idno>
		<title level="m">Privacy via the Johnson-Lindenstrauss transform</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards a learning theory of cause-effect inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Privacy: Theory meets practice on the map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 24th International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mechanism design via differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">48th Annual IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel mean shrinkage estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random features for large scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning in a large function space: Privacy-preserving mechanisms for SVM learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computing functions of random variables via Reproducing Kernel Hilbert Space representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Consistent Kernel Mean Estimation for Functions of Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ścibior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Hilbert space embedding for distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALT</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimal rates for random Fourier features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Szabo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Universality, characteristic kernels and RKHS embedding of measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Minimax estimation of kernel mean embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A statistical framework for differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Differential privacy with compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Symposium on Information Theory</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
