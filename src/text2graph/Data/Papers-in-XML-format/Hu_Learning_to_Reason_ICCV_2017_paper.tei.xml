<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Reason: End-to-End Module Networks for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<email>rohrbach@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Reason: End-to-End Module Networks for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual Question Answering (VQA) requires joint comprehension of images and text. This comprehension often depends on compositional reasoning, for example locating multiple objects in a scene and inspecting their properties or comparing them to one another ( <ref type="figure">Figure 1</ref>). While conventional deep networks have shown promising VQA performance <ref type="bibr" target="#b8">[9]</ref>, there is limited evidence that they are capable of explicit compositional reasoning <ref type="bibr" target="#b14">[15]</ref>. Much of the success of state-of-the-art approaches to VQA instead comes from their ability to discover statistical biases in the data distribu- There is a shiny object that is right of the gray metallic cylinder; does it have the same size as the large rubber sphere?</p><p>Figure 1: For each instance, our model predicts a computational expression and a sequence of attentive module parameterizations. It uses these to assemble a concrete network architecture, and then executes the assembled neural module network to output an answer for visual question answering.</p><p>(The example shows a real structure predicted by our model, with text attention maps simplified for clarity.)</p><p>tion <ref type="bibr" target="#b9">[10]</ref>. And to the extent that such approaches are capable of more sophisticated reasoning, their monolithic structure makes these behaviors difficult to understand and explain. Additionally, they rely on the same non-modular network structure for all input questions. In this paper, we propose End-to-End Module Networks (N2NMNs): a class of models capable of predicting novel modular network architectures directly from textual input and applying them to images in order to solve question answering tasks. In contrast to previous work, our approach learns to both parse the language into linguistic structures and compose them into appropriate layouts.</p><p>The present work synthesizes and extends two recent modular architectures for visual problem solving. Standard neural module networks (NMNs) <ref type="bibr" target="#b2">[3]</ref> already provide a technique for constructing dynamic network structures from collections of composable modules. However, previous work relies on an external parser to process input text and obtain the module layout. This is a serious limitation, because off-the-shelf language parsers are not designed for language and vision tasks and must therefore be modified using handcrafted rules that often fail to predict valid layouts <ref type="bibr" target="#b14">[15]</ref>. Meanwhile, the compositional modular network <ref type="bibr" target="#b11">[12]</ref> proposed for grounding referring expressions in images does not need a parser, but is restricted to a fixed (subject, relationship, object) structure. None of the existing methods can learn to predict a suitable structure for every input in an end-to-end manner.</p><p>Our contributions are 1) a method for learning a layout policy that dynamically predicts a network structure for each instance, without the aid of external linguistic resources at test time and 2) a new module parameterization that uses a soft attention over question words rather than hard-coded word assignments. Experiments show that our model is capable of directly predicting expert-provided network layouts with near-perfect accuracy, and even improving on expertdesigned networks after a period of exploration. We obtain state-of-the-art results on the recently released CLEVR dataset by a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Neural module networks. The recently proposed neural module network (NMN) architecture <ref type="bibr" target="#b2">[3]</ref>-a general class of recursive neural networks <ref type="bibr" target="#b21">[22]</ref>-provides a framework for constructing deep networks with dynamic computational structure. In an NMN model, every input is associated with a layout that provides a template for assembling an instancespecific network from a collection of shallow network fragments called modules. These modules can be jointly trained across multiple structures to provide reusable, compositional behaviors. Existing work on NMNs has focused on natural language question answering applications, in which a linguistic analysis of the question is used to generate the layout, and the resulting network applied to some world representation (either an image or knowledge base) to produce an answer. The earliest work on NMNs <ref type="bibr" target="#b2">[3]</ref> used fixed rule-based layouts generated from dependency parses <ref type="bibr" target="#b26">[27]</ref>. Later work on "dynamic" module networks (D-NMNs) <ref type="bibr" target="#b1">[2]</ref> incorporated a limited form of layout prediction by learning to rerank a list of three to ten candidates, again generated by rearranging modules predicted by a dependency parse. Like D-NMNs, the present work attempts to learn an optimal layout predictor jointly with module behaviors themselves. Here, however, we tackle a considerably more challenging prediction problem: our approach learns to optimize over the full space of network layouts rather than acting as a reranker, and requires no parser at evaluation time.</p><p>We additionally modify the representation of the assembled module networks themselves: where <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b1">[2]</ref> parameterized individual modules with a fixed embedding supplied by the parser, here we predict these parameters jointly with network structures using a soft attention mechanism. This parameterization resembles the approach used in the "compositional modular network" architecture <ref type="bibr" target="#b11">[12]</ref> for grounding referential expressions. However, the model proposed in <ref type="bibr" target="#b11">[12]</ref> is restricted to a fixed layout structure of (subject, relationship, object) for every referential expression, and includes no structure search. Learning network architectures. More generally than these dynamic / modular approaches, a long line of research focuses on generic methods for automatically discovering neural network architectures from data. Past work includes techniques for optimizing over the space of architectures using evolutionary algorithms <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>, Bayesian methods <ref type="bibr" target="#b5">[6]</ref>, and reinforcement learning <ref type="bibr" target="#b27">[28]</ref>. The last of these is most closely related to our approach in this paper: both learn a controller RNN to output a network structure, train a neural network with the generated structure, and use the accuracy of the generated network to optimize the controller RNN. A key difference between <ref type="bibr" target="#b27">[28]</ref> and the layout policy optimization in our work is that <ref type="bibr" target="#b27">[28]</ref> learns a fixed layout (network architecture) that is applied to every instance, while our model learns a layout policy that dynamically predicts a specific layout tailored to each individual input example. Visual question answering. The visual question answering task <ref type="bibr" target="#b18">[19]</ref> is generally motivated as a test to measure the capacity of deep models to reason about linguistic and visual inputs jointly <ref type="bibr" target="#b18">[19]</ref>. Recent years have seen a proliferation of datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref> and approaches, including models based on differentiable memory <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>, dynamic prediction of question-specific computations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, and core improvements to the implementation of the multi-modal representation and attention mechanism <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. Together, these approaches have produced substantial gains over the initial baseline results published with the first VQA datasets.</p><p>It has been less clear, however, that these improvements correspond to an improvement in the reasoning abilities of models. Recent work has found that it is possible to do quite well on many visual QA problems by simply memorizing statistics about question / answer pairs <ref type="bibr" target="#b9">[10]</ref> (suggesting that limited visual reasoning is involved), and that models with bag-of-words text representations perform competitively against more sophisticated approaches <ref type="bibr" target="#b13">[14]</ref> (suggesting that limited linguistic compositionality is involved). To address this concern, newer visual question answering datasets have focused on exploring specific phenomena in compositionality and generalization; examples include the SHAPES dataset <ref type="bibr" target="#b2">[3]</ref>, the VQAv2 dataset <ref type="bibr" target="#b9">[10]</ref>, and the CLEVR dataset <ref type="bibr" target="#b14">[15]</ref>. The last of these appears to present the greatest challenges to standard VQA approaches and the hardest reasoning problems in general.</p><p>Most previous work on this task other than NMN uses a fixed inference structure to answer every question. However, the optimal reasoning procedure may vary greatly from question to question, so it is desirable to have inference structures that are specific to the input question. Concurrent with our work, <ref type="bibr" target="#b15">[16]</ref> proposes a similar model to ours. Our model is different from <ref type="bibr" target="#b15">[16]</ref> in that we use a set of specialized modules with soft attention mechanism to provide textual parameters for each module, while <ref type="bibr" target="#b15">[16]</ref> uses a generic mod- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image features</head><p>Image encoder (CNN) <ref type="figure">Figure 2</ref>: Model overview. Our approach first computes a deep representation of the question, and uses this as an input to a layout-prediction policy implemented with a recurrent neural network. This policy emits both a sequence of structural actions, specifying a template for a modular neural network in reverse Polish notation, and a sequence of attentive actions, extracting parameters for these neural modules from the input sentence. These two sequences are passed to a network builder, which dynamically instantiates an appropriate neural network and applies it to the input image to obtain an answer.</p><p>ule implementation with textual parameters hard-coded in module instantiation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">End-to-End Module Networks</head><p>We propose End-to-End Module Networks (N2NMNs) to address compositionality in visual reasoning tasks. Our model consists of two main components: a set of co-attentive neural modules that provide parameterized functions for solving sub-tasks, and a layout policy to predict a questionspecific layout from which a neural network is dynamically assembled. An overview of our model is shown in <ref type="figure">Figure 2</ref>.</p><p>Given an input question, such as how many other things are there of the same size as the matte ball?, our layout policy first predicts a coarse functional expression like count(relocate(find()) that describes the structure of the desired computation, Next, some subset of function applications within this expression (here relocate and find) receive parameter vectors predicted from text (here perhaps vector representations of matte ball and size, respectively). Then a network is assembled with the modules according to this layout expression to output an answer.</p><p>We describe the implementation details of each neural module f m in Sec. 3.1, and our layout policy in Sec. 3.2. In Sec. 3.3, we present a reinforcement learning approach to jointly optimize the neural modules and the layout policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attentional neural modules</head><p>Our model involves a set of neural modules that can be dynamically assembled into a neural network. A neural module m is a parameterized function y = f m (a 1 , a 2 , . . . ; x vis , x txt , θ m ) that takes zero, one or multiple tensors a 1 , a 2 , . . . as input, using its internal parameter θ m and features x vis and x txt from the image and question to perform some computation on the input, and outputs a tensor y. In our implementation, each input tensor a i is an image attention map over the convolutional image feature grid, and the output tensor y is either an image attention map, or a probability distribution over possible answers. <ref type="table" target="#tab_1">Table 1</ref> shows the set of modules in our N2NMNs model, along with their implementation details. We assign a name to each module according to its input and output type and potential functionality, such as find or describe. However, we note that each module is in itself merely a function with parameters, and we do not restrict its behavior during training. In addition to the input tensors (that are outputs from other modules), a module m can also use two additional feature vectors x vis and x (m) txt , where x vis is the spatial feature map extracted from the image with a convolutional neural network, and x (m) txt is a textual vector for this module m that contains information extracted from the question q. In addition, and and or take two image attention maps as inputs, and return their intersection or union respectively.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, the find module outputs an attention map over the image and can be potentially used to localize some objects or attributes. The relocate module transforms the input image attention map and outputs a new attention map, which can be useful for spatial or relationship inference. Also the filter module reuses find and and, and can be used to simplify the layout expression. We use two classes of modules to infer an answer from a single attention map: the first class has the instances exist and count (instances share the same structure, but have different parameters). They are used for simple inference by looking only at the attention map. The second class, describe, is for more complex inference where visual appearance is needed. Similarly, for pairwise comparison over two attention maps we also have two classes of available modules with (compare) or without (eq count,more,less) access to visual features.</p><p>The biggest difference in module implementation between this work and <ref type="bibr" target="#b2">[3]</ref> is the textual component. Hard-   <ref type="formula" target="#formula_3">.2)</ref>, and obtain the textual feature x txt for each module:</p><formula xml:id="formula_0">= W T 1 (W2sum(a ⊙ xvis) ⊙ W3xtxt) [eq count, more, less] a1, a2 (none) ans y = W T 1 vec(a1) + W T 2 vec(a2) compare a1, a2 xvis, xtxt ans y = W T 1 (W2sum(a1 ⊙ xvis) ⊙ W3sum(a2 ⊙ xvis) ⊙ W4xtxt)</formula><formula xml:id="formula_1">x (m) txt = T i=1 α (m) i w i (1)</formula><p>where w i is the word embedding vector for word i in the question. At runtime, the modules can be assembled into a network according to a layout l, which is a computation expression consisting of modules, such as f m2 (f m4 (f m1 ), f m3 (f m1 , f m1 )), where each of f m1 , · · · , f m4 is one of the modules in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Layout policy with sequence-to-sequence RNN</head><p>We would like to predict the most suitable reasoning structure tailored to each question. For an input question q such as What object is next to the table?, our layout policy outputs a probability distribution p(l|q), and we can sample from p(l|q) to obtain high probability layout l such as describe(relocate(find())) that are effective for answering the question q. Then, a neural network is assembled according to the predicted layout l to output an answer.</p><p>Unlike in <ref type="bibr" target="#b1">[2]</ref> where the layout search space is restricted to a few parser candidates, in this work, we search over a much larger layout space: in our model, the layout policy p(l|q; θ layout ) predicts a distribution over the space of all possible layouts. Every possible layout l is an expression that consists of neural modules, such as f m2 (f m1 , f m3 (f m1 , f m1 )), and can be represented as a syntax tree. So each layout expression can be mapped one-toone into a linearized sequence l = {m (t) } using Reverse Polish Notation <ref type="bibr" target="#b6">[7]</ref> (the post-order traversal over the syntax tree). <ref type="figure" target="#fig_2">Figure 3</ref> shows an example for an expression and its linearized module token sequence.</p><p>After linearizing each layout l into a sequence of module tokens {m (t) }, the layout prediction problem turns into a sequence-to-sequence learning problem from questions to module tokens. We address this problem using the attentional Recurrent Neural Network <ref type="bibr" target="#b4">[5]</ref>. First, we embed every word i in the question into a vector w i (also embedding all module tokens similarly), and use a multi-layer LSTM network as the encoder of the input question. For a question q with T words, the encoder LSTM outputs a length-T se-</p><formula xml:id="formula_2">quence [h 1 , h 2 , · · · , h T ].</formula><p>The decoder is a LSTM network that has the same structure as the encoder but different parameters. Similar to <ref type="bibr" target="#b4">[5]</ref>, at each time step in the decoder LSTM, a soft attention map over the input sequence is predicted. At decoder time-step t, the attention weights α ti of input word at position i ∈ {1, · · · , T } are predicted as</p><formula xml:id="formula_3">u ti = v T tanh(W 1 h i + W 2 h t )<label>(2)</label></formula><formula xml:id="formula_4">α ti = exp(u ti ) T j=1 exp(u tj )<label>(3)</label></formula><p>where h i and h t are LSTM outputs at encoder time-step i and decoder time-step t, respectively, and v, W 1 and W 2 are model parameters to be learned from data. Then a context vector c t is obtained as</p><formula xml:id="formula_5">T i=1</formula><p>α ti h i , and the probability for the next module token m (t) is predicted from h t and c t as p(m (t) |m <ref type="bibr" target="#b0">(1)</ref> , · · · , m (t−1) , q) = softmax(W 3 h t + W 4 c t ). We sample from p(m (t) |m <ref type="bibr" target="#b0">(1)</ref> , · · · , m (t−1) , q) to discretely get the next token m (t) , and also construct its textual input x (t) txt according to Eqn. 1 using the attention weights α ti in Eqn. 3. The probability of a layout l is p(l|q) =</p><formula xml:id="formula_6">m (t) ∈l p(m (t) |m (1) , · · · , m (t−1) , q)</formula><p>. At test time, we deterministically predict a maximum-probability layout l from p(l|q) using beam search, and assemble a neural network according to l to output an answer for the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">End-to-end training</head><p>During training, we jointly learn the layout policy p(l|q) and the parameters in each neural module, and minimize the expected loss from the layout policy. Let θ be all the parameters in our model. Suppose we obtain a layout l sampled from p(l|q; θ) and receive a final question answering loss L(θ, l; q, I) on question q and image I after predicting an answer using the network assembled with l. Our training loss function L(θ) is as follows.</p><formula xml:id="formula_7">L(θ) = E l∼p(l|q;θ) [L(θ, l; q, I)]<label>(4)</label></formula><p>where we use the softmax loss over the output answer scores asL(θ, l; q, I) in our implementation. The loss function in L(θ) is not fully differentiable since the layout l is discrete, so one cannot train it with full back-propagation. We optimize L(θ) using backpropagation for differentiable parts, and policy gradient method in reinforcement learning for non-differentiable part. The gradient ∇ θ L of the loss L(θ) is ∇ θ L = E l∼p(l|q;θ) L (θ, l)∇ θ log p(l|q; θ) + ∇ θL (θ, l) which can be estimated using Monte-Carlo sampling as</p><formula xml:id="formula_8">∇ θ L ≈ 1 M M m=1 L (θ, l m )∇ θ log p(l m |q; θ) + ∇ θL (θ, l m )<label>(5)</label></formula><p>where both log p(l m |q; θ) andL(θ, l m ) are fully differentiable so the above equation can be computed with backpropagation, allowing end-to-end training for the entire model. We use M = 1 in our implementation.</p><p>To reduce the variance of the estimated gradient, we introduce a simple baseline b, by replacingL(θ, l m ) with L(θ, l m ) − b in Eqn. 5, where b is implemented as an exponential moving average over the recent lossL(θ, l m ). We also use an entropy regularization α = 0.005 over the policy p(l|q) to encourage exploration through the layout space.</p><p>Behavioral cloning from expert polices. Optimizing the loss function in Eqn. 4 from scratch is a challenging reinforcement learning problem: one needs to simultaneously learn the parameters in the sequence-to-sequence RNN to optimize the layout policy and textual attention weights to construct the textual features x (m) txt for each module, and also the parameters in the neural modules. This is more challenging than a typical reinforcement learning scenario where one only needs to learn a policy.</p><p>On the other hand, the learning would be easier if we have some additional knowledge of module layout. While we do not want to restrict the layout search space to only a few candidates from the parser as in <ref type="bibr" target="#b1">[2]</ref>, we can treat these candidate layouts as an existing expert policy that can be used to provide additional supervision. More generally, if there is an expert policy p e (l|q) that predicts a reasonable layout l from the question, we can first pre-train our model by behavioral cloning from p e . This can be done by minimizing the KL-divergence D KL (p e ||p) between the expert policy p e and our layout policy p, and simultaneously minimizing the question answering lossL(θ, l; q, I) with l obtained from p e . This supervised behavioral cloning from the expert policy can provide a good set of initial parameters in our sequenceto-sequence RNN and each neural module. Note that the above behavioral cloning procedure is only done at training time to obtain a supervised initialization our model, and the expert policy is not used at test time.</p><p>The expert policy is not necessarily optimal, so behavioral cloning itself is not sufficient for learning the most suitable layout for each question. After learning a good initialization by cloning the expert policy, our model is further trained endto-end with gradient ∇ θ L computed using Eqn. 5, where now the layout l is sampled from the layout policy p(l|q) in our model, and the expert policy p e can be discarded.</p><p>We train our models using the Adam Optimizer <ref type="bibr" target="#b16">[17]</ref> in all of our experiments. Our model is implemented using TensorFlow <ref type="bibr" target="#b0">[1]</ref> and our code is available at http: //ronghanghu.com/n2nmn/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first analyze our model on a relatively small SHAPES dataset <ref type="bibr" target="#b2">[3]</ref>, and then apply our model to two large-scale datasets: CLEVR <ref type="bibr" target="#b14">[15]</ref> and VQA <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis on the SHAPES dataset</head><p>The SHAPES dataset for visual question answering (collected in <ref type="bibr" target="#b2">[3]</ref>) consists of 15616 image-question pairs with 244 unique questions. Each image consists of shapes of different colors and sizes aligned on a 3 by 3 grid. Despite its relatively small size, effective reasoning is needed to successfully answer questions like "is there a red triangle above a blue shape?". The dataset also provides a ground-truth parsing result for each question, which is used to train the NMN model in <ref type="bibr" target="#b2">[3]</ref>.</p><p>We analyze our method on the SHAPES dataset under two settings. In the first setting, we train our model using behavioral cloning from an expert layout policy as described in Sec. 3.3. An expert layout policy p e is constructed by mapping the the ground-truth parsing for each question to a module layout in the same way as in <ref type="bibr" target="#b2">[3]</ref>. Note that unlike <ref type="bibr" target="#b2">[3]</ref>, in this setting we only need to query the expert policy at training time. At test time, we obtain the layout l from the learned layout policy p(l|q) in our model, while NMN <ref type="bibr" target="#b2">[3]</ref> still needs to access the ground-truth parsing at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy NMN <ref type="bibr" target="#b2">[3]</ref> 90.80% ours -behavioral cloning from expert 100.00% ours -policy search from scratch 96.19% <ref type="table">Table 2</ref>: Performance of our model on the SHAPES dataset. "ours -behavioral cloning from expert" corresponds to the supervised behavioral cloning from the expert policy p e , and "ours -policy search from scratch" is directly optimizing the layout policy without utilizing any expert policy.</p><p>is a circle below a square?</p><p>is a square left of right of a green shape?</p><p>predicted layout and answer behavior cloning from the expert policy exist(and(find(), relocate(find()))) ans_output: "yes" policy search from scratch (without expert policy) exist(relocate(find())) ans_output: "yes"</p><p>behavior cloning from the expert policy exist(and(find(), relocate(relocate(find())))) ans_output: "no" policy search from scratch (without expert policy) exist(find()) ans_output: "no"</p><p>image and question In the second setting, we train our model without using any expert policy, and directly perform policy optimization by minimizing the loss function L(θ) in Eqn. 4 with gradient ∇ θ L in Eqn. 5. For both settings, we use a simple randomly initialized two-layer convolutional neural network to extract visual features from the image, trained together with other parts of our model.</p><p>The results are summarized in <ref type="table">Table 2</ref>. In the first setting, we find that our model ("ours -behavioral cloning from expert") already achieves 100% accuracy. While this shows that the expert policy constructed from ground-truth parsing is quite effective on this dataset, the higher performance of our model compared to the previous NMN <ref type="bibr" target="#b2">[3]</ref> also suggests that our implementation of modules is more effective than <ref type="bibr" target="#b2">[3]</ref>, since the NMN is also trained with the same expert module layout obtained from the ground-truth parsing. In the second setting, our model achieves a good performance on this dataset by performing policy search from scratch without resorting to any expert policy. <ref type="figure" target="#fig_3">Figure 4</ref> shows some examples of predicted layouts and answers on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on the CLEVR dataset</head><p>We evaluate our End-to-End Module Networks on the recently proposed CLEVR dataset <ref type="bibr" target="#b14">[15]</ref> with 100,000 images and 853,554 questions. The images in this dataset are photorealistic rendered images with objects of different shapes, colors, materials and sizes and possible occlusions, and the questions in this dataset are synthesized with functional programs. Compared to other datasets for visual question answering such as <ref type="bibr" target="#b3">[4]</ref>, the CLEVR dataset focuses mostly on the reasoning ability. The questions in the CLEVR dataset have much longer question length, and require handling long and complex inference chains to get an answer, such as "what size is the cylinder that is left of the brown metal thing that is left of the big sphere?" and "there is an object in front of the blue thing; does it have the same shape as the tiny cyan thing that is to the right of the gray metal ball?".</p><p>In our experiment on this dataset, we resize each image to 480 × 320, and extract a 15 × 10 convolutional feature map from each image by forwarding the image through the VGG-16 network <ref type="bibr" target="#b20">[21]</ref> trained on ImageNET classification, and take the 512-channel pool5 output. To help reason about spatial properties, we add two extra x = i 15 and y = j 10 dimensions to each location (i, j) on the feature map similar to <ref type="bibr" target="#b12">[13]</ref>, so the final visual feature x vis on each image is a 15 × 10 × 514 tensor. Each question word is embedded to a 300-dimensional vector initialized from scratch. We use a batch size of 64 during training.</p><p>In the first training stage, behavioral cloning is used with an expert layout policy as described in Sec. 3.3. We construct an expert layout policy p e that deterministically maps a question q into a layout l e by converting the annotated functional programs in this dataset into a module layout with manually defined rules: first, the program chain is simplified to keep all intermediate computation in the image attention domain, and then each function type is mapped to a module in <ref type="table" target="#tab_1">Table 1</ref> that has the same number of inputs and closest potential behavior.</p><p>While the manually specified expert policy p e obtained in this way might not be optimal, it is sufficient to provide supervision to learn good initial model parameters that can be further optimized in the later stage. During behavioral cloning, we train our model with two losses added together: the first loss is the KL-divergence D KL (p e ||p) = − log(p(l = l e |q)), which corresponds to maximizing the probability of the expert layout l e in our policy p(l|q) from the sequence-to-sequence RNN, and the second loss is the question answering lossL(θ, l e ; q, I) for question q and image I, where the layout l e is obtained from the expert. Note that the second lossL(θ, l e ; q, I) also affects the parameters in the sequence-to-sequence RNN through the textual attention in Eqn. 3.</p><p>After the first training stage, we discard the expert policy and continue to train our model for a second stage with endto-end reinforcement learning, using the gradient in Eqn. 5. In this stage, the model is no longer constrained to get close to the expert, but is encouraged to explore the layout space and search for the optimal layout of each question.</p><p>As a baseline, we also train our model without using any expert policy, and directly perform policy search from scratch by minimizing the loss function L(θ) in Eqn. 4.  <ref type="figure">Figure 5</ref>: Question answering examples on the CLEVR dataset. On the left, it can be seen that the model successfully locates the matte green ball, attends to all the other objects of the same size, and then correctly identifies that there are 4 such objects (excluding the initial ball). On the right, it can be seen the various modules similarly assume intuitive semantics. Of particular interest is the second find module, which picks up the word right in addition to metallic red thing: this suggests that model can use the fact that downstream computation will look to the right of the detected object to focus its initial search in the left half of the image, a behavior supported by our attentive approach but not a conventional linguistic analysis of the question.  We evaluate our model on the test set of CLEVR. <ref type="table" target="#tab_4">Table 3</ref> shows the detailed performance of our model and previous methods on each question type, where "ours -policy search from scratch" is the baseline using pure reinforcement learning without resorting to the expert, "ours -cloning expert" is the supervised behavioral cloning from the constructed expert policy in the first stage, and "ours -policy search after cloning" is our model further trained for the second training stage. It can be seen that without using any expert demonstrations, our method with policy optimization from scratch already achieves higher performance than most previous work, and our model trained in the first behavioral cloning stage outperforms the previous approaches by a large margin in overall accuracy. This indicates that our neural modules are capable of reasoning for complex questions in the dataset like "does the block that is to the right of the big cyan sphere have the same material as the large blue thing?" Our model also outperforms the NMN baseline <ref type="bibr" target="#b2">[3]</ref> trained on the same expert layout as used in our model <ref type="bibr" target="#b0">1</ref> . This shows that our soft attention module parameterization is better than the hard-coded textual parameters in NMN. <ref type="figure">Figure 5</ref> shows some question answering examples with our model.</p><p>By comparing "ours -policy search after cloning" with "ours -cloning expert" in <ref type="table" target="#tab_4">Table 3</ref>, it can be seen that the performance consistently improves after end-to-end training with policy search using reinforcement learning in the second training stage, with especially large improvement on the compare color type of questions, indicating that the original expert policy is not optimal, and we can improve upon it with policy search over the entire layout space. <ref type="figure">Figure 6</ref> shows an example before and after end-to-end optimization.  <ref type="figure">Figure 6</ref>: An example illustrating the layout change before (top row) and after (middle row) the second stage of end-to-end optimization with reinforcement learning. After end-to-end learning, a new filter module is inserted by the layout policy to remove the attention over the non-object area before feeding it into the final compare module, correcting the previous error. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation on the VQA dataset</head><p>We also evaluate our method on the VQA dataset <ref type="bibr" target="#b3">[4]</ref> with real images. On the VQA dataset, although there are no underlying functional programs annotation for the questions, we can still construct an expert layout policy using a syntactic parse of questions as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. We extract visual features from the ResNet-152 network <ref type="bibr" target="#b10">[11]</ref>, and train our model in the same way as in Sec. 4.2. On this dataset, however, we find that the second training stage of policy search after cloning does not lead to noticeable improvement in the accuracy, so we only evaluate our model with behavioral cloning from the expert layout obtained by syntactic parsing. Unlike previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, the syntactic parser is only used during the training stage and is not needed at test time.</p><p>The results are summarized in <ref type="table">Table 4</ref>, where our method significantly outperforms NMN <ref type="bibr" target="#b2">[3]</ref> and D-NMN <ref type="bibr" target="#b1">[2]</ref> that also use modular structures. Compared with MCB <ref type="bibr" target="#b8">[9]</ref> (the VQA 2016 challenge winner method) trained on the same ResNet-152 image features, our model achieves comparable performance while being more interpretable as one can Method Accuracy MCB <ref type="bibr" target="#b8">[9]</ref> 64.7 NMN <ref type="bibr" target="#b2">[3]</ref> 57.3 D-NMN <ref type="bibr" target="#b1">[2]</ref> 57.9 ours 64.2 <ref type="table">Table 4</ref>: Evaluation of our method on the VQA dataset. Our model outperforms previous work NMN and D-NMN and achieves comparable performance as MCB.</p><p>explicitly see the underlying reasoning procedure. <ref type="figure" target="#fig_4">Figure 7</ref> shows a prediction example on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present the End-to-End Module Networks for visual question answering. Our model uses a set of neural modules to break down complex reasoning problems posed in textual questions into a few sub-tasks connected together, and learns to predict a suitable layout expression for each question using a layout policy implemented with a sequence-to-sequence RNN. During training, the model can be first trained with behavioral cloning from an expert layout policy, and further optimized end-to-end using reinforcement learning. Experimental results demonstrate that our model is capable of handling complicated reasoning problems, and the end-to-end optimization of the neural modules and layout policy can lead to significant further improvement over behavioral cloning from expert layouts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>How many other things are of the same size as the green matte ball? How many other things are of the same size as the green matte ball?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example showing how an arbitrary layout expression can be linearized as a sequence of module tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of layouts predicted by our model on the SHAPES dataset, under two training settings (Sec. 4.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example from our model on the VQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>The full list of neural modules in our model. Each module takes 0, 1 or 2 attention maps (and also visual and textual features) as input, and outputs either an attention map a out or a score vector y for all possible answers. The operator ⊙ is element-wise multiplication, and sum is summing the result over spatial dimensions. The vec operation is flattening an attention map into a vector, and adding two extra dimensions: the max and min over attention map. coded textual components are used in [3], for example, describe['shape'] and describe['where'] are two dif- ferent instantiations that have different parameters. In con- trast, our model obtains the textual input using soft attention over question words similar to [12]. For each module m, we predict an attention map α</figDesc><table>(m) 
i 

over the T question words 
(in Sec. 3</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of our method and previous work on CLEVR test set. With policy search after cloning, the accuracies are consistently improved on all questions types, with large improvement on some question types like compare color.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>question: do the small cylinder that is in front of the small green thing and the object right of the green cylinder have the same material? ground-truth answer: no</figDesc><table>image 
layout 
find[0] 
relocate[1] filter[2] 
find[3] 
relocate[4] compare[5] 

compare[5]( 
filter[2]( 
relocate[1]( 
find[0]())), 
relocate[4]( 
find[3]())) 

"yes" 

image 
layout 
find[0] 
relocate[1] filter[2] 
find[3] 
relocate[4] filter[5] compare[6] 

compare[6]( 
filter[2]( 
relocate[1]( 
find[0]())), 
filter[5]( 
relocate[4]( 
find[3]()))) 

"no" 

after 2 nd 
training 
stage 

before 2 nd 
training 
stage 

textual 
attention 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The question parsing in the original NMN implementation does not work on the CLEVR dataset, as confirmed in [15]. For fair comparison with NMN, we train NMN using the same expert layout as our model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An analysis of a logical machine using parenthesis-free notation. Mathematical tables and other aids to computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Burks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="53" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neuroevolution: from architectures to learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dürr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mattiussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1612.00837</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="727" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical CoAttention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling systems with internal state using evolino</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th annual conference on Genetic and evolutionary computation</title>
		<meeting>the 7th annual conference on Genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1795" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and accurate shift-reduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
