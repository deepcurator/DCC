<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritambhara</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshdeep</forename><surname>Sekhon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
							<email>yanjun@virginia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The past decade has seen a revolution in genomic technologies that enabled a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what the relevant factors are and how they work together. Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach, AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long Short-Term Memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in humans. Not only is the proposed architecture more accurate, but its attention scores provide a better interpretation than state-of-the-art feature visualization methods such as saliency maps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DNA. These spatial re-arrangements result in certain DNA regions becoming accessible or restricted and therefore affecting expressions of genes in the neighborhood region. Researchers have established the "Histone Code Hypothesis" that explores the role of histone modifications in controlling gene regulation. Unlike genetic mutations, chromatin changes such as histone modifications are potentially reversible ( <ref type="bibr" target="#b4">[5]</ref>). This crucial difference makes the understanding of how chromatin factors determine gene regulation even more impactful because this knowledge can help developing drugs targeting genetic diseases.</p><p>At the whole genome level, researchers are trying to chart the locations and intensities of all the chemical modifications, referred to as marks, over the chromatin <ref type="bibr" target="#b3">4</ref> . Recent advances in nextgeneration sequencing have allowed biologists to profile a significant amount of gene expression and chromatin patterns as signals (or read counts) across many cell types covering the full human genome. These datasets have been made available through large-scale repositories, the latest being the Roadmap Epigenome Project (REMC, publicly available) ( <ref type="bibr" target="#b17">[18]</ref>). REMC recently released 2,804 genome-wide datasets, among which 166 datasets are gene expression reads (RNA-Seq datasets) and the rest are signal reads of various chromatin marks across 100 different "normal" human cells/tissues <ref type="bibr" target="#b17">[18]</ref>.</p><p>The fundamental aim of processing and understanding this repository of "big" data is to understand gene regulation. For each cell type, we want to know which chromatin marks are the most important and how they work together in controlling gene expression. However, previous machine learning studies on this task either failed to model spatial dependencies among marks or required additional feature analysis to explain the predictions (Section 4). Computational tools should consider two important properties when modeling such data.</p><p>• First, signal reads for each mark are spatially structured and high-dimensional. For instance, to quantify the influence of a histone modification mark, learning methods typically need to use as input features all of the signals covering a DNA region of length 10, 000 base pair (bp) <ref type="bibr" target="#b4">5</ref> centered at the transcription start site (TSS) of each gene. These signals are sequentially ordered along the genome direction. To develop "epigenetic" drugs, it is important to recognize how a chromatin mark's effect on regulation varies over different genomic locations.</p><p>• Second, various types of marks exist in human chromatin that can influence gene regulation. For example, each of the five standard histone proteins can be simultaneously modified at multiple different sites with various kinds of chemical modifications, resulting in a large number of different histone modification marks. For each mark, we build a feature vector representing its signals surrounding a gene's TSS position. When modeling genome-wide signal reads from multiple marks, learning algorithms should take into account the modular nature of such feature inputs, where each mark functions as a module. We want to understand how the interactions among these modules influence the prediction (gene expression).</p><p>In this paper we propose an attention-based deep learning model, AttentiveChrome, that learns to predict the expression of a gene from an input of histone modification signals covering the gene's neighboring DNA region. By using a hierarchy of multiple LSTM modules, AttentiveChrome can discover interactions among signals of each chromatin mark, and simultaneously learn complex dependencies among different marks. Two levels of "soft" attention mechanisms are trained, <ref type="bibr" target="#b0">(1)</ref> to attend to the most relevant regions of a chromatin mark, and (2) to recognize and attend to the important marks. Through predicting and attending in one unified architecture, AttentiveChrome allows users to understand how chromatin marks control gene regulation in a cell. In summary, this work makes the following contributions:</p><p>• AttentiveChrome provides more accurate predictions than state-of-the-art baselines. Using datasets from REMC, we evaluate AttentiveChrome on 56 different cell types (tasks).</p><p>• We validate and compare interpretation scores using correlation to a new mark signal from REMC (not used in modeling). AttentiveChrome's attention scores provide a better interpretation than state-of-the-art methods for visualizing deep learning models. • AttentiveChrome can model highly modular inputs where each module is highly structured.</p><p>AttentiveChrome can explain its decisions by providing "what" and "where" the model has focused <ref type="bibr" target="#b3">4</ref> In biology this field is called epigenetics. "Epi" in Greek means over. The epigenome in a cell is the set of chemical modifications over the chromatin that alter gene expression. on. This flexibility and interpretability make this model an ideal approach for many real-world applications.</p><p>• To the authors' best knowledge, AttentiveChrome is the first attention-based deep learning method for modeling data from molecular biology.</p><p>In the following sections, we denote vectors with bold font and matrices using capital letters. To simplify notation, we use "HM" as a short form for the term "histone modification".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Long Short-Term Memory (LSTM) Networks</head><p>Recurrent neural networks (RNNs) have been widely used in applications such as natural language processing due to their abilities to model sequential dependencies. Given an input matrix X of size n in × T , an RNN produces a matrix H of size d × T , where n in is the input feature size, T is the length of input feature , and d is the RNN embedding size. At each timestep t ∈ {1, · · · , T }, an RNN takes an input column vector x t ∈ R nin and the previous hidden state vector h t−1 ∈ R d and produces the next hidden state h t by applying the following recursive operation:</p><formula xml:id="formula_0">h t = σ(Wx t + Uh t−1 + b) = −−−−→ LST M (x t ),<label>(1)</label></formula><p>where W, U, b are the trainable parameters of the model, and σ is an element-wise nonlinearity function. Due to the recursive nature, RNNs can capture the complete set of dependencies among all timesteps being modeled, like all spatial positions in a sequential sample. To handle the "vanishing gradient" issue of training basic RNNs, Hochreiter et al. <ref type="bibr" target="#b12">[13]</ref> proposed an RNN variant called the Long Short-term Memory (LSTM) network.</p><p>An LSTM layer has an input-to-state component and a recurrent state-to-state component like that in Eq. (1). Additionally, it has gating functions that control when information is written to, read from, and forgotten. Though the LSTM formulation results in a complex form of Eq. (1) (see Supplementary), when given input vector x t and the state h t−1 from previous time step t − 1, an LSTM module also produces a new state h t . The embedding vector h t encodes the learned representation summarizing feature dependencies from the time step 0 to the time step t. For our task, we call each bin position on the genome coordinate a "time step". Input and output formulation for the task: We use the same feature inputs and outputs as done previously in DeepChrome ( <ref type="bibr" target="#b28">[29]</ref>). Following Cheng et al. <ref type="bibr" target="#b6">[7]</ref>, the gene expression prediction is formulated as a binary classification task whose output represents if the gene expression of a gene is high(+1) or low(-1). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the input feature of a sample (a particular gene) is denoted as a matrix X of size M × T . Here M denotes the number of HM marks we consider in the input. T is the total number of bin positions we take into account from the neighboring region of a gene's TSS site on the genome. We refer to this region as the 'gene region' in the rest of the paper. x j denotes the j-th row vector of X whose elements are sequentially structured (signals from the j-th HM mark) j ∈ {1, ..., M }. x j t in matrix X represents the signal from the t-th bin of the j-th HM mark. t ∈ {1, ..., T }. We assume our training set D contains N tr labeled pairs. We denote the n-th pair as (X (n) , y (n) ), X (n) is a matrix of size M × T and y (n) ∈ {−1, +1}, where n ∈ {1, ..., N tr }.</p><p>An end-to-end deep architecture for predicting and attending jointly: AttentiveChrome learns to predict the expression of a gene from an input of HM signals covering its gene region. First, the signals of each HM mark are fed into a separate LSTM network to encode the spatial dependencies among its bin signals, and then another LSTM is used to model how multiple factors work together for predicting gene expression. Two levels of "soft" attention mechanisms are trained and dynamically predicted for each gene: (1) to attend to the most relevant positions of an HM mark, and <ref type="formula">(2)</ref>  </p><formula xml:id="formula_1">← − h j t , each of size d. − → h j t = −−−−→ LST M j (x j t ) and ← − h j t = ←−−−− LST M j (x j t ).</formula><p>The final embedding vector at the t-th position is the concatenation h</p><formula xml:id="formula_2">j t = [ − → h j t , ← − h j t ].</formula><p>By coupling these LSTM-based HM encoders with the final classification, they can learn to embed each HM mark by extracting the dependencies among bins that are essential for the prediction task.</p><p>Bin-Level Attention, α-attention: Although the LSTM can encode dependencies among the bins, it is difficult to determine which bins are most important for prediction from the LSTM. To automatically and adaptively highlight the most relevant bins for each sample, we use "soft" attention to learn the importance weights of bins. This means when representing j-th HM mark, AttentiveChrome follows a basic concept that not all bins contribute equally to the encoding of the entire j-th HM mark. The attention mechanism can help locate and recognize those bins that are important for the current gene sample of interest from j-th HM mark and can aggregate those important bins to form an embedding vector. This extraction is implemented through learning a weight vector α j of size T for the j-th HM mark. For t ∈ {1, ..., T }, α j t represents the importance of the t-th bin in the j-th HM. It is computed as:</p><formula xml:id="formula_3">α j t = exp(W b h j t ) T i=1 exp(W b h j i )</formula><p>. α j t is a scalar and is computed by considering all bins' embedding vectors {h</p><formula xml:id="formula_4">j 1 , · · · , h j T }.</formula><p>The context parameter W b is randomly initialized and jointly learned with the other model parameters during training. Our intuition is that through W b the model will automatically learn the context of the task (e.g., type of a cell) as well as the positional relevance to the context simultaneously. Once we have the importance weight of each bin position, we can represent the entire j-th HM mark as a weighted sum of all its bin embeddings:</p><formula xml:id="formula_5">m j = T t=1 α j t × h j t .</formula><p>Essentially the attention weights α j t tell us the relative importance of the t-th bin in the representation m j for the current input X (both h j t and α j t depend on X). HM-Level Encoder Using Another LSTM: We aim to capture the dependencies among HMs as some HMs are known to work together to repress or activate gene expression <ref type="bibr" target="#b5">[6]</ref>. Therefore, next we model the joint dependencies among multiple HM marks (essentially, learn to represent a set). Even though there exists no clear order among HMs, we assume an imagined sequence as {HM 1 , HM 2 , HM 3 , ..., HM M } 6 . We implement another bi-directional LSTM encoder, this time on the imagined sequence of HMs using the representations m j of the j-th HMs as LSTM inputs (Supplementary <ref type="figure">Figure S:</ref>2 (e)). Setting the embedding size as d , this set-based encoder, we denote as LST M s , encodes the j-th HM as:</p><formula xml:id="formula_6">s j = [ − −−−− → LST M s (m j ), ← −−−− − LST M s (m j )].</formula><p>Differently from m j , s j encodes the dependencies between the j-th HM and other HM marks. <ref type="bibr" target="#b5">6</ref> We tried several different architectures to model the dependencies among HMs, and found no clear ordering. </p><formula xml:id="formula_7">× × × × × Support Vector Machine ([7]) × Bin-specific × × Random Forest ([10]) × Best-bin × × × Rule Learning ([12]) × × × × DeepChrome-CNN [29] Automatic × AttentiveChrome Automatic</formula><p>HM-Level Attention, β-attention: Now we want to focus on the important HM markers for classifying a gene's expression as high or low. We do this by learning a second level of attention among HMs. Similar to learning α j t , we learn another set of weights β j for j ∈ {1, · · · , M } representing the importance of HM j . β i is calculated as:</p><formula xml:id="formula_8">β j = exp(Wss j ) M i=1 exp(Wss i )</formula><p>. The HM-level context parameter W s learns the context of the task and learns how HMs are relevant to that context. W s is randomly initialized and jointly trained. We encode the entire "gene region" into a hidden representation v as a weighted sum of embeddings from all HM marks: v = M j=1 β j s j . We can interpret the learned attention weight β i as the relative importance of HM i when blending all HM marks to represent the entire gene region for the current gene sample X.</p><p>Training AttentiveChrome End-to-End: The vector v summarizes the information of all HMs for a gene sample. We feed it to a simple classification module f (Supplementary <ref type="figure">Figure S:</ref>2(f)) that computes the probability of the current gene being expressed high or low:</p><formula xml:id="formula_9">f (v) = softmax(W c v + b c ).</formula><p>W c and b c are learnable parameters. Since the entire model, including the attention mechanisms, is differentiable, learning end-to-end is trivial by using backpropagation <ref type="bibr" target="#b20">[21]</ref>. All parameters are learned together to minimize a negative log-likelihood loss function that captures the difference between true labels y and predicted scores from f (.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Connecting to Previous Studies</head><p>In recent years, there has been an explosion of deep learning models that have led to groundbreaking performance in many fields such as computer vision <ref type="bibr" target="#b16">[17]</ref>, natural language processing <ref type="bibr" target="#b29">[30]</ref>, and computational biology <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Attention-based deep models: The idea of attention in deep learning arises from the properties of the human visual system. When perceiving a scene, the human vision gives more importance to some areas over others <ref type="bibr" target="#b8">[9]</ref>. This adaptation of "attention" allows deep learning models to focus selectively on only the important features. Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation <ref type="bibr" target="#b3">[4]</ref>, object recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>, image caption generation <ref type="bibr" target="#b32">[33]</ref>, question answering <ref type="bibr" target="#b29">[30]</ref>, text document classification <ref type="bibr" target="#b33">[34]</ref>, video description generation <ref type="bibr" target="#b34">[35]</ref>, visual question answering - <ref type="bibr" target="#b31">[32]</ref>, or solving discrete optimization <ref type="bibr" target="#b30">[31]</ref>. Attention brings in two benefits: (1) By selectively focusing on parts of the input during prediction the attention mechanisms can reduce the amount of computation and the number of parameters associated with deep learning model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>. (2) Attention-based modeling allows for learning salient features dynamically as needed <ref type="bibr" target="#b33">[34]</ref>, which can help improve accuracy. Different attention mechanisms have been proposed in the literature, including 'soft' attention <ref type="bibr" target="#b3">[4]</ref>, 'hard' attention <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24]</ref>, or 'location-aware' <ref type="bibr" target="#b7">[8]</ref>. Soft attention <ref type="bibr" target="#b3">[4]</ref> calculates a 'soft' weighting scheme over all the component feature vectors of input. These weights are then used to compute a weighted combination of the candidate feature vectors. The magnitude of an attention weight correlates highly with the degree of significance of the corresponding component feature vector to the prediction. Inspired by <ref type="bibr" target="#b33">[34]</ref>, AttentiveChrome uses two levels of soft attention for predicting gene expression from HM marks.</p><p>Visualizing and understanding deep models: Although deep learning models have proven to be very accurate, they have widely been viewed as "black boxes". Researchers have attempted to develop separate visualization techniques that explain a deep classifier's decisions. Most prior studies have focused on understanding convolutional neural networks (CNN) for image classifications, including techniques such as "deconvolution" <ref type="bibr" target="#b35">[36]</ref>, "saliency maps" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> and "class optimization" based visualisation <ref type="bibr" target="#b27">[28]</ref>. The "deconvolution' approach <ref type="bibr" target="#b35">[36]</ref> maps hidden layer representations back to the input space for a specific example, showing those features of an image that are important for classification. "Saliency maps" <ref type="bibr" target="#b27">[28]</ref> use a first-order Taylor expansion to linearly approximate the deep network and seek most relevant input features. The "class optimization" based visualization <ref type="bibr" target="#b27">[28]</ref> tries to find the best example (through optimization) that maximizes the probability of the class of interest. Recent studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> explored the interpretability of recurrent neural networks (RNN) for text-based tasks. Moreover, since attention in models allows for automatically extracting salient features, attention-coupled neural networks impart a degree of interpretability. By visualizing what the model attends to in <ref type="bibr" target="#b33">[34]</ref>, attention can help gauge the predictive importance of a feature and hence interpret the output of a deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning in bioinformatics:</head><p>Deep learning is steadily gaining popularity in the bioinformatics community. This trend is credited to its ability to extract meaningful representations from large datasets. For instance, multiple recent studies have successfully used deep learning for modeling protein sequences <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref> and DNA sequences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, predicting gene expressions <ref type="bibr" target="#b28">[29]</ref>, as well as understanding the effects of non-coding variants <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Previous machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by Dong et al. <ref type="bibr" target="#b10">[11]</ref>) including linear regression <ref type="bibr" target="#b13">[14]</ref>, support vector machines <ref type="bibr" target="#b6">[7]</ref>, random forests <ref type="bibr" target="#b9">[10]</ref>, rule-based learning <ref type="bibr" target="#b11">[12]</ref> and CNNs <ref type="bibr" target="#b28">[29]</ref>. These studies designed different feature selection strategies to accommodate a large amount of histone modification signals as input. The strategies vary from using signal averaging across all relevant positions, to a 'best position' strategy that selected the input signals at the position with the highest correlation to the target gene expression and automatically learning combinatorial interactions among histone modification marks using CNN (called DeepChrome <ref type="bibr" target="#b28">[29]</ref>). DeepChrome outperformed all previous methods (see Supplementary) on this task and used a class optimization-based technique for visualizing the learned model. However, this class-level visualization lacks the necessary granularity to understand the signals from multiple chromatin marks at the individual gene level. <ref type="table" target="#tab_0">Table 1</ref> compares previous learning studies on the same task with AttentiveChrome across seven desirable model properties. The columns indicate properties (1) whether the study has a unified end-to-end architecture or not, (2) if it captures non-linearity among features, (3) how has the bin information been incorporated, (4) if representation of features is modeled on local and (5) global scales, (6) whether gene expression prediction is provided, (7) if combinatorial interactions among histone modifications are modeled, and finally (8) if the model is interpretable. AttentiveChrome is the only model that exhibits all seven properties. Additionally, Section 5 compares the attention weights from AttentiveChrome with the visualization from "saliency map" and "class optimization." Using the correlation to one additional HM mark from REMC, we show that AttentiveChrome provides better interpretation and validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>Dataset: Following DeepChrome <ref type="bibr" target="#b28">[29]</ref>, we downloaded gene expression levels and signal data of five core HM marks for 56 different cell types archived by the REMC database <ref type="bibr" target="#b17">[18]</ref>. Each dataset contains information about both the location and the signal intensity for a mark measured across the whole genome. The selected five core HM marks have been uniformly profiled across all 56 cell types in the REMC study <ref type="bibr" target="#b17">[18]</ref>. These five HM marks include (we rename these HMs in our analysis for readability): H3K27me3 as H reprA , H3K36me3 as H struct , H3K4me1 as H enhc , H3K4me3 as H prom , and H3K9me3 as H reprB . HMs H reprA and H reprB are known to repress the gene expression, H prom activates gene expression, H struct is found over the gene body, and H enhc sometimes helps in activating gene expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of the Dataset:</head><p>We divided the 10, 000 base pair DNA region (+/ − 5000 bp) around the transcription start site (TSS) of each gene into bins, with each bin containing 100 continuous bp). For each gene in a specific celltype, the feature generation process generated a 5 × 100 matrix, X, where columns represent T (= 100) different bins and rows represent M (= 5) HMs. For each cell type, the gene expression has been quantified for all annotated genes in the human genome and has been normalized. As previously mentioned, we formulated the task of gene expression prediction as a binary classification task. Following <ref type="bibr" target="#b6">[7]</ref>, we used the median gene expression across all genes for a particular cell type as the threshold to discretize expression values. For each cell type, we divided </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Variations and Two Baselines:</head><p>In Section 3, we introduced three main components of AttentiveChrome to handle the task of predicting gene expression from HM marks: LSTMs, attention mechanisms, and hierarchical attention. To investigate the performance of these components, our experiments compare multiple AttentiveChrome model variations plus two standard baselines.</p><p>• DeepChrome <ref type="bibr" target="#b28">[29]</ref>: The temporal (1-D) CNN model used by Singh et al. <ref type="bibr" target="#b28">[29]</ref> for the same classification task. This study did not consider the modular property of HM marks.</p><p>• LSTM: We directly apply an LSTM on the input matrix X without adding any attention. This setup does not consider the modular property of each HM mark, that is, we treat the signals of all HMs at t-th bin position as the t-th input to LSTM.</p><p>• LSTM-Attn: We add one attention layer on the baseline LSTM model over input X. This setup does not consider the modular property of HM marks.</p><p>• CNN-Attn: We apply one attention layer over the CNN model from DeepChrome <ref type="bibr" target="#b28">[29]</ref>, after removing the max-pooling layer to allow bin-level attention for each bin. This setup does not consider the modular property of HM marks.</p><p>• LSTM-α, β: As introduced in Section 3, this model uses one LSTM per HM mark and add one α-attention per mark. Then it uses another level of LSTM and β-attention to combine HMs.</p><p>• CNN-α, β: This considers the modular property among HM marks. We apply one CNN per HM mark and add one α-attention per mark. Then it uses another level of CNN and β-attention to combine HMs.</p><p>• LSTM-α: This considers the modular property of HM marks. We apply one LSTM per HM mark and add one α-attention per mark. Then, the embedding of HM marks is concatenated as one long vector and then fed to a 2-layer fully connected MLP.</p><p>We use datasets across 56 cell types, comparing the above methods over each of the 56 different tasks.</p><p>Model Hyperparameters: For AttentiveChrome variations, we set the bin-level LSTM embedding size d to 32 and the HM-level LSTM embedding size as 16. Since we implement a bi-directional LSTM, this results in each embedding vector h t as size 64 and embedding vector m j as size 32. Therefore, we set the context vectors, W b and W s , to size 64 and 32 respectively. Performance Evaluation: <ref type="table" target="#tab_1">Table 2</ref> compares different variations of AttentiveChrome using summarized AUC scores across all 56 cell types on the test set. We find that overall the LSTM-attention based models perform better than CNN-based and LSTM baselines. CNN-attention model gives worst performance. To add the bin-level attention layer to the CNN model, we removed the max-pooling layer. We hypothesize that the absence of max-pooling is the cause behind its low performance. LSTM-α has better empirical performance than the LSTM-α, β model. We recommend the use of the proposed AttentiveChrome LSTM-α, β (from here on referred to as AttentiveChrome) for hypothesis generation because it provides a good trade-off between AUC and interpretability. Also, while the performance improvement over DeepChrome <ref type="bibr" target="#b28">[29]</ref> is not large, AttentiveChrome is better as it allows interpretability to the "black box" neural networks. We use the average read counts of H active across all 100 bins and for all the active genes (gene=ON) in the three selected cell types to compare different visualization methods. We compare the attention α-maps of the best performing LSTM-α and AttentiveChrome models with the other two popular visualization techniques: (1) the Class-based optimization method and (2) the Saliency map applied on the baseline DeepChrome-CNN model. We take the importance weights calculated by all visualization methods for our active input mark, H prom , across 100 bins and then calculate their Pearson correlation to H active counts across the same 100 bins. H active counts indicate the actual active regions. <ref type="table" target="#tab_2">Table  3</ref> reports the correlation coefficients between H prom weights and read coverage of H active . We observe that attention weights from our models consistently achieve the highest correlation with the actual active regions near the gene, indicating that this method can capture the important signals for predicting gene activity. Interestingly, we observe that the saliency map on the DeepChrome achieves a higher correlation with H active than the Class-based optimization method for two cell types: H1-hESC (stem cell) and K562 (leukemia cell).</p><p>Next, we obtain the attention weights learned by AttentionChrome, representing the important bins and HMs for each prediction of a particular gene as ON or OFF. For a specific gene sample, we can visualize and inspect the bin-level and HM-level attention vectors α j t and β j generated by AttentionChrome. In <ref type="figure" target="#fig_5">Figure 2</ref>(a), we plot the average bin-level attention weights for each HM for cell type GM12878 (blood cell) by averaging α-maps of all predicted "ON" genes (top) and "OFF" genes (bottom). We see that on average for "ON" genes, the attention profiles near the TSS region are well defined for H prom , H enhc , and H struct . On the contrary, the weights are low and close to uniform for H reprA and H reprB . This average trend reverses for "OFF" genes in which H reprA and H reprB seem to gain more importance over H prom , H enhc , and H struct . These observations make sense biologically as H prom , H enhc , and H struct are known to encourage gene activation while H reprA and H reprB are known to repress the genes 8 . On average, while H prom is concentrated near the TSS region, other HMs like H struct show a broader distribution away from the TSS. In summary, the importance of each HM and its position varies across different genes. E.g., H enhc can affect a gene from a distant position.</p><p>In <ref type="figure" target="#fig_5">Figure 2(b)</ref>, we plot the average read coverage of H active (top) for the same 100 bins, that we used for input signals, across all the active genes (gene=ON) for GM12878 cell type. We also plot the bin-level attention weights α j t for AttentiveChrome (bottom) averaged over all genes predicted as ON for GM12878. Visually, we can tell that the average H prom profile is similar to H active . This   , with j ∈ {1, ..., 5} for an important differentially regulated gene (PAX5) across three blood lineage cell types: H1-hESC (stem cell), GM12878 (blood cell), and K562 (leukemia cell). The trend of HM-level β j weights for PAX5 have been verified through biological literature. observation makes sense because H prom is related to active regions for "ON" genes. Thus, validating our results from <ref type="table" target="#tab_2">Table 3</ref>. <ref type="figure" target="#fig_5">Figure 2</ref>(c) we demonstrate the advantage of AttentiveChrome over LSTM-α model by printing out the β j weights for genes with differential expressions across the three cell types. That is, we select genes with varying ON(+1)/OFF(−1) states across the three chosen cell types using a heatmap. <ref type="figure" target="#fig_5">Figure 2</ref>(c) visualizes the β j weights for a certain differentially regulated gene, PAX5. PAX5 is critical for the gene regulation when stem cells convert to blood cells ( <ref type="bibr" target="#b24">[25]</ref>). This gene is OFF in the H1-hESC cell stage (left column) but turns ON when the cell develops into GM12878 cell (middle column). The β j weight of repressor mark H reprA is high when gene=OFF in H1-hESC (left column). This same weight decreases when gene=ON in GM12878 (middle column). In contrast, the β j weight of the promoter mark H prom increases from H1-hESC (left column) to GM12878 (middle column). These trends have been observed in <ref type="bibr" target="#b24">[25]</ref> showing that PAX5 relates to the conversion of chromatin states: from a repressive state (H prom (H3K4me3):−, H reprA (H3K27me3):+) to an active state (H prom (H3K4me3):+, H reprA (H3K27me3):−). This example shows that our β j weights visualize how different HMs work together to influence a gene's state (ON/OFF). We would like to emphasize that the attention weights on both bin-level (α-map) and HM-level (β-map) are gene (i.e. sample) specific. The proposed AttentiveChrome model provides an opportunity for a plethora of downstream analyses that can help us understand the epigenomic mechanisms better. Besides, relevant datasets are big and noisy. A predictive model that automatically selects and visualizes essential features can significantly reduce the potential manual costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finally in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented AttentiveChrome, an attention-based deep-learning approach that handles prediction and understanding in one architecture. The advantages of this work include:</p><p>• AttentiveChrome provides more accurate predictions than state-of-the-art baselines <ref type="table" target="#tab_1">(Table 2</ref>).</p><p>• The attention scores of AttentiveChrome provide a better interpretation than saliency map and class optimization <ref type="table" target="#tab_2">(Table 3</ref>). This allows us to view what the model 'sees' when making its prediction.</p><p>• AttentiveChrome can model highly modular feature inputs in which each is sequentially structured.</p><p>• To the authors' best knowledge, AttentiveChrome is the first implementation of deep attention mechanism for understanding data about gene regulation. We can gain insights and understand the predictions by locating 'what' and 'where' AttentiveChrome has focused <ref type="figure" target="#fig_5">(Figure 2</ref>). Many real-world applications are seeking such knowledge from data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed AttentiveChrome framework. It includes 5 important parts: (1) Bin-level LSTM encoder for each HM mark; (2) Bin-level α-Attention across all bin positions of each HM mark; (3) HM-level LSTM encoder encoding all HM marks; (4) HM-level β-Attention among all HM marks; (5) the final classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3</head><label></label><figDesc>AttentiveChrome: A Deep Model for Joint Classification and Visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>then to recognize and attend to the relevant marks. In summary, AttentiveChrome consists of five main modules (see Supplementary Figure S:2): (1) Bin-level LSTM encoder for each HM mark; (2) Bin-level Attention on each HM mark; (3) HM-level LSTM encoder encoding all HM marks; (4) HM-level Attention over all the HM marks; (5) the final classification module. We describe the details of each component as follows: Bin-Level Encoder Using LSTMs: For a gene of interest, the j-th row vector, x j , from X includes a total of T elements that are sequentially ordered along the genome coordinate. Considering the sequential nature of such signal reads, we treat each element (essentially a bin position) as a 'time step' and use a bidirectional LSTM to model the complete dependencies among elements in x j . A bidirectional LSTM contains two LSTMs, one in each direction (see Supplementary Figure S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Best viewed in color) (a) Bin-level attention weights (α j t ) from AttentiveChrome averaged for all genes when predicting gene=ON and gene=OFF in GM12878 cell type. (b) Top: Cumulative H active signal across all active genes. Bottom: Plot of the bin-level attention weights (α j t ). These weights are averaged for gene=ON predictions. H prom weights are concentrated near the TSS and corresponds well with the H active indicating actual activity near the gene. This indicates that AttentiveChrome is focusing on the correct bin positions for this case (c) Heatmaps visualizing the HM-level weights (β j ), with j ∈ {1, ..., 5} for an important differentially regulated gene (PAX5) across three blood lineage cell types: H1-hESC (stem cell), GM12878 (blood cell), and K562 (leukemia cell). The trend of HM-level β j weights for PAX5 have been verified through biological literature. observation makes sense because H prom is related to active regions for "ON" genes. Thus, validating our results from Table 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparison of previous studies for the task of quantifying gene expression using histone modification marks (adapted from [29]). AttentiveChrome is the only model that exhibits all 8 desirable properties.</figDesc><table>Computational Study 
Unified 
Non-
linear 

Bin-Info 
Representation Learning 
Prediction 
Feature 
Inter. 

Interpretable 

Neighbor 
Bins 

Whole 
Region 

Linear Regression ([14]) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>AUC score performances for different variations of AttentiveChrome and baselines</figDesc><table>Baselines 
AttentiveChrome Variations 

Model DeepChrome 
(CNN) 
[29] 

LSTM CNN-
Attn 

CNN-
α, β 

LSTM-
Attn 

LSTM-
α 

LSTM-
α, β 

Mean 0.8008 
0.8052 0.7622 0.7936 0.8100 0.8133 0.8115 
Median 0.8009 
0.8036 0.7617 0.7914 0.8118 0.8143 0.8123 
Max 
0.9225 
0.9185 0.8707 0.9059 0.9155 0.9218 0.9177 
Min 
0.6854 
0.7073 0.6469 0.7001 0.7237 0.7250 0.7215 

Improvement over DeepChrome [29] 36 
0 
16 
49 
50 
49 
(out of 56 cell types) 

our set of 19,802 gene samples into three separate, but equal-size folds for training (6601 genes), 
validation (6601 genes), and testing (6600 genes) respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Pearson Correlation values between weights assigned for H prom (active HM) by different visualization techniques and H active read coverage (indicating actual activity near "ON" genes) for predicted "ON" genes across three major cell types.</figDesc><table>Viz. Methods 
H1-hESC GM12878 K562 

α Map (LSTM-α) 
0.8523 
0.8827 
0.9147 
α Map (LSTM-α, β) 
0.8995 
0.8456 
0.9027 
Class-based Optimization (CNN) 0.0562 
0.1741 
0.1116 
Saliency Map (CNN) 
0.1822 
-0.1421 
0.2238 

Using Attention Scores for Interpretation: Unlike images and text, the results for biology are hard 
to interpret by just looking at them. Therefore, we use additional evidence from REMC as well as 
introducing a new strategy to qualitatively and quantitatively evaluate the bin-level attention weights 
or α-map LSTM-α model and AttentiveChrome. To specifically validate that the model is focusing 
its attention at the right bins, we use the read counts of a new HM signal -H3K27ac from REMC 
database. We represent this HM as H active because this HM marks the region that is active when the 
gene is "ON". H3K27ac is an important indication of activity in the DNA regions and is a good source 
to validate the results. We did not include H3K27ac Mark as input because it has not been profiled 
for all 56 cell types we used for prediction. However, the genome-wide reads of this HM mark are 
available for three important cell types in the blood lineage: H1-hESC (stem cell), GM12878 (blood 
cell), and K562 (leukemia cell). We, therefore, chose to compare and validate interpretation in these 
three cell types. This HM signal has not been used at any stage of the model training or testing. We 
use it solely to analyze the visualization results. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>H reprA H struct H enhc H prom H reprBH reprA H struct H enhc H prom H reprB</figDesc><table>Bins (t) 
Avg. Attention Weights 

Bins (t) 
Avg. Attention Weights 

(b) 
(a) 

Genes=ON 

Genes=OFF 

Cell Type: GM12878 

TSS 

TSS 

H1-hESC 
GM12878 
K562 

Gene = OFF ON OFF 

(c) 

H reprA 

H struct 

H enhc 

H prom 

H reprB 

Maps 

Gene: PAX5 

0.008 

0.014 

0.010 

0.012 

0 
20 
40 
60 
80 
100 
Attention Weights 

H active 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We can view W b as 1 × 64 matrix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The small dips at the TSS in both subfigures of Figure 2(a) are caused by missing signals at the TSS due to the inherent nature of the sequencing experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Predicting the sequence specificities of dna-and rna-binding proteins by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Weirauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How to explain individual classification decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Mãžller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regulation of chromatin by histone modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Bannister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kouzarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Polycomb repressive complex 2 and h3k27me3 cooperate with h3k9 methylation to maintain heterochromatin protein 1α at chromatin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Boros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nausica</forename><surname>Arnoult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Stroobant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-François</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anabelle</forename><surname>Decottignies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular and cellular biology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3662" to="3674" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A statistical framework for modeling gene expression using chromatin features and application to modencode datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koon-Kiu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Y</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Rozowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentionbased models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Control of goal-directed and stimulus-driven attention in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Corbetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon L Shulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling gene expression using chromatin features in various cellular contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Melissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Greven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Djebali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gingeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roderic</forename><surname>Gerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Guigó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The correlation between histone modifications and gene expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epigenomics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="116" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combinatorial roles of dna methylation and histone modifications on gene expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Bich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rania</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Tu</forename><surname>Mohammed Kotb Hassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Some Current Advanced Researches on Information and Computer Science in Vietnam</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histone modification levels are predictive for gene expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><surname>Karlić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho-Ryun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vlahoviček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vingron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2926" to="2931" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">L</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rinn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Cold Spring Harbor Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrative analysis of 111 reference human epigenomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Meuleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Bilenky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Heravi-Moussavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Kheradpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7539</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep motif: Visualizing genomic sequence classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritambhara</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep motif dashboard: Visualizing and understanding genomic sequences using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritambhara</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Must-cnn: A multilayer shift-and-stitch deep convolutional architecture for sequence-based protein structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The transcription factor pax5 regulates its target genes by recruiting chromatin-modifying proteins in committed b cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Mcmanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgia</forename><surname>Salvagiotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasna</forename><surname>Medvedovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiromi</forename><surname>Tagoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinrad</forename><surname>Busslinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The EMBO journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2388" to="2404" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Alex Graves, and others. Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Danq: a hybrid convolutional and recurrent deep neural network for quantifying the function of dna sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="107" to="107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepchrome: deep-learning for predicting gene expression from histone modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritambhara</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="639" to="648" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep supervised and convolutional generative stochastic network for protein secondary structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1347</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Predicting effects of noncoding variants with deep learning-based sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Nature Publishing Group</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="931" to="934" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
