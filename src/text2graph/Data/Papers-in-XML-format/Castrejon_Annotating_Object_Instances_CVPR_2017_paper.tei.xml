<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Annotating Object Instances with a Polygon-RNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Castrejón</surname></persName>
							<email>castrejon@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
							<email>kkundu@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>fidler@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Annotating Object Instances with a Polygon-RNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose an approach for semi - </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic image segmentation has been receiving significant attention in the community <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>. With new benchmarks such as Cityscapes <ref type="bibr" target="#b5">[6]</ref>, object instance segmentation is also gaining steam <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. Most of the recent approaches are based on neural networks, achieving impressive performance for these tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>. Deep learning approaches are, however, data hungry and their performance is strongly correlated with the amount of available training data. This requires the community to annotate large-scale datasets which is both time consuming and expensive. Our goal in this paper is to make this process faster, while yielding ground-truth as precise as the one available in the current datasets.</p><p>There have been several attempts at reducing the dependency on very detailed annotation such as object segmentation masks. In the weakly-supervised setting, approaches aim at learning segmentation models from weak annotation such as image tags or bounding boxes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, the authors rely on scribbles, one on each object, while <ref type="bibr" target="#b0">[1]</ref> requires only a single point on the object. While these ap- <ref type="figure">Figure 1</ref>. Given a bounding box, we automatically predict the polygon outlining the object instance inside the box, using our Polygon-RNN. Our method is designed to facilitation annotation, and easily incorporates user corrections of points to improve the overall object's polygon. Our method cuts down the number of required annotation clicks by a factor of 4.74.</p><p>proaches hold promise, their performance is not yet competitive with fully supervised approaches. Other work exploits easier-to-obtain ground-truth such as bounding boxes, and produces (noisy) labeling inside each box with a GrabCut type of approach <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref>. It has been shown that such annotation can serve as useful auxilary data to train neural segmentation networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b28">29]</ref>. Yet, these segmentations cannot be used as official ground-truth for a benchmark due to its inherent imprecisions.</p><p>Most of the large-scale segmentation datasets have been collected by having annotators outline the objects with a polygon <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">37]</ref>. Since typically objects are connected and without holes, polygons provide a way of annotating an object with a relatively small number of clicks, typically around 30 to 40 per object. In this paper, we propose an interactive segmentation method that produces highly accurate and structurally coherent object annotations, and reduces annotation time by a factor of 4.7.</p><p>Given a ground-truth bounding box, our method generates a polygon outlining the object instance using a Recurrent Neural Network, which we call Polygon-RNN. Our approach takes as input an image crop and sequentially pro-duces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentations as desired by the annotator. We show that our annotation approach speeds up annotation process by factor of 4.7, while achieving 78.4% agreement with original groundtruth, matching the typical agreement of human annotators. We plan to release our code and create a web-annotation interface running our model at the backend. Please refer to our project page: http://www.cs.toronto.edu/ polyrnn. We hope this will cut down annotation time and cost of segmentation benchmarks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our approach is related to work on semi-automatic image annotation and object instance segmentation.</p><p>Semi-automatic annotation. There has been significant effort at making pixel-level image labeling faster for the annotators. In <ref type="bibr" target="#b1">[2]</ref>, the authors used scribbles as seeds to model the appearance of foreground and background, and performed segmentation via graph-cuts by combining appearance cues and a smoothness term <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr" target="#b18">[19]</ref> uses multiple scribbles on the object and background and exploits motion cues to annotate an object in a video. Scribbles were also recently used in <ref type="bibr" target="#b14">[15]</ref> to train CNNs for semantic image segmentation. GrabCut <ref type="bibr" target="#b24">[25]</ref> exploits annotations in the form of 2D bounding boxes, and performs per-pixel labeling with foreground/background models using EM. Building on top of this idea, <ref type="bibr" target="#b22">[23]</ref> combined GrabCut with CNN to segment medical images. In <ref type="bibr" target="#b3">[4]</ref>, the authors exploited 3D bounding boxes and a point cloud to facilitate labeling. A different type of approach has been to exploit multiple bounding boxes and perform co-segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Since most of these approaches define a graphical model at the pixel-level, with the smoothness term as the main relation among pixels, it is hard to incorporate shape priors. These are particularly important in ambiguous regions caused by shadows, image saturation or low-resolution of the object. Furthermore, nothing prevents these models to provide labelings with holes. If the method makes mistakes in outlining the object, the human annotator has a hard and tedious work to correct for such mistakes. Thus, these methods have mainly been used to produce additional, yet noisy training examples, but their output is typically not accurate enough to serve as official ground-truth of a benchmark.</p><p>Annotation tools. <ref type="bibr" target="#b31">[32]</ref> labeled clothing in images by performing annotation at the superpixel-level. This makes the labeling process more efficient, but inherently depends on the superpixel scale and thus typically merges small objects or parts. This issue was somewhat resolved in <ref type="bibr" target="#b21">[22]</ref> by labeling videos at multiple superpixel scales.</p><p>Object instance segmentation. Our work is also related to object instance segmentation. Most of these approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref>] operate on the pixel-level, typically exploiting a CNN inside a box or a patch to perform the labeling. Work most related to ours is <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28]</ref> which aims to produce a polygon around an object. These approaches start by detecting edge fragments and find an optimal cycle that links the edges into a coherent region. In <ref type="bibr" target="#b6">[7]</ref>, the authors propose a method that produces superpixels in the from of small polygons which they combine into object regions with the aim to label aerial images. In our work, we predict the polygon around the object directly, using a carefully designed RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Polygon-RNN</head><p>Our goal is to create an efficient annotation tool for labeling object instances with polygons. As is typical in an annotation setting, we assume that the user provides the bounding box around the object. Given the image patch inside the box, our method predicts a (closed) polygon outlining the object using a Recurrent Neural Network. We allow the user to correct a predicted vertex of the polygon at any time step if needed, which we integrate in our prediction task.</p><p>We parametrize the polygon as a sequence of 2D vertices (c t ) t∈N , c ∈ R 2 . We assume the polygon is closed, i.e., there is an edge between any two consecutive vertices, as well as the last and the first vertices. Note that a closed polygon is a cycle and thus has multiple equivalent parametrizations obtained by choosing any of the vertices as the beginning of the sequence, as well as selecting the orientation of the sequence. Here, we fix the polygon to always follow the clockwise orientation, but the starting point of the sequence can be any of the vertices.</p><p>Our model is an RNN, that predicts a vertex at every time step. As input in each step of the RNN we use a CNN representation of the image crop, as well as the vertices predicted one and two time steps ago, plus the first point. By explicitly providing information of the past two points we help the RNN to follow a particular orientation of the polygon. On the other hand, the first vertex helps the RNN to decide when to close (finish) the polygon. We train the RNN+CNN model end-to-end. This essentially helps the CNN to be fine-tuned to object boundaries, while the RNN learns to follow these boundaries and exploits its recurrent nature to also encode priors on object shapes. Our model thus returns a structurally coherent representation of the object. We name our model Polygon-RNN. <ref type="figure" target="#fig_0">Figure 2</ref> shows the overview of the model. We next describe each component of the model in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>We start by providing details on the image representation via a CNN, and then explain the design of the RNN. Our Polygon-RNN model. At each time step of the RNN-decoder (right), we feed in an image representation using a modified VGG architecture. Our RNN is a two-layer convolutional LSTM with skip-connection from one and two time steps ago. At the output at each time step, we predict the spatial location of the new vertex of the polygon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Image Representation via a CNN with Skip Connections</head><p>We adopt the VGG-16 architecture <ref type="bibr" target="#b26">[27]</ref> and modify it for the purpose of our task. We first remove the fully connected layers as well as the last max-pooling layer, pool5. The output of this modified network has a downsampling factor of 16. We then add additional convolutional layers with skipconnections that fuse information from the previous layers and upscale the output by factor of 2 (downsampling factor of 8 wrt to the original size of the image crop, which is always scaled to 224 × 224). This allows the CNN to extract features that contain both low-level information about the edges and corners, as well as semantic information about the object. The latter helps the model to "see" the object, while the former helps it to follow the object's boundaries. We employ a similar architecture for the skipconnections as the one in <ref type="bibr" target="#b20">[21]</ref>. The design guideline is to first process the features in the skip-layers using another convolutional layer, then concatenate all outputs, and finally process this concatenated feature using another convolutional layer. We employ convolutional filters with a kernel size of 3×3, followed by a ReLU non-linearity. Concatenation layers join the channels of different outputs into a single tensor. Since we use features from multiple skiplayers which have different spatial dimensions, we employ bilinear upsampling or max-pooling in order to get outputs that all have the same spatial resolution. We refer the reader to <ref type="figure" target="#fig_0">Fig. 2</ref> for a visualization and further details about the architecture (the CNN is highlighted in green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">RNN for Vertex Prediction</head><p>An RNN is a powerful representation of time-series data, as it carries more complex information about the history by employing linear and non-linear functions. In our case, we hope the RNN to capture the shape of the object and thus make coherent predictions even in ambiguous cases such as for example shadows and saturation.</p><p>In particular, we employ a Convolutional LSTM <ref type="bibr" target="#b29">[30]</ref> in our model, and use it as a decoder. ConvLSTMs operate in 2D, which allows us to preserve the spatial information received from the CNN. Furthermore, a ConvLSTM employs convolutions, thus greatly reducing the number of parameters to be learned compared to using a fully-connected RNN. In its simplest form, a ConvLSTM (single layer) computes the hidden state h t given the input x t according to the following equations:</p><formula xml:id="formula_0">    i t f t o t g t     = W h * h t−1 + W x * x t + b (1) c t = σ(f t ) ⊙ c t−1 + σ(i t ) ⊙ tanh(g t ) h t = σ(o t ) ⊙ tanh(c t )</formula><p>Here i, f , o denote the input, forget, and output gate, h is the hidden state and c is the cell state. σ denotes the sigmoid function, ⊙ indicates an element-wise product and * a convolution. W h denotes the hidden-to-state convolution kernel and W x the input-to-state convolution kernel.</p><p>In particular, we model the polygon with a two-layer ConvLSTM with kernel size of 3×3 and 16 channels, which outputs a vertex at each time step. We formulate the vertex prediction as a classification task. Specifically, we represent our output at time step t as one-hot encoding of a D ×D +1 grid, where the D×D dimensions represent the possible 2D positions of the vertex, and the last dimension corresponds to the end-of-sequence token (i.e., polygon is closed). The position of the vertices are thus quantized to the resolution of the output grid. Let y t denote the one-hot encoding of a vertex, output at time step t.</p><p>Our ConvLSTM gets as input a tensor x t at time step t, that concatenates multiple features: the CNN feature repre-sentation of the image, y t−1 and y t−2 , i.e., a one-hot encoding of the previous predicted vertex and the vertex predicted from two time steps ago, as well as the one-hot encoding of the first predicted vertex y 1 .</p><p>Given two consecutive vertices, the next vertex on the polygon is uniquely defined. However, this is not the case for the first vertex, since any vertex of the polygon can serve as a starting point (polygon is a cycle). We thus treat the starting point as special, and predict it in the following way. We reuse the same architecture of the CNN as in Sec. 3.1.1, but add two layers, each of dimension D × D. One branch predicts object boundaries while the other takes as input the output of the boundary-predicting layer as well as the image features and predicts the vertices of the polygon. We treat both, the boundary and vertices as a binary classification problem in each cell in the output grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>To train our model we use cross-entropy at each time step of the RNN. In order to not over-penalize the incorrect predictions that are close to the ground-truth vertex, we smooth our target distribution at each time step. We assign non-zero probability mass to those locations that are within a distance of 2 in our D × D output grid.</p><p>We follow the typical training regime where we make predictions at each time step but feed in ground-truth vertex information to the next. We train our model using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with a batch size b = 8 and an initial learning rate of λ = 1e − 4. We decay the learning rate after 10 epochs by a factor of 10 and use the default values of β 1 = 0.9 and β 2 = 0.999.</p><p>For the task of first vertex prediction, we train another CNN using a multi-task loss. In particular, we use the logistic loss for every location in the grid. As groundtruth for the object boundaries, we draw the edges of the ground-truth polygon, and use the vertices of the polygon as ground-truth for the vertex layer. Our full model takes approximately a day to train on a Nvidia Titan-X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference and Annotators in the Loop</head><p>Inference in our model is done by taking the vertex with the highest log-prob at each time step of the RNN. This allows for a simple annotation interface: the annotator can correct the prediction at any time step, and we feed in the corrected vertex to the next time-step of the RNN (instead of the prediction). This puts the model back "on the right track". Typical inference time is 250 ms per object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>We predict the polygon at resolution D × D. In our experiments we used D = 28, corresponding to an 8x downsampling factor with the input resolution and matching the resolution of the ConvLSTM. We perform polygon simplification with zero error in the quantized grid to eliminate  vertices that lie on a line and to remove multiple vertices that would fall in the same grid position as a result of the quantization process.</p><p>We perform three different types of data augmentation: (1) we randomly flip the image crop and the corresponding polygon annotation, (2) we randomly select the amount of context expansion (enlarging the box) between 10% and 20% of the original bounding box and (3) we randomly select the starting vertex of our polygon annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We evaluate our approach for the task of object instance annotation on the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, and provide additional results on KITTI <ref type="bibr" target="#b8">[9]</ref>. Note that in all our experiments we assume to be given a ground-truth box around the object. Our goal then is to provide a polygon outlining this object as accurately as possible and with minimal number of clicks required from the annotator. We report our performance with the standard IOU measure, as well as the number of vertex corrections of the predicted polygon. A box around the object in principle requires two additional clicks. However, boxes are typically much easier and cheaper to obtain using crowd-sourcing services such as AMT, while for most major segmentation benchmarks, polygons have been collected with high quality (in-house) annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cityscapes Dataset</head><p>We evaluate our approach on the Cityscapes instance segmentation dataset <ref type="bibr" target="#b5">[6]</ref>. This dataset has images taken  <ref type="table">Table 1</ref>. The Cityscapes dataset has instances with a large variation in their sizes. We show the distribution of instances for different lengths of the longest side of the box, in <ref type="figure" target="#fig_1">Fig. 3</ref>. We observe a large variance, from 28 pixels to 1792 pixels. Cityscapes provides instance segmentation ground truth both in terms of a pixel labeling as well as in terms of polygons. In the former, each pixel can correspond to at most one instance, thus representing the visible portion of the object. However, Cityscapes' polygons typically also capture some occluded parts of an instance, since the annotation tool performed depth ordering of objects to effectively remove the occluded portions <ref type="bibr" target="#b5">[6]</ref>. We process the polygons to recreate the layering effect and obtain polygons representing only the visible portions of each object. The average number of vertices from the resulting polygons are shown in <ref type="table">Table 2</ref>. Since objects can be broken into multiple components due to occlusion, component-wise statistics treats each component as a single example, while instance-wise statistics treats the entire instance as an example. Based on this statistics, we choose a hard limit of 70 time steps for our RNN, taking also GPU memory requirements into account.</p><p>Evaluation Metrics: We measure two aspects of our predicted annotations. For evaluating their quality, we use the intersection over union (IoU) metric, computed on a perinstance basis, and averaging across all instances. This is a strict measure since the small objects are penalized the same as the large instances. For evaluating the amount of human action required to correct our annotations, we simulate an annotator that corrects a point each time the predicted vertex deviates from the GT vertex more than a threshold. We then report the number of corrections (measured as clicks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prediction Mode</head><p>We first sanity check the performance of our model without any interaction from the annotator, i.e., we predict the full polygon automatically. We will refer to this setting as the prediction mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We use the recently proposed DeepMask <ref type="bibr" target="#b19">[20]</ref> and SharpMask <ref type="bibr" target="#b20">[21]</ref> as state-of-the-art baselines. Given an input image patch, DeepMask uses a CNN to output a pixel labeling of an object, and does so agnostic to the class. Sharpmask extends Deepmask by clever upsampling of the output to obtain the labeling at a much higher resolution (160 vs 56). Note that in their original approach, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> exhaustively sample patches at different scales over the entire image. Here, we use ground-truth boxes when reporting performance for their approach. Further, DeepMask and SharpMask use a 50 layer ResNet <ref type="bibr" target="#b9">[10]</ref> architecture, which has been trained on the COCO <ref type="bibr" target="#b15">[16]</ref> dataset. We fine-tune this network on our Cityscapes split in two steps. In the first step, we fine-tune the feed-forward ResNet architecture for 150 epochs, followed by fine-tuning the weights for the Sharpmask's upsampling layers, for 70 epochs. This two step process is in the same spirit as that suggested in the paper. Note that while these two approaches perform well in labeling the pixels, their output cannot easily be corrected by an annotator in cases when mistakes occur. This is in contrast to our approach, which efficiently integrates a human in the loop in order to get high quality annotations.</p><p>We use two additional baselines, SquareBox and Dilation10. SquareBox is a simple baseline where the full box is labeled as the object. Instead of taking the tight-fit box, we reduce the dimensions of the box, keeping the same aspect ratio. Based on the validation set, we get the best results by choosing 80% of the original box. If an instance has multiple components, we fit a box for each individual component as opposed to using the full box. This baseline mimics the scenario, in which the object is modeled simply as a box rather than a polygon. For the Dilation10 baseline, we use the segmentation results from <ref type="bibr" target="#b32">[33]</ref>, which was trained on the Cityscapes segmentation dataset. For each bounding box, we consider the pixels belonging to the respective object category as the instance mask.</p><p>Quantitative Results: We report the IoU metric in Table 3. We outperform the baselines in 6 out of 8 categories, as well as in the average across all classes. We perform particularly well in car, person, and rider, outperforming Sharpmask by 12%, 7%, and 6%, respectively. This is particularly impressive since Sharpmask uses a more powerful ResNet architecture (we use VGG).</p><p>Effect of object size: <ref type="figure" target="#fig_2">In Fig. 4</ref>, we see how our model performs w.r.t baselines on different instance sizes. For small instances our model performs significantly better than the baselines. For larger objects, the baselines have an ad-  <ref type="table">Table 4</ref>. Annotator in the loop: Average number of corrections per instance and IoU, computed across all classes. Threshold indicates chessboard distance to the closest GT vertex.</p><p>vantage due to larger output resolution. This effect is most notable for classes such as bus and train, in which our model obtains lower IOU compared to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Annotator in the loop</head><p>The main advantage of our model is that it allows a human annotator to easily interfere if a mistake occurs. In particular, at each RNN time step, the annotator has the possibility to correct a misplaced vertex. The correction is fed to the model at the next time step replacing the model's prediction, effectively helping the model to get back to the right track. Our goal is to obtain high quality annotations while minimizing annotation time.</p><p>We analyze how many clicks are needed to obtain different levels of segmentation accuracy. We perform such analysis by simulating an annotator: we correct a prediction if it deviates from the ground truth vertex by a certain distance. Distances are computed at the model output resolution using the chessboard metric. In our experiments we compare the corrected predictions using distance thresholds T ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In <ref type="table">Table 4</ref> we show the resulting IoU given different thresholds on the distance. We can observe a tradeoff between the number of corrections and these metrics.</p><p>To put our results in perspective, we hired an experienced, high-quality annotator. We asked the annotator to annotate all car (including van) instances in 10 randomly selected Cityscapes images from our validation split. We perform two experiments: in the first experiment, the annotator is asked to annotate objects by free-viewing of the full image. In the second experiment, we crop the image patches using the Cityscapes boxes, and place a blue dot on the instance to disambiguate annotation. We take a crop with 15% of context around the box and scale it to size 224x224. The annotator used the LabelMe tool <ref type="bibr" target="#b25">[26]</ref> for annotation.</p><p>In <ref type="table" target="#tab_3">Table 5</ref> we report the IoU achieved by the human annotator as well as the mean number of clicks per instance  in each experiment. We can observe that the agreement achieved in IoU is 69.5% in the free-viewing regime, and 78.60% when shown the crops (our regime). This number sheds light on what we are to expect from automatic methods in general, and points to some ambiguities in the task. It also indicates that benchmarks should collect multiple annotations of images to reduce the variations and biases across the annotators. We hope our approach will make such data collection feasible and affordable. Notice that our model achieves a higher agreement (82%) by requiring only 4.6 clicks on average, which is a factor of 7.3 speed-up in annotation time. Even at agreement as high as 87.7, the annotation speed-up factor is still 3.6. This showcases the effectiveness of our model as an annotation tool. For all the categories in Cityscapes and following the same procedure, we require only 9.39 clicks on average to obtain 78.40% IoU agreement, obtaining a speed-up factor of 4.74.</p><p>Comparison with Grabcut. We also compare the performance of our approach with another semi automatic method on a set of 54 randomly chosen instances. We used the OpenCV implementation of Grabcut <ref type="bibr" target="#b24">[25]</ref> for this experiment. On average, using Grabcut the annotators needed 42.2s and 17.5 clicks per instance, and obtained an average of 70.7% IoU agreement with the Cityscapes GT. On the same set of images, our model achieves IoUs ranging from 79.7% to 85.8%, with 5.0 clicks (T=4) to 9.6 clicks (T=1), respectively. Our expert human annotator needed 87.6 clicks to obtain an IoU of 77.6% (without using any   <ref type="table">Table 6</ref>. Car annotation results on the KITTI dataset.</p><p>semi automatic tool). Since our model requires much less human intervention than <ref type="bibr" target="#b24">[25]</ref> (5 vs 17.5 clicks) and requires comparable inference time per click, we expect that in a real world scenario our method would be much faster.</p><p>Qualitative Results: In <ref type="figure" target="#fig_4">Fig. 6</ref> we show examples of images annotated with our method. We remind the reader, that this labeling is obtained by exploiting the GT bounding boxes. In particular, we here show the predictions obtained without any corrections (0 clicks). Our model is able to correctly segment instances with a variety of shapes and sizes. For large instances the quantization error introduced by the output resolution of our model becomes apparent. Increasing the output resolution is subject of ongoing work. The main challenges are memory considerations as well as challenges due to longer sequences (polygons have more vertices) that the network would need to predict.</p><p>In <ref type="figure">Fig. 7</ref> we compare annotations of example instances more closely by zooming in on each object. We can inspect the agreement between the GT annotation and our in-house annotator, as well as the quality of the predictions obtained by PolygonRNN with and without corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Annotating KITTI Instances</head><p>We also evaluate how well our model that was trained on Cityscapes generalizes to an unseen dataset. We use KITTI for this experiment, which has 741 annotated instances provided by <ref type="bibr" target="#b3">[4]</ref>. We report the results in <ref type="table">Table 6</ref>. The object instances in KITTI are usually larger than those found in Cityscapes, making Deepmask and SharpMask perform very similarly. Note that <ref type="bibr" target="#b3">[4]</ref>, also a method for semiautomatic annotation, exploited Velodyne point clouds to perform their labeling, which puts it with an unfair advantage. Our model is further penalized by its lower resolution output. Still, their performance is lower than our fully automatic approach. With only 5.84 clicks on mean per instance our models achieves an IOU comparable to the human annotation agreement, thus reducing the annotation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we proposed an approach to facilitate annotation of object instances. Our Polygon-RNN predicts a polygon outlining an object, and easily incorporates corrections from an annotator in the loop. We show annotation speed-up of factor 4.74 while achieving the same annotation agreement as that between human annotators. The main advantage of our approach is that it produces structurally plausible annotations of objects, and allows us to achieve a desired annotation accuracy by requiring only a few clicks by the annotator. Additional experiments show that our approach generalizes across different datasets, thus showcasing its power as a generic annotation tool. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Annotator Ours (Automatic) Ours (T=1) <ref type="figure">Figure 7</ref>. We look at a few instances in more detail. In the first column we show the GT annotation, while in the second column we show the polygons from the in-house annotator. We observe that these segmentations are high quality but differ in uncertain areas such as the base of the car. In the third column we show the PolygonRNN prediction without human intervention. Finally, in the fourth column we show a corrected prediction. We can observe that the segmentation is refined to better outline the car mirrors or wheels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Our Polygon-RNN model. At each time step of the RNN-decoder (right), we feed in an image representation using a modified VGG architecture. Our RNN is a two-layer convolutional LSTM with skip-connection from one and two time steps ago. At the output at each time step, we predict the spatial location of the new vertex of the polygon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Distribution of instances across different sizes: The longest side on the X axis are multiples of 28 pixels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. IoU vs size of instance comparing different approaches. Here, ours is run in prediction mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Annotator in the loop: We show IoU as a function of the number of clicks/corrections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative results in prediction mode: We show polygons for all classes in the original image. Note that our approach uses GT boxes as input. (left) we show the GT labeling of the image, (right) we show our polygons without any human intervention. The GT images contain 38, 12, 28 and 16 instances, and required 985, 308, 580 and 338 clicks respectively from their Cityscapes annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Split # Img. Person Rider Car Truck Bus Train Mbike BikeTable 1. Number of object instances per class in Cityscapes.</figDesc><table>Train 2711 16452 1575 24982 455 352 136 657 3400 
Val. 264 
1462 180 1962 27 27 32 
78 258 
Test 500 
3395 544 4658 93 98 23 149 1167 

Mode 
Car Truck Train Bike Prsn. Rider Mbike Bus Avg. 
Comp-wise 24.3 27.2 23.6 24.2 27.9 31.6 29.2 26.1 26.8 
Inst-wise 31.7 41.7 66.6 40.0 35.0 44.7 45.7 50.8 44.5 

Table 2. Average number of vertices in polygon annotations for dif-
ferent object classes in Cityscapes. 

from 27 cities in Germany and neighboring countries. It 
contains 2975 training, 500 validation and 1525 test im-
ages. Since we do not have ground truth instances on the 
test set, we use an alternative split, where the 500 origi-
nal validation images form our test set. We then split the 
original training set and select the images from two cities 
(Weimar and Zurich) as our validation, while the remaining 
cities become our training set. The dataset has annotations 
for eight object categories: person, rider, car, truck, bus, 
train, motorcycle and bicycle. The number of instances for 
each of these classes in our split is shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>Our model vs Annotator Agreement: We hired a highly 
trained annotator to label car instances on additional 10 images 
(101 instances). We report IoU agreement with Cityscapes GT, 
and report polygon statistics. We compare our approach with the 
agreement between the human annotators. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We acknowledge the support from NSERC, and thank Relu Patrascu for infrastructure support. L.C. was supported by a La Caixa Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beat the mturkers: Automatic image labeling from weak 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards large-scale city reconstruction from satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lafarge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results. 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active image segmentation propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation propagation in imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuettel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-automatic video object segmentation by advanced manipulation of segmentation hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A F</forename><surname>Guiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl Workshop on Content-Based Multimedia Indexing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deepcut: Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Passeratpalmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07866</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08250</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Free-shape polygonal object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.05096</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tell me what you see and i will show you where it is</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Super-edge grouping for object localization by combining appearance and shape information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
