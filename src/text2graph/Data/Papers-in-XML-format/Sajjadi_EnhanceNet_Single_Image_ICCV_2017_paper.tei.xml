<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
							<email>msajjadi@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems Spemanstr. 34</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems Spemanstr. 34</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
							<email>mhirsch@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems Spemanstr. 34</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single image super-resolution is the task of inferring a high-resolution image from</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Enhancing and recovering a high-resolution (HR) image from a low-resolution (LR) counterpart is a theme both of science fiction movies and of the scientific literature. In the latter, it is known as single image super-resolution (SISR), a topic that has enjoyed much attention and progress in recent years. The problem is inherently ill-posed as no unique solution exists: when downsampled, a large number of different HR images can give rise to the same LR image. For high magnification ratios, this one-to-many mapping problem becomes worse, rendering SISR a highly intricate problem. Despite considerable progress in both reconstruction accuracy and speed of SISR, current state-of-the-art methods are still far from image enhancers like the one operated by Harrison Ford alias Rick Deckard in the iconic Blade Runner movie from 1982. A crucial problem is the loss of high-frequency information for large downsampling factors rendering textured regions in super-resolved images blurry, overly smooth, and unnatural in appearance (c.f . <ref type="figure" target="#fig_0">Fig. 1</ref>, left, the new state of the art by PSNR, ENet-E).</p><p>The reason for this behavior is rooted in the choice of the objective function that current state-of-the-art methods employ: most systems minimize the pixel-wise mean squared error (MSE) between the HR ground truth image and its reconstruction from the LR observation, which has however been shown to correlate poorly with human perception of image quality <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54]</ref>. While easy to minimize, the optimal MSE estimator returns the mean of many possible solutions which makes SISR results look unnatural and implausible (c.f . <ref type="figure" target="#fig_1">Fig. 2</ref>). This regression-to-the-mean problem in the context of super-resolution is a well-known fact, however, modeling the high-dimensional multi-modal distribution of natural images remains a challenging problem.</p><p>In this work we pursue a different strategy to improve the perceptual quality of SISR results. Using a fully convolutional neural network architecture, we propose a novel modification of recent texture synthesis networks in combination with adversarial training and perceptual losses to produce realistic textures at large magnification ratios. The method works on all RGB channels simultaneously and produces sharp results for natural images at a competitive speed. Trained with suitable combinations of losses, we reach state-of-the-art results both in terms of PSNR and using perceptual metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The task of SISR has been studied for decades <ref type="bibr" target="#b22">[23]</ref>. Early interpolation methods such as bicubic and Lanczos <ref type="bibr" target="#b10">[11]</ref> are based on sampling theory but often produce blurry results with aliasing artifacts in natural images. A large number of high-performing algorithms have since been proposed <ref type="bibr" target="#b34">[35]</ref>, see also the recent surveys by Nasrollahi and Moeslund <ref type="bibr" target="#b36">[37]</ref> and Yang et al. <ref type="bibr" target="#b56">[57]</ref>.</p><p>In recent years, popular approaches include exemplarbased models that either exploit recurrent patches of different scales within a single image <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b55">56]</ref> or learn mappings between low and high resolution pairs of image patches in external databases <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b62">63]</ref>. They further include dictionary-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref> that learn a sparse representation of image patches as a combination of dictionary atoms, as well as neural network-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b61">62]</ref> which apply convolutional neural networks (CNNs) to the task of SISR. Some approaches are specifically designed for fast inference times <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. Thus far, realistic textures in the context of high-magnification SISR have only been achieved by user-guided methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>More specifically, Dong et al. <ref type="bibr" target="#b7">[8]</ref> apply shallow networks to the task of SISR by training a CNN via backpropagration to learn a mapping from the bicubic interpolation of the LR input to a high-resolution image. Later works successfully apply deeper networks and the current state of the art in SISR measured by PSNR is based on deep CNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>As these models are trained through MSE minimization, the results tend to be blurry and lack high-frequency textures due to the afore-mentioned regression-to-the-mean problem. Alternative perceptual losses have been proposed for CNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref> where the idea is to shift the loss from the image-space to a higher-level feature space of an object recognition system like VGG <ref type="bibr" target="#b48">[49]</ref>, resulting in sharper results despite lower PSNR values.</p><p>CNNs have also been found useful for the task of texture synthesis <ref type="bibr" target="#b14">[15]</ref> and style transfer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">53]</ref>, however these methods are constrained to the setting of a single network learning to produce only a single texture and have so far not been applied to SISR. Adversarial networks <ref type="bibr" target="#b17">[18]</ref> have recently been shown to produce sharp results in a number of image generation tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b65">66]</ref> but have so far only been applied in the context of super-resolution in a highly constrained setting for the task of face hallucination <ref type="bibr" target="#b61">[62]</ref>.</p><p>Concurrently and independently to our research, in an unpublished work, Ledig et al. <ref type="bibr" target="#b28">[29]</ref> developed an approach that is similar to ours: inspired by Johnson et al. <ref type="bibr" target="#b23">[24]</ref>, they train feed-forward CNNs using a perceptual loss in conjunction with an adversarial network. However, in contrast to our work, they do not explicitly encourage local matching of texture statistics which we found to be an effective means to produce more realistic textures and to further reduce visually implausible artifacts without the need for additional regularization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Single image super-resolution</head><formula xml:id="formula_0">A high resolution image I HR ∈ [0, 1]</formula><p>αw×αh×c is downsampled to a low resolution image</p><formula xml:id="formula_1">I LR = d α (I HR ) ∈ [0, 1] w×h×c (1)</formula><p>using some downsampling operator</p><formula xml:id="formula_2">d α : [0, 1] αw×αh×c → [0, 1] w×h×c<label>(2)</label></formula><p>for a fixed scaling factor α &gt; 1, image width w, height h and color channels c. The task of SISR is to provide an approximate inverse f ≈ d −1 estimating I HR from I LR :</p><formula xml:id="formula_3">f(I LR ) = I est ≈ I HR .<label>(3)</label></formula><p>This problem is highly ill-posed as the downsampling operation d is non-injective and there exists a very large number of possible images I est for which d(I est ) = I LR holds. Recent learning approaches aim to approximate f via multi-layered neural networks by minimizing the Euclidean loss ||I est − I HR || 2 2 between the current estimate and the ground truth image. While these models reach excellent results as measured by PSNR, the resulting images tend to look blurry and lack high frequency textures present in the original images. This is a direct effect of the high ambiguity in SISR: since downsampling removes high frequency information from the input image, no method can hope to reproduce all fine details with pixel-wise accuracy. Therefore, even state-of-the-art models learn to produce the mean of all possible textures in those regions in order to minimize the Euclidean loss for the output image.</p><p>To illustrate this effect, we designed a simple toy example in <ref type="figure" target="#fig_1">Fig. 2</ref>, where all high frequency information is lost by downsampling. The optimal solution with respect to the Euclidean loss is simply the average of all possible images while more advanced loss functions lead to more realistic, albeit not pixel-perfect reproductions.</p><p>Output size  <ref type="table">Table 1</ref>. Our generative fully convolutional network architecture for 4x super-resolution which only learns the residual between the bicubic interpolation of the input and the ground truth. We use 3×3 convolution kernels, 10 residual blocks and RGB images (c = 3).</p><formula xml:id="formula_4">Layer w × h × c Input I LR w × h × 64</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>Our network architecture in <ref type="table">Table 1</ref> is inspired by Long et al. <ref type="bibr" target="#b31">[32]</ref> and Johnson et al. <ref type="bibr" target="#b23">[24]</ref> since feed-forward fully convolutional neural networks exhibit a number of useful properties for the task of SISR. The exclusive use of convolutional layers enables training of a single model for an input image of arbitrary size at a given scaling factor α while the feed-forward architecture results in an efficient model at inference time since the LR image only needs to be passed through the network once to get the result. The exclusive use of 3×3 filters is inspired by the VGG architecture <ref type="bibr" target="#b48">[49]</ref> and allows for deeper models at a low number of parameters in the network.</p><p>As the LR input is smaller than the output image, it needs to be upsampled at some point to produce a high-resolution image estimate. It may seem natural to simply feed the bicubic interpolation of the LR image into the network <ref type="bibr" target="#b7">[8]</ref>. However, this introduces redundancies to the input image and leads to a higher computational cost. For convolutional neural networks, Long et al. <ref type="bibr" target="#b31">[32]</ref> use convolution transpose layers 1 which upsample the feature activations inside the network. This circumvents the nuisance of having to feed a large image with added redundancies into the CNN and allows most computation to be done in the LR image space, resulting in a smaller network and larger receptive fields of the filters relative to the output image.</p><p>However, convolution transpose layers have been reported to produce checkerboard artifacts in the output, ne- cessitating an additional regularization term in the output such as total variation <ref type="bibr" target="#b42">[43]</ref>. Odena et al. <ref type="bibr" target="#b37">[38]</ref> replace the convolution transpose layers with nearest-neighbor upsampling of the feature activations in the network followed by a single convolution layer. In our network architecture, this approach still produces checkerboard-artifacts for some specific loss functions, however we found that it obviates the need for an additional regularization term in our more complex models. To further reduce artifacts, we add a convolution layer after all upsampling blocks in the HR image space as this helps to avoid regular patterns in the output. Training deep networks, we found residual blocks <ref type="bibr" target="#b19">[20]</ref> to be beneficial for faster convergence compared to stacked convolution layers. A similarly motivated idea proposed by Kim et al. <ref type="bibr" target="#b24">[25]</ref> is to learn only the residual image by adding the bicubic interpolation of the input to the model's output, so that it does not need to learn the identity function for I LR . While the residual blocks that make up a main part of our network already only add residual information, we found that applying this idea helps stabilize training and reduce color shifts in the output during training.</p><formula xml:id="formula_5">(I) (II) (III) (IV) IHR ILR Iest MSE Iest Adv.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training and loss functions</head><p>In this section, we introduce the loss terms used to train our network. Various combinations of these losses and their effects on the results are discussed in Sec. 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pixel-wise loss in the image-space</head><p>As a baseline, we train our model with the pixel-wise MSE</p><formula xml:id="formula_6">L E = ||I est − I HR || 2 2 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">||I|| 2 2 = 1 whc w,h,c (I w,h,c ) 2 .<label>(5)</label></formula><p>Bicubic</p><p>ENet-E ENet-PAT Ground Truth <ref type="figure">Figure 3</ref>. Our results on an image from ImageNet for 4x super-resolution. Despite reaching state-of-the-art results by PSNR, ENet-E produces an unnatural and blurry image while ENet-PAT reproduces faithful high-frequency information, resulting in a photorealistic image, at first glance almost indistinguishable from the ground truth image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Perceptual loss in feature space</head><p>Dosovitskiy and Brox <ref type="bibr" target="#b9">[10]</ref> as well as Johnson et al. <ref type="bibr" target="#b23">[24]</ref> propose a perceptual similarity measure. Rather than computing distances in image space, both I est and I HR are first mapped into a feature space by a differentiable function φ before computing their distance.</p><formula xml:id="formula_8">L P = ||φ(I est ) − φ(I HR )|| 2 2<label>(6)</label></formula><p>This allows the model to generate outputs that may not match the ground truth image with pixel-wise accuracy but instead encourages the network to produce images that have similar feature representations. For the feature map φ, we use a pre-trained implementation of the popular VGG-19 network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>. It consists of stacked convolutions coupled with pooling layers to gradually decrease the spatial dimension of the image and to extract higher-level features in higher layers. To capture both low-level and high-level features, we use a combination of the second and fifth pooling layers and compute the MSE on their feature activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Texture matching loss</head><p>Gatys et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> demonstrate how convolutional neural networks can be used to create high quality textures. Given a target texture image, the output image is generated iteratively by matching statistics extracted from a pre-trained network to the target texture. As statistics, correlations between the feature activations φ(I) ∈ R n×m at a given VGG layer with n features of length m are used:</p><formula xml:id="formula_9">L T = ||G(φ(I est )) − G(φ(I HR ))|| 2 2 ,<label>(7)</label></formula><p>with Gram matrix G(F ) = F F T ∈ R n×n . As it is based on iterative optimization, this method is slow and only works if a target texture is provided at test time. Subsequent works train a feed-forward network that is able to synthesize a global texture (e.g., a given painting style) onto other images <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">53]</ref>, however a single network again only produces a single texture, and textures in all input images are replaced by the single style that the network has been trained for.</p><p>We propose using the style transfer loss for SISR: Instead of supplying our network with matching highresolution textures during inference, we compute the texture loss L T patch-wise during training to enforce locally similar textures between I est and I HR . The network therefore learns to produce images that have the same local textures as the high-resolution images during training. While the task of generating arbitrary textures is more demanding than single-texture synthesis, the LR image and high-level contextual cues give our network more information to work with, enabling it to generate varying high resolution textures. Empirically, we found a patch size of 16×16 pixels to result in the best balance between faithful texture generation and the overall perceptual quality of the images. For results with different patch sizes and further details on the implementation, we refer the reader to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Adversarial training</head><p>Adversarial training <ref type="bibr" target="#b17">[18]</ref> is a recent technique that has proven to be a useful mechanism to produce realistically looking images. In the original setting, a generative network G is trained to learn a mapping from random vectors z to a data space of images x that is determined by the selected training dataset. Simultaneously, a discriminative network D is trained to distinguish between real images x from the dataset and generated samples G(z). This approach leads to a minimax game in which the generator is trained to minimize</p><formula xml:id="formula_10">L A = − log(D(G(z)))<label>(8)</label></formula><p>while the discriminator minimizes</p><formula xml:id="formula_11">L D = − log(D(x)) − log(1 − D(G(z))).<label>(9)</label></formula><p>In the SISR setting, G is our generative network as shown in <ref type="figure" target="#fig_0">Fig. 1, i.</ref>e., the input to G is now an LR image I LR instead of a noise vector z and its desired output is a suitable realistic high-resolution image I est .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>ENet-E ENet-P ENet-PA ENet-PAT IHR <ref type="figure">Figure 4</ref>. Comparing the results of our model trained with different losses at 4x super-resolution on images from ImageNet. ENet-P's result looks slightly sharper than ENet-E's, but it also produces unpleasing checkerboard artifacts. ENet-PA produces images that are significantly sharper but contain unnatural textures while we found that ENet-PAT generates more realistic textures, resulting in photorealistic images close to the original HR images. Results with further combinations of losses and different parameters are shown in the supplementary.</p><p>Following common practice <ref type="bibr" target="#b40">[41]</ref>, we apply leaky ReLU activations <ref type="bibr" target="#b33">[34]</ref> and use strided convolutions to gradually decrease the spatial dimensions of the image in the discriminative network as we found deeper architectures to result in images of higher quality. Perhaps surprisingly, we found dropout not to be effective at preventing the discriminator from overpowering the generator. Instead, the following learning strategy yields better results and a more stable training: we keep track of the average performance of the discriminator on true and generated images within the previous training batch and only train the discriminator in the subsequent step if its performance on either of those two samples is below a threshold. The full architecture and further details are specified in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>In Sec. 5.1, we investigate the performance of our architecture trained with different combinations of the previously introduced loss functions. After identifying the best performing models, Sec. 5.2 gives a comprehensive qualitative and quantitative evaluation of our approach. Additional experiments, comparisons and results at various scaling factors are given in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of different losses</head><p>We compare the performance of our network trained with the combinations of loss functions listed in Tab. 2. The results are shown in <ref type="figure">Fig. 4</ref> and Tab. 3 while more results on Enet-EA, ENet-EAT and ENet-PAT trained with different parameters are given in the supplementary.</p><p>Using the perceptual loss in ENet-P yields slightly sharper results than ENet-E but it produces artifacts with- out adding new details in textured areas. Even though the perceptual loss is invariant under perceptually similar transformations, the network is given no incentive to produce realistic textures when trained with the perceptual loss alone.</p><formula xml:id="formula_12">Network Loss Description ENet-E L E Baseline with MSE ENet-P L P Perceptual loss ENet-EA L E + L A ENet-E + adversarial ENet-PA L P + L A ENet-P + adversarial ENet-EAT L E + L A + L T ENet-EA + texture loss ENet-PAT L P + L A + L T ENet-PA + texture loss</formula><p>ENet-PA produces greatly sharper images by adding high frequency details to the output. However, the network sometimes produces unpleasing high-frequency noise to smooth regions and it seems to add high frequencies at random edges resulting in halos and sharpening artifacts in some cases. The texture loss helps ENet-PAT create locally meaningful textures and greatly reduces the artifacts. For some images, the results are almost indistinguishable from the ground truth even at a high magnification ratio of 4.</p><p>Unsurprisingly, ENet-E yields the highest PSNR as it is optimized specifically for that measure. Although ENet-PAT produces perceptually more realistic images, the PSNR is much lower as the reconstructions are not pixel-accurate. As shown in the supplementary, SSIM and IFC <ref type="bibr" target="#b45">[46]</ref>   <ref type="table">Table 3</ref>. PSNR for our architecture trained with different combinations of losses at 4x super resolution. ENet-E yields the highest PSNR values since it is trained towards minimizing the per-pixel distance to the ground truth. The models trained with the perceptual loss all yield lower PSNRs as it allows for deviations in pixel intensities from the ground truth. It is those outliers that significantly lower the PSNR scores. The texture loss increases the PSNR values by reducing the artifacts from the adversarial loss term. Best results shown in bold. <ref type="figure">Figure 5</ref> gives an overview of different approaches including the current state of the art by PSNR <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> on the zebra image from Set14 which is particularly well-suited for a visual comparison since it contains both smooth and sharp edges, textured regions as well as repeating patterns. Previous methods have gradually improved on edge reconstruction, but even the state-of-the-art model DRCN suffers from blur in regions where the LR image doesn't provide any high frequency information. While ENet-E reproduces slightly sharper edges, the results exhibit the same characteristics as previous approaches. The perceptual loss from Johnson et al. <ref type="bibr" target="#b23">[24]</ref> produces only a slightly sharper image than ENet-E. On the other hand, ENet-PAT is the only model that produces significantly sharper images with realistic textures. Comparisons with further works including Johnson et al. <ref type="bibr" target="#b23">[24]</ref>, Bruna et al. <ref type="bibr" target="#b3">[4]</ref> and Romano et al. <ref type="bibr" target="#b41">[42]</ref> are shown in <ref type="figure">Fig. 6</ref> and in the supplementary. <ref type="table" target="#tab_4">Table 4</ref> summarizes the PSNR values of our approach in comparison to other approaches including the previous state of the art on various popular SISR benchmarks. ENet-E achieves state-of-the-art results on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with other approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Quantitative results by PSNR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Object recognition performance</head><p>It is known that super-resolution algorithms can be used as a preprocessing step to improve the performance of other image-related tasks such as face recognition <ref type="bibr" target="#b11">[12]</ref>. We propose to use the performance of state-of-the-art object recognition models as a metric to evaluate image reconstruction algorithms, especially for models whose performance is not captured well by PSNR, SSIM or IFC.</p><p>For evaluation, any pre-trained object recognition model M and labeled set of images may be used. The image restoration models to be evaluated are applied on a degraded version of the dataset and the reconstructed images are fed into M . The hypothesis is that the performance of powerful object recognition models shows a meaningful correlation with the human perception of image quality that may complement pixel-based benchmarks such as PSNR.</p><p>Similar indirect metrics have been applied in previous works, e.g., optical character recognition performance has been utilized to compare the quality of text deblurring algorithms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b54">55]</ref> and face-detection performance has been used for the evaluation of super-resolution algorithms <ref type="bibr" target="#b29">[30]</ref>. The performance of object recognition models has been used for the indirect evaluation of image colorization <ref type="bibr" target="#b64">[65]</ref>, where black and white images were colorized to improve object detection rates. Namboodiri et al. <ref type="bibr" target="#b35">[36]</ref> apply a metric similar to ours to evaluate SISR algorithms and found it to be a better metric than PSNR or SSIM for evaluating the perceptual quality of super-resolved images.</p><p>For our comparison, we use ResNet-50 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> as this class of models has achieved state-of-the-art performance by winning the 2015 Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b43">[44]</ref>. For the evaluation, we use the first 1000 images in the ILSVRC 2016 CLS-LOC validation dataset <ref type="bibr" target="#b1">2</ref> where each image has exactly one out of 1000 labels. The original images are scaled to 224×224 for the baseline and downsampled to 56×56 for a scaling factor of 4. We report the mean top-1 and top-5 errors as well as the mean confidence that ResNet reports on correct classifications. The results are shown in Tab. 5. In our comparison, some of the results roughly coincide with the PSNR scores, with bicubic interpolation resulting in the worst performance followed by DRCN <ref type="bibr" target="#b25">[26]</ref> and PSyCo <ref type="bibr" target="#b39">[40]</ref> which yield visually comparable images and hence similar scores as our ENet-E network. However, our models ENet-EA, ENet-PA and ENet-PAT produce images of higher perceptual quality which is reflected in higher classification scores despite their low PSNR scores. This indicates that the object recognition benchmark matches human perception better than PSNR does. The high scores of ENet-PAT are not a result of overfitting due to being trained with VGG, since even ENet-EA (which is not trained with VGG) gains higher scores than e.g. ENet-E, which has the highest PSNR but lower scores under this metric.</p><p>While we observe that the object recognition performance roughly coincides with the human perception of image quality in this benchmark for super-resolution, we leave a more detailed analysis of this evaluation metric on other image restoration problems to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>Glasner <ref type="bibr" target="#b16">[17]</ref> Kim <ref type="bibr" target="#b26">[27]</ref> SCSR <ref type="bibr" target="#b59">[60]</ref> SelfEx <ref type="bibr" target="#b21">[22]</ref> SRCNN <ref type="bibr" target="#b7">[8]</ref> PSyCo <ref type="bibr" target="#b39">[40]</ref> VDSR <ref type="bibr" target="#b24">[25]</ref> DRCN <ref type="bibr" target="#b25">[26]</ref> ENet-E ENet-PAT IHR <ref type="figure">Figure 5</ref>. A comparison of previous methods with our results at 4x super-resolution on an image from Set14. Previous methods have continuously improved upon the restoration of sharper edges yielding higher PSNR's, a trend that ENet-E continues with slightly sharper edges and finer details (e.g., area below the eye). With our texture-synthesizing approach, ENet-PAT is the only method that yields sharp lines and reproduces textures, resulting in the most realistic looking image. Furthermore, ENet-PAT produces high-frequency patterns missing completely in the LR image, e.g., lines on the zebra's forehead or the grass texture, showing that the model is capable of detecting and generating patterns that lead to a realistic image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Evaluation of perceptual quality</head><p>To further validate the perceptual quality of our results, we conducted a user study on the ImageNet dataset from the previous section. As a representative for models that minimize the Euclidean loss, we compare ENet-E as the new state of the art in PSNR performance with the images generated by ENet-PAT which have a PSNR comparable to images upsampled with bicubic interpolation. The subjects were shown the ground truth image along with the superresolution results of both ENet-E and ENet-PAT at 4x superresolution side-by-side, and were asked to select the image that looks more similar to the ground truth. In 49 survey responses for a total of 843 votes, subjects selected the image produced by ENet-PAT 91.0% of the time, underlining the perceptual quality of our results. For a screenshot of the survey and an analysis on the images where the blurry result by ENet-E was prefered, we refer the reader to the supplementary material.  <ref type="table">Table 5</ref>. ResNet object recognition performance and reported confidence on pictures from the ImageNet dataset downsampled to 56×56 before being upscaled by a factor of 4 using different algorithms. The baseline shows ResNet's performance on the original 224×224 sized images. Compared to PSNR, the scores correlate better with the human perception of image quality: ENet-E achieves only slightly higher scores than DRCN or PSyCo since all these models minimize pixel-wise MSE. On the other hand, ENet-PAT achieves higher scores as it produces sharper images and more realistic textures. The good results of ENet-EA which is trained without VGG indicate that the high scores of ENet-PAT are not solely due to being trained with VGG, but likely a result of sharper images. Best results shown in bold.</p><p>Johnson et al. <ref type="bibr" target="#b23">[24]</ref> ENet-PAT IHR <ref type="figure">Figure 6</ref>. Comparing our model with a result from Johnson et al. <ref type="bibr" target="#b23">[24]</ref> on an image from BSD100 at 4x super-resolution. ENet-PAT's result looks more natural and does not contain checkerboard artifacts despite the lack of an additional regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training and inference speed</head><p>For training, we use all color images in MSCOCO <ref type="bibr" target="#b30">[31]</ref> that have at least 384 pixels on the short side resulting in roughly 200k images. All images are cropped centrally to a square and then downsampled to 256×256 to reduce noise and JPEG artifacts. During training, we fix the size of the input I LR to 32×32. As the scale of objects in the MSCOCO dataset is too small when downsampled to such a small size, we downsample the 256×256 images by α and then crop these to patches of size 32×32. After training the model for any given scaling factor α, the input to the fully convolutional network at test time can be an image of arbitrary dimensions w×h which is then upscaled to (αw)×(αh).</p><p>We trained all models for a maximum of 24 hours on an Nvidia K40 GPU using TensorFlow <ref type="bibr" target="#b1">[2]</ref>, though convergence rates depend on the applied combination of loss functions. Although not optimized for efficiency, our network is compact and quite fast at test time. The final trained model is only 3.1MB in size and processes images in 9ms (Set5), 18ms (Set14), 12ms (BSD100) and 59ms (Urban100) on average per image at 4x super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion, limitations and future work</head><p>We have proposed an architecture that is capable of producing state-of-the-art results by both quantitative and qualitative measures by training with a Euclidean loss or a novel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>ENet-PAT IHR <ref type="figure">Figure 7</ref>. Failure case on an image from BSD100. ENet-PAT has learned to continue high-frequency patterns since they are often lost in ILR at smaller scales. While that works out extremely well in most cases (c.f . zebra's forehead <ref type="figure">Fig. 5</ref>), the model fails in this notable case since IHR is actually smooth in that region.</p><p>combination of adversarial training, perceptual losses and a newly proposed texture transfer loss for super-resolution.</p><p>Once trained, the model interpolates full color images in a single forward-pass at competitive speeds. As SISR is a heavily ill-posed problem, some limitations remain. While images produced by ENet-PAT look realistic, they do not match the ground truth images on a pixelwise basis. Furthermore, the adversarial training sometimes produces artifacts in the output which are greatly reduced but not fully eliminated with the addition of the texture loss.</p><p>We noted an interesting failure on an image in the BSD100 dataset that is shown in <ref type="figure">Fig. 7</ref>, where the model continues a pattern visible in the LR image onto smooth areas. This is a result of the model learning to hallucinate textures that occur frequently between pairs of LR and HR images such as repeating stripes that fade in the LR image as they increasingly shrink in size.</p><p>While the model is already competitive in terms of its runtime, future work may decrease the depth of the network and apply shrinking methods to speed up the model to real-time performance on high-resolution data: adding a term for temporal consistency could then enable the model to be used for video super-resolution. We refer the reader to the supplementary material for more results, further details and additional comparisons. A reference implementation of ENet-PAT can be found on the project website at http://webdav.tue.mpg.de/pixel/enhancenet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparing the new state of the art by PSNR (ENet-E) with the sharper, perceptually more plausible result produced by ENet-PAT at 4x super-resolution on an image from ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Toy example to illustrate the effect of the Euclidean loss and how maximizing the PSNR does not lead to realistic results. (I) The HR images consist of randomly placed vertical and horizontal bars of 1×2 pixels. (II) In ILR, the original orientations cannot be distinguished anymore since both types of bars turn into a single pixel. (III) A model trained to minimize the Euclidean loss produces the mean of all possible solutions since this yields the lowest MSE but the result looks clearly different from the original images IHR. (IV) Training a model with an adversarial loss ideally results in a sharp image that is impossible to distinguish from the original HR images, although it does not match IHR exactly since the model cannot know the orientation of each bar. Intriguingly, this result has a lower PSNR than the blurry MSE sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Conv Residual image I res Output I est = I bicubic + I res</figDesc><table>Conv, ReLU 
Residual: Conv, ReLU, Conv 
. . . 

2w × 2h × 64 
2x nearest neighbor upsampling 
Conv, ReLU 

4w × 4h × 64 

2x nearest neighbor upsampling 
Conv, ReLU 
Conv, ReLU 

4w × 4h × c 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>The same network trained with varying loss functions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>which have been found to correlate better with human perception [57] also do not capture the perceptual quality of the results, so we provide alternative quantitative evaluations that agree better with human perception in Sec. 5.2.2 and 5.2.3. Dataset Bicubic ENet-E ENet-P ENet-EA ENet-PA ENet-EAT ENet-PAT</figDesc><table>Set5 
28.42 
31.74 
28.28 
28.15 
27.20 
29.26 
28.56 
Set14 
26.00 
28.42 
25.64 
25.94 
24.93 
26.53 
25.77 
BSD100 
25.96 
27.50 
24.73 
25.71 
24.19 
25.97 
24.93 
Urban100 
23.14 
25.66 
23.75 
23.56 
22.51 
24.16 
23.54 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>PSNR for different methods at 4x super-resolution. ENet-E achieves state-of-the-art results on all datasets. Best performance shown in bold. Further results as well as SSIM and IFC scores on varying scaling factors are given in the supplementary.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Evaluation Bicubic DRCN [26] PSyCo [40] ENet-E ENet-EA ENet-PA ENet-PAT Baseline</figDesc><table>Top-1 error 
0.506 
0.477 
0.454 
0.449 
0.407 
0.429 
0.399 
0.260 
Top-5 error 
0.266 
0.242 
0.224 
0.214 
0.185 
0.199 
0.171 
0.072 
Confidence 
0.754 
0.727 
0.728 
0.754 
0.760 
0.783 
0.797 
0.882 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Long et al. [32] introduce them as deconvolution layers which may be misleading since no actual deconvolution is performed. Other names for convolution transpose layers include upconvolution, fractionally strided convolution or simply backwards convolution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the validation dataset since the annotations for the test dataset are not released. However, even a potential bias of the ResNet-model would not invalidate the results, since higher scores only imply that the upscaled images are closer to the originals under the proposed metric.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vgg19</surname></persName>
		</author>
		<ptr target="https://github.com/machrisaa/tensorflow-vgg" />
		<imprint>
			<date type="published" when="2016-06-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution with deep convolutional sufficient statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="https://github.com/ry/tensorflow-resnet" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>ResNet in tensorflow</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluation of image resolution and super-resolution on face recognition performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image upsampling via texture hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for direct text deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradiš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemcík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving resolution by image registration. CVGIP: Graphical models and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="231" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Perceptual image quality assessment using a normalized laplacian pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802v3</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Superresolved faces for improved face recognition from surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometry constrained sparse coding for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Super-resolution Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Systematic evaluation of super-resolution using classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VCIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Super-resolution: A comprehensive survey. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1423" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://distill.pub/2016/deconv-checkerboard/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PSyCo: Manifold span reduction for super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">RAISR: Rapid and accurate image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Isidoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. De</forename><surname>Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Local-and holisticstructure preserving image super resolution via deep joint component learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Super resolution using edge prior and single image detail synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning high-order filters for efficient blind deconvolution of document photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting selfsimilarities for single frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Single-image superresolution: a benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image superresolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Landmark image super-resolution by retrieving web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4865" to="4878" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-scale dictionary for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
