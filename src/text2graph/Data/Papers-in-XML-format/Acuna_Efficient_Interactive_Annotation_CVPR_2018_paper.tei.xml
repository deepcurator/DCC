<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
							<email>davidj@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
							<email>linghuan@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Kar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>fidler@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detailed reasoning about structures in images is a necessity for numerous computer vision applications. For example, it is crucial in the domain of autonomous driving to localize and outline all cars, pedestrians, and miscellaneous static and dynamic objects <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>. For mapping, there is a need to obtain detailed footprints of buildings and roads from aerial/satellite imagery <ref type="bibr" target="#b33">[34]</ref>, while medical/healthcare domains require automatic methods to precisely outline cells, tissues and other relevant structures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Neural networks have proven to be an effective way of inferring semantic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref> and object instance segmentation <ref type="bibr">Figure 1:</ref> We introduce Polygon-RNN++, an interactive object annotation tool. We make several advances over <ref type="bibr" target="#b3">[4]</ref>, allowing us to annotate objects faster and more accurately. Furthermore, we exploit a simple online fine-tuning method to adapt our model from one dataset to efficiently annotate novel, out-of-domain datasets.</p><p>information <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref> in challenging imagery. It is well known that the amount and variety of data that the networks see during training drastically affects their performance at run time. Collecting ground truth instance masks, however, is an extremely time consuming task, typically requiring human annotators to spend 20-30 seconds per object in an image.</p><p>To this end, in <ref type="bibr" target="#b3">[4]</ref>, the authors introduced Polygon-RNN, a conceptual model for semi-automatic and interactive labeling to help speed up object annotation. Instead of producing pixel-wise segmentation of an object as in existing interactive tools such as Grabcut <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b3">[4]</ref> predicts the vertices of a polygon that outlines the object. The benefits of using a polygon representation are three-fold, 1) it is sparse (only a few vertices represent regions with a large number of pixels), 2) it is easy for an annotator to interact with, and 3) it allows for efficient interaction, typically requiring only a few corrections from the annotator <ref type="bibr" target="#b3">[4]</ref>. Using their model, the authors have shown high annotation speed-ups on two autonomous driving datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>In this work, we introduce several improvements to the Polygon-RNN model. In particular, we 1) make a few changes to the neural network architecture, 2) propose a better learning algorithm to train the model using reinforcement learning, and 3) show how to significantly increase the out-put resolution of the polygon (one of the main limitations of the original model) using a Graph Neural Network <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17]</ref>. We analyze the robustness of our approach to noise, and its generalization capabilities to out-of-domain imagery.</p><p>In the fully automatic mode (no annotator in the loop), our model achieves significant improvements over the original approach, outperforming it by 10% mean IoU on the Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref>. In interactive mode, our approach requires 50% fewer clicks as compared to <ref type="bibr" target="#b3">[4]</ref>. To demonstrate generalization, we use a model trained on the Cityscapes dataset to annotate a subset of a scene parsing dataset <ref type="bibr" target="#b40">[41]</ref>, aerial imagery <ref type="bibr" target="#b32">[33]</ref>, and two medical datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref>. The model significantly outperforms strong pixel-wise labeling baselines, showcasing that it inherently learns to follow object boundaries, thus generalizing better. We further show that a simple online fine-tuning approach achieves high annotation speed-ups on out-of-domain dataset annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Interactive annotation. Since object instance segmentation is time consuming to annotate manually, several works have aimed at speeding up this process using interactive techniques. In seminal work, <ref type="bibr" target="#b1">[2]</ref> used scribbles to model the appearance of foreground/background, and performed segmentation via graph-cuts <ref type="bibr" target="#b2">[3]</ref>. This idea was extended by <ref type="bibr" target="#b19">[20]</ref> to use multiple scribbles on both the object and background, and was demonstrated in annotating objects in videos. GrabCut <ref type="bibr" target="#b29">[30]</ref> exploited 2D bounding boxes provided by the annotator, and performed pixel-wise foreground/background labeling using EM. <ref type="bibr" target="#b24">[25]</ref> combined GrabCut with CNNs to annotate structures in medical imagery. Most of these works operate on the pixel level, and typically have difficulties in cases where foreground and background have similar color.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, the authors used polygons instead. The main power of using such a representation is that it is sparse; only a few vertices of a polygon represent large image regions. This allows the user to easily introduce corrections, by simply moving the wrong vertices. An RNN also effectively captures typical shapes of objects as it forms a non-linear sequential representation of shape. This is particularly important in ambiguous regions, ie shadows and saturation, where boundaries cannot be observed. We follow this line of work, and introduce several important modifications to the architecture and training. Furthermore, the original model was only able to make prediction at a low resolution (28 × 28), thus producing blocky polygons for large objects. Our model significantly increases the output resolution (112 × 112).</p><p>Object instance segmentation. Most approaches to object instance segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18</ref>] operate on the pixel-level. Many rely on object detection, and use a convnet over a box proposal to perform the labeling <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref>. In <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b32">33]</ref>, the authors produce a polygon around an object. These approaches first detect boundary fragments, followed by finding an optimal cycle linking the boundaries into object regions. <ref type="bibr" target="#b8">[9]</ref> produce superpixels in the form of small polygons which are further combined into an object. Here, as in <ref type="bibr" target="#b3">[4]</ref> we use neural networks to produce polygons, and in particular tackle the interactive labeling scenario which has not been explored in these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Polygon-RNN++</head><p>In this section, we introduce Polygon-RNN++. Following <ref type="bibr" target="#b3">[4]</ref>, our model expects an annotator to provide a bbox around the object of interest. We extract an image crop enclosed by the 15% enlarged box. We use a CNN+RNN architecture as in <ref type="bibr" target="#b3">[4]</ref>, with a CNN serving as an image feature extractor, and the RNN decoding one polygon vertex at a time. Output vertices are represented as locations in a grid.</p><p>The full model is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. Our redesigned encoder produces image features that are used to predict the first vertex. The first vertex and the image features are then fed to the recurrent decoder. Our RNN exploits visual attention at each time step to produce polygon vertices. A learned evaluator network selects the best polygon from a set of candidates proposed by the decoder. Finally, a graph neural network re-adjusts polygons, augmented with additional vertices, at a higher resolution.</p><p>This model naturally incorporates a human in the loop, allowing the annotator to correct an erroneously predicted vertex. This vertex is then fed back to the model, helping the model to correct its prediction in the next time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual Encoder with Skip Connections</head><p>Most networks perform repeated down-sampling operations at consecutive layers of a CNN, which impacts the effective output resolution in tasks such as image segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. In order to alleviate this issue, we follow <ref type="bibr" target="#b6">[7]</ref> and modify the ResNet-50 architecture <ref type="bibr" target="#b12">[13]</ref> by reducing the stride of the network and introducing dilation factors. This allows us to increase the resolution of the output feature map without reducing the receptive field of individual neurons. We also remove the original average pooling and FC layers.</p><p>We further add a skip-layer architecture <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40]</ref> which aims to capture both, low-level details such as edges and corners, as well as high-level semantic information. In <ref type="bibr" target="#b3">[4]</ref>, the authors perform down-sampling in the skip-layer architecture, built on top of VGG, before concatenating the features from different layers. Instead, we concatenate all the outputs of the skip layers at the highest possible resolution, and use a combination of conv layers and max-pooling operations to obtain the final feature map. We employ conv filters with a kernel size of 3 × 3, batch normalization <ref type="bibr" target="#b13">[14]</ref> and ReLU non-linearities. In cases where the skip-connections have different spatial dimensions, we use bilinear upsampling before concatenation. The architecture is shown in <ref type="figure">Fig. 4</ref>. We refer to the final feature map as the skip features.   <ref type="figure">Figure 4</ref>: Residual Encoder architecture. Blue tensor is fed to GNN, while the orange tensor is input to the RNN decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Decoder</head><p>As in <ref type="bibr" target="#b3">[4]</ref>, we use a Recurrent Neural Network to model the sequence of 2D vertices of the polygon outlining an object. In line with previous work, we also found that the use of Convolutional LSTM <ref type="bibr" target="#b35">[36]</ref> is essential: 1) to preserve spatial information and 2) to reduce the number of parameters to be learned. In our RNN, we further add an attention mechanism, as well as predict the first vertex within the same network (unlike <ref type="bibr" target="#b3">[4]</ref> which has two separate networks).</p><p>We use a two-layer ConvLTSM with a 3 × 3 kernel with 64 and 16 channels, respectively. We apply batch norm <ref type="bibr" target="#b13">[14]</ref> at each time step, without sharing mean/variance estimates across time steps. We represent our output at time step t as a one-hot encoding of (D × D) + 1 elements, where D is the resolution at which we predict. In our experiments, D is set to 28. The first D × D dimensions represent the possible vertex positions and the last dimension corresponds to the end-of-seq token that signals that the polygon is closed. Attention Weighted Features: In our RNN, we exploit a mechanism akin to attention. In particular, at time step t, we compute the weighted feature map as,</p><formula xml:id="formula_0">α t = softmax(f att (x, f 1 (h 1,t−1 ), f 2 (h 2,t−1 ))) F t = x • α t (1)</formula><p>where • is the Hadamard product, x is the skip feature tensor, and h 1,t , h 2,t are the hidden state tensors from the twolayer ConvLSTM. Here, f 1 and f 2 map h 1,t and h 2,t to R D×D×128 using one fully-connected layer. f att takes the sum of its inputs and maps it to D × D through a fully connected layer, giving one "attention" weight per location.</p><p>Intuitively, we use the previous RNN hidden state to gate certain locations in the image feature map, allowing the RNN to focus only on the relevant information in the next time step. The gated feature map F t is then concatenated with one-hot encodings of the two previous vertices y t−1 , y t−2 and the first vertex y 0 , and passed to the RNN at time step t. First Vertex: Given a previous vertex and an implicit direction, the next vertex of a polygon is always uniquely defined, except for the first vertex. To tackle this problem, the authors in <ref type="bibr" target="#b3">[4]</ref> treated the first vertex as a special case and used an additional architecture (trained separately) to predict it. In our model, we add another branch from the skip-layer architecture, constituting of two layers each of dimension D × D. Following <ref type="bibr" target="#b3">[4]</ref>, the first layer predicts edges, while the second predicts the vertices of the polygon. At test time, the first vertex is sampled from the final layer of this branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training using Reinforcement Learning</head><p>In <ref type="bibr" target="#b3">[4]</ref>, the authors trained the model using the crossentropy loss at each time step. However, such training has two major limitations: 1) MLE over-penalizes the model (for example when the predicted vertex is on an edge of the GT polygon but is not one of the GT vertices), and 2) it optimizes a metric that is very different from the final evaluation metric (i.e. IoU). Further, the model in <ref type="bibr" target="#b3">[4]</ref> was trained following a typical training regime where the GT vertex is fed to the next time step instead of the model's prediction. This training regime, called teacher forcing creates a mismatch between training and testing known as the exposure bias problem <ref type="bibr" target="#b25">[26]</ref>.</p><p>In order to mitigate these problems, we only use MLE training as an initialization stage. We then reformulate the polygon prediction task as a reinforcement learning problem and fine-tune the network using RL. During this phase, we let the network discover policies that optimize the desirable, yet non-differentiable evaluation metric (IoU) while also exposing it to its own predictions during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Problem formulation</head><p>We view our recurrent decoder as a sequential decision making agent. The parameters θ of our encoder-decoder architecture define its policy p θ for selecting the next vertex v t . At the end of the sequence, we obtain a reward r. We compute our reward as the IoU between the mask enclosed by the generated polygon and the ground-truth mask m. To maximize the expected reward, our loss function becomes</p><formula xml:id="formula_1">L(θ) = −E v s ∼p θ [r(v s , m)]<label>(2)</label></formula><p>where v s = (v </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Self-Critical Training with Policy Gradients</head><p>Using the REINFORCE trick <ref type="bibr" target="#b34">[35]</ref> to compute the gradients of the expectation, we have</p><formula xml:id="formula_2">∇L(θ) = −E v s ∼p θ [r(v s , m)∇ log p θ (v s )]<label>(3)</label></formula><p>In practice, the expected gradient is computed using simple Monte-Carlo sampling with a single sample. This procedure is known to exhibit high variance and is highly unstable without proper context-dependent normalization. A natural way to deal with this is to use a learned baseline which is subtracted from the reward. In this work, we follow the selfcritical method <ref type="bibr" target="#b27">[28]</ref> and use the test-time inference reward of our model as the baseline. Accordingly, we reformulate the gradient of our loss function to be</p><formula xml:id="formula_3">∇L(θ) = −[(r(v s , m) − r(v s , m))∇ log p θ (v s )] (4)</formula><p>where r(v s , m) is the reward obtained using greedy decoding. To control the level of randomness in the vertices explored by the model, we introduce a temperature parameter τ in the softmax of the policy. This ensures that the sampled vertices lead to well behaved polygons. We set τ = 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluator Network</head><p>Smart choice of the first vertex is crucial as it biases the initial predictions of the RNN, when the model does not have a strong history to reason about the object to annotate. This is particularly important in cases of occluding objects. It is desirable for the first vertex to be far from the occlusion boundaries so that the model follows the object of interest. In RNNs, beam search is typically used to prune off improbable sequences. However, since classical beam search uses log probabilities to evaluate beams, it does not directly apply to our model which aims to optimize IoU. A point on an occlusion boundary generally exhibits a strong edge and thus would have a high log probability during prediction, reducing the chances of it being pruned by beam search.</p><p>In order to solve this problem, we propose to use an evaluator network at inference time, aiming to effectively choose among multiple candidate polygons. Our evaluator network takes as input the skip features, the last state tensor of the ConvLSTM, and the predicted polygon, and tries to estimate its quality by predicting its IoU with GT. The network has two 3 × 3 convolutional layers followed by a FC layer, forming another branch in the model. <ref type="figure">Fig. 3</ref> depicts its architecture. While the full model can be trained end-to-end during the RL step, we choose to train the evaluator network separately after the RL fine-tuning has converged.</p><p>During training, we minimize the mean squared error</p><formula xml:id="formula_4">L(φ) = [p(φ, v s ) − IoU(m v s , m)] 2<label>(5)</label></formula><p>where p is the network's predicted IoU, m v s is the mask for the sampled vertices and m is the ground-truth mask. To ensure diversity in the vertices seen, we sample polygons with τ = 0.3. We emphasize that we do not use this network as a baseline estimator during the RL training step since we found that the self-critical method produced better results.</p><p>Inference: At test time, we take K top scoring first vertex predictions. For each of these, we generate polygons via classical beam-search (using log prob with a beam-width B). This yields K different polygons, one for each first vertex candidate. We use the evaluator network to choose the best polygon. In our experiments, we use K = 5. While one could use the evaluator network instead of beam-search at each time step, this would lead to impractically long inference times. Our faster full model (using B = K = 1) runs at 295ms per object instance on a Titan XP.</p><p>Annotator in the Loop: We follow the same protocol as in <ref type="bibr" target="#b3">[4]</ref>, where the annotator corrects the vertices in sequential order. Each correction is then fed back to the model, which re-predicts the rest of the polygon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Upscaling with a Graph Neural Network</head><p>The model described above produces polygons at a resolution of D × D, where we set D to be 28 to satisfy memory bounds and to keep the cardinality of the output space amenable. In this section, we exploit a Gated Graph Neural Network (GGNN) <ref type="bibr" target="#b16">[17]</ref>, in order to generate polygons at a much higher resolution. GNN has been proven efficient for semantic segmentation <ref type="bibr" target="#b23">[24]</ref>, where it was used at pixel-level.</p><p>Note that when training the RNN decoder, the GT polygons are simplified at their target resolution (co-linear vertices are removed) to alleviate the ambiguity of the prediction task. Thus, at a higher resolution, the object may have additional vertices, thus changing the topology of the polygon.</p><p>Our upscaling model takes as input the sequence of vertices generated by the RNN decoder. We treat these vertices as nodes in a graph. To model finer details at a higher resolution, we add a node in between two consecutive nodes, with its location being in the middle of their corresponding edge. We also connect the last and the first vertex, effectively converting the sequence into a cycle. We connect neighboring nodes using 3 different types of edges, as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p><p>GGNN defines a propagation model that extends RNNs to arbitrary graphs, effectively propagating information between nodes, before producing an output at each node. Here, we aim to predict the relative offset of each node (vertex) at a higher resolution. The model is visualized in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p><p>Gated Graph Neural Network: For completeness, we briefly summarize the GGNN model <ref type="bibr" target="#b16">[17]</ref>. GGNN uses a graph {V, E}, where V and E are the sets of nodes and edges, respectively. It consists of a propagation model performing message passing in the graph, and an output model for prediction tasks. We represent the initial state of a node v as x v and the hidden state of node v at time step t as h recurrence of the propagation model is</p><formula xml:id="formula_5">h 0 v = [x ⊤ v , 0] ⊤ a t v = A ⊤ v: [h t−1 1 ⊤ , ..., h t−1 |V | ⊤ ] ⊤ + b h t v = f GRU (h t−1 v , a t v )<label>(6)</label></formula><p>The matrix A ∈ R |V |×2N |V | determines how the nodes in the graph communicate with each other, where N represents the number of different edge types. Messages are propagated for T steps. The output for node v is then defined as</p><formula xml:id="formula_6">h v = tan(f 1 (h T v )) out v = f 2 (h v )<label>(7)</label></formula><p>Here, f 1 and f 2 are MLP, and out v is v's desired output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PolygonRNN++ with GGNN:</head><p>To get observations for our GGNN model, we add another branch on top of our skiplayer architecture, specifically, from the 112 × 112 × 256 feature map (marked in blue in <ref type="figure">Fig. 4</ref>). We exploit a conv layer with 256 filters of size 15 × 15, giving us a feature map of size 112 × 112 × 256. For each node v in the graph, we extract a S × S patch around the scaled (v x , v y ) location, giving us the observation vector x v . After propagation, we predict the output of a node v as a location in a D ′ × D ′ spatial grid. We make this grid relative to the location (v x , v y ), rendering the prediction task to be a relative displacement with respect to its initial position. This prediction is treated as a classification task and the model is trained with the cross entropy loss. In particular, in order to train our model, we first take predictions from the RNN decoder, and correct a wrong prediction if it deviates from the ground-truth vertex by more than a threshold. The targets for training our GGNN are then the relative displacements of each of these vertices with respect to their corresponding ground-truth vertices. Implementation details: We set S to 1 and D ′ to 112. While our model supports much higher output resolutions, we found that larger D ′ did not improve results. The hidden state of the GRU in the GGNN has 256 dimensions. We use T = 5 propagation steps. In the output model, f 1 is a 256 × 256 FC layer and f 2 is a 256 × 15 × 15 MLP. In training, we take the predictions from the RNN, and replace vertices with GT vertices if they deviate by more than 3 cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Annot. New Domains via Online Fine-Tuning</head><p>We now also tackle the scenario in which our model is trained on one dataset, and is used to annotate a novel dataset. As the new data arrives, the annotator uses our model to annotate objects and corrects wrong predictions when necessary. We propose a simple approach to fine-tune our model in such a scenario, in an online fashion.</p><p>Let us denote C as the number of chunks the new data is divided into, CS as the chunk size, N EV as the number of training steps for the evaluator network and N M LE , N RL as the number of training steps for each chunk with MLE and RL, respectively. Our online fine-tuning is described in Algorithm 1 where P redictAndCorrect refers to the (simulated) annotator in the loop. Because we train on corrected data, we smooth our targets for MLE training with a manhattan distance transform truncated at distance 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we provide an extensive evaluation of our model. We report both automatic and interactive instance annotation results on the challenging Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref> and compare with strong pixel-wise methods. We then characterize the generalization capability of our model with evaluation on the KITTI dataset <ref type="bibr" target="#b9">[10]</ref> and four out-of-domain datasets spanning general scenes <ref type="bibr" target="#b40">[41]</ref>, aerial <ref type="bibr" target="#b32">[33]</ref>, and medical imagery <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref>. Finally, we evaluate our online fine-tuning scheme, demonstrating significant decrease in annotation time for novel datasets. Note that as in <ref type="bibr" target="#b3">[4]</ref>, we assume that user-provided ground-truth boxes around objects are given. We further analyze robustness of our model to noise with respect to these boxes, mimicking noisy annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">In-Domain Annotation</head><p>We first evaluate our approach in training and evaluating on the same domain. This mimics the scenario where one takes an existing dataset, and uses it to annotate novel images from the same domain. In particular, we use the Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref>, which is currently one of the most comprehensive benchmarks for instance segmentation. It contains 2975 training, 500 validation and 1525 test images with 8 semantic classes. To ensure a fair comparison, we follow the same alternative split proposed by <ref type="bibr" target="#b3">[4]</ref>. As in <ref type="bibr" target="#b3">[4]</ref>, we preprocess    the ground-truth polygons according to depth ordering to obtain polygons for only the visible regions of each instance. Evaluation Metrics: We utilize two quantitative measures to evaluate our model. 1) We use the intersection over union (IoU) metric to evaluate the quality of the generated polygons and 2) we calculate the number of annotator clicks required to correct the predictions made by the model. We describe the correction protocol in detail in a subsequent section. Baselines: Following <ref type="bibr" target="#b3">[4]</ref>, we compare with DeepMask <ref type="bibr" target="#b20">[21]</ref>, SharpMask <ref type="bibr" target="#b21">[22]</ref>, as well as Polygon-RNN <ref type="bibr" target="#b3">[4]</ref> as state-of-the-art baselines. Note that the first two approaches are pixel-wise methods and errors in their output cannot easily be corrected by an annotator. To be fair, we only compare our automatic mode with their approaches. In their original approach, <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> exhaustively sample patches at different scales over the entire image. Here, we evaluate <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> by providing exact ground-truth boxes to their models. As per <ref type="bibr" target="#b3">[4]</ref>, we also include SquareBox, which considers the provided bounding box as its prediction. Automatic Mode: We compare Polygon-RNN++ to the baselines in <ref type="table">Table 1</ref>, and ablate the use of each of the components in our model. Here, Residual Polygon-RNN refers to the original model with our image encoder instead of VGG. Our full approach outperforms all competitors by 10% IoU, and achieves best performance for each class. Moreover, Polygon-RNN++ surpasses the reported human agreement <ref type="bibr" target="#b3">[4]</ref> of 78.6% IoU on cars, on average. Using human agreement on cars as a proxy, we observe that the model also obtains human-level performance for truck and bus. Interactive Mode: The interactive mode aims to minimize annotation time while obtaining high quality annotations. Following the simulation proposed in <ref type="bibr" target="#b3">[4]</ref>, we calculate the number of annotator clicks required to correct predictions from the model. The annotator corrects a prediction if it deviates from the corresponding GT vertex by a min distance of T , where the hyperparameter T governs the quality of the produced annotations. For fair comparison, distances are computed using manhattan distance at the model output resolution using distance thresholds T ∈ [1, 2, 3, 4], as in <ref type="bibr" target="#b3">[4]</ref>.</p><p>We introduce another threshold T 2 , defined as the IoU between the predicted polygon and the GT mask. We consider polygons with agreement above T 2 unnecessary for the annotator to interfere. We use this threshold due to the somewhat unsatisfactory correction simulation above: for example, if the predicted vertex falls along a GT edge, this vertex is in fact correct and should not be corrected. Note that when T 2 = 1, simulation is equivalent to the one above.</p><p>In <ref type="figure" target="#fig_4">Fig. 6</ref>, we compare the average number of clicks per instance required to annotate all classes on the Cityscapes val set with different values of T 2 . Using T 2 = 1, we see that our model outperforms <ref type="bibr" target="#b3">[4]</ref>, requiring fewer clicks to obtain the same IoU. Even at T 2 = 0.8 our model is still more accurate than <ref type="bibr" target="#b3">[4]</ref> at T 2 = 1.0. At T 2 = 0.7, we achieve over 80% mIoU with only 5 clicks per object on average, which is a reduction of more than 50% over <ref type="bibr" target="#b3">[4]</ref>. <ref type="figure" target="#fig_5">Fig. 7</ref> shows frequency of required corrections for different T at T 2 = 0.8. In Sec.4.4, we show results with real human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to bbox noise:</head><p>We analyze the effect of noise in the bbox provided to the model. We randomly expand the box by a percentage of its width and height. Results in <ref type="table" target="#tab_5">Table 4</ref> illustrates that our model is very robust to some amount of noise (0-5%). Even in the presence of moderate and large noise (5-10%,10-15%), it outperforms the reported performance of previous baselines which use perfect boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Instance-Level Segmentation</head><p>We evaluate our model on (automatic) full-image instance segmentation, by exploiting FasterRCNN <ref type="bibr" target="#b26">[27]</ref> to detect objects. Polygon-RNN++ with FasterRCNN achieves 22.8% AP and 42.6% AP 50 on Cityscapes test. Following <ref type="bibr" target="#b17">[18]</ref>, we also add semantic segmentation <ref type="bibr" target="#b39">[40]</ref> to post-process the results. We simply perform a logical "and" operation be-     tween the predicted class-semantic map and our prediction. We achieve 25.49% AP and 45.47% AP 50 on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cross-Domain Evaluation</head><p>Here we analyze performance on different datasets that capture both shifts in environment (KITTI) and domain (general scenes, aerial, medical). We first use our model trained on Cityscapes without any fine-tuning on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI [10]:</head><p>We use Polygon-RNN++ to annotate 741 instances of KITTI <ref type="bibr" target="#b9">[10]</ref> provided by <ref type="bibr" target="#b4">[5]</ref>. The results in automatic mode are reported in <ref type="table" target="#tab_4">Table 3</ref> and the performance with a human in the loop is illustrated in <ref type="figure" target="#fig_6">Fig. 8</ref>. Our method outperforms all baselines showcasing its robustness to change in environment while being in a similar domain. With an annotator in the loop, our model requires on average 5 fewer clicks than <ref type="bibr" target="#b3">[4]</ref> to achieve the same IoU. It achieves human level agreement of 85% as reported by <ref type="bibr" target="#b4">[5]</ref> by requiring only 2 clicks on average by the (simulated) annotator. Out-of-Domain Imagery We consider datasets with a large domain shift wrt the Cityscapes dataset in order to evaluate the generalization capabilities of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE20K [41]:</head><p>The ADE20K dataset is a challenging general scene parsing dataset containing 20,210 images in the training set, 2,000 images in the validation set, and 3,000 images in the testing set. We select the following subset of categories from the validation set: television receiver, bus, car, oven, person and bicycle in our evaluation.</p><p>Aerial <ref type="bibr" target="#b32">[33]</ref>: Rooftop dataset consists of 65 aerial images of rural scenes containing several building rooftops, a majority of which exhibit fairly complex polygonal geometry. Performance for this dataset is reported for the test set.</p><p>Medical Imagery <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b10">11]</ref>: We use two medical segmentation datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> and <ref type="bibr" target="#b10">[11]</ref> for our experiments. The former, used in the Left Ventricle Segmentation Challenge <ref type="bibr" target="#b31">[32]</ref>, divides the data of 200 patients equally in the training and validation sets. We report the performance on a subset of the validation set which only includes the outer contours that segment the epicardium. The latter provides two image stacks (training and testing) each containing 20 sections from serial section Transmission Electron Microscopy (ssTEM) images of the ventral nerve cord. We use the mitochondria and synapse segmentations from this data to test our model. Since ground-truth instances for the test set are not publicly available, we evaluate on the training set.</p><p>Quantitative Results: For out-of-domain datasets, we introduce a baseline (named Ellipse) which fits an ellipse into the GT bounding box which is motivated by the observation that many instances in <ref type="bibr" target="#b31">[32]</ref> are ellipses. We show results with perfect and expanded bounding boxes (expansion similar to our model) for Square Box and Ellipse. DeepMask and SharpMask were evaluated with perfect bounding boxes with the threshold suggested by the authors. <ref type="table" target="#tab_3">Table 2</ref>, demonstrates high generalization capabilities of our model.</p><p>Online Fine-tuning: In these experiments, our simulated annotator has parameters T = 1 and T 2 = 0.8. <ref type="figure" target="#fig_7">Fig. 9</ref> reports the percentage of clicks saved with respect to GT polygons for our Cityscapes model and the online fine-tuned models. We see that our adaptive approach overcomes stark domain shifts with as few as one chunk of data (40 images for Sunnybrook, 3 for ssTEM, 200 for ADE and 20 for Aerial) showcasing strong generalization. Overall, we show at least 65% overall reduction in the number of clicks across all datasets, with the numbers almost at 100% for the Sunnybrook Cardiac MR dataset. We believe these results pave the way towards a real annotation tool that can learn along with the annotator and significantly reduce human effort. <ref type="figure" target="#fig_3">Fig. 10</ref> shows example predictions obtained in automatic mode on Cityscapes. We illustrate the improvements from specific parts of the model in <ref type="figure" target="#fig_3">Fig. 11</ref>. We see how using RL and the evaluator network leads to crisper predictions, while the GGNN upscales, adds points and builds a polygon resembling human annotation. <ref type="figure" target="#fig_0">Fig. 12</ref> showcases automatic predictions from PolygonRNN++ on the out-of-domain datasets. We remind the reader that this labeling is obtained by exploiting GT bounding boxes, and no fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Interaction with Human Annotators</head><p>We also conducted a small scale experiment with real human annotators in the loop. To this end, we implemented a very simple annotation tool that runs our model at the backend. We use 54 car instances from Cityscapes as per <ref type="bibr" target="#b3">[4]</ref>. We asked two human subjects to annotate these interactively using our model, and two to annotate manually. While we explain how the tool works, we do not train the annotators to use our tool. All our annotators were in-house.</p><p>Timing begins when an annotator first clicks on an object, and stops when the "submit" button is clicked. While using our model, the annotator needs to draw a bounding box around the object, which we include in our reported timing. Note that we display the object to the annotator by cropping an image inside an enlarged box. Thus our annotators are fast in drawing the boxes, taking around 2 seconds on average.</p><p>We report results in <ref type="table" target="#tab_7">Table 5</ref>. Annotators are 3x faster when using our model, with only slightly lower IoU agreement with GT. Note that our tool has scope for improvement in various engineering aspects. <ref type="bibr" target="#b3">[4]</ref> reported that on these examples, human subjects needed on average 42.2 sec per object using GrabCut <ref type="bibr" target="#b29">[30]</ref>, and obtained a lower IoU (70.7).  We also investigate cross-domain annotation. In particular, we use the ADE20k dataset and our model trained on Cityscapes (no fine-tuning). We randomly chose a total of 40 instances of car, person, sofa and dog. Here car and person are two classes seen in Cityscapes (i.e., person ∼ pedestrian in Cityscapes), and sofa and dog are unseen categories. From results in <ref type="table" target="#tab_7">Table 5</ref>, we observe that the humans were still faster when using our tool, but less so, as expected. Limitations: Our model predicts one polygon per box, typically annotating the more central object. If one object breaks the other, our approach tends to predict the occluded object as a single polygon. As a result, current failures cases are mostly multi-component objects. Note also that we do not handle holes which do not appear in our tested datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed Polygon-RNN++, a model for object instance segmentation that can be used to interactively annotate segmentation datasets. The model builds on top of Polygon-RNN <ref type="bibr" target="#b3">[4]</ref>, but introduces several important improvements that significantly outperform the previous approach in both, automatic and interactive modes. We further show generalization of our model to novel domains. We also show that with a simple online fine-tuning scheme, our model can be used to effectively adapt to novel, out-of-domain datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Polygon-RNN++ model (figures best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 , ..., v s T ), and v s t is the vertex sampled from the model at time t. Here, r = IoU(mask(v s ), m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: GGNN model: We take predicted polygon from RNN (orange vertices), and add midpoints (in blue) between every pair of consecutive vertices (orange). Our GGNN has three types of edges (red, blue, green), each having its own weights for message propagation. Black dashed arrows pointing out of the nodes (middle diagram) indicate that the GGNN aims to predict the relative location for each of the nodes (vertices), after completing propagation. Right is the high resolution polygon output by the GGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Online Fine Tuning on New Datasets bestPoly = cityscapesPoly; while currChunk in (1..C) do rawData = readChunk(currChunk); data = P redictAndCorrect(rawData, bestPoly); data += SampleF romSeenData(CS); newPoly = T rain M LE (data, N M LE , bestPoly); newPoly = T rain RL (data, N RL , newPoly); newPoly = T rain EV (data, N EV , newPoly); bestPoly = newPoly; end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Interactive mode on Cityscapes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Interactive mode on Citysc. (T 2 = 0.8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Interactive mode on KITTI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Percentage clicks saved with online fine-tuning on out-of-domain datasets (Plots share legend and y axis)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 11 :Figure 12 :Figure 13 :</head><label>10111213</label><figDesc>Figure 10: Qualitative results in automatic mode on Cityscapes. Left: requiring GT boxes; Right: FasterRCNN + PolygonRNN++</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Performance (IoU in % in val test) on all the Cityscapes classes in automatic mode. All methods exploit GT boxes.</figDesc><table>Model 

Bicycle 
Bus 
Person 
Train 
Truck 
Motorcycle 
Car 
Rider 
Mean 

Square Box 
35.41 
53.44 
26.36 
39.34 
54.75 
39.47 
46.04 
26.09 
40.11 
DeepMask [21] 
47.19 
69.82 
47.93 
62.20 
63.15 
47.47 
61.64 
52.20 
56.45 
SharpMask [22] 
52.08 
73.02 
53.63 
64.06 
65.49 
51.92 
65.17 
56.32 
60.21 
Polygon-RNN [4] 
52.13 
69.53 
63.94 
53.74 
68.03 
52.07 
71.17 
60.58 
61.40 

Residual Polygon-RNN 
54.86 
69.56 
67.05 
50.20 
66.80 
55.37 
70.05 
63.40 
62.16 
+ Attention 
56.47 
73.57 
68.15 
53.31 
74.08 
57.34 
75.13 
65.42 
65.43 
+ RL 
57.38 
75.99 
68.45 
59.65 
76.31 
58.26 
75.68 
65.65 
67.17 
+ Evaluator Network 
62.34 
79.63 
70.80 
62.82 
77.92 
61.69 
78.01 
68.46 
70.21 
+ GGNN 
63.06 
81.38 
72.41 
64.28 
78.90 
62.01 
79.08 
69.95 
71.38 
Table 1: 4 
6 
8 
10 
12 
14 
16 
AVG NUM CLICKS 

76 

78 

80 

82 

84 

86 

AVG IOU 

Annotator in the loop 

PolyRNN 
Ours T_2=1 
Ours T_2=0.9 
Ours T_2=0.8 
Ours T_2=0.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Out-of-domain automatic mode performance 

Model 
IoU (%) 

DeepMask [21] 
78.3 
SharpMask [22] 
78.8 
Beat The MTurkers [5] 
73.9 
Polygon-RNN [4] 
74.22 
Ours w/o GGNN 
81.40 
Ours w/ GGNN 
83.14 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Car annot. results on KITTI in 
automatic mode (no fine-tuning, 0 clicks) 

Bbox Noise IoU (%) 

0% 
71.38 
0-5% 
70.54 
5-10% 
68.07 
10-15% 
64.80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Robustness to Bound-
ing Box noise on Cityscapes (in 
% of side length at each vertex) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Real Human Experiment: In-domain on 50 randomly chosen Cityscapes car instances (left) and Out-of-domain on 40 randomly chosen ADE20K instances (right). No fine-tuning was used in ADE experiment.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We gratefully acknowledge NVIDIA for donating several GPUs used in this research. We also thank Kaustav Kundu for his help and advice, and Relu Patrascu for infrastructure support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Annotating object instances with a polygon-rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beat the mturkers: Automatic image labeling from weak 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards large-scale city reconstruction from satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lafarge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fetter</surname></persName>
		</author>
		<title level="m">Segmented anisotropic ssTEM dataset of neural tissue. figshare</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rationale and Design for the Defibrillators to Reduce Risk by Magnetic Resonance Imaging Evaluation (DE-TERMINE) Trial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Kadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Bonow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schaechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Subacius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Daubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cardiovasc Electrophysiol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="982" to="989" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepcut: Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Passeratpalmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Trans. on Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Sequence level training with recurrent neural networks. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08250</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A collaborative resource to build consensus for automated left ventricular segmentation of cardiac MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suinesiaputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Al-Agamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Fahmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Medranogracia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Kadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Margeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="62" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Free-shape polygonal object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Torontocity: Seeing the world with a million eyes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheverie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Super-edge grouping for object localization by combining appearance and shape information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Scene parsing through ade20k dataset. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
