<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
							<email>helge.rhodin@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<email>pascal.fua@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D reconstruction</term>
					<term>semi-supervised training</term>
					<term>representation learning</term>
					<term>monocular human pose reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Modern 3D human pose estimation techniques rely on deep networks, which require large amounts of training data. While weaklysupervised methods require less supervision, by utilizing 2D poses or multi-view imagery without annotations, they still need a sufficiently large set of samples with 3D annotations for learning to succeed. In this paper, we propose to overcome this problem by learning a geometry-aware body representation from multi-view images without annotations. To this end, we use an encoder-decoder that predicts an image from one viewpoint given an image from another viewpoint. Because this representation encodes 3D geometry, using it in a semi-supervised setting makes it easier to learn a mapping from it to 3D human pose. As evidenced by our experiments, our approach significantly outperforms fully-supervised methods given the same amount of labeled data, and improves over other semi-supervised methods while using as little as 1% of the labeled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most current monocular solutions to 3D human pose estimation rely on methods based on convolutional neural networks (CNNs). With networks becoming ever more sophisticated, the main bottleneck now is the availability of sufficiently large training datasets, which typically require a large annotation effort. While such an effort might be practical for a handful of subjects and specific motions such as walking or running, covering the whole range of human body shapes, appearances, and poses is infeasible.</p><p>Weakly-supervised methods that reduce the amount of annotation required to achieve a desired level of performance are therefore valuable. For example, methods based on articulated 3D skeletons can be trained not only with actual 3D annotations but also using 2D annotations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54]</ref> and multi-view footage <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47]</ref>. Some methods dispense with 2D annotations altogether and instead exploit multi-view geometry in sequences acquired by synchronized cameras <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b54">55]</ref>. However, these methods still require a good enough 3D training set to initialize (a) During training, we first learn a geometry-aware representation using unlabeled multi-view images. We then use a small amount of supervision to learn a mapping from our representation to actual 3D poses, which only requires a shallow network and therefore a limited amount of supervision. (b) At run-time, we compute the latent representation of the test image and feed it to the shallow network to compute the pose. (c) By contrast, most state-of-the-art approaches train a network to regress directly from the images to the 3D poses, which requires a much deeper network and therefore more training data.</p><p>the learning process, which sets limits on the absolute gain that can be achieved from using unlabeled examples.</p><p>In this paper, we propose to use images of the same person taken from multiple views to learn a latent representation that, as shown on the left side of <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, captures the 3D geometry of the human body. Learning this representation does not require any 2D or 3D pose annotation. Instead, we train an encoder-decoder to predict an image seen from one viewpoint from an image captured from a different one. As sketched on the right side of <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, we can then learn to predict a 3D pose from this latent representation in a supervised manner. The crux of our approach, however, is that because our latent representation already captures 3D geometry, the mapping to 3D pose is much simpler and can be learned using much fewer examples than existing methods that rely on multiview supervision <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b54">55]</ref>, and more generally most state-of-the-art methods that attempt to regress directly from the image to the 3D pose.</p><p>As can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>, our latent representation resembles a volumetric 3D shape. While such shapes can be obtained from silhouettes <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b44">45]</ref>, body outlines are typically difficult to extract from natural images. By contrast, learning our representation does not require any silhouette information. Furthermore, at test time, it can be obtained from a monocular view of the person. Finally, it can also be used for novel view synthesis (NVS) and outperforms existing encoderdecoder algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23]</ref> qualitatively on natural images.</p><p>Our contribution is therefore a latent variable body model that can be learned without 2D or 3D annotations, encodes both 3D pose and appearance, and can be integrated into semi-supervised approaches to reduce the required amount of supervised training data. We demonstrate this on the well-known Human3.6Million <ref type="bibr" target="#b12">[13]</ref> dataset and show that our method drastically outperforms fully supervised methods in 3D pose reconstruction accuracy when only few labeled examples are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In the following, we first review the literature on semi-supervised approaches to monocular 3D human pose estimation, which is most closely related to our goal. We then discuss approaches that, like us, make use of geometric representations, both in and out of the context of human pose estimation, and finally briefly review the novel view synthesis literature that has inspired us.</p><p>Semi-supervised human pose estimation. While most current human pose estimation methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref> are fully supervised, relying on large training sets annotated with ground-truth 3D positions coming from multi-view motion capture systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b11">12]</ref>, several methods have recently been proposed to limit the requirement for labeled data. In this context, foreground and background augmentation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> and the use of synthetic datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref> focus on increasing the training set size. Unfortunately, these methods do not generalize well to new motions, apparels, and environments that are different from the simulated data. Since larger and less constrained datasets for 2D pose estimation exist, they have been used for transfer learning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b21">22]</ref> and to provide re-projection constraints <ref type="bibr" target="#b53">[54]</ref>. Furthermore, given multiple views of the same person, 3D pose can be triangulated from 2D detections <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref> and a 2D pose network can be trained to be view-consistent after bootstrapping from annotations. Nevertheless, these methods still require 2D annotation in images capturing the target motion and appearance. By contrast, the methods of <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b54">55]</ref> exploit multi-view geometry in sequences acquired by synchronized cameras, thus removing the need for 2D annotations. However, in practice, they still require a large enough 3D training set to initialize and constrain the learning process. We will show that our geometry-aware latent representation learned from multi-view imagery but without annotations allows us to train a 3D pose estimation network using much less labeled data.</p><p>Geometry-aware representations. Multi-view imagery has long been used to derive volumetric representations of 3D human pose from silhouettes, for example by carving out the empty space. This approach can be used in conjunction with learning-based methods <ref type="bibr" target="#b43">[44]</ref>, by defining constraints based on perspective view rays <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b14">15]</ref>, orthographic projections <ref type="bibr" target="#b49">[50]</ref>, or learned projections <ref type="bibr" target="#b28">[29]</ref>. It can even be extended to the single-view training-scenario if the distribution of the observed shape can be inferred prior to reconstruction <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b7">8]</ref>. The main drawback of these methods, however, is that accurate silhouettes are difficult to automatically extract in natural scenes, which limits their applicability.</p><p>Another approach to encoding geometry relies on a renderer that generates images from a 3D representation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">52]</ref> and can function as a decoder in an autoencoder setup <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>. For simple renderers, the rendering function can even be learned <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and act as an encoder. When put together, such learned encoders and decoders have been used for unsupervised learning, both with GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref> and without them <ref type="bibr" target="#b16">[17]</ref>. In <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40]</ref>, a CNN was trained to map to and from spherical mesh representations without supervision. While these methods also effectively learn a geometry-aware representation based on images, they have only been applied to well-constrained problems, such as face modeling. As such, it is unclear how they would generalize to the much larger degree of variability of 3D human poses.</p><p>Novel view synthesis. Our approach borrows ideas from the novel view synthesis literature, which is devoted to the task of creating realistic images from previously unseen viewpoints. Most recent techniques rely on encoder-decoder architectures, where the latent code is augmented with view change information, such as yaw angle, and the decoder learns to reconstruct the encoded image from a new perspective <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Large view changes are difficult. They have been achieved by relying on a recurrent network that performs incremental rotation steps <ref type="bibr" target="#b50">[51]</ref>. Optical flow information <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b52">53]</ref> and depth maps <ref type="bibr" target="#b6">[7]</ref> have been used to further improve the results. While the above-mentioned techniques were demonstrated on simple objects, methods dedicated to generating images of humans have been proposed. However, most of these methods use additional information as input, such as part-segmentations <ref type="bibr" target="#b17">[18]</ref> and 2D poses <ref type="bibr" target="#b18">[19]</ref>. Here, we build on the approaches of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref> that have been designed to handle large viewpoint changes. We describe these methods and our extensions in more detail in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Geometry-Aware Latent Representation</head><p>Our goal is to design a latent representation L that encodes 3D pose, along with shape and appearance information, and can be learned without any 2D or 3D pose annotations. To achieve this, we propose to make use of sequences of images acquired from multiple synchronized and calibrated cameras. To be useful, such footage requires care during the setup and acquisition process. However, the amount of effort involved is negligible compared to what is needed to annotate tens of thousands of 2D or 3D poses.</p><p>For L to be practical, it must be easy to decode into its individual components. To this end, we learn from the images separate representations for the body's 3D pose and geometry, its appearance, and that of the background. We will refer to them as L 3D , L app , and B, respectively. Let us assume that we are given a set, U = {(I , of N u image pairs without annotations, where the i and j superscripts refer to the cameras used to capture the images, and the subscript t to the acquisition time. Let R i→j be the rotation matrix from the coordinate system of camera i to that of camera j. We now turn to the learning of the individual components of L. </p><formula xml:id="formula_0">= D θ d (L)</formula><p>. θ e and θ d are learned by minimizing I −Î 2 on average over a training set U . To leverage multi-view geometry, we take our inspiration from Novel View Synthesis methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11]</ref> that rely on training encoder-decoders on multiple views of the same object, such as a car or a chair. Let (I i t , I j t ) ∈ U be two images taken from different viewpoints but at the same time t. Since we are given the rotation matrix R i→j connecting the two viewpoints, we could feed this information as an additional input to the encoder and decoder and train them to encode I i t and resynthesize I j t , as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Then, novel views of the object could be rendered by varying the rotation parameter R i→j . However, this does not force the latent representation to encode 3D information explicitly. To this end, we model the latent representation L 3D ∈ R 3×N as a set of N points in 3D space by designing the encoder E θe and decoder D θe so that they have a three channel output and input, respectively, as shown on the left side of <ref type="figure" target="#fig_2">Fig. 2</ref>. This enables us to model the view-change as a proper 3D rotation by matrix multiplication of the encoder output by the rotation matrix before using it as input to the decoder. Formally, the output of the resulting autoencoder A θe,θ d can be written as</p><formula xml:id="formula_1">A θe,θ d (I i t , R i→j ) = D θ d (R i→j L 3D i,t ), with L 3D i,t = E θe (I i t ) ,<label>(1)</label></formula><p>and the weights θ d and θ e are optimized to minimize A θe,θ d (I i t , R i→j )−I j t over the training set U . In this setup, which was also used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref> and is inspired by <ref type="bibr" target="#b10">[11]</ref>, the decoder D does not need to learn how to rotate the input to a new view but only how to decode the 3D latent vector L 3D . This means that the encoder is forced to map to a proper 3D latent space, that is, one that can still be decoded To encode subject identity, we split the latent space into a 3D geometry part and an appearance part. The latter is not rotated, but swapped between two time frames t and t ′ depicting the same subject, so as to enforce it not to contain geometric information.</p><p>by D after an arbitrary rotation. However, while L 3D now encodes multi-view geometry, it also encodes the background and the person's appearance. Our goal now is to isolate them from L 3D and to create two new vectors B and L app that encode the latter two so that L 3D only represents geometry and 3D pose.</p><p>Factoring out the background. Let us assume that we can construct background images B j , for example by taking the median of all the images taken from a given viewpoint j. To factor them out, we introduce in the decoder a direct connection to the target background B j , as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. More specifically, we concatenate the background image with the output of the decoder and use an additional 1 × 1 convolutional layer to synthesize the decoded image. This frees the rest of the network from having to learn about the background and ensures that the L 3D vector we learn does not contain information about it anymore.</p><p>Factoring out appearance. To separate appearance from geometry in our latent representation, we break up the output of the encoder E into two separate vectors L 3D and L app that should describe pose and appearance, respectively. To enforce this separation, we train simultaneously on two frames I t and I t ′ depicting the same subject at different times, t and t ′ , as depicted in <ref type="figure" target="#fig_3">Fig. 3</ref> </p><formula xml:id="formula_2">: I i t → (L 3D i,t , L app i,t )</formula><p>and the decoder D θ d accepts these plus the background as inputs, after swapping appearance and rotating the geometric representation for two views i and j. We therefore write the output of our encoder-decoder as</p><formula xml:id="formula_3">A θe,θ d (I i t , R i→j , L app k,t ′ , B j ) = D θ d (R i→j L 3D i,t , L app k,t ′ , B j ) .<label>(2)</label></formula><p>The viewpoint k can be arbitrary. The critical point is that it was acquired at time t ′ = t such that the poses at t and t ′ are uncorrelated. Thus, only time-invariant appearance features are encoded into L app . A similar exchange of information has been performed before in <ref type="bibr" target="#b27">[28]</ref> for analogy transformations. It is related to works that separate facial identity, pose and illumination <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined optimization. To train</head><note type="other">A with sequences featuring several people and backgrounds, we randomly select mini-batches of Z triplets (I i t , I j t , I k t ′ ) in U , with t = t ′ , from individual sequences. In other words, all three views feature the same person. The first two are taken at the same time but from different viewpoints. The third is taken at a different time and from an arbitrary viewpoint k. For each such mini-batch, we compute the loss</note><formula xml:id="formula_4">E θ d ,θe = 1 Z I i t ,I j t ,I k t ′ ∈U t =t ′ A θe,θ d (I i t , R i→j , L app k,t ′ , B j ) − I j t ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">L k,t ′ = (L 3D k,t ′ , L app k,t ′ ) is</formula><note type="other">the output of encoder E θe applied to image I k t ′ , B j is the background in view j, and R i→j denotes the rotation from view i to view j. Note that we apply E twice, to obtain L 3D i,t and L app k,t ′ in Eq. 3 while ignoring L app i,t and L</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D</head><p>k,t ′ with the swap discussed above. At training time, we minimize a total loss that is the sum of the pixel-wise error E θ d ,θe of Eq. 3 and a second term obtained by first applying a Resnet with 18 layers trained on ImageNet on the output and target image and then computing the feature difference after the second block level, as previously done with VGG by <ref type="bibr" target="#b22">[23]</ref>. All individual pixel and feature differences are averaged and their influence is balanced by weighting the feature loss by two. We experiment with L1 and L2 norms. The L1 norm in combination with the additional feature term allows for crisper decodings and improved pose reconstruction.</p><p>Translation and Augmentation. Object scale and translation in depth direction are inherently ambiguous for monocular reconstruction and NVS. To make our model invariant instead of ambiguous to these effects, we use the crop information provided in the training datasets. We compute the rotation between two views with respect to the crop center instead of the image center and shear the cropped image so that it appears as if it were taken from a virtual camera pointing in the crop direction. With the human in the same position and scale, these crops remove the need to model object and camera translation. We also apply random in-plane rotations to increase view diversity. As a result, R i→j and B j depend on time t, but we neglect this in our notation for readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">3D Human Pose Estimation</head><p>Recall that our ultimate goal is to infer the 3D pose of a person from a monocular image. Since L 3D can be rotated and used to generate novel views, we are already part way there. Being a 3 × N matrix, it can be understood as a set of N 3D points, but these do not have any semantic meaning. However, in most practical applications, one has to infer a pre-defined representation, such as a skeleton with K major human body joints, encoded as a vector p ∈ R 3K . To instantiate such a representation, we need a mapping F : L 3D → R 3K , which can be thought as a different decoder that reconstructs 3D poses instead of images. To learn it, we rely on supervision. However, as we will see in the results section, the necessary amount of human annotations is much smaller than what would have been required to learn the mapping directly from the images, as in many other recent approaches to human pose estimation.</p><p>Let L = {(I t , p t )} Ns t=1 be a small set of N s labeled examples made of image pairs and corresponding ground-truth 3D poses. We model F as a deep network with parameters θ f . We train it by minimizing the objective function</p><formula xml:id="formula_6">E θ f = 1 N s Ns t=1 F θ f (L 3D t ) − p t , with (L 3D t , ·) = E θe (I t ) .<label>(4)</label></formula><p>Because our latent representation L 3D already encodes human 3D pose and shape, F can be implemented as a simple two-layer fully-connected neural network. Together with the encoder-decoder introduced in Section 3, which is trained in an unsupervised manner, they form the semi-supervised setup depicted by <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. In other words, our unsupervised representation does a lot of the hard-work in the difficult task of lifting the image to a 3D representation, which makes the final mapping comparatively easy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we first evaluate our approach on the task of 3D human pose estimation, which is our main target application, and show that our representation enables us to use far less annotated training data than state-of-the-art approaches to achieve better accuracy. We then evaluate the quality of our latent space itself and show that it does indeed encode geometry, appearance, and background separately.</p><p>Dataset. We use the well-known Human3.6M (H36M) <ref type="bibr" target="#b11">[12]</ref> dataset. It is recorded in a calibrated multi-view studio and ground-truth human poses are available for all frames. This makes it easy to compare different levels of supervision, unsupervised, semi-supervised, or fully supervised. As in previous approaches <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>, we use the bounding boxes provided with the dataset to crop images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semi-Supervised Human Pose Estimation</head><p>Our main focus is semi-supervised human pose estimation. We now demonstrate that, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, recent state-of-the-art methods can do better than us when large amounts of annotated training data are available. However, as we use fewer and fewer of these annotations, the accuracy of the baselines suffers greatly whereas ours does not, which confers a significant advantage in situations where annotations are hard to obtain. We now explain in detail how the graphs of <ref type="figure" target="#fig_4">Fig. 4</ref> were produced and further discuss their meaning.  Metrics. We evaluate pose prediction accuracy in terms of the mean per joint prediction error (MPJPE), and its normalized variants N-MPJPE and P-MPJPE, where poses are aligned to the ground truth in the least-square sense either in scale only or in scale, rotation and translation, respectively, before computing the MPJPE. The latter is also known as Procrustes alignment. We do this over 16 major human joints and all positions are centered at the pelvis, as in <ref type="bibr" target="#b53">[54]</ref>. Our results are consistent across all metrics, as shown in <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>.</p><p>Baselines. We compare our approach against the state-of-the-art semi-supervised method of <ref type="bibr" target="#b30">[31]</ref>, which uses the same input as ours and outputs normalized poses. We will refer to it as RhodinCVPR. We also use the popular ResNetbased architecture <ref type="bibr" target="#b21">[22]</ref> to regress directly from the image to the 3D pose, as shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, we will refer to this as Resnet.</p><p>Note that even higher accuracies on H36M than those of RhodinCVPR and Resnet have been reported in the literature <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27]</ref> but they depend both on more complex architectures and using additional information such as labeled 2D poses <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref> or semantic segmentation <ref type="bibr" target="#b26">[27]</ref>, which is not our point here. We want to show that when using only 3D annotations and not many of them are available, our representation still allows us to perform well.</p><p>Implementation. We base our encoder-decoder architecture on the UNet <ref type="bibr" target="#b33">[34]</ref> network, which was used to perform a similar task in <ref type="bibr" target="#b18">[19]</ref>. We simply remove the skip connections to force the encoding of all information into the latent spaces and reduce the number of feature channels by half.</p><p>Concretely the encoder E consists of four blocks of two convolutions, where each two convolutions are followed by max pooling. The resulting convolutional features are of dimension 512 × 16 × 16 for an input image resolution of 128 × 128 pixels. These are mapped to L app ∈ R 128 and L 3D ∈ R 200×3 by a single fullyconnected layer followed by dropout with probability 0.3. The decoder D maps L 3D to a feature map of dimension (512 − 128) × 16 × 16 with a fully-connected layer followed by ReLU and dropout and duplicates L app to form a spatial uniform map of size 128 × 16 × 16. These two maps are concatenated and then reconstructed by four blocks of two convolutions, where the first convolution is preceded by bilinear interpolation and all other pairs by up-convolutions. Each convolution is followed by batch-normalization and ReLU activation functions. We also experimented with a variant in which the encoder E is an off-the shelf Resnet with fifty layers <ref type="bibr" target="#b9">[10]</ref>, pre-trained on ImageNet, and the decoder is the same as before. We will refer to these two versions as OursUnet and OursResnet, respectively.</p><p>The pose decoder F is a fully-connected network with two hidden layers of dimension 2048. The ground-truth poses in the least-squares loss of Eq. 4 are defined as root-centered 3D poses. Poses and images are normalized by their mean and standard deviation on the training set. We use mini-batches of size 32 and the Adam optimizer with learning rate 10 −3 for optimization of θ e , θ d and θ f .</p><p>Dataset splits. On H36M, we take the unlabeled set U used to learn our representation to be the complete training set-S1, S5, S6, S7 and S8, where SN refers to all sequences of the N th subject-but without the available 3D labels. To provide the required supervision to train the shallow network of <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, we then define several scenarios.</p><p>-Fully supervised training with the 3D annotation of all five training subjects.</p><p>-We use all the 3D annotations for S1; S1 and S5; or S1, S5 and S6.</p><p>-We use only 50%, 10%, 5%, 1% or 0.1% of the 3D annotations for S1. In all cases we used S9 and S11 for testing. We subsampled the test and training videos at 10ps to reduce redundancy and validation time. The resulting numbers of annotated images we used are shown along the x-axis of <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>Comparison to the state of the art. RhodinCVPR is the only method that is designed to leverage unlabeled multi-view footage without using a supplemental 2D dataset <ref type="bibr" target="#b30">[31]</ref>. OursUnet outperforms it significantly, e.g., on labeled subject S1 by 13.6 mm (8.9% relative improvement) and OursResnetL1 even attains a gain of 35.7 mm (23.3% relative improvement). The fact that the Resnet architecture, training procedure, and dataset split is the same for our method and RhodinCVPR evidences that this gain is due to our new way of exploiting the unlabeled examples, thus showing the effectiveness of learning a geometry-aware latent representation in an unsupervised manner. Discussion and Ablation Study. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, when more than 300,000 annotated images are used the baselines outperform us. However, their accuracy decreases rapidly when fewer are available and our approach then starts dominating. It only loses accuracy very slowly down to 5,000 images and still performs adequately given only 500.</p><p>We used the L2 loss in Eq. 3 by default since our main goal is 3D pose estimation, not NVS quality. Interestingly, however, using the L1 loss makes reconstructions not only crisper but also 3D poses estimates more accurate. It improves pose accuracy consistently by about 5%, shown as OursResnetL1 in <ref type="figure" target="#fig_4">Fig. 4</ref>. Unless indicated otherwise, all results are produced with the L2 metric.</p><p>To better evaluate different aspects of our approach, we use the OursUnet version to conduct an ablation study whose results we report in <ref type="table">Table 1</ref>. In short, not separating the background and appearance latent spaces reduces N-MPJPE by 14 mm and P-MPJPE by more than 12 mm. Using two hidden layers in F instead of one increases accuracy by 12 mm. The loss term based on ResNet-18 features not only leads to crisper NVS results but also improves pose estimation by 9 mm. Using bilinear upsampling instead of deconvolution for all decoding layers reduces performance by 4 mm. The largest decrease in accuracy by far, 46.1mm, occurs when we use our standard OursUnet architecture but without our geometry-aware 3D latent space. It appears in the last line of the table on the left and strongly suggests that using our latent representation has more impact than tweaking the architecture in various ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating the Latent Representation Qualitatively</head><p>We now turn to evaluating the quality of our latent representation as such with a number of experiments on OursUnet. We show that geometry can be separated from appearance and background and that this improves results. The quality of the synthesized images is best seen in the supplemental videos.</p><p>Method N-MPJPE P-MPJPE OursUnet ⋆ 145.6 112.2 OursUnet ⋆ , w/o appearance space, as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref> 159.0 117.1 OursUnet ⋆ , w/o background handling, as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref> 159.6 124.6 OursUnet ⋆ , w/o 3D latent space, as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> 191.7 139.0 ⋆ no rotation augmentation. Errors are reported in mm.  <ref type="table">Table 1</ref>: Ablation study, using S1 for semi-supervised training. The extensions to the NVS methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> and <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref> as well as further model choices improve accuracy. Novel View Synthesis. Recall from Section 3 that E encodes the image into variables L 3D and L app , which are meant to represent geometry and appearance, respectively. To check that this is indeed the case, we multiply L 3D by different rotation matrices R and feed the result along with the original L app to D. For comparison purposes, in <ref type="figure">Fig. 6</ref>, we synthesize rotated images without using our geometry-aware latent space, that is, as in <ref type="bibr" target="#b36">[37]</ref>. The resulting images are far blurrier than those of OursResnet. <ref type="figure">Fig. 6</ref> further shows that results degrade without the background handling, that is, as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11]</ref>. Using the L1 instead of L1 loss further improves reconstruction quality. Test subjects wear clothes that differ in color and shape from those seen in the training data. As a result, the geometry in the synthesized images remains correct, but the appearance ends up being a mixture of training appearances that approximates the unseen appearance. Arguably, using more than the five subjects that appear in the training set should result in a better encoding of appearance, which is something we plan to investigate in future work.</p><p>Appearance and background switching. Let I j and I g be two images of subjects j and g and (</p><formula xml:id="formula_7">L 3D j , L app j , B j ) = E(I j ) and (L 3D g , L app g , B g ) = E(I g ) their encodings.</formula><p>Re-encoding using L 3D of one and L app of the other yields results such as those depicted by <ref type="figure" target="#fig_7">Fig. 7</ref>. Note that the appearance of one is correctly transferred to the pose of the other while the geometry remains intact under rotation. This method could be used to generate additional training data, by changing the appearance of an existing multi-view sequence to synthesize images of the same motion being performed by multiple actors. The same pose can be decoded to different identities by blending the appearance latent vectors. In the first row, both subjects appear in the training set. In the second row, they are from the test set. Bottom two rows. We generate rotated views of the test subject and its transferred appearance, to demonstrate that appearance can be changed without affecting 3D pose.</p><p>Input New view White Picture Bg. pic. Input New view White Picture Bg. pic. Similarly, we can switch backgrounds instead of appearances before decoding the latent vectors, as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. In one case, we make the background white and in the other we use a natural scene. In the first case, dark patches are visible below the subject, evidently modeling shadowing effects that were learned implicitly. In the second case, the green trees tend to be rendered as orange because our training scenes were mostly reddish-problem a larger training database would almost certainly cure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generalization and Limitations</head><p>To analyze the scalability of the unsupervised training we tested using only four out of the five unsupervised training subjects. The additional subject improves OurResnet drastically by 16 N-MPJPE. This indicates that training is not yet saturated and much higher accuracies seem possible by leveraging huge unsupervised sets. These, be it indoors or outdoors, are relatively easy to obtain. In the data we used, some of the images contain a chair on which the subject sits. Interestingly, as shown in <ref type="figure">Fig. 9</ref>, the chair appearance and 3D position is faithfully reconstructed by our method. This suggests that it is not specific to human poses and can generalize to rigid objects as well as multiple object classes. In future work, we intend to apply it to such more generic problems.</p><p>We further tested our method on the MPI-INF-3DHP (3DHP) <ref type="bibr" target="#b20">[21]</ref> dataset, which features more diverse clothing and viewpoints, such as low-hanging and ceiling cameras, and is therefore well suited to probe extreme conditions for NVS. Without changing any parameter, OursResnet is able to synthesis view transformations in roll, yaw and pitch, as shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. On H36M, pitch transformation could not be learned due to the solely chest-height training views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced an approach to learning a geometry-aware representation of the human body in an unsupervised manner, given only multi-view imagery. Our experiments have shown that this representation is effective both as an intermediate one for 3D pose estimation and for novel view synthesis. For pose estimation, our semi-supervised approach performs much better than state-ofthe-art methods when only very little annotated data is available. In future work, we will extend its range by learning an equivalent latent representation for much larger multi-view datasets but still in an unsupervised manner.</p><p>Acknowledgment. This work was supported in part by a Microsoft Joint Research Project.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Approach. (a) During training, we first learn a geometry-aware representation using unlabeled multi-view images. We then use a small amount of supervision to learn a mapping from our representation to actual 3D poses, which only requires a shallow network and therefore a limited amount of supervision. (b) At run-time, we compute the latent representation of the test image and feed it to the shallow network to compute the pose. (c) By contrast, most state-of-the-art approaches train a network to regress directly from the images to the 3D poses, which requires a much deeper network and therefore more training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Representation learning. We learn a representation that encodes geometry and thereby 3D pose information in an unsupervised manner. Our method (Left) extends a conventional auto encoder (Right) with a 3D latent space, rotation operation, and background fusion module. The 3D rotation enforces explicit encoding of 3D information. The background fusion enables application to natural images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Appearance representation learning. To encode subject identity, we split the latent space into a 3D geometry part and an appearance part. The latter is not rotated, but swapped between two time frames t and t ′ depicting the same subject, so as to enforce it not to contain geometric information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: (a) Performance as function of the number of training samples. When using all the available annotated 3D data in H36M, that is, 370,000 images, RhodinCVPR and Resnet yield a better accuracy than our approach. However, when the number of training examples drops below 180'000 the baselines' accuracy degrades significantly, whereas OursResnet degrades much more gracefully and our accuracy becomes significantly better. (b) This improvement is consistent across metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Novel viewpoint synthesis. Top row. Each one of the three image pairs of image to the left of it comprise an original image acquired from a different viewpoint and the image synthesized from the input image i. Bottom row. We can also synthesize images for previously unseen viewpoints and remove the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 5 depicts such synthesized novel views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Appearance separation. Top two rows. The same pose can be decoded to different identities by blending the appearance latent vectors. In the first row, both subjects appear in the training set. In the second row, they are from the test set. Bottom two rows. We generate rotated views of the test subject and its transferred appearance, to demonstrate that appearance can be changed without affecting 3D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Background separation. The background is handled separately from the foreground an can be chosen arbitrary at decoding time. From left to right, input image, decoded on the input background, on a novel view, on a white, and on a picture. The first row features someone from the training set and the second row from the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig. 9: Foreground objects are reconstructed too, if seen in training and testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. While the decoder uses L3D t ′ , as before, it swaps L app t and L app t ′ . In other words, the decoder uses Lto resynthesize frame t and LIn practice, the encoder E has two outputs, that is, E θe</figDesc><table>3D 
t 

and L 

3D 
t 

and L 

app 
t ′ 

3D 
t ′ 

and L 

app 
t 

for frame t 
′ . Assuming that the person's appearance does not change 
drastically between t and t 
′ and that differences in the images are caused by 3D 
pose changes, this results in L 
3D encoding pose while L 
app encodes appearance. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Fig. 6: Ablation study. First row. Without background handling, as used in [4, 49], the synthesized foreground pose appears fuzzy. Second row. Without a geometry- aware latent space, as used by [36, 37], results are inaccurate and blurred in new views. Third row. OursResnet captures pose and appearance accurately, but contours are still blurred. Fourth row. OursResnetL1 produces crisper and more accurate results.</figDesc><table>Input 
0 

• 

45 

• 

90 

• 

135 

• 

180 

• 

225 

• 

270 

• 

315 

• 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<title level="m">3D Morphable Models as Spatial Transformer Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Synthesizing Training Images for Boosting Human 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Transformation Properties of Learned Visual Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to Generate Chairs with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to Generate Chairs, Tables and Cars with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="692" to="705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<title level="m">Deepstereo: Learning to Predict New Views from the World&apos;s Imagery</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
	<note>Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05872</idno>
		<title level="m">3d shape induction from 2d views of multiple objects</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerven</surname></persName>
		</author>
		<title level="m">Deep Disentangled Representations for Volumetric Reconstruction</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="266" to="279" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transforming Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iterated Second-Order Label Sensitive Pooling for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Panoptic Studio: A Massively Multiview System for Social Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="364" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<title level="m">versefacenet: Deep Single-Shot Inverse Face Rendering from a Single Image</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<title level="m">Deep Convolutional Inverse Graphics Network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">arXiv</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Generative Model of People in Clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose Guided Person Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="405" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Simple Yet Effective Baseline for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular 3D Human Pose Estimation in the Wild Using Improved CNN Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vnect: Real-Time 3D Human Pose Estimation with a Single RGB Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformation-Grounded Image Generation Network for Novel 3D View Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="702" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coarse-ToFine Volumetric Prediction for Single-Image 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kostas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A recurrent encoder-decoder network for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Deep Visual Analogy-Making. In: Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1252" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of 3D Structure from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Egocap: Egocentric Marker-Less Motion Capture with Two Fisheye Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Monocular 3D Human Pose Estimation from Multi-View Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spoerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mocap Guided Data Augmentation for 3D Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lcr-Net: Localization-ClassificationRegression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural Face Editing with Intrinsic Image Disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Single-view to multi-view: Reconstructing unseen views with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>CoRR abs/1511.06702</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object frames by dense equivariant image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="844" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks by factorized spatial embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00295</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for poseinvariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Multi-View Consistency as Supervisory Signal for Learning Shape and Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5242" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction Without 3D Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentangling with recurrent transformations for 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1099" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04886</idno>
		<title level="m">Multi-view image generation from a single-view</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Weakly-Supervised Transfer for 3D Human Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>We</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for 3d keypoint prediction from a single depth scan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05765</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
