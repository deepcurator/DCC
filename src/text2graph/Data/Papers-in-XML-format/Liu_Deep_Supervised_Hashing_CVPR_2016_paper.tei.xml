<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Supervised Hashing for Fast Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haomiao</forename><surname>Liu</surname></persName>
							<email>haomiao.liu@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Supervised Hashing for Fast Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, hundreds of thousands of images are uploaded to the Internet every day, making it extremely difficult to find relevant images according to different users' request. For example, content based image retrieval retrieves images that are similar to a given query image, where "similar" may refer to visually similar or semantically similar. Suppose that both the images in the database and the query image are represented by real-valued features, the simplest way of looking for relevant images is by ranking the database images according to their distances to the query image in the feature space, and returning the closest ones. However, for a database with millions of images, which is quite common nowadays, even a linear search through the database would cost a great deal of time and memory.</p><p>To address the inefficiency of real-valued features, hashing approaches are proposed to map images to compact binary codes that approximately preserve the data structure in the original space, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref> for example. Since the images are represented by binary codes instead of realvalued features, the time and memory costs of searching can be greatly reduced. However, the retrieval performance of most existing hashing methods heavily depends on the features they use, which are basically extracted in an unsupervised manner, thus more suitable for dealing with the visual similarity search rather than the semantic similarity search. On the other hand, recent progress in image classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8]</ref>, object detection <ref type="bibr" target="#b25">[26]</ref>, face recognition <ref type="bibr" target="#b23">[24]</ref>, and many other vision tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> demonstrate the impressive learning power of CNNs. In these different tasks, the CNNs can be viewed as a feature extractor guided by the objective functions specifically designed for the individual tasks. The successful applications of CNNs in various tasks imply that the features learned by CNNs can well capture the underlying semantic structure of images in spite of significant appearance variations.</p><p>Inspired by the robustness of CNN features, we propose a binary code learning framework by exploiting the CNN structure, named Deep Supervised Hashing (DSH). In our method, first we devise a CNN model which takes image pairs along with labels indicating whether the two images are similar as training inputs, and produces binary codes as outputs, as shown in <ref type="figure">Figure 1</ref>. In practice, we generate image pairs online so that many more image pairs can be utilized in the training stage. The loss function is designed to pull the network outputs of similar images together and push the outputs of dissimilar ones far away, so that the learned Hamming space can well approximate the semantic structure of images. To avoid optimizing the nondifferentiable loss function in Hamming space, the network  <ref type="figure">Figure 1</ref>. The network structure used in our method. The network consists of 3 convolution-pooling layers and 2 fully connected layers. The filters in convolution layers are of size 5 × 5 with stride 1 (32, 32, and 64 filters in the three convolution layers respectively), and pooling over 3 × 3 patches with stride 2. The first fully connected layer contains 500 nodes, and the second (output layer) has k (the code length) nodes. The loss function is designed to learn similarity-preserving binary-like codes by exploiting discriminability terms and a regularizer. Binary codes are obtained by quantizing the network outputs of images.</p><p>outputs are relaxed to real values, while simultaneously a regularizer is imposed to encourage the real-valued outputs to approach the desired discrete values. Under this framework, images can be easily encoded by first propagating through the network and then quantizing the network outputs to binary codes representation. The rest of the paper is organised as follows: Section 2 discusses the related works to our method. Section 3 describes DSH in detail. Section 4 extensively evaluates the proposed method on two large scale datasets. Section 5 gives concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Many hashing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14]</ref> have been proposed to boost the performance of approximate nearest neighbor search due to their low time and space complexity. In the early years, researchers mainly focused on data-independent hashing methods, such as a family of methods known as Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b3">[4]</ref>. LSH methods use random projections to produce hashing bits. It has been proven theoretically that as the code length grows, the Hamming distance between two binary codes asymptotically approaches their corresponding distance in the feature space. However, LSH methods usually require long codes to achieve satisfactory performance, which demands for large amount of memory.</p><p>To produce more compact binary codes, data-dependent hashing methods are proposed. Such methods attempt to learn similarity-preserving hashing functions from a training set. These methods can be further divided into unsupervised methods and supervised (semi-supervised) methods. Unsupervised methods only make use of unlabelled training data to learn hash functions. For example, Spectral Hashing (SH) <ref type="bibr" target="#b27">[28]</ref> minimizes the weighted Hamming distance of image pairs, where the weights are defined to be the similarity metrics of image pairs; Iterative Quantization (ITQ) <ref type="bibr" target="#b5">[6]</ref> tries to minimize the quantization error on projected image descriptors so as to alleviate the information loss caused by the discrepancy between the real-valued feature space and the binary Hamming space.</p><p>To deal with more complicated semantic similarity, supervised methods are proposed to take advantage of label information, such as category labels. CCA-ITQ <ref type="bibr" target="#b5">[6]</ref>, which is an extension of ITQ, uses label information to find better projections for the image descriptors; Predictable Discriminative Binary Code (DBC) <ref type="bibr" target="#b21">[22]</ref> looks for hyperplanes that separate categories with large margin as hash functions; Minimal Loss Hashing (MLH) <ref type="bibr" target="#b19">[20]</ref> optimizes upper bound of a hinge-like loss to learn the hash functions. On the other hand, Semi-Supervised Hashing (SSH) <ref type="bibr" target="#b26">[27]</ref> makes use of the abundant unlabelled data to regularize the hashing functions. While the above methods use linear projections as hashing functions, they can hardly deal with linearly inseparable data. To overcome this limitation, Supervised Hashing with Kernels (KSH) <ref type="bibr" target="#b16">[17]</ref> and Binary Reconstructive Embedding (BRE) <ref type="bibr" target="#b12">[13]</ref> are proposed to learn similaritypreserving hashing functions in kernel space; Deep Hashing (DH) <ref type="bibr" target="#b2">[3]</ref> exploits a non-linear deep network to produce binary codes. Most hashing methods relax the binary codes to real-values in optimization and quantize the model outputs to produce binary codes. However, there is no guarantee that the optimal real-valued codes are still optimal after quantization. Methods such as Discrete Graph Hashing (DGH) <ref type="bibr" target="#b15">[16]</ref> and Supervised Discrete Hashing (SDH) <ref type="bibr" target="#b22">[23]</ref> are proposed to directly optimize the binary codes to overcome the shortcomings of relaxation, and achieves improved retrieval performance.</p><p>While the aforementioned hashing methods have certainly achieved success to some extent, they all use hand-crafted features, which cannot capture the semantic information beneath the drastic appearance variations in real-world data and thus limit the retrieval accuracy of the learned binary codes. To tackle this issue, most recently, several CNNbased hashing methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref> are proposed to learn image representations together with binary codes using the promising CNNs. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref> enforce the network to learn binary-like outputs that preserve the semantic relations of image-triplets; <ref type="bibr" target="#b28">[29]</ref> trains a CNN to fit the binary codes computed from the pairwise similarity matrix; <ref type="bibr" target="#b14">[15]</ref> trains the model with a binary-like hidden layer as features for image classification tasks. By coupling image feature extraction and binary code learning, these methods have shown greatly improved retrieval accuracy. Nevertheless, there still exist some shortcomings with the training objectives of these methods that limit their practical retrieval performance, as will detailed in our experiments. In addition, the non-linear activations they employ to approximate the quantization step operate at the cost of possibly slowing down the network training <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to learn compact binary codes for images such that: (a) similar images should be encoded to similar binary codes in Hamming space, and vice versa; (b) the binary codes could be computed efficiently.</p><p>Although many hashing methods have been proposed to learn similarity-preserving binary codes, they suffer from the limitations of either hand-crafted features or linear projections. The powerful non-linear models known as CNNs have facilitated the recent successes in computer vision community on various tasks. To this end, we propose to use the CNN illustrated in <ref type="figure">Figure 1</ref> to learn discriminative image representations and compact binary codes simultaneously, which can break out the limitations of both handcrafted features and linear models. Our method first trains the CNN using image pairs and the corresponding similarity labels. Here the loss function is elaborately designed to learn similarity-preserving binary-like image representations. Then the CNN outputs are quantized to generate binary codes for new-coming images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Loss Function</head><p>Let Ω be the RGB space, our goal is to learn a mapping from Ω to k-bit binary code: F : Ω → {+1, −1} k , such that similar (either in terms of visually similar or semantically similar) images are encoded to similar binary codes. For this purpose, the codes of similar images should be as close as possible, while the codes of dissimilar images being far away. Based on this objective, the loss function is naturally designed to pull the codes of similar images together, and push the codes of dissimilar images away from each other.</p><p>Specifically, for a pair of images I 1 , I 2 ∈ Ω and the corresponding binary network outputs b 1 , b 2 ∈ {+1, −1} k , we define y = 0 if they are similar, and y = 1 otherwise. The loss with respect to the pair of images is defined as:</p><formula xml:id="formula_0">L(b 1 , b 2 , y) = 1 2 (1 − y)D h (b 1 , b 2 ) + 1 2 y max(m − D h (b 1 , b 2 ), 0) s.t. b j ∈ {+1, −1} k , j ∈ {1, 2}<label>(1)</label></formula><p>where D h (· , ·) denotes the Hamming distance between two binary vectors, and m &gt; 0 is a margin threshold parameter. The first term punishes similar images mapped to different binary codes, and the second term punishes dissimilar images mapped to close binary codes when their Hamming distance falls below the margin threshold m. Here it is worth noting that to avoid collapsed solution, our loss function takes a contrastive loss form as <ref type="bibr" target="#b6">[7]</ref> where only those dissimilar pairs having their distance within a radius are eligible to contribute to the loss function. Suppose that there are N training pairs randomly selected from the training images {(I i,1 , I i,2 , y i )|i = 1, ..., N }, our goal is to minimize the overall loss function:</p><formula xml:id="formula_1">L = N i=1 L(b i,1 , b i,2 , y i ) s.t. b i,j ∈ { + 1, −1} k , i ∈ {1, ..., N }, j ∈ {1, 2}<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relaxation</head><p>It would be preferable if one can directly optimize Eqn.(2), however it is infeasible because the binary constraints on b i,j requires thresholding the network outputs (e.g. with signum function), and will make it intractable to train the network with back propagation algorithm. Some recent works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16]</ref> propose to directly optimize the binary codes, however, due to the memory limitation, CNN models can only be trained with mini-batches, and the optimality of the produced binary codes is questionable when the batch size is very small compared to the whole training set.</p><p>On the other hand, if one totally ignores the binary constraints, it would result in suboptimal binary codes due to the discrepancy between the Euclidean space and the Hamming space. A commonly used relaxation scheme is to utilize sigmoid or tanh function to approximate the thresholding procedure. Nevertheless, working with such non-linear functions would inevitably slow down or even restrain the convergence of the network <ref type="bibr" target="#b11">[12]</ref>. To overcome such limitation, in this work we propose to impose a regularizer on the real-valued network outputs to approach the desired discrete values (+1/-1). To be specific, we replace the Hamming distance in Eqn. <ref type="formula" target="#formula_0">(1)</ref> by Euclidean distance, and impose an additional regularizer to replace the binary constraints, then Eqn. <ref type="formula" target="#formula_0">(1)</ref> is rewritten as:</p><formula xml:id="formula_2">L r (b 1 , b 2 , y) = 1 2 (1 − y)||b 1 − b 2 || 2 2 + 1 2 y max(m − ||b 1 − b 2 || 2 2 , 0) +α(|| |b 1 | − 1|| 1 + || |b 2 | − 1|| 1 ) (3)</formula><p>where the subscript r denotes the relaxed loss function, 1 is a vector of all ones, || · || 1 is the L1-norm of vector, | · | is the element-wise absolute value operation, and α is a weighting parameter that controls the strength of the regularizer.</p><p>Here we use L2-norm to measure the distance between network outputs because the subgradients produced by lower-order norms treat the image pairs with different distances equally and thus make no use of the information involved in different distance magnitudes. While higher-order norms are also feasible, more computations will be incurred accordingly at the same time. As for the regularizer, L1-norm is chosen rather than higher-order norms for its much less computational cost, which can favorably accelerate the training process.</p><p>By substituting Eqn. <ref type="formula">(3)</ref> into Eqn. <ref type="formula" target="#formula_1">(2)</ref>, we rewrite the relaxed overall loss function as follows:</p><formula xml:id="formula_3">L r = N i=1 { 1 2 (1 − y i )||b i,1 − b i,2 || 2 2 + 1 2 y i max(m − ||b i,1 − b i,2 || 2 2 , 0) + α(|| |b i,1 | − 1|| 1 + || |b i,2 | − 1|| 1 )} (4)</formula><p>With this objective function, the network is trained using back-propagation algorithm with mini-batch gradient descent method. To do so, the gradients of Eqn.(4) w.r.t. b i,j , ∀i, j need to be computed. Since the max operation and the absolute value operation in the objective function is non-differentiable at some certain points, we use subgradients instead, and define the subgradients to be 1 at such points. The subgradients of the first two terms of Eqn. <ref type="bibr" target="#b3">(4)</ref> and that of the third term (i.e. the regularizer) are respectively written as:</p><formula xml:id="formula_4">∂T erm 1 ∂b i,j = (−1) j+1 (1 − y i )(b i,1 − b i,2 ) ∂T erm 2 ∂b i,j = (−1) j y i (b i,1 − b i,2 ), ||b i,1 − b i,2 || 2 2 &lt; m 0 , otherwise ∂Regularizer ∂b i,j = αδ(b i,j )<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">δ(x) = 1, − 1 ≤ x ≤ 0 or x ≥ 1 −1, otherwise<label>(6)</label></formula><p>is applied element-wisely. With the computed subgradients over mini-batches, the rest of the back-propagation can be done in standard manner.</p><p>Discussion: With such a framework, the binary codes of images are easily obtained with sign(b). Note that unlike existing CNN-based hashing methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>, our method does not use saturating non-linearities, e.g. tanh or sigmoid, to approximate the quantization step because these nonlinearities are likely to slow down the training process <ref type="bibr" target="#b11">[12]</ref>. Experiments in Section 4.2 will validate the advantage of the regularizer over saturating nonlinearities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>Network parameters: Our DSH method is implemented with Caffe 1 <ref type="bibr" target="#b9">[10]</ref>. The network structure is illustrated in <ref type="figure">Figure 1</ref>, which consists of three convolution-pooling layers followed by two fully connected layers. The convolution layers use 32, 32, and 64 5 × 5 filters with stride 1 respectively, and the pooling is performed over 3×3 windows with stride 2. The first fully connected layer contains 500 nodes, and the second contains k nodes, where k is the length of binary code. All the convolution layers and the first fully connected layer are equipped with the ReLU <ref type="bibr" target="#b18">[19]</ref>.</p><p>The weight layers are initialized with "Xavier" initialization <ref type="bibr" target="#b4">[5]</ref>. During training, the batch size is set to 200, momentum to 0.9, and weight decay to 0.004. The initial learning rate is set to 10 −3 and decreases by 40% after every 20,000 iterations (150,000 iterations in total). The margin m in Eqn.(4) is heuristically set to m = 2k to encourage the codes of dissimilar images to differ in no less than k 2 bits. Training methodology: An intuitive way to train the network is to use the Siamese structure <ref type="bibr" target="#b6">[7]</ref> and generate image pairs offline. However, with such a scheme, processing n images could only produce n 2 valid image pairs, and storing the image pairs would be very space intensive. To make better use of computational resources and storage space, we propose to generate image pairs online by exploiting all the unique pairs in each mini-batch. To cover those image pairs across batches, in each iteration the training images are randomly selected from the whole training set. By doing so, our method alleviates the need to store the whole pair-wise similarity matrix, thus being scalable to large-scale datasets.</p><p>Moreover, to learn models corresponding to different code lengths, if one chooses to train each model from scratch, it would be severely wasteful since the preceding layers can be shared by these models. Besides, as the code length grows, the model would contain more parameters in the output layer, and thus gets prone to overfitting. To overcome such limitations, we propose to first train a network with a few nodes in the output layer, and then finetune it to obtain the target model with the desired code length. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Datasets and Evaluation Metrics</head><p>We verify the effectiveness of our proposed method and compare with other state-of-the-art methods on two widely used datasets: (1) CIFAR-10 <ref type="bibr" target="#b10">[11]</ref>. This dataset consists of 60,000 32 × 32 images belonging to 10 mutually exclusive categories (6,000 images per category). The images are directly used as input for those competing CNN-based methods as well as our DSH. For conventional hashing methods, the images are represented by 512-D GIST descriptors <ref type="bibr" target="#b20">[21]</ref> following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>. (2) NUS-WIDE <ref type="bibr" target="#b0">[1]</ref>. This dataset contains 269,648 images collected from Flickr. The associations between images and 81 concepts are manually annotated. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>, we use the images associated with the 21 most frequent concepts, where each of these concepts associates with at least 5,000 images, resulting in a total of 195,834 images. The images are warped to 64 × 64 before inputting to the CNN-based methods. For conventional hashing methods, images are represented by the provided 225-D normalized block-wise color moments features.</p><p>In our experiments, similarity labels are defined by semantic-level labels. For CIFAR-10, images from the same category are considered semantically similar, and vice versa. The officially provided train/test split was used for experiments, namely, 50,000 images for training the models and 10,000 images for evaluating. For NUS-WIDE, if two images share at least one positive label, they are considered similar, and dissimilar otherwise. We randomly sampled 10,000 images to form the test query set, and used the rest as training set.</p><p>Following previous works, the evaluation metrics used are: the mean Average Precision (mAP) for different code lengths, precision-recall curves (48-bit), and mean precision within Hamming radius 2 for different code lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of the Regularizer</head><p>In this part, we validate the effectiveness of the proposed regularizer, and compare it with the standard relaxation scheme used in existing CNN-based hashing methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref>. Without loss of generality, we only test the case when k = 12, and set m = 24 in our DSH according to Section 3.3. The sigmoid relaxed models were trained almost the same as ours except for using sigmoid function as the activation of the output layer and setting α = 0. We test these models with m = {1, 2, 3, 6} (note that the maximum distance between network outputs of these models is k).</p><p>The retrieval mAP of different models are listed in Table 1. <ref type="figure">Figure 2</ref> shows the distribution of network outputs on the test set of CIFAR-10 under different settings (more results are provided in supplementary materials). We make three observations from the comparison results: First, without regularization (α = 0), the network outputs concentrate on the quantization threshold 0 <ref type="figure">(Fig.2a)</ref>, thus it is likely that neighboring points in the output space are quantized to very different binary codes; Second, imposing the regularizer (α = {0.001, 0.01, 0.1}, <ref type="figure">Fig.2b,c,d</ref>) can reduce the discrepancy between the real-valued output space and the Hamming space, and the retrieval performances can be improved significantly when setting α under a reasonable range (e.g. [0.001, 0.01]); Third, with proper settings of m, the sigmoid relaxed model can learn binary-like outputs <ref type="figure">(Fig.2e,f,g</ref>). Nonetheless, the retrieval performances of such codes are much inferior to our best-performing ones and are sensitive to m. Increasing the number of training iterations and carefully tuning m might improve the performance of the sigmoid relaxed models, however, it would take much more time to obtain a satisfactory model. Based on the above observations, we empirically set α = 0.01 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Online vs. Offline Image Pair Generation</head><p>This part compares the convergence behavior of our online image pair generating scheme against the alternative Siamese scheme, as described in Section 3.3. Both schemes employed the same network structure and hyperparameters as detailed in Section 3.3 (k = 12, m = 24). Due to limited storage space, 10 million image pairs were generated offline for the Siamese scheme, and the learning rate policy was tuned accordingly. For fair comparison, we input the same number of images to both schemes in each iteration (200 images for our online scheme and 100 image pairs for the alternative Siamese scheme). Since the computations mainly take place in the convolution-pooling layers, the computation costs of the two schemes are approximately the same. <ref type="figure">Figure 3</ref> shows the training loss against the number of iterations on both datasets. As can be seen, our online training scheme converges much faster than the Siamese alternative, since our online scheme has the capacity to utilize much more image pairs in each iteration, which offers more information about the semantic relations between different images. Besides, by sampling from the whole training set  <ref type="table">Table 2</ref>. Comparison of retrieval performance (mAP) of the models trained from scratch and the finetuned models.</p><p>in each iteration, our scheme can make use of more image pairs than the offline generated 10 million pairs for Siamese, and thus satisfactorily converges to a lower loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Finetuning vs. Training From Scratch</head><p>As mentioned in Section 3.3, if the last fully connected layer contains a large number of nodes, training the model from scratch may lead to overfitting. To get a clear understanding of the situation, in this part, we compare the mod-  els finetuned from a pretrained network against the models trained from scratch. Specifically, We first trained four models that produces {12, 24, 36, 48}-bit binary codes respectively (the first four rows in <ref type="table">Table 2</ref>). Then we replaced the last fully connected layer of the 12-bit model with a larger one, and finetuned it to get another group of {24, 36, 48}-bit models (the last three rows in <ref type="table">Table 2</ref>). For finetuning, the learning rate was set to 10 −3 for the last fully connected layer and 10 −4 for the preceding layers, and decreased by a factor of 0.6 after every 4,000 iterations. The model was trained with 30,000 iterations in total.</p><p>The retrieval mAPs on both datasets are listed in Table 2. It can be found that as the code length grows, the retrieval performance of finetuned models consistently improves, while the performance of models trained from scratch falls, especially on the NUS-WIDE dataset with a large drop. To take a closer look at the situation, we analyze the training/test loss on two example models, namely, the 48-bit model trained from scratch, and the finetuned 48-bit model. <ref type="figure" target="#fig_2">Figure 4</ref> shows the loss against the number of iterations for the two models on CIFAR-10. It is clear that on the first model (trained from scratch), the training loss keeps decreasing, while the test loss decreases as expected at first but increases after about 30,000 iterations, indicating overfitting on the training set. As comparison, on the second model (finetuned), the test loss decreases at first and then favorably stablizes after only a few thousand iterations. Such observations suggest that the different models with various code length can share those preceding layers to reduce training cost as well as to alleviate overfitting. For more results please refer to supplementary materials.</p><p>Moreover, we investigate network ensembles, which are widely used in classification tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8]</ref>, for retrieval problem. Specifically, we trained four 12-bit models with different random initializations, and concatenated the quantized network outputs as binary codes. Under the same code length, the ensemble codes further improve the retrieval performance of the finetuned codes by up to 0.04 in mAP, verifying the effectiveness of network ensembles in retrieval task (the details are provided in supplementary materials). One possible explanation is that the multiple networks can capture complementary image characteristics due to random initialization. Nevertheless, since exploiting network ensembles leads to multiple times of training cost, we adopt the finetuned models in the following experiments for efficiency consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the State-of-the-art</head><p>Comparative methods: We compare our method with LSH <ref type="bibr" target="#b3">[4]</ref>, SH <ref type="bibr" target="#b27">[28]</ref>, ITQ <ref type="bibr" target="#b5">[6]</ref>, CCA-ITQ <ref type="bibr" target="#b5">[6]</ref>, MLH <ref type="bibr" target="#b19">[20]</ref>, BRE <ref type="bibr" target="#b12">[13]</ref>, KSH <ref type="bibr" target="#b16">[17]</ref>, CNNH <ref type="bibr" target="#b28">[29]</ref>, DLBHC <ref type="bibr" target="#b14">[15]</ref>, and DNNH <ref type="bibr" target="#b13">[14]</ref>. These methods were all implemented using source codes provided by the authors except for DNNH 2 . For fair comparison, all the CNN-based methods, including CNNH, DLBHC, DNNH, and DSH, used the same network structure, as described in Section 3.3. Note that while more complicated network structures can be also feasible, we chose to work with a relatively simple one for fast evaluation.</p><p>Training set: We aim to use the whole training data to train models for all methods if possible. However, due to the huge amount of memory demanded by MLH, KSH and CNNH (O(N 2 ), where N is the number of training images), in our experiments, we randomly selected a 20K subset from each dataset to train models for these three methods, which costs more than 10GB of memory.</p><p>Parameter settings: The parameters of those comparative methods were all set based on the authors' suggestions in the original publications. In particular, we found the divide-and-encode structure devised in DNNH <ref type="bibr" target="#b13">[14]</ref> largely degraded the retrieval mAP on CIFAR-10 (about 0.07) and brought marginal improvement on NUS-WIDE (0.01 ∼ 0.03) in our experiments, thus we report the performance of the fully connected version for simplicity.</p><p>Results: The comparisons of our method against the others are shown in <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_3">Figure 5</ref>. In general, those CNN-based methods outperform the conventional hash learning methods on both datasets by a large margin, validating the advantage of learning image representations over using hand-crafted features. Moreover, we investigate some conventional hashing methods trained with CNN features, although the performances were significantly improved, they were still inferior to our DSH, suggesting that our end-to-end learning scheme is advantageous (the details are provided in the supplementary materials).</p><p>Among the CNN-based methods, it is observed that our DSH yields the highest accuracy in most cases. The performance gaps between these methods mainly come from the differences in their training objectives: CNNH trains the model to fit the pre-computed discriminative binary codes. However, as the binary code generation and the network learning are isolated, a mismatch exists between the two stages; DLBHC trains the model with a binary-like hidden layer as features for classification tasks , thus encoding dissimilar images to similar binary codes would not be punished as long as the classification accuracy is unaffected; While DNNH uses triplet-based constraints (rather than the pairwise constraints we adopt) to describe more complex semantic relations, training its network becomes more difficult, due to the sigmoid non-linearity and the parameterized piece-wise threshold function used in the output layer. As a result, DNNH performs inferior to our DSH method, especially on CIFAR-10, where the triplet-based constraints cannot provide more information than the pairwise ones since the images only have category labels (some real retrieval cases are provided in the supplementary materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison of Encoding Time</head><p>In real-world applications, generating binary codes for new-coming images should be fast. In this part, we compare the encoding time of our DSH method and 7 other supervised hashing methods: CCA-ITQ <ref type="bibr" target="#b5">[6]</ref>, MLH <ref type="bibr" target="#b19">[20]</ref>, BRE <ref type="bibr" target="#b12">[13]</ref>, KSH <ref type="bibr" target="#b16">[17]</ref>, CNNH <ref type="bibr" target="#b28">[29]</ref>, DLBHC <ref type="bibr" target="#b14">[15]</ref>, and DNNH <ref type="bibr" target="#b13">[14]</ref>, including the linear and non-linear conventional hashing methods along with the state-of-the-art CNN-based methods. For thorough comparison, we report the encoding time of CNN-based methods both on CPU and GPU, and the feature extraction time for conventional hashing methods (using the publicly available code of GIST feature extraction <ref type="bibr" target="#b20">[21]</ref>). Since we used the authors' provided features for NUS-WIDE and only extracted features for CIFAR-10, all comparisons were conducted on CIFAR-10. Without loss of generality, we only report the timings of 24-bit and 48-bit codes. The binary codes of all CNN-based methods were generated with the same version of Caffe. The experiments were carried out on a PC with Intel i7-4770, 32GB RAM, and NVIDIA Titan Black with CUDA-7.0 and cuDnn v3.0.  <ref type="table">Table 3</ref>. Comparison of retrieval mAP of our DSH method and the other hashing methods on CIFAR-10 and NUS-WIDE.  The logarithmic encoding time (in microseconds, base 10) of such hashing methods is shown in <ref type="figure" target="#fig_4">Figure 6</ref>, where results were obtained by averaging over the whole test set. CNN-based methods take almost the same time to encode a single image with varying code lengths, since the computations mainly take place in the common preceding layers. In general, when only considering generating binary codes from model inputs, even the GPU accelerated version of CNN-based methods are slower than the conventional methods by at least an order of magnitude. However, taking the feature extraction time into consideration, the CNN-based methods are 10x faster than the conventional hashing methods. Moreover, the conventional hashing methods usually require several types of features to achieve comparable retrieval performance to CNN-based methods, which further slows down the whole encoding procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We attribute the promising retrieval performance of DSH to three aspects: First, the coupling of non-linear feature learning and hash coding for extracting task-specific image representations; Second, the proposed regularizer for reducing the discrepancy between the real-valued network output space and the desired Hamming space; Third, the online generated dense pairwise supervision for well describing the desired Hamming space. In terms of efficiency, experiments have shown that the proposed method encodes new-coming images even faster than conventional hashing methods. Since our current framework is relatively general, more complex network structure can also be easily exploited. In addition, preliminary study of "network ensembles" in this work has proven it a promising way that is worth our future investigation to further boost retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Distribution of network outputs on the test query set of CIFAR-10. (a)-(d) the models using our proposed regularizer under different settings of α, (e)-(h) the sigmoid relaxed models under different settings of m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of (a) the model trained from scratch and (b) the finetuned model in terms of training/test loss, on the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of retrieval performance of our DSH method and the other hashing methods on CIFAR-10 (results on NUS-WIDE are provided in supplementary materials). (a) PR curves (48-bit). (b) Mean precision within Hamming radius 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Time cost to encode one new-coming image (microseconds) on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Method CIFAR-10 12-bit 24-bit 36-bit 48-bit NUS-WIDE 12-bit 24-bit 36-bit 48-bit</figDesc><table>LSH [4] 
0.1277 0.1367 0.1407 0.1492 
0.3329 0.3392 0.3450 0.3474 
SH [28] 
0.1319 0.1278 0.1364 0.1320 
0.3401 0.3374 0.3343 0.3332 
ITQ [6] 
0.1080 0.1088 0.1117 0.1184 
0.3425 0.3464 0.3522 0.3576 
CCA-ITQ [6] 
0.1653 0.1960 0.2085 0.2176 
0.3874 0.3977 0.4146 0.4188 
MLH [20] 
0.1844 0.1994 0.2053 0.2094 
0.3829 0.3930 0.3959 0.3990 
BRE [13] 
0.1589 0.1632 0.1697 0.1717 
0.3556 0.3581 0.3549 0.3592 
KSH [17] 
0.2948 0.3723 0.4019 0.4167 
0.4331 0.4592 0.4659 0.4692 
CNNH [29] 
0.5425 0.5604 0.5640 0.5574 
0.4315 0.4358 0.4451 0.4332 
DLBHC [15] 
0.5503 0.5803 0.5778 0.5885 
0.4663 0.4728 0.4921 0.4916 
DNNH [14] 
0.5708 0.5875 0.5899 0.5904 
0.5471 0.5367 0.5258 0.5248 
DSH 
0.6157 0.6512 0.6607 0.6755 
0.5483 0.5513 0.5582 0.5621 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code of our DSH with running samples are available at http://vipl.ict.ac.cn/resources/codes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since the source code of DNNH is not publicly available, we used our own implementation of this method for experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval</title>
		<meeting>the ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1042" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3270" to="3278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning of binary hash codes for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discrete graph hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2074" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-11</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attribute discovery via predictable discriminative binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="876" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large-scale search. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bitscalable deep hashing with regularized similarity learning for image retrieval and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
