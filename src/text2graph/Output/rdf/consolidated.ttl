@prefix : <https://github.com/deepcurator/DCC/> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xml: <http://www.w3.org/XML/1998/namespace> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

: a owl:Ontology ;
    rdfs:label "DeepSciKG" ;
    rdfs:comment "The first iteration of the Siemens Ontology Schema to represent the multimodal curated elements from the DARPA ASKE Project." .

<https://github.com/deepcurator/DCC/1807.03039v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1807.03039v2_benchmarks>,
        <https://github.com/deepcurator/DCC/1807.03039v2_convolution>,
        <https://github.com/deepcurator/DCC/1807.03039v2_flow>,
        <https://github.com/deepcurator/DCC/1807.03039v2_generative_model>,
        <https://github.com/deepcurator/DCC/1807.03039v2_generative_models>,
        <https://github.com/deepcurator/DCC/1807.03039v2_images>,
        <https://github.com/deepcurator/DCC/1807.03039v2_improvement>,
        <https://github.com/deepcurator/DCC/1807.03039v2_log-likelihood>,
        <https://github.com/deepcurator/DCC/1807.03039v2_model>,
        <https://github.com/deepcurator/DCC/1807.03039v2_our_method>,
        <https://github.com/deepcurator/DCC/1807.03039v2_that>,
        <https://github.com/deepcurator/DCC/1807.03039v2_this>,
        <https://github.com/deepcurator/DCC/1807.03039v2_training>,
        <https://github.com/deepcurator/DCC/1807.03039v2_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp0> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp1> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp10> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp3> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp4> a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp5> a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp6> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp8> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp9> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Text0> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Text8> :partOf <https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_algorithm>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_covariance>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_effectiveness>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_efficiency>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_feature>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_features>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_image>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_images>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matching>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matrix>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_method>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_methods>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_network>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_optimization>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_our_method>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_quality>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_textures>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_that>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_these>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_this>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_training>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_transfer>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_visual_quality>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp0> a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp1> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp12> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp13> a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp15> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp17> a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp18> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp19> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp2> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp3> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp4> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp5> a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp7> a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp8> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp9> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Text3> :partOf <https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_approach>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_architectures>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_diversity>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_image>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_input>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_learns>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_map>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_method>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_methods>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_model>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_modeling>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_network>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_one>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_other>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_our_method>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_outputs>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problem>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problems>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_results>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_this>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_time>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_training>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp0> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp1> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp11> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp12> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp13> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp15> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp16> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp2> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp3> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp4> a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp5> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp6> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp7> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp8> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp9> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text0> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text1> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text10> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text4> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text5> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text6> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text7> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Text8> :partOf <https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_architecture>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_audio>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_benchmarks>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_convolutional_neural_networks>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_correlations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_data>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_deep_learning>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_dynamics>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_first>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_images>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_information>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_interactions>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_layers>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_learn>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_locations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_model>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_modeling>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_principles>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_representations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_state>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structural_variations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structured_data>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_that>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_those>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_video>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_adversarial>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_analogy>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_approach>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_baselines>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_cifar-10>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_data>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_database>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_datasets>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_divergences>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_evaluations>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experimental_results>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experiments>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_gan>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_imagenet>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kl>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kullback-leibler>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_mnist>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_network>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_novel_approach>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_objective_function>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_our_method>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_problem>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_properties>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_quality>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_rewards>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_scale>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_state>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_that>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_theoretical_analysis>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_these>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_this>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_two>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp0> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp1> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp2> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp3> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp4> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp6> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp7> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Comp8> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_Text0> :partOf <https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_activity_recognition>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_convergence>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_data>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_datasets>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_environments>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_features>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_first>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_framework>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_gradient>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_hierarchical>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_kitti>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_layers>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_loss_function>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_methods>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_model>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_models>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_priori>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_processing>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_region>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_target>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tasks>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_this>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tracking>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_training>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_two>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_videos>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_6898-hierarchical-attentive-recurrent-tracking-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1_Comp0> :partOf <https://github.com/deepcurator/DCC/:fig_6898-hierarchical-attentive-recurrent-tracking-Figure3-1> .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1_Comp1> :partOf <https://github.com/deepcurator/DCC/:fig_6898-hierarchical-attentive-recurrent-tracking-Figure3-1> .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_accuracy>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_adversarial>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approach>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approaches>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_cifar-10>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data_augmentation>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_datasets>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain_experts>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_experiments>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_f1>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_functions>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_image>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_improvements>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_input>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_medical_imaging>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_method>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_model>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_operations>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_relation_extraction>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_results>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_state>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_task>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_technique>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_text>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_that>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_this>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_time>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_training>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformation_model>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformations>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_unlabeled_data>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_data>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_dynamics>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_encoder>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_end-to-end>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_framework>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_methods>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_missing_data>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_model>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_objects>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representation>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representations>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_state>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_systems>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_tasks>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_that>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_time>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_two>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_unsupervised_learning>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_video>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_videos> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_architecture>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_convolutional_neural_network>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_datasets>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_deep_learning>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_formulations>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graph>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graphs>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_learn>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_matrix>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_models>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_neural_network>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_novel_approach>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_our_method>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_parameters>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_priors>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_recurrent_neural_network>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_smoothness>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_state>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structured>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structures>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_system>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_systems>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_techniques>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_that>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these_techniques> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithm>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithms>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_architectures>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_complex>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_computations>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_data>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_flexibility>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_hardware>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_implementation>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_models>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_neural_network>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_operations>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_structure>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_task>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tasks>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tensorflow>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_that>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_them>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_this>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp0> a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp1> :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp2> a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp3> a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp4> a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp5> a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp6> a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp7> a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp8> :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp9> :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Text0> :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Text1> :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Text2> :partOf <https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_accuracy>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_approaches>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_architectures>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_benchmarks>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_input>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_interactions>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_model>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_modeling>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_models>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network_models>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_optimization>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_problem>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_research>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_results>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_scalability>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_state>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_systems>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_that>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_these>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_this>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Comp0> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Comp1> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Comp2> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Comp3> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Comp5> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Comp7> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Comp8> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Text0> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Text1> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Text2> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_Text4> :partOf <https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_architecture>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanism>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanisms>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu_score>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_complex>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutional_neural_networks>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutions>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_decoder>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_encoder>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_ensembles>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_french>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_machine_translation_tasks>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_model>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_models>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_network>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_quality>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_results>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_state>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_task>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_that>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_these>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_time>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_training>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_two> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1>,
        <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7192-value-prediction-network> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7192-value-prediction-network_architecture>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_baselines>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_deep_reinforcement_learning_(rl)>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_dynamics>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_environment>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_experimental_results>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_learns>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_methods>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_model>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_model-based>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_model-free>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_network>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_neural_network>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_observations>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_planning>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction_model>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_predictions>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_representation>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_rewards>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_rl>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_state>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_states>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_that>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_way> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Comp0> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Comp2> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Comp3> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Comp4> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Comp5> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Text0> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Text1> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_Text6> :partOf <https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_architecture>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_baselines>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_concepts>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_feature>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_input>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_inputs>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_language>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_maps>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_mechanism>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_models>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_network>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_processing>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_question_answering>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_representation>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_resnet>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_tasks>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_that>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_this>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_two>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_vision>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_we> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1>,
        <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_approaches>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_architectures>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_complex>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_components>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_deep_learning>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_framework>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_global_context>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_graph>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_images>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_interactions>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model_design>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_modeling>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_objects>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction_model>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_principles>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_results>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scene>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scenes>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_state>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_structured>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_task>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_that>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_this>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_understanding>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:AbstractText a owl:Class ;
    rdfs:subClassOf :Text .

:AdadeltaOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:AdagradOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:AdamOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp1 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp2 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp4 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp6 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> .

:BodyText a owl:Class ;
    rdfs:subClassOf :Text .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_approach,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_baseline,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concept,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concepts,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_datasets,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_effectiveness,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_embeddings,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_end-to-end,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_experiments,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_grounding,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_images,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_improvement,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_layers,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_learns,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_model,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_phrases,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_prior_works,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_region,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representation,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representations,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_solution,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_subspaces,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_text,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_that,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_them,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp5 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text10 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text11 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text12 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text15 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text16 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Text17 :partOf <https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Busta_Deep_TextSpotter_An_ICCV_2017_paper_accuracy,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_cnn,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_data,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_datasets,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_end-to-end,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_input,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_magnitude,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_method,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_methods,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_per,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_resolution,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_scene,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_state,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_structure,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_text,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_text_recognition,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_that,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_training,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_two ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp2 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp6 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> .

:BytesList a owl:Class ;
    rdfs:subClassOf :train .

:Camgoz_Neural_Sign_Language_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Camgoz_Neural_Sign_Language_CVPR_2018_paper_language ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:CheckpoingManager a owl:Class ;
    rdfs:subClassOf :train .

:Checkpoint a owl:Class ;
    rdfs:subClassOf :train .

:CheckpointSaverHook a owl:Class ;
    rdfs:subClassOf :train .

:CheckpointSaverListener a owl:Class ;
    rdfs:subClassOf :train .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp14 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp3 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp4 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp6 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_applications,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_approaches,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_benchmarks,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_cameras,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_data,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_experiments,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_first,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_information,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_learn,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_networks,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_one,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_point_clouds,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_policies,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_quality,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_research,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_scale,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_semantic,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_tasks,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_that,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_this,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_understanding,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_video,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_videos,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_vision,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text12 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text15 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text16 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text21 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text23 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text26 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> .

:Chen_Multi-View_3D_Object_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp1 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp14 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp2 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp8 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp9 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text10 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text11 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text12 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text15 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_natural_images,
        :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_research,
        :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_text ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1>,
        <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp12 a :FlattenBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp14 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp17 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp19 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp20 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp21 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp25 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp27 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp5 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp6 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp9 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text11 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_Comp2 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1> .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1> .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_accuracies,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_algorithms,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_approach,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_cifar-10,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_classification,
        <https://github.com/deepcurator/DCC/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_convolutional_neural_networks_(cnns)>,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_imagenet,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_method,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_methods,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_model,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_models,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_optimization,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_our_method,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_reinforcement_learning,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_rl,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search_space,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_sequential_model-based,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_state,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_strategy,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structure,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structures,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_that,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_this,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_times,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_way,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1> .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1> .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1> .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1> .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1> .

:ChiefSessionCreator a owl:Class ;
    rdfs:subClassOf :train .

:ClusterDef a owl:Class ;
    rdfs:subClassOf :train .

:ClusterSpec a owl:Class ;
    rdfs:subClassOf :train .

:Coordinator a owl:Class ;
    rdfs:subClassOf :train .

:EmbedBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Example a owl:Class ;
    rdfs:subClassOf :train .

:ExponentialMovingAverage a owl:Class ;
    rdfs:subClassOf :train .

:Fan_A_Point_Set_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Fan_A_Point_Set_CVPR_2017_paper_data,
        :Fan_A_Point_Set_CVPR_2017_paper_deep_neural_networks ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp4 a :DropoutBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> .

:Feature a owl:Class ;
    rdfs:subClassOf :train .

:FeatureList a owl:Class ;
    rdfs:subClassOf :train .

:FeatureLists a owl:Class ;
    rdfs:subClassOf :train .

:Features a owl:Class ;
    rdfs:subClassOf :train .

:FeedFnHook a owl:Class ;
    rdfs:subClassOf :train .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_annotations,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_classification,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_deep_neural_network,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_environment,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_image,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_interaction,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_map,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_methods,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_navigation,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_network,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_our_method,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_parameters,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_problem,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_results,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_rgb,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_robot,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_scale,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_segmentation,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_semantic_labels,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_structure,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_surfaces,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_tasks,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_that,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_this,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_vision,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp14 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp19 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp20 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp21 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp4 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> .

:FinalOpsHook a owl:Class ;
    rdfs:subClassOf :train .

:FloatList a owl:Class ;
    rdfs:subClassOf :train .

:FtrlOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:GenericTerm a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> .

:GlobalStepWaiterHook a owl:Class ;
    rdfs:subClassOf :train .

:GradientDescentOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_3d_reconstruction,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_adversarial,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotation,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotations,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_data,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_images,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_measure,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_models,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_paradigms,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_pose,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_structure,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_supervision,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_task,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_that,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_these,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_this,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_training,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_transfer_learning,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_unified_framework,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp10 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp12 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp14 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp15 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp17 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp19 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp21 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp4 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp7 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp8 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp9 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp14 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp19 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp20 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp21 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp22 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp23 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp24 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp25 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp26 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp27 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp28 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp29 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp30 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp31 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp32 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp33 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp34 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp35 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp36 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp37 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp38 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp39 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp40 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp41 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp42 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp43 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp44 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp45 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp46 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp47 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp48 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp6 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :He_Deep_Residual_Learning_CVPR_2016_paper_accuracy,
        :He_Deep_Residual_Learning_CVPR_2016_paper_cifar-10,
        :He_Deep_Residual_Learning_CVPR_2016_paper_classification_task,
        :He_Deep_Residual_Learning_CVPR_2016_paper_deep_representations,
        :He_Deep_Residual_Learning_CVPR_2016_paper_ensemble,
        :He_Deep_Residual_Learning_CVPR_2016_paper_framework,
        :He_Deep_Residual_Learning_CVPR_2016_paper_functions,
        :He_Deep_Residual_Learning_CVPR_2016_paper_imagenet,
        :He_Deep_Residual_Learning_CVPR_2016_paper_improvement,
        :He_Deep_Residual_Learning_CVPR_2016_paper_inputs,
        :He_Deep_Residual_Learning_CVPR_2016_paper_layers,
        :He_Deep_Residual_Learning_CVPR_2016_paper_networks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_neural_networks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_object_detection,
        :He_Deep_Residual_Learning_CVPR_2016_paper_recognition_tasks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_representations,
        :He_Deep_Residual_Learning_CVPR_2016_paper_segmentation,
        :He_Deep_Residual_Learning_CVPR_2016_paper_tasks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_that,
        :He_Deep_Residual_Learning_CVPR_2016_paper_these,
        :He_Deep_Residual_Learning_CVPR_2016_paper_those,
        :He_Deep_Residual_Learning_CVPR_2016_paper_training,
        :He_Deep_Residual_Learning_CVPR_2016_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1>,
        <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1>,
        <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1_Comp0 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp0 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp2 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp22 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp23 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp24 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp26 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp29 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp31 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp32 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp35 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp38 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp39 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp40 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp42 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp43 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp44 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp46 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp47 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp57 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp6 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp63 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp70 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp73 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp74 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp75 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp77 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp81 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp82 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp83 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp84 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp85 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp86 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Text22 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Text23 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Text24 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Text25 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Text27 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Text30 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_Comp3 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1> .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_Comp4 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp12 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp13 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp14 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp4 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp6 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp9 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_computation,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_imagenet,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_improvements,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_models,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_state,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_them ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp1 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp3 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp5 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity <https://github.com/deepcurator/DCC/Hui_Fast_and_Accurate_CVPR_2018_paper_deep_convolutional_neural_networks_(cnns)> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp10 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp12 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp14 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp19 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp20 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp21 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp22 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp23 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp24 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp25 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp26 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp27 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp28 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp29 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp6 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp7 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp8 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> .

:Int64List a owl:Class ;
    rdfs:subClassOf :train .

:JobDef a owl:Class ;
    rdfs:subClassOf :train .

:Kong_RON_Reverse_Connection_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1> .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1> .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp2 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1> .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp3 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1> .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1> .

:LSTMSeqBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_accuracy,
        :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_image_super-resolution ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp1 a :DeconvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp4 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp7 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text14 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text15 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text16 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text17 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text18 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text19 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_image_classification,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_models,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_noise,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_problem,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_this,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp0 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp2 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp4 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_3d_reconstructions,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_component,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_deep_learning,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_distances,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edge,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edges,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_features,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_first,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_learn,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_loss_function,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_minimize,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_network,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_neural_network,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_our_method,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_patches,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_point_clouds,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_results,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_state,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_surface,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_technique,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_this,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp3 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp7 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp8 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text10 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text11 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text12 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text10 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text11 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text16 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text17 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper_face_recognition ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp3 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> .

:LoggingTensorHook a owl:Class ;
    rdfs:subClassOf :train .

:LooperThread a owl:Class ;
    rdfs:subClassOf :train .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1> .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1> .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1> .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1> .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp10 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp13 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp14 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp2 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp9 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text10 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text14 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text15 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1>,
        <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp0 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp1 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp14 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp19 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp2 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp3 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> .

:Metric a owl:Class ;
    rdfs:subClassOf :TextEntity .

:MomentumOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:MonitoredSession a owl:Class ;
    rdfs:subClassOf :train .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Mousavian_3D_Bounding_Box_CVPR_2017_paper_image,
        :Mousavian_3D_Bounding_Box_CVPR_2017_paper_method,
        :Mousavian_3D_Bounding_Box_CVPR_2017_paper_object_detection,
        :Mousavian_3D_Bounding_Box_CVPR_2017_paper_pose_estimation ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp8 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> .

:NanLossDuringTrainingError a owl:Class ;
    rdfs:subClassOf :train .

:NanTensorHook a owl:Class ;
    rdfs:subClassOf :train .

:Optimizer a owl:Class ;
    rdfs:subClassOf :train .

:OtherScientificTerm a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1>,
        <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp5 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> .

:ProfilerHook a owl:Class ;
    rdfs:subClassOf :train .

:ProximalAdagradOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:ProximalGradientDescentOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp7 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text12 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text14 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text16 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text18 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text19 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text20 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text21 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text22 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> .

:QueueRunner a owl:Class ;
    rdfs:subClassOf :train .

:RMSPropOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:RNNBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:RNNSeqBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Saver a owl:Class ;
    rdfs:subClassOf :train .

:SaverDef a owl:Class ;
    rdfs:subClassOf :train .

:Scaffold a owl:Class ;
    rdfs:subClassOf :train .

:SecondOrStepTimer a owl:Class ;
    rdfs:subClassOf :train .

:SequenceExample a owl:Class ;
    rdfs:subClassOf :train .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_adaptation,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_approaches,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_automatic,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_commercial_systems,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_conditions,
        <https://github.com/deepcurator/DCC/Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_convolutional_neural_network_(cnn)>,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_datasets,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_experimental_results,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_image,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_images,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_manual_annotations,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_method,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_methods,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_ones,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_region,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_regions,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_state,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_system,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_that,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp0 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp10 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp14 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp15 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp17 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp3 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp6 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> .

:Server a owl:Class ;
    rdfs:subClassOf :train .

:ServerDef a owl:Class ;
    rdfs:subClassOf :train .

:SessionCreator a owl:Class ;
    rdfs:subClassOf :train .

:SessionManager a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunArgs a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunContext a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunHook a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunValues a owl:Class ;
    rdfs:subClassOf :train .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp14 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp19 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp20 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp21 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp22 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp23 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp24 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp25 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp26 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp27 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp28 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp29 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp30 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp31 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp32 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp33 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp34 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> .

:SingularMonitoredSession a owl:Class ;
    rdfs:subClassOf :train .

:SourceCodeFile a owl:Class .

:StepCounterHook a owl:Class ;
    rdfs:subClassOf :train .

:StopAtStepHook a owl:Class ;
    rdfs:subClassOf :train .

:SummarySaverHook a owl:Class ;
    rdfs:subClassOf :train .

:Supervisor a owl:Class ;
    rdfs:subClassOf :train .

:SyncReplicasOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1>,
        <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1>,
        <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1>,
        <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1>,
        <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1>,
        <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1>,
        <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp3 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp7 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp8 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp9 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp5 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp7 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp6 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp8 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp11 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp9 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp0 a :ConcatBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp7 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp2 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp3 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp4 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_Comp3 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1>,
        <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1>,
        <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1>,
        <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp1 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp10 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp12 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp14 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp15 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp16 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp2 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp3 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp4 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp6 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp7 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp8 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_Comp0 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp11 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp2 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp3 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp4 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp6 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp8 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp9 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp10 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp11 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp3 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp7 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity <https://github.com/deepcurator/DCC/Tai_MemNet_A_Persistent_ICCV_2017_paper_deep_convolutional_neural_networks_(cnns)> ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1>,
        <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp2 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Text21 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp10 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp8 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Text14 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_image_deblurring,
        :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_scheme ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp19 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp21 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp22 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp4 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp1 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp6 a :UnpoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp10 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp14 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp15 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp6 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> .

:TitleText a owl:Class ;
    rdfs:subClassOf :Text .

:Toderici_Full_Resolution_Image_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1>,
        <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp11 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp12 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp14 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp2 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp6 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp8 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp9 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp2 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp4 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> .

:UserDefined a owl:Class ;
    rdfs:subClassOf :CodeEntity .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp15 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp16 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp18 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp19 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp20 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp7 a :FlattenBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp8 a :RnnSeqBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp0 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp1 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp10 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp11 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp13 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp15 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp16 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp17 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp2 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp4 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp5 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp6 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp7 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp8 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp9 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> .

:VocabInfo a owl:Class ;
    rdfs:subClassOf :train .

:WorkerSessionCreator a owl:Class ;
    rdfs:subClassOf :train .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_accuracy,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_approach,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_architecture,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computation_efficiency,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computer_vision_tasks,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_convolutional_neural_network,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_efficiency,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_end-to-end,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_face,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_feature,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_gradient,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_interference,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_module,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_multi-task_learning_methods,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_optimum,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_other,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_parameters,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_problem,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_quality,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_relevance,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_retrieval,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_scales,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_task,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_tasks,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_that,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_they,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_this,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_those,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_time,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_training,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_two,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp2 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp4 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp3 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp4 a :PoolingBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_alignment,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_attention_mechanism,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_audio,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_correlations,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_distance,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_event,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_experiments,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_features,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_information,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modalities,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modality,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modeling,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_network,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_objects,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_problem,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_semantics,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_tasks,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_that,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_this,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_two,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_unconstrained_videos,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_video,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1>,
        <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp10 a :DeconvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp13 a :FlattenBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp7 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp2 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp3 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp7 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp8 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp1 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp2 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp3 a :DeconvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp4 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp7 a :DeconvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp8 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Text12 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Comp2 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text10 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text11 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Text9 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp2 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_adversarial,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_baseline,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cameras,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cnn_embeddings,
        <https://github.com/deepcurator/DCC/Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_convolutional_neural_network_(cnn)>,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_data,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_datasets,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_gan,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_generation,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_images,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_improvement,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_method,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_model,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_network,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_other,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_outliers,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_person_re-identification,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_problem,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_regularization,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_representation_learning,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_scale,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_smoothing,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_task,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_that,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_this,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_training,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Comp0 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1> .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Comp1 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1> .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Comp2 a :LossBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1> .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1> .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1> .

:amos17b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :amos17b_architecture,
        :amos17b_constraints,
        :amos17b_convex,
        :amos17b_data,
        :amos17b_function,
        :amos17b_image_completion,
        :amos17b_improvement,
        :amos17b_input,
        :amos17b_inputs,
        :amos17b_methods,
        :amos17b_models,
        :amos17b_network,
        :amos17b_networks,
        :amos17b_neural_network,
        :amos17b_neural_network_architectures,
        :amos17b_neural_networks,
        :amos17b_optimization,
        :amos17b_optimization_algorithms,
        :amos17b_others,
        :amos17b_parameters,
        :amos17b_prediction,
        :amos17b_problems,
        :amos17b_reinforcement_learning,
        :amos17b_state,
        :amos17b_structured,
        :amos17b_that,
        :amos17b_these,
        :amos17b_this,
        :amos17b_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1>,
        <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:amos17b-Figure1-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> .

:amos17b-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> .

:amos17b-Figure1-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> .

:amos17b-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> .

:amos17b-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> .

:amos17b-Figure1-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> .

:amos17b-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> .

:amos17b-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> .

:amos17b-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> .

:amos17b-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> .

:amos17b-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> .

:amos17b-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> .

:amos17b-Figure2-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> .

:amos17b-Figure2-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> .

:and a owl:ObjectProperty .

:arxivId a owl:DatatypeProperty .

:author a owl:ObjectProperty .

:authorName a owl:DatatypeProperty .

:authorOf a owl:ObjectProperty ;
    rdfs:domain :PublicationAuthor ;
    rdfs:range :Publication .

:brukhim18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :brukhim18a_approach,
        :brukhim18a_approaches,
        :brukhim18a_architecture,
        :brukhim18a_baselines,
        :brukhim18a_benchmarks,
        :brukhim18a_classification,
        :brukhim18a_constraints,
        :brukhim18a_deep_learning,
        :brukhim18a_framework,
        :brukhim18a_machine_learning,
        :brukhim18a_model,
        :brukhim18a_modeling,
        :brukhim18a_models,
        :brukhim18a_outputs,
        :brukhim18a_prediction,
        :brukhim18a_prediction_models,
        :brukhim18a_problems,
        :brukhim18a_results,
        :brukhim18a_state,
        :brukhim18a_structured,
        :brukhim18a_that,
        :brukhim18a_them,
        :brukhim18a_this,
        :brukhim18a_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:brukhim18a-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> .

:brukhim18a-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> .

:brukhim18a-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> .

:brukhim18a-Figure1-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> .

:brukhim18a-Figure1-1_Comp6 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> .

:brukhim18a-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> .

:calls a owl:ObjectProperty ;
    rdfs:domain :Function ;
    rdfs:range :Function .

:cites a owl:ObjectProperty .

:compare a owl:ObjectProperty .

:conferenceSeries a owl:DatatypeProperty .

:conjunction a owl:ObjectProperty .

:donahue17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :donahue17a_approaches,
        :donahue17a_audio,
        :donahue17a_chart,
        :donahue17a_charts,
        :donahue17a_convolutional_neural_networks,
        :donahue17a_features,
        :donahue17a_generative_model,
        :donahue17a_lstm,
        :donahue17a_music,
        :donahue17a_platform,
        :donahue17a_rhythm,
        :donahue17a_task,
        :donahue17a_that,
        :donahue17a_two,
        :donahue17a_video,
        :donahue17a_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1>,
        <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:donahue17a-Figure5-1_Comp0 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp1 a :DenseBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp2 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp3 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp4 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp5 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure5-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> .

:donahue17a-Figure7-1_Comp0 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Comp1 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Comp2 a :RnnBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Comp3 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:donahue17a-Figure7-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> .

:feature-of a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:featureOf a owl:ObjectProperty .

:followedBy a owl:ObjectProperty .

:githubrepo a owl:DatatypeProperty .

:hasCaptionText a owl:DatatypeProperty ;
    rdfs:domain :CaptionText ;
    rdfs:subPropertyOf :hasText .

:hasComponent a owl:ObjectProperty .

:hasDataFlow a owl:DatatypeProperty .

:hasEntity a owl:ObjectProperty ;
    rdfs:domain :Modality ;
    rdfs:range :TextEntity .

:hasFigureId a owl:DatatypeProperty ;
    rdfs:domain :ImageComponent .

:hasFile a owl:DatatypeProperty .

:hasFunction a owl:ObjectProperty ;
    rdfs:domain :app,
        :audio,
        :strings,
        :summary,
        :sysconfig,
        :tf ;
    rdfs:range :decode_wav,
        :encode_wav,
        :run .

:hasModality a owl:ObjectProperty ;
    rdfs:domain :Publication ;
    rdfs:range :Code,
        :Figure,
        :Modality,
        :Text .

:hasPublicationId a owl:DatatypeProperty ;
    rdfs:domain :Publication .

:hasRepository a owl:ObjectProperty ;
    rdfs:domain :Publication ;
    rdfs:range :Repository .

:hasTitle a owl:DatatypeProperty .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp0 a :OutputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp2 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp4 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp7 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp8 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Comp9 a :NormBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure1-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7181-attention-is-all-you-need-Figure2-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp10 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure1-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Comp0 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Comp3 a :ActivationBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Comp9 a :LSTMBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Text5 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_7237-modulating-early-visual-processing-by-language-Figure3-1_Text6 :partOf <https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text13 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text14 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text15 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text16 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text7 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:hfig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> .

:isDeprecated a owl:DatatypeProperty .

:isExperimental a owl:DatatypeProperty .

:kalchbrenner17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kalchbrenner17a_action,
        :kalchbrenner17a_approaches,
        :kalchbrenner17a_architecture,
        :kalchbrenner17a_color,
        :kalchbrenner17a_discrete,
        :kalchbrenner17a_estimates,
        :kalchbrenner17a_model,
        :kalchbrenner17a_motion,
        :kalchbrenner17a_moving_mnist,
        :kalchbrenner17a_network,
        :kalchbrenner17a_objects,
        :kalchbrenner17a_state,
        :kalchbrenner17a_structure,
        :kalchbrenner17a_tensors,
        :kalchbrenner17a_that,
        :kalchbrenner17a_time,
        :kalchbrenner17a_video,
        :kalchbrenner17a_videos ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:kalchbrenner17a-Figure2-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> .

:kalchbrenner17a-Figure2-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> .

:kalchbrenner17a-Figure2-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> .

:kalchbrenner17a-Figure2-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> .

:kalchbrenner17a-Figure2-1_Comp4 a :ConvBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> .

:kalchbrenner17a-Figure2-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> .

:mirhoseini17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :mirhoseini17a_approach,
        :mirhoseini17a_classification,
        :mirhoseini17a_device,
        :mirhoseini17a_devices,
        :mirhoseini17a_distributed,
        :mirhoseini17a_environment,
        :mirhoseini17a_graph,
        :mirhoseini17a_graphs,
        :mirhoseini17a_hardware,
        :mirhoseini17a_heuristics,
        :mirhoseini17a_imagenet,
        :mirhoseini17a_inception,
        :mirhoseini17a_language_modeling,
        :mirhoseini17a_learns,
        :mirhoseini17a_lstm,
        :mirhoseini17a_method,
        :mirhoseini17a_methods,
        :mirhoseini17a_model,
        :mirhoseini17a_models,
        :mirhoseini17a_neural_machine_translation,
        :mirhoseini17a_neural_networks,
        :mirhoseini17a_operations,
        :mirhoseini17a_our_method,
        :mirhoseini17a_parameters,
        :mirhoseini17a_reward,
        :mirhoseini17a_rnn,
        :mirhoseini17a_sequence-to-sequence,
        :mirhoseini17a_signal,
        :mirhoseini17a_tensorflow,
        :mirhoseini17a_that,
        :mirhoseini17a_these,
        :mirhoseini17a_this,
        :mirhoseini17a_time,
        :mirhoseini17a_training,
        :mirhoseini17a_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure1-1>,
        <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure5-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:mirhoseini17a-Figure1-1_Comp0 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure1-1> .

:mirhoseini17a-Figure1-1_Comp1 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure1-1> .

:mirhoseini17a-Figure1-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure1-1> .

:mirhoseini17a-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure1-1> .

:mirhoseini17a-Figure5-1_Text0 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure5-1> .

:mirhoseini17a-Figure5-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure5-1> .

:mirhoseini17a-Figure5-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure5-1> .

:mirhoseini17a-Figure5-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure5-1> .

:part-of a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:partOf a owl:ObjectProperty .

:platform a owl:DatatypeProperty .

:sameAs a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:used-for a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:usedFor a owl:ObjectProperty .

:venueOfPublication a owl:ObjectProperty .

:wang18b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wang18b_alternative,
        :wang18b_datasets,
        :wang18b_deep_predictive_models,
        :wang18b_dynamics,
        :wang18b_flows,
        :wang18b_gradient,
        :wang18b_inputs,
        :wang18b_long-term,
        :wang18b_lstm,
        :wang18b_lstms,
        :wang18b_model,
        :wang18b_modeling,
        :wang18b_motions,
        :wang18b_network,
        :wang18b_outputs,
        :wang18b_prediction,
        :wang18b_recurrent_network,
        :wang18b_results,
        :wang18b_short-term,
        :wang18b_state,
        :wang18b_structure,
        :wang18b_time,
        :wang18b_video,
        :wang18b_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:wang18h a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wang18h_audio,
        :wang18h_corpus,
        :wang18h_data,
        :wang18h_embeddings,
        :wang18h_interpretable,
        :wang18h_learn,
        :wang18h_model,
        :wang18h_noise,
        :wang18h_results,
        :wang18h_state,
        :wang18h_system,
        :wang18h_text,
        :wang18h_that,
        :wang18h_they,
        :wang18h_this,
        :wang18h_transfer,
        :wang18h_ways,
        :wang18h_we ;
    :hasModality <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:wang18h-Figure1-1_Comp0 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp1 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp10 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp11 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp12 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp13 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp14 a :InputBlock ;
    :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp15 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp16 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp17 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp18 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp19 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp2 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp20 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp3 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp4 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp5 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp6 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp7 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp8 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Comp9 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Text1 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Text2 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Text3 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Text4 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:wang18h-Figure1-1_Text8 :partOf <https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> .

:yearOfPublication a owl:DatatypeProperty .

<https://github.com/deepcurator/DCC/1807.03039v2_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/1807.03039v2_convolution> a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/1807.03039v2_flow> a :Other ;
    :hasText "flow" .

<https://github.com/deepcurator/DCC/1807.03039v2_generative_model> a :Method ;
    :hasText "generative_model" .

<https://github.com/deepcurator/DCC/1807.03039v2_generative_models> a :Method ;
    :hasText "generative_models" .

<https://github.com/deepcurator/DCC/1807.03039v2_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1807.03039v2_improvement> a :Eval ;
    :hasText "improvement" .

<https://github.com/deepcurator/DCC/1807.03039v2_log-likelihood> a :Other ;
    :hasText "log-likelihood" .

<https://github.com/deepcurator/DCC/1807.03039v2_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1807.03039v2_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1807.03039v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1807.03039v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1807.03039v2_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1807.03039v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_covariance> a :Other ;
    :hasText "covariance" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matching> a :Task ;
    :hasText "matching" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matrix> a :Generic ;
    :hasText "matrix" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_optimization> a :Task ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_textures> a :Other ;
    :hasText "textures" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_transfer> a :Other ;
    :hasText "transfer" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_visual_quality> a :Eval ;
    :hasText "visual_quality" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_diversity> a :Other ;
    :hasText "diversity" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_map> a :Eval ;
    :hasText "map" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_outputs> a :Generic ;
    :hasText "outputs" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problems> a :Generic ;
    :hasText "problems" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_audio> a :Material ;
    :hasText "audio" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_convolutional_neural_networks> a :Method ;
    :hasText "convolutional_neural_networks" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_correlations> a :Other ;
    :hasText "correlations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_locations> a :Generic ;
    :hasText "locations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_principles> a :Generic ;
    :hasText "principles" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structural_variations> a :Other ;
    :hasText "structural_variations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structured_data> a :Material ;
    :hasText "structured_data" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_those> a :Generic ;
    :hasText "those" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_video> a :Material ;
    :hasText "video" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_analogy> a :Other ;
    :hasText "analogy" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_database> a :Generic ;
    :hasText "database" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_divergences> a :Generic ;
    :hasText "divergences" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_evaluations> a :Generic ;
    :hasText "evaluations" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experimental_results> a :Generic ;
    :hasText "experimental_results" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_gan> a :Method ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_imagenet> a :Material ;
    :hasText "imagenet" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kl> a :Generic ;
    :hasText "kl" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kullback-leibler> a :Generic ;
    :hasText "kullback-leibler" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_novel_approach> a :Generic ;
    :hasText "novel_approach" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_objective_function> a :Other ;
    :hasText "objective_function" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_properties> a :Generic ;
    :hasText "properties" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_rewards> a :Eval ;
    :hasText "rewards" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_theoretical_analysis> a :Generic ;
    :hasText "theoretical_analysis" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_activity_recognition> a :Task ;
    :hasText "activity_recognition" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_environments> a :Generic ;
    :hasText "environments" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_hierarchical> a :Generic ;
    :hasText "hierarchical" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_kitti> a :Material ;
    :hasText "kitti" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_loss_function> a :Method ;
    :hasText "loss_function" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_priori> a :Other ;
    :hasText "priori" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_region> a :Generic ;
    :hasText "region" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tracking> a :Task ;
    :hasText "tracking" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_videos> a :Material ;
    :hasText "videos" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data_augmentation> a :Method ;
    :hasText "data_augmentation" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain_experts> a :Other ;
    :hasText "domain_experts" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_f1> a :Eval ;
    :hasText "f1" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_improvements> a :Eval ;
    :hasText "improvements" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_medical_imaging> a :Material ;
    :hasText "medical_imaging" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_relation_extraction> a :Task ;
    :hasText "relation_extraction" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_technique> a :Generic ;
    :hasText "technique" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_text> a :Material ;
    :hasText "text" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformation_model> a :Method ;
    :hasText "transformation_model" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformations> a :Generic ;
    :hasText "transformations" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_unlabeled_data> a :Other ;
    :hasText "unlabeled_data" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_missing_data> a :Material ;
    :hasText "missing_data" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_unsupervised_learning> a :Method ;
    :hasText "unsupervised_learning" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_video> a :Material ;
    :hasText "video" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_videos> a :Material ;
    :hasText "videos" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_convolutional_neural_network> a :Method ;
    :hasText "convolutional_neural_network" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_formulations> a :Generic ;
    :hasText "formulations" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graphs> a :Other ;
    :hasText "graphs" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_matrix> a :Generic ;
    :hasText "matrix" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_novel_approach> a :Generic ;
    :hasText "novel_approach" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_priors> a :Other ;
    :hasText "priors" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_recurrent_neural_network> a :Method ;
    :hasText "recurrent_neural_network" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_smoothness> a :Other ;
    :hasText "smoothness" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structured> a :Generic ;
    :hasText "structured" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structures> a :Generic ;
    :hasText "structures" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these_techniques> a :Generic ;
    :hasText "these_techniques" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithms> a :Generic ;
    :hasText "algorithms" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_computations> a :Generic ;
    :hasText "computations" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_flexibility> a :Eval ;
    :hasText "flexibility" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_hardware> a :Generic ;
    :hasText "hardware" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_implementation> a :Material ;
    :hasText "implementation" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tensorflow> a :Other ;
    :hasText "tensorflow" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network_models> a :Method ;
    :hasText "neural_network_models" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_optimization> a :Task ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_research> a :Generic ;
    :hasText "research" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_scalability> a :Eval ;
    :hasText "scalability" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanism> a :Method ;
    :hasText "attention_mechanism" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanisms> a :Method ;
    :hasText "attention_mechanisms" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu> a :Eval ;
    :hasText "bleu" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu_score> a :Eval ;
    :hasText "bleu_score" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutional_neural_networks> a :Method ;
    :hasText "convolutional_neural_networks" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutions> a :Method ;
    :hasText "convolutions" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_decoder> a :Method ;
    :hasText "decoder" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_ensembles> a :Task ;
    :hasText "ensembles" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_french> a :Material ;
    :hasText "french" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_machine_translation_tasks> a :Task ;
    :hasText "machine_translation_tasks" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_deep_reinforcement_learning_(rl)> a :Method ;
    :hasText "deep_reinforcement_learning_(rl)" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_environment> a :Generic ;
    :hasText "environment" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_experimental_results> a :Generic ;
    :hasText "experimental_results" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_model-based> a :Generic ;
    :hasText "model-based" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_model-free> a :Generic ;
    :hasText "model-free" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_observations> a :Generic ;
    :hasText "observations" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_planning> a :Other ;
    :hasText "planning" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction_model> a :Method ;
    :hasText "prediction_model" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_rewards> a :Eval ;
    :hasText "rewards" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_rl> a :Task ;
    :hasText "rl" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_states> a :Generic ;
    :hasText "states" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_concepts> a :Other ;
    :hasText "concepts" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_maps> a :Eval ;
    :hasText "maps" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_mechanism> a :Generic ;
    :hasText "mechanism" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_question_answering> a :Task ;
    :hasText "question_answering" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_resnet> a :Method ;
    :hasText "resnet" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_vision> a :Generic ;
    :hasText "vision" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_components> a :Generic ;
    :hasText "components" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_global_context> a :Other ;
    :hasText "global_context" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model_design> a :Generic ;
    :hasText "model_design" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction_model> a :Method ;
    :hasText "prediction_model" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_principles> a :Generic ;
    :hasText "principles" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scene> a :Generic ;
    :hasText "scene" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scenes> a :Generic ;
    :hasText "scenes" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_structured> a :Generic ;
    :hasText "structured" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_understanding> a :Task ;
    :hasText "understanding" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_we> a :Generic ;
    :hasText "we" .

:BasicLSTMCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:BasicRNNCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_baseline a :Generic ;
    :hasText "baseline" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concept a :Other ;
    :hasText "concept" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concepts a :Other ;
    :hasText "concepts" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_embeddings a :Generic ;
    :hasText "embeddings" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_grounding a :Task ;
    :hasText "grounding" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_improvement a :Eval ;
    :hasText "improvement" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_layers a :Other ;
    :hasText "layers" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_phrases a :Generic ;
    :hasText "phrases" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_prior_works a :Generic ;
    :hasText "prior_works" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_region a :Generic ;
    :hasText "region" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_solution a :Generic ;
    :hasText "solution" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_subspaces a :Other ;
    :hasText "subspaces" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_text a :Material ;
    :hasText "text" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_cnn a :Method ;
    :hasText "cnn" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_data a :Generic ;
    :hasText "data" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_input a :Generic ;
    :hasText "input" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_magnitude a :Other ;
    :hasText "magnitude" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_method a :Generic ;
    :hasText "method" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_per a :Eval ;
    :hasText "per" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_resolution a :Eval ;
    :hasText "resolution" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_scene a :Generic ;
    :hasText "scene" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_state a :Generic ;
    :hasText "state" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_structure a :Generic ;
    :hasText "structure" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_text a :Material ;
    :hasText "text" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_text_recognition a :Task ;
    :hasText "text_recognition" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_training a :Task ;
    :hasText "training" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_two a :Generic ;
    :hasText "two" .

:Camgoz_Neural_Sign_Language_CVPR_2018_paper_language a :Generic ;
    :hasText "language" .

:CaptionText a owl:Class ;
    rdfs:subClassOf :Text .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_cameras a :Other ;
    :hasText "cameras" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_information a :Generic ;
    :hasText "information" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_one a :Generic ;
    :hasText "one" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_point_clouds a :Other ;
    :hasText "point_clouds" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_policies a :Other ;
    :hasText "policies" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_research a :Generic ;
    :hasText "research" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_scale a :Other ;
    :hasText "scale" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_natural_images a :Material ;
    :hasText "natural_images" .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_research a :Generic ;
    :hasText "research" .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_text a :Material ;
    :hasText "text" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_accuracies a :Eval ;
    :hasText "accuracies" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_optimization a :Task ;
    :hasText "optimization" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_rl a :Task ;
    :hasText "rl" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search a :Task ;
    :hasText "search" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search_space a :Other ;
    :hasText "search_space" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_sequential_model-based a :Method ;
    :hasText "sequential_model-based" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_times a :Generic ;
    :hasText "times" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Code a owl:Class ;
    rdfs:subClassOf :Modality .

:DeviceWrapper a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:DropoutBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:DropoutWrapper a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:Fan_A_Point_Set_CVPR_2017_paper_data a :Generic ;
    :hasText "data" .

:Fan_A_Point_Set_CVPR_2017_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_environment a :Generic ;
    :hasText "environment" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_interaction a :Generic ;
    :hasText "interaction" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_navigation a :Task ;
    :hasText "navigation" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_robot a :Generic ;
    :hasText "robot" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_semantic_labels a :Other ;
    :hasText "semantic_labels" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_surfaces a :Other ;
    :hasText "surfaces" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:GRUCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_3d_reconstruction a :Method ;
    :hasText "3d_reconstruction" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotation a :Task ;
    :hasText "annotation" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_measure a :Generic ;
    :hasText "measure" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_paradigms a :Generic ;
    :hasText "paradigms" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_transfer_learning a :Method ;
    :hasText "transfer_learning" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_unified_framework a :Method ;
    :hasText "unified_framework" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:He_Deep_Residual_Learning_CVPR_2016_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:He_Deep_Residual_Learning_CVPR_2016_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:He_Deep_Residual_Learning_CVPR_2016_paper_classification_task a :Task ;
    :hasText "classification_task" .

:He_Deep_Residual_Learning_CVPR_2016_paper_deep_representations a :Method ;
    :hasText "deep_representations" .

:He_Deep_Residual_Learning_CVPR_2016_paper_ensemble a :Task ;
    :hasText "ensemble" .

:He_Deep_Residual_Learning_CVPR_2016_paper_framework a :Generic ;
    :hasText "framework" .

:He_Deep_Residual_Learning_CVPR_2016_paper_functions a :Generic ;
    :hasText "functions" .

:He_Deep_Residual_Learning_CVPR_2016_paper_imagenet a :Material ;
    :hasText "imagenet" .

:He_Deep_Residual_Learning_CVPR_2016_paper_improvement a :Eval ;
    :hasText "improvement" .

:He_Deep_Residual_Learning_CVPR_2016_paper_inputs a :Generic ;
    :hasText "inputs" .

:He_Deep_Residual_Learning_CVPR_2016_paper_layers a :Other ;
    :hasText "layers" .

:He_Deep_Residual_Learning_CVPR_2016_paper_networks a :Generic ;
    :hasText "networks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_neural_networks a :Method ;
    :hasText "neural_networks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_object_detection a :Task ;
    :hasText "object_detection" .

:He_Deep_Residual_Learning_CVPR_2016_paper_recognition_tasks a :Task ;
    :hasText "recognition_tasks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_representations a :Generic ;
    :hasText "representations" .

:He_Deep_Residual_Learning_CVPR_2016_paper_segmentation a :Task ;
    :hasText "segmentation" .

:He_Deep_Residual_Learning_CVPR_2016_paper_tasks a :Generic ;
    :hasText "tasks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_that a :Generic ;
    :hasText "that" .

:He_Deep_Residual_Learning_CVPR_2016_paper_these a :Generic ;
    :hasText "these" .

:He_Deep_Residual_Learning_CVPR_2016_paper_those a :Generic ;
    :hasText "those" .

:He_Deep_Residual_Learning_CVPR_2016_paper_training a :Task ;
    :hasText "training" .

:He_Deep_Residual_Learning_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_computation a :Other ;
    :hasText "computation" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_improvements a :Eval ;
    :hasText "improvements" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_models a :Generic ;
    :hasText "models" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_state a :Generic ;
    :hasText "state" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_them a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/Hui_Fast_and_Accurate_CVPR_2018_paper_deep_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "deep_convolutional_neural_networks_(cnns)" .

:ImageComponent a owl:Class .

:LSTMCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:LSTMStateTuple a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_image_super-resolution a :Task ;
    :hasText "image_super-resolution" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_models a :Generic ;
    :hasText "models" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_noise a :Other ;
    :hasText "noise" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_3d_reconstructions a :Task ;
    :hasText "3d_reconstructions" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_component a :Generic ;
    :hasText "component" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_distances a :Other ;
    :hasText "distances" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edge a :Other ;
    :hasText "edge" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edges a :Other ;
    :hasText "edges" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_loss_function a :Method ;
    :hasText "loss_function" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_minimize a :Task ;
    :hasText "minimize" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_patches a :Generic ;
    :hasText "patches" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_point_clouds a :Other ;
    :hasText "point_clouds" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_surface a :Other ;
    :hasText "surface" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_technique a :Generic ;
    :hasText "technique" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper_face_recognition a :Task ;
    :hasText "face_recognition" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_method a :Generic ;
    :hasText "method" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:MultiRNNCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:PublicationAuthor a owl:Class .

:RNNCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:Repository a owl:Class .

:ResidualWrapper a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_adaptation a :Task ;
    :hasText "adaptation" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_automatic a :Task ;
    :hasText "automatic" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_commercial_systems a :Method ;
    :hasText "commercial_systems" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_conditions a :Generic ;
    :hasText "conditions" .

<https://github.com/deepcurator/DCC/Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_experimental_results a :Generic ;
    :hasText "experimental_results" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_manual_annotations a :Task ;
    :hasText "manual_annotations" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_region a :Generic ;
    :hasText "region" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_system a :Generic ;
    :hasText "system" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/Tai_MemNet_A_Persistent_ICCV_2017_paper_deep_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "deep_convolutional_neural_networks_(cnns)" .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_image_deblurring a :Task ;
    :hasText "image_deblurring" .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_scheme a :Generic ;
    :hasText "scheme" .

:UnpoolingBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computation_efficiency a :Eval ;
    :hasText "computation_efficiency" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computer_vision_tasks a :Task ;
    :hasText "computer_vision_tasks" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_face a :Other ;
    :hasText "face" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_gradient a :Other ;
    :hasText "gradient" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_interference a :Other ;
    :hasText "interference" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_multi-task_learning_methods a :Task ;
    :hasText "multi-task_learning_methods" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_optimum a :Other ;
    :hasText "optimum" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_relevance a :Eval ;
    :hasText "relevance" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_scales a :Other ;
    :hasText "scales" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_alignment a :Task ;
    :hasText "alignment" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_audio a :Material ;
    :hasText "audio" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_correlations a :Other ;
    :hasText "correlations" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_distance a :Other ;
    :hasText "distance" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_event a :Other ;
    :hasText "event" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modalities a :Generic ;
    :hasText "modalities" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modality a :Other ;
    :hasText "modality" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modeling a :Task ;
    :hasText "modeling" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_semantics a :Other ;
    :hasText "semantics" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_unconstrained_videos a :Material ;
    :hasText "unconstrained_videos" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_baseline a :Generic ;
    :hasText "baseline" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cameras a :Other ;
    :hasText "cameras" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cnn_embeddings a :Other ;
    :hasText "cnn_embeddings" .

<https://github.com/deepcurator/DCC/Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_data a :Generic ;
    :hasText "data" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_gan a :Method ;
    :hasText "gan" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_generation a :Task ;
    :hasText "generation" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_images a :Material ;
    :hasText "images" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_improvement a :Eval ;
    :hasText "improvement" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_method a :Generic ;
    :hasText "method" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_model a :Generic ;
    :hasText "model" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_network a :Generic ;
    :hasText "network" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_other a :Generic ;
    :hasText "other" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_outliers a :Other ;
    :hasText "outliers" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_person_re-identification a :Task ;
    :hasText "person_re-identification" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_regularization a :Other ;
    :hasText "regularization" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_representation_learning a :Task ;
    :hasText "representation_learning" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_scale a :Other ;
    :hasText "scale" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_smoothing a :Method ;
    :hasText "smoothing" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_task a :Generic ;
    :hasText "task" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_training a :Task ;
    :hasText "training" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:all_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:amos17b_architecture a :Generic ;
    :hasText "architecture" .

:amos17b_constraints a :Generic ;
    :hasText "constraints" .

:amos17b_convex a :Other ;
    :hasText "convex" .

:amos17b_data a :Generic ;
    :hasText "data" .

:amos17b_function a :Generic ;
    :hasText "function" .

:amos17b_image_completion a :Task ;
    :hasText "image_completion" .

:amos17b_improvement a :Eval ;
    :hasText "improvement" .

:amos17b_input a :Generic ;
    :hasText "input" .

:amos17b_inputs a :Generic ;
    :hasText "inputs" .

:amos17b_methods a :Generic ;
    :hasText "methods" .

:amos17b_models a :Generic ;
    :hasText "models" .

:amos17b_network a :Generic ;
    :hasText "network" .

:amos17b_networks a :Generic ;
    :hasText "networks" .

:amos17b_neural_network a :Method ;
    :hasText "neural_network" .

:amos17b_neural_network_architectures a :Generic ;
    :hasText "neural_network_architectures" .

:amos17b_neural_networks a :Method ;
    :hasText "neural_networks" .

:amos17b_optimization a :Task ;
    :hasText "optimization" .

:amos17b_optimization_algorithms a :Method ;
    :hasText "optimization_algorithms" .

:amos17b_others a :Generic ;
    :hasText "others" .

:amos17b_parameters a :Generic ;
    :hasText "parameters" .

:amos17b_prediction a :Task ;
    :hasText "prediction" .

:amos17b_problems a :Generic ;
    :hasText "problems" .

:amos17b_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:amos17b_state a :Generic ;
    :hasText "state" .

:amos17b_structured a :Generic ;
    :hasText "structured" .

:amos17b_that a :Generic ;
    :hasText "that" .

:amos17b_these a :Generic ;
    :hasText "these" .

:amos17b_this a :Generic ;
    :hasText "this" .

:amos17b_we a :Generic ;
    :hasText "we" .

:atrous_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:atrous_conv2d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:autograph a owl:Class ;
    rdfs:subClassOf :Module .

:avg_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool1d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool3d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:batch_norm_with_global_normalization a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:batch_normalization a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:bias_add a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:bidirectional_dynamic_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:bitwise a owl:Class ;
    rdfs:subClassOf :Module .

:brukhim18a_approach a :Generic ;
    :hasText "approach" .

:brukhim18a_approaches a :Generic ;
    :hasText "approaches" .

:brukhim18a_architecture a :Generic ;
    :hasText "architecture" .

:brukhim18a_baselines a :Generic ;
    :hasText "baselines" .

:brukhim18a_benchmarks a :Material ;
    :hasText "benchmarks" .

:brukhim18a_classification a :Task ;
    :hasText "classification" .

:brukhim18a_constraints a :Generic ;
    :hasText "constraints" .

:brukhim18a_deep_learning a :Generic ;
    :hasText "deep_learning" .

:brukhim18a_framework a :Generic ;
    :hasText "framework" .

:brukhim18a_machine_learning a :Task ;
    :hasText "machine_learning" .

:brukhim18a_model a :Generic ;
    :hasText "model" .

:brukhim18a_modeling a :Task ;
    :hasText "modeling" .

:brukhim18a_models a :Generic ;
    :hasText "models" .

:brukhim18a_outputs a :Generic ;
    :hasText "outputs" .

:brukhim18a_prediction a :Task ;
    :hasText "prediction" .

:brukhim18a_prediction_models a :Method ;
    :hasText "prediction_models" .

:brukhim18a_problems a :Generic ;
    :hasText "problems" .

:brukhim18a_results a :Generic ;
    :hasText "results" .

:brukhim18a_state a :Generic ;
    :hasText "state" .

:brukhim18a_structured a :Generic ;
    :hasText "structured" .

:brukhim18a_that a :Generic ;
    :hasText "that" .

:brukhim18a_them a :Generic ;
    :hasText "them" .

:brukhim18a_this a :Generic ;
    :hasText "this" .

:brukhim18a_we a :Generic ;
    :hasText "we" .

:collapse_repeated a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:compat a owl:Class ;
    rdfs:subClassOf :Module .

:compute_accidental_hits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:config a owl:Class ;
    rdfs:subClassOf :Module .

:contrib a owl:Class ;
    rdfs:subClassOf :Module .

:conv1d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv1d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d_backprop_filter a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d_backprop_input a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv3d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv3d_backprop_filter_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv3d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:convolution a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:crelu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_beam_search_decoder a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_beam_search_decoder_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_greedy_decoder a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_loss_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_unique_labels a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:data a owl:Class ;
    rdfs:subClassOf :Module .

:debugging a owl:Class ;
    rdfs:subClassOf :Module .

:depth_to_space a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_backprop_filter a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_backprop_input a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_native a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_native_backprop_filter a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_native_backprop_input a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:dilation2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:distribute a owl:Class ;
    rdfs:subClassOf :Module .

:distributions a owl:Class ;
    rdfs:subClassOf :Module .

:donahue17a_approaches a :Generic ;
    :hasText "approaches" .

:donahue17a_audio a :Material ;
    :hasText "audio" .

:donahue17a_chart a :Other ;
    :hasText "chart" .

:donahue17a_charts a :Other ;
    :hasText "charts" .

:donahue17a_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:donahue17a_features a :Generic ;
    :hasText "features" .

:donahue17a_generative_model a :Method ;
    :hasText "generative_model" .

:donahue17a_lstm a :Method ;
    :hasText "lstm" .

:donahue17a_music a :Generic ;
    :hasText "music" .

:donahue17a_platform a :Generic ;
    :hasText "platform" .

:donahue17a_rhythm a :Material ;
    :hasText "rhythm" .

:donahue17a_task a :Generic ;
    :hasText "task" .

:donahue17a_that a :Generic ;
    :hasText "that" .

:donahue17a_two a :Generic ;
    :hasText "two" .

:donahue17a_video a :Material ;
    :hasText "video" .

:donahue17a_we a :Generic ;
    :hasText "we" .

:dropout a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:dtypes a owl:Class ;
    rdfs:subClassOf :Module .

:dynamic_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:elu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:embedding_lookup a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:embedding_lookup_sparse a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:erosion2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:errors a owl:Class ;
    rdfs:subClassOf :Module .

:estimator a owl:Class ;
    rdfs:subClassOf :Module .

:experimental a owl:Class ;
    rdfs:subClassOf :Module .

:feature_column a owl:Class ;
    rdfs:subClassOf :Module .

:fixed_unigram_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:fractional_avg_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:fractional_max_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:fused_batch_norm a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:gfile a owl:Class ;
    rdfs:subClassOf :Module .

:graph_util a owl:Class ;
    rdfs:subClassOf :Module .

:hasText a owl:DatatypeProperty ;
    rdfs:domain :Text .

:image a owl:Class ;
    rdfs:subClassOf :Module .

:in_top_k a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:initializers a owl:Class ;
    rdfs:subClassOf :Module .

:io a owl:Class ;
    rdfs:subClassOf :Module .

:kalchbrenner17a_action a :Generic ;
    :hasText "action" .

:kalchbrenner17a_approaches a :Generic ;
    :hasText "approaches" .

:kalchbrenner17a_architecture a :Generic ;
    :hasText "architecture" .

:kalchbrenner17a_color a :Other ;
    :hasText "color" .

:kalchbrenner17a_discrete a :Generic ;
    :hasText "discrete" .

:kalchbrenner17a_estimates a :Generic ;
    :hasText "estimates" .

:kalchbrenner17a_model a :Generic ;
    :hasText "model" .

:kalchbrenner17a_motion a :Generic ;
    :hasText "motion" .

:kalchbrenner17a_moving_mnist a :Material ;
    :hasText "moving_mnist" .

:kalchbrenner17a_network a :Generic ;
    :hasText "network" .

:kalchbrenner17a_objects a :Generic ;
    :hasText "objects" .

:kalchbrenner17a_state a :Generic ;
    :hasText "state" .

:kalchbrenner17a_structure a :Generic ;
    :hasText "structure" .

:kalchbrenner17a_tensors a :Generic ;
    :hasText "tensors" .

:kalchbrenner17a_that a :Generic ;
    :hasText "that" .

:kalchbrenner17a_time a :Generic ;
    :hasText "time" .

:kalchbrenner17a_video a :Material ;
    :hasText "video" .

:kalchbrenner17a_videos a :Material ;
    :hasText "videos" .

:keras a owl:Class ;
    rdfs:subClassOf :Module .

:l2_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:l2_normalize a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:layers a owl:Class ;
    rdfs:subClassOf :Module .

:leaky_relu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:learned_unigram_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:linalg a owl:Class ;
    rdfs:subClassOf :Module .

:lite a owl:Class ;
    rdfs:subClassOf :Module .

:local_response_normalization a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:log_poisson_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:log_softmax a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:log_uniform_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:logging a owl:Class ;
    rdfs:subClassOf :Module .

:lookup a owl:Class ;
    rdfs:subClassOf :Module .

:losses a owl:Class ;
    rdfs:subClassOf :Module .

:lrn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:manip a owl:Class ;
    rdfs:subClassOf :Module .

:math a owl:Class ;
    rdfs:subClassOf :Module .

:max_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool1d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool3d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool_with_argmax a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_poolv2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:metrics a owl:Class ;
    rdfs:subClassOf :Module .

:mirhoseini17a_approach a :Generic ;
    :hasText "approach" .

:mirhoseini17a_classification a :Task ;
    :hasText "classification" .

:mirhoseini17a_device a :Generic ;
    :hasText "device" .

:mirhoseini17a_devices a :Generic ;
    :hasText "devices" .

:mirhoseini17a_distributed a :Generic ;
    :hasText "distributed" .

:mirhoseini17a_environment a :Generic ;
    :hasText "environment" .

:mirhoseini17a_graph a :Other ;
    :hasText "graph" .

:mirhoseini17a_graphs a :Other ;
    :hasText "graphs" .

:mirhoseini17a_hardware a :Generic ;
    :hasText "hardware" .

:mirhoseini17a_heuristics a :Method ;
    :hasText "heuristics" .

:mirhoseini17a_imagenet a :Material ;
    :hasText "imagenet" .

:mirhoseini17a_inception a :Method ;
    :hasText "inception" .

:mirhoseini17a_language_modeling a :Task ;
    :hasText "language_modeling" .

:mirhoseini17a_learns a :Task ;
    :hasText "learns" .

:mirhoseini17a_lstm a :Method ;
    :hasText "lstm" .

:mirhoseini17a_method a :Generic ;
    :hasText "method" .

:mirhoseini17a_methods a :Generic ;
    :hasText "methods" .

:mirhoseini17a_model a :Generic ;
    :hasText "model" .

:mirhoseini17a_models a :Generic ;
    :hasText "models" .

:mirhoseini17a_neural_machine_translation a :Task ;
    :hasText "neural_machine_translation" .

:mirhoseini17a_neural_networks a :Method ;
    :hasText "neural_networks" .

:mirhoseini17a_operations a :Generic ;
    :hasText "operations" .

:mirhoseini17a_our_method a :Other ;
    :hasText "our_method" .

:mirhoseini17a_parameters a :Generic ;
    :hasText "parameters" .

:mirhoseini17a_reward a :Eval ;
    :hasText "reward" .

:mirhoseini17a_rnn a :Method ;
    :hasText "rnn" .

:mirhoseini17a_sequence-to-sequence a :Method ;
    :hasText "sequence-to-sequence" .

:mirhoseini17a_signal a :Generic ;
    :hasText "signal" .

:mirhoseini17a_tensorflow a :Other ;
    :hasText "tensorflow" .

:mirhoseini17a_that a :Generic ;
    :hasText "that" .

:mirhoseini17a_these a :Generic ;
    :hasText "these" .

:mirhoseini17a_this a :Generic ;
    :hasText "this" .

:mirhoseini17a_time a :Generic ;
    :hasText "time" .

:mirhoseini17a_training a :Task ;
    :hasText "training" .

:mirhoseini17a_we a :Generic ;
    :hasText "we" .

:moments a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:nce_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:nest a owl:Class ;
    rdfs:subClassOf :Module .

:normalize_moments a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:profiler a owl:Class ;
    rdfs:subClassOf :Module .

:python_io a owl:Class ;
    rdfs:subClassOf :Module .

:quantization a owl:Class ;
    rdfs:subClassOf :Module .

:quantized_avg_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:quantized_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:quantized_max_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:quantized_relu_x a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:queue a owl:Class ;
    rdfs:subClassOf :Module .

:ragged a owl:Class ;
    rdfs:subClassOf :Module .

:random a owl:Class ;
    rdfs:subClassOf :Module .

:raw_ops a owl:Class ;
    rdfs:subClassOf :Module .

:raw_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:relu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:relu6 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:relu_layer a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:resource_loader a owl:Class ;
    rdfs:subClassOf :Module .

:safe_embedding_lookup_sparse a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sampled_softmax_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:saved_model a owl:Class ;
    rdfs:subClassOf :Module .

:selu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:separable_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sets a owl:Class ;
    rdfs:subClassOf :Module .

:sigmoid a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sigmoid_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:signal a owl:Class ;
    rdfs:subClassOf :Module .

:softmax a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softmax_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softmax_cross_entropy_with_logits_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softplus a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softsign a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:space_to_batch a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:space_to_depth a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sparse a owl:Class ;
    rdfs:subClassOf :Module .

:sparse_softmax_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:spectral a owl:Class ;
    rdfs:subClassOf :Module .

:static_bidirectional_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:static_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:static_state_saving_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sufficient_statistics a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:tanh a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:test a owl:Class ;
    rdfs:subClassOf :Module .

:top_k a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:tpu a owl:Class ;
    rdfs:subClassOf :Module .

:uniform_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:user_ops a owl:Class ;
    rdfs:subClassOf :Module .

:version a owl:Class ;
    rdfs:subClassOf :Module .

:wang18b_alternative a :Generic ;
    :hasText "alternative" .

:wang18b_datasets a :Material ;
    :hasText "datasets" .

:wang18b_deep_predictive_models a :Method ;
    :hasText "deep_predictive_models" .

:wang18b_dynamics a :Generic ;
    :hasText "dynamics" .

:wang18b_flows a :Other ;
    :hasText "flows" .

:wang18b_gradient a :Other ;
    :hasText "gradient" .

:wang18b_inputs a :Generic ;
    :hasText "inputs" .

:wang18b_long-term a :Generic ;
    :hasText "long-term" .

:wang18b_lstm a :Method ;
    :hasText "lstm" .

:wang18b_lstms a :Method ;
    :hasText "lstms" .

:wang18b_model a :Generic ;
    :hasText "model" .

:wang18b_modeling a :Task ;
    :hasText "modeling" .

:wang18b_motions a :Generic ;
    :hasText "motions" .

:wang18b_network a :Generic ;
    :hasText "network" .

:wang18b_outputs a :Generic ;
    :hasText "outputs" .

:wang18b_prediction a :Task ;
    :hasText "prediction" .

:wang18b_recurrent_network a :Method ;
    :hasText "recurrent_network" .

:wang18b_results a :Generic ;
    :hasText "results" .

:wang18b_short-term a :Generic ;
    :hasText "short-term" .

:wang18b_state a :Generic ;
    :hasText "state" .

:wang18b_structure a :Generic ;
    :hasText "structure" .

:wang18b_time a :Generic ;
    :hasText "time" .

:wang18b_video a :Material ;
    :hasText "video" .

:wang18b_we a :Generic ;
    :hasText "we" .

:wang18h_audio a :Material ;
    :hasText "audio" .

:wang18h_corpus a :Material ;
    :hasText "corpus" .

:wang18h_data a :Generic ;
    :hasText "data" .

:wang18h_embeddings a :Generic ;
    :hasText "embeddings" .

:wang18h_interpretable a :Generic ;
    :hasText "interpretable" .

:wang18h_learn a :Task ;
    :hasText "learn" .

:wang18h_model a :Generic ;
    :hasText "model" .

:wang18h_noise a :Other ;
    :hasText "noise" .

:wang18h_results a :Generic ;
    :hasText "results" .

:wang18h_state a :Generic ;
    :hasText "state" .

:wang18h_system a :Generic ;
    :hasText "system" .

:wang18h_text a :Material ;
    :hasText "text" .

:wang18h_that a :Generic ;
    :hasText "that" .

:wang18h_they a :Generic ;
    :hasText "they" .

:wang18h_this a :Generic ;
    :hasText "this" .

:wang18h_transfer a :Other ;
    :hasText "transfer" .

:wang18h_ways a :Generic ;
    :hasText "ways" .

:wang18h_we a :Generic ;
    :hasText "we" .

:weighted_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:weighted_moments a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:with_space_to_batch a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:xla a owl:Class ;
    rdfs:subClassOf :Module .

:xw_plus_b a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:zero_fraction a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:CodeEntity a owl:Class .

:Function a owl:Class .

:app a owl:Class ;
    rdfs:subClassOf :Module .

:audio a owl:Class ;
    rdfs:subClassOf :Module .

:decode_wav a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:encode_wav a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:nn a owl:Class ;
    rdfs:subClassOf :Module .

:run a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:strings a owl:Class ;
    rdfs:subClassOf :Module .

:summary a owl:Class ;
    rdfs:subClassOf :Module .

:sysconfig a owl:Class ;
    rdfs:subClassOf :Module .

<https://github.com/deepcurator/DCC/:fig_6898-hierarchical-attentive-recurrent-tracking-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1> a :Figure .

:FlattenBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:tf a owl:Class .

:DeconvBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_mirhoseini17a-Figure5-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1> a :Figure .

:Modality a owl:Class .

:Text a owl:Class ;
    rdfs:subClassOf :Modality .

<https://github.com/deepcurator/DCC/:fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Fan_A_Point_Set_CVPR_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_brukhim18a-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_kalchbrenner17a-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_amos17b-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_amos17b-Figure2-1> a :Figure .

:TextEntity a owl:Class .

<https://github.com/deepcurator/DCC/:fig_7192-value-prediction-network-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1> a :Figure .

:DenseBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_6860-dual-discriminator-generative-adversarial-nets-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1> a :Figure .

:ConcatBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:rnn_cell a owl:Class ;
    rdfs:subClassOf :nn .

<https://github.com/deepcurator/DCC/:fig_Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_donahue17a-Figure7-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_1807.03039v2-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1> a :Figure .

:LossBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:OutputBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_7237-modulating-early-visual-processing-by-language-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1> a :Figure .

:PoolingBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_donahue17a-Figure5-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_6642-universal-style-transfer-via-feature-transforms-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_7181-attention-is-all-you-need-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1> a :Figure .

:FigureComponent a owl:Class .

:NormBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1> a :Figure .

:ActivationBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1> a :Figure .

:InputBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_6650-toward-multimodal-image-to-image-translation-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_wang18h-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1> a :Figure .

:LSTMBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

<https://github.com/deepcurator/DCC/:fig_Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/:fig_Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1> a :Figure .

:Module a owl:Class ;
    rdfs:subClassOf :tf .

:train a owl:Class ;
    rdfs:subClassOf :Module .

:Method a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Material a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Publication a owl:Class .

:Task a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Figure a owl:Class ;
    rdfs:subClassOf :Modality .

:TensorFlowDefined a owl:Class ;
    rdfs:subClassOf :CodeEntity .

:ConvBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

[] a owl:AllDisjointClasses ;
    owl:members ( :BasicLSTMCell :BasicRNNCell :DeviceWrapper :DropoutWrapper :GRUCell :LSTMCell :LSTMStateTuple :MultiRNNCell :RNNCell :ResidualWrapper ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :Modality :TextEntity :tf ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :all_candidate_sampler :atrous_conv2d :atrous_conv2d_transpose :avg_pool :avg_pool1d :avg_pool2d :avg_pool3d :avg_pool_v2 :batch_norm_with_global_normalization :batch_normalization :bias_add :bidirectional_dynamic_rnn :collapse_repeated :compute_accidental_hits :conv1d :conv1d_transpose :conv2d :conv2d_backprop_filter :conv2d_backprop_input :conv2d_transpose :conv3d :conv3d_backprop_filter_v2 :conv3d_transpose :conv_transpose :convolution :crelu :ctc_beam_search_decoder :ctc_beam_search_decoder_v2 :ctc_greedy_decoder :ctc_loss :ctc_loss_v2 :ctc_unique_labels :decode_wav :depth_to_space :depthwise_conv2d :depthwise_conv2d_backprop_filter :depthwise_conv2d_backprop_input :depthwise_conv2d_native :depthwise_conv2d_native_backprop_filter :depthwise_conv2d_native_backprop_input :dilation2d :dropout :dynamic_rnn :elu :embedding_lookup :embedding_lookup_sparse :encode_wav :erosion2d :fixed_unigram_candidate_sampler :fractional_avg_pool :fractional_max_pool :fused_batch_norm :in_top_k :l2_loss :l2_normalize :leaky_relu :learned_unigram_candidate_sampler :local_response_normalization :log_poisson_loss :log_softmax :log_uniform_candidate_sampler :lrn :max_pool :max_pool1d :max_pool2d :max_pool3d :max_pool_with_argmax :max_poolv2 :moments :nce_loss :normalize_moments :pool :quantized_avg_pool :quantized_conv2d :quantized_max_pool :quantized_relu_x :raw_rnn :relu :relu6 :relu_layer :run :safe_embedding_lookup_sparse :sampled_softmax_loss :selu :separable_conv2d :sigmoid :sigmoid_cross_entropy_with_logits :softmax :softmax_cross_entropy_with_logits :softmax_cross_entropy_with_logits_v2 :softplus :softsign :space_to_batch :space_to_depth :sparse_softmax_cross_entropy_with_logits :static_bidirectional_rnn :static_rnn :static_state_saving_rnn :sufficient_statistics :tanh :top_k :uniform_candidate_sampler :weighted_cross_entropy_with_logits :weighted_moments :with_space_to_batch :xw_plus_b :zero_fraction ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :app :audio :autograph :bitwise :compat :config :contrib :data :debugging :distribute :distributions :dtypes :errors :estimator :experimental :feature_column :gfile :graph_util :image :initializers :io :keras :layers :linalg :lite :logging :lookup :losses :manip :math :metrics :nest :nn :profiler :python_io :quantization :queue :ragged :random :raw_ops :resource_loader :saved_model :sets :signal :sparse :spectral :strings :summary :sysconfig :test :tpu :train :user_ops :version :xla ) .

