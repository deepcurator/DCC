@prefix : <https://github.com/deepcurator/DCC/> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xml: <http://www.w3.org/XML/1998/namespace> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

: a owl:Ontology ;
    rdfs:label "DeepSciKG" ;
    rdfs:comment "The first iteration of the Siemens Ontology Schema to represent the multimodal curated elements from the DARPA ASKE Project." .

<https://github.com/deepcurator/DCC/1712.09913v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1712.09913v2_architecture>,
        <https://github.com/deepcurator/DCC/1712.09913v2_convex>,
        <https://github.com/deepcurator/DCC/1712.09913v2_generalization>,
        <https://github.com/deepcurator/DCC/1712.09913v2_loss_function>,
        <https://github.com/deepcurator/DCC/1712.09913v2_loss_functions>,
        <https://github.com/deepcurator/DCC/1712.09913v2_method>,
        <https://github.com/deepcurator/DCC/1712.09913v2_network>,
        <https://github.com/deepcurator/DCC/1712.09913v2_shape>,
        <https://github.com/deepcurator/DCC/1712.09913v2_skip_connections>,
        <https://github.com/deepcurator/DCC/1712.09913v2_structure>,
        <https://github.com/deepcurator/DCC/1712.09913v2_that>,
        <https://github.com/deepcurator/DCC/1712.09913v2_these>,
        <https://github.com/deepcurator/DCC/1712.09913v2_this>,
        <https://github.com/deepcurator/DCC/1712.09913v2_training>,
        <https://github.com/deepcurator/DCC/1712.09913v2_training_parameters>,
        <https://github.com/deepcurator/DCC/1712.09913v2_visualization_methods>,
        <https://github.com/deepcurator/DCC/1712.09913v2_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1801.07791v3> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1801.07791v3_cnns>,
        <https://github.com/deepcurator/DCC/1801.07791v3_convolution>,
        <https://github.com/deepcurator/DCC/1801.07791v3_correlation>,
        <https://github.com/deepcurator/DCC/1801.07791v3_data>,
        <https://github.com/deepcurator/DCC/1801.07791v3_datasets>,
        <https://github.com/deepcurator/DCC/1801.07791v3_feature>,
        <https://github.com/deepcurator/DCC/1801.07791v3_features>,
        <https://github.com/deepcurator/DCC/1801.07791v3_framework>,
        <https://github.com/deepcurator/DCC/1801.07791v3_generalization>,
        <https://github.com/deepcurator/DCC/1801.07791v3_input>,
        <https://github.com/deepcurator/DCC/1801.07791v3_kernels>,
        <https://github.com/deepcurator/DCC/1801.07791v3_learn>,
        <https://github.com/deepcurator/DCC/1801.07791v3_method>,
        <https://github.com/deepcurator/DCC/1801.07791v3_methods>,
        <https://github.com/deepcurator/DCC/1801.07791v3_operations>,
        <https://github.com/deepcurator/DCC/1801.07791v3_point_cloud>,
        <https://github.com/deepcurator/DCC/1801.07791v3_problems>,
        <https://github.com/deepcurator/DCC/1801.07791v3_shape_information>,
        <https://github.com/deepcurator/DCC/1801.07791v3_state>,
        <https://github.com/deepcurator/DCC/1801.07791v3_tasks>,
        <https://github.com/deepcurator/DCC/1801.07791v3_that>,
        <https://github.com/deepcurator/DCC/1801.07791v3_them>,
        <https://github.com/deepcurator/DCC/1801.07791v3_these>,
        <https://github.com/deepcurator/DCC/1801.07791v3_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1802.05074v4> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1802.05074v4_adaptation>,
        <https://github.com/deepcurator/DCC/1802.05074v4_architectures>,
        <https://github.com/deepcurator/DCC/1802.05074v4_cnns>,
        <https://github.com/deepcurator/DCC/1802.05074v4_computational_cost>,
        <https://github.com/deepcurator/DCC/1802.05074v4_counterparts>,
        <https://github.com/deepcurator/DCC/1802.05074v4_datasets>,
        <https://github.com/deepcurator/DCC/1802.05074v4_gradient>,
        <https://github.com/deepcurator/DCC/1802.05074v4_hyperparameters>,
        <https://github.com/deepcurator/DCC/1802.05074v4_loss_function>,
        <https://github.com/deepcurator/DCC/1802.05074v4_mnist>,
        <https://github.com/deepcurator/DCC/1802.05074v4_ones>,
        <https://github.com/deepcurator/DCC/1802.05074v4_others>,
        <https://github.com/deepcurator/DCC/1802.05074v4_resnets>,
        <https://github.com/deepcurator/DCC/1802.05074v4_scheme>,
        <https://github.com/deepcurator/DCC/1802.05074v4_stochastic_gradient_descent> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1802.05335v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1802.05335v2_approaches>,
        <https://github.com/deepcurator/DCC/1802.05335v2_autoencoder>,
        <https://github.com/deepcurator/DCC/1802.05335v2_computation>,
        <https://github.com/deepcurator/DCC/1802.05335v2_datasets>,
        <https://github.com/deepcurator/DCC/1802.05335v2_edge>,
        <https://github.com/deepcurator/DCC/1802.05335v2_image>,
        <https://github.com/deepcurator/DCC/1802.05335v2_inference_problem>,
        <https://github.com/deepcurator/DCC/1802.05335v2_input>,
        <https://github.com/deepcurator/DCC/1802.05335v2_learn>,
        <https://github.com/deepcurator/DCC/1802.05335v2_missing_data>,
        <https://github.com/deepcurator/DCC/1802.05335v2_modalities>,
        <https://github.com/deepcurator/DCC/1802.05335v2_model>,
        <https://github.com/deepcurator/DCC/1802.05335v2_network>,
        <https://github.com/deepcurator/DCC/1802.05335v2_paradigm>,
        <https://github.com/deepcurator/DCC/1802.05335v2_parameters>,
        <https://github.com/deepcurator/DCC/1802.05335v2_representation>,
        <https://github.com/deepcurator/DCC/1802.05335v2_representations>,
        <https://github.com/deepcurator/DCC/1802.05335v2_results>,
        <https://github.com/deepcurator/DCC/1802.05335v2_segmentation>,
        <https://github.com/deepcurator/DCC/1802.05335v2_state>,
        <https://github.com/deepcurator/DCC/1802.05335v2_supervised_learning>,
        <https://github.com/deepcurator/DCC/1802.05335v2_supervision>,
        <https://github.com/deepcurator/DCC/1802.05335v2_tasks>,
        <https://github.com/deepcurator/DCC/1802.05335v2_that>,
        <https://github.com/deepcurator/DCC/1802.05335v2_these>,
        <https://github.com/deepcurator/DCC/1802.05335v2_this>,
        <https://github.com/deepcurator/DCC/1802.05335v2_training>,
        <https://github.com/deepcurator/DCC/1802.05335v2_transformations>,
        <https://github.com/deepcurator/DCC/1802.05335v2_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1802.06006v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1802.06006v2_adaptation>,
        <https://github.com/deepcurator/DCC/1802.06006v2_approach>,
        <https://github.com/deepcurator/DCC/1802.06006v2_approaches>,
        <https://github.com/deepcurator/DCC/1802.06006v2_audio_samples>,
        <https://github.com/deepcurator/DCC/1802.06006v2_audios>,
        <https://github.com/deepcurator/DCC/1802.06006v2_generative_model>,
        <https://github.com/deepcurator/DCC/1802.06006v2_input>,
        <https://github.com/deepcurator/DCC/1802.06006v2_interfaces>,
        <https://github.com/deepcurator/DCC/1802.06006v2_memory>,
        <https://github.com/deepcurator/DCC/1802.06006v2_model>,
        <https://github.com/deepcurator/DCC/1802.06006v2_network>,
        <https://github.com/deepcurator/DCC/1802.06006v2_quality>,
        <https://github.com/deepcurator/DCC/1802.06006v2_resource>,
        <https://github.com/deepcurator/DCC/1802.06006v2_speaker_adaptation>,
        <https://github.com/deepcurator/DCC/1802.06006v2_system>,
        <https://github.com/deepcurator/DCC/1802.06006v2_that>,
        <https://github.com/deepcurator/DCC/1802.06006v2_this>,
        <https://github.com/deepcurator/DCC/1802.06006v2_time>,
        <https://github.com/deepcurator/DCC/1802.06006v2_training>,
        <https://github.com/deepcurator/DCC/1802.06006v2_two>,
        <https://github.com/deepcurator/DCC/1802.06006v2_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1>,
        <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1>,
        <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1_0> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1_1> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1_2> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1_3> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1_Comp1> a :PoolingBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1_Comp2> a :ConvBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_0> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_1> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_10> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_11> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_12> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_13> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_14> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_15> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_16> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_17> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_19> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_2> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_22> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_3> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_4> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_5> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_6> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_7> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_8> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_9> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_Comp5> a :ActivationBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_Comp6> a :PoolingBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_Comp8> a :ConvBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1_Comp9> a :ConvBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_0> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_1> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_10> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_2> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_3> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_4> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_5> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_6> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_8> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_9> :partOf <https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_Comp2> a :PoolingBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_Comp3> a :ConvBlock .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1_Comp5> a :ConvBlock .

<https://github.com/deepcurator/DCC/1802.07044> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1802.07044_approach>,
        <https://github.com/deepcurator/DCC/1802.07044_bounds>,
        <https://github.com/deepcurator/DCC/1802.07044_compression>,
        <https://github.com/deepcurator/DCC/1802.07044_data>,
        <https://github.com/deepcurator/DCC/1802.07044_deep_learning>,
        <https://github.com/deepcurator/DCC/1802.07044_deep_networks>,
        <https://github.com/deepcurator/DCC/1802.07044_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/1802.07044_methods>,
        <https://github.com/deepcurator/DCC/1802.07044_minimize>,
        <https://github.com/deepcurator/DCC/1802.07044_minimum_description_length_principle>,
        <https://github.com/deepcurator/DCC/1802.07044_model>,
        <https://github.com/deepcurator/DCC/1802.07044_neural_networks>,
        <https://github.com/deepcurator/DCC/1802.07044_other>,
        <https://github.com/deepcurator/DCC/1802.07044_parameters>,
        <https://github.com/deepcurator/DCC/1802.07044_that>,
        <https://github.com/deepcurator/DCC/1802.07044_theory>,
        <https://github.com/deepcurator/DCC/1802.07044_these>,
        <https://github.com/deepcurator/DCC/1802.07044_this>,
        <https://github.com/deepcurator/DCC/1802.07044_training>,
        <https://github.com/deepcurator/DCC/1802.07044_viewpoint>,
        <https://github.com/deepcurator/DCC/1802.07044_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1802.07191> :hasFigure <https://github.com/deepcurator/DCC/1802.07191-Figure15-1>,
        <https://github.com/deepcurator/DCC/1802.07191-Figure19-1>,
        <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_0> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_11> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_14> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_18> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_2> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_21> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_23> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_3> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_6> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_7> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_9> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure15-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1_Comp11> a :LossBlock .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_13> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_14> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_15> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_16> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_18> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_2> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_20> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_21> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_22> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_24> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_25> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_28> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_29> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_3> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_30> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_4> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_5> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_6> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure19-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_Comp13> a :ActivationBlock .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1_Comp25> a :ActivationBlock .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_0> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_12> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_13> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_2> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_4> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_8> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_9> :partOf <https://github.com/deepcurator/DCC/1802.07191-Figure3-1> .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_Comp8> a :LossBlock .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1_Comp9> a :LossBlock .

<https://github.com/deepcurator/DCC/1803.00404v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1803.00404v2_adversarial>,
        <https://github.com/deepcurator/DCC/1803.00404v2_adversarial_examples>,
        <https://github.com/deepcurator/DCC/1803.00404v2_applications>,
        <https://github.com/deepcurator/DCC/1803.00404v2_architectures>,
        <https://github.com/deepcurator/DCC/1803.00404v2_cifar-10>,
        <https://github.com/deepcurator/DCC/1803.00404v2_classification>,
        <https://github.com/deepcurator/DCC/1803.00404v2_classifiers>,
        <https://github.com/deepcurator/DCC/1803.00404v2_computer_vision_tasks>,
        <https://github.com/deepcurator/DCC/1803.00404v2_datasets>,
        <https://github.com/deepcurator/DCC/1803.00404v2_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/1803.00404v2_dnns>,
        <https://github.com/deepcurator/DCC/1803.00404v2_image>,
        <https://github.com/deepcurator/DCC/1803.00404v2_imagenet>,
        <https://github.com/deepcurator/DCC/1803.00404v2_inputs>,
        <https://github.com/deepcurator/DCC/1803.00404v2_learn>,
        <https://github.com/deepcurator/DCC/1803.00404v2_mnist>,
        <https://github.com/deepcurator/DCC/1803.00404v2_models>,
        <https://github.com/deepcurator/DCC/1803.00404v2_network>,
        <https://github.com/deepcurator/DCC/1803.00404v2_optimization_problem>,
        <https://github.com/deepcurator/DCC/1803.00404v2_our_method>,
        <https://github.com/deepcurator/DCC/1803.00404v2_predictions>,
        <https://github.com/deepcurator/DCC/1803.00404v2_problem>,
        <https://github.com/deepcurator/DCC/1803.00404v2_regularizations>,
        <https://github.com/deepcurator/DCC/1803.00404v2_regularizer>,
        <https://github.com/deepcurator/DCC/1803.00404v2_results>,
        <https://github.com/deepcurator/DCC/1803.00404v2_systems>,
        <https://github.com/deepcurator/DCC/1803.00404v2_that>,
        <https://github.com/deepcurator/DCC/1803.00404v2_this>,
        <https://github.com/deepcurator/DCC/1803.00404v2_training>,
        <https://github.com/deepcurator/DCC/1803.00404v2_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1803.06373v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1803.06373v1_accuracy>,
        <https://github.com/deepcurator/DCC/1803.06373v1_adversarial>,
        <https://github.com/deepcurator/DCC/1803.06373v1_adversarial_examples>,
        <https://github.com/deepcurator/DCC/1803.06373v1_counterparts>,
        <https://github.com/deepcurator/DCC/1803.06373v1_datasets>,
        <https://github.com/deepcurator/DCC/1803.06373v1_examples>,
        <https://github.com/deepcurator/DCC/1803.06373v1_imagenet>,
        <https://github.com/deepcurator/DCC/1803.06373v1_improvement>,
        <https://github.com/deepcurator/DCC/1803.06373v1_method>,
        <https://github.com/deepcurator/DCC/1803.06373v1_scale>,
        <https://github.com/deepcurator/DCC/1803.06373v1_state>,
        <https://github.com/deepcurator/DCC/1803.06373v1_technique>,
        <https://github.com/deepcurator/DCC/1803.06373v1_techniques>,
        <https://github.com/deepcurator/DCC/1803.06373v1_that>,
        <https://github.com/deepcurator/DCC/1803.06373v1_this>,
        <https://github.com/deepcurator/DCC/1803.06373v1_training>,
        <https://github.com/deepcurator/DCC/1803.06373v1_two>,
        <https://github.com/deepcurator/DCC/1803.06373v1_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1804.09170v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1804.09170v2_algorithms>,
        <https://github.com/deepcurator/DCC/1804.09170v2_baselines>,
        <https://github.com/deepcurator/DCC/1804.09170v2_benchmarks>,
        <https://github.com/deepcurator/DCC/1804.09170v2_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/1804.09170v2_examples>,
        <https://github.com/deepcurator/DCC/1804.09170v2_experiments>,
        <https://github.com/deepcurator/DCC/1804.09170v2_face>,
        <https://github.com/deepcurator/DCC/1804.09170v2_framework>,
        <https://github.com/deepcurator/DCC/1804.09170v2_methods>,
        <https://github.com/deepcurator/DCC/1804.09170v2_platform>,
        <https://github.com/deepcurator/DCC/1804.09170v2_real-world_applications>,
        <https://github.com/deepcurator/DCC/1804.09170v2_research>,
        <https://github.com/deepcurator/DCC/1804.09170v2_supervised_learning>,
        <https://github.com/deepcurator/DCC/1804.09170v2_tasks>,
        <https://github.com/deepcurator/DCC/1804.09170v2_techniques>,
        <https://github.com/deepcurator/DCC/1804.09170v2_that>,
        <https://github.com/deepcurator/DCC/1804.09170v2_them>,
        <https://github.com/deepcurator/DCC/1804.09170v2_these>,
        <https://github.com/deepcurator/DCC/1804.09170v2_unlabeled_data>,
        <https://github.com/deepcurator/DCC/1804.09170v2_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.07445v3> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.07445v3_approaches>,
        <https://github.com/deepcurator/DCC/1805.07445v3_autoencoders>,
        <https://github.com/deepcurator/DCC/1805.07445v3_bound>,
        <https://github.com/deepcurator/DCC/1805.07445v3_bounds>,
        <https://github.com/deepcurator/DCC/1805.07445v3_continuous>,
        <https://github.com/deepcurator/DCC/1805.07445v3_datasets>,
        <https://github.com/deepcurator/DCC/1805.07445v3_discrete>,
        <https://github.com/deepcurator/DCC/1805.07445v3_implementation>,
        <https://github.com/deepcurator/DCC/1805.07445v3_latent_variables>,
        <https://github.com/deepcurator/DCC/1805.07445v3_lower_bound>,
        <https://github.com/deepcurator/DCC/1805.07445v3_methods>,
        <https://github.com/deepcurator/DCC/1805.07445v3_mnist>,
        <https://github.com/deepcurator/DCC/1805.07445v3_prior>,
        <https://github.com/deepcurator/DCC/1805.07445v3_priors>,
        <https://github.com/deepcurator/DCC/1805.07445v3_relaxations>,
        <https://github.com/deepcurator/DCC/1805.07445v3_results>,
        <https://github.com/deepcurator/DCC/1805.07445v3_that>,
        <https://github.com/deepcurator/DCC/1805.07445v3_these>,
        <https://github.com/deepcurator/DCC/1805.07445v3_training>,
        <https://github.com/deepcurator/DCC/1805.07445v3_transformations>,
        <https://github.com/deepcurator/DCC/1805.07445v3_two> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.07674> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.07674_data>,
        <https://github.com/deepcurator/DCC/1805.07674_experiments>,
        <https://github.com/deepcurator/DCC/1805.07674_features>,
        <https://github.com/deepcurator/DCC/1805.07674_gan>,
        <https://github.com/deepcurator/DCC/1805.07674_gans>,
        <https://github.com/deepcurator/DCC/1805.07674_gaussian_mixture_model>,
        <https://github.com/deepcurator/DCC/1805.07674_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/1805.07674_metric>,
        <https://github.com/deepcurator/DCC/1805.07674_metrics>,
        <https://github.com/deepcurator/DCC/1805.07674_objective_function>,
        <https://github.com/deepcurator/DCC/1805.07674_our_method>,
        <https://github.com/deepcurator/DCC/1805.07674_pairwise_distance>,
        <https://github.com/deepcurator/DCC/1805.07674_structure>,
        <https://github.com/deepcurator/DCC/1805.07674_synthetic_data>,
        <https://github.com/deepcurator/DCC/1805.07674_that>,
        <https://github.com/deepcurator/DCC/1805.07674_theoretical_analysis>,
        <https://github.com/deepcurator/DCC/1805.07674_this>,
        <https://github.com/deepcurator/DCC/1805.07674_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.07932v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.07932v1_computational_cost>,
        <https://github.com/deepcurator/DCC/1805.07932v1_datasets>,
        <https://github.com/deepcurator/DCC/1805.07932v1_flickr30k_entities_datasets>,
        <https://github.com/deepcurator/DCC/1805.07932v1_information>,
        <https://github.com/deepcurator/DCC/1805.07932v1_input>,
        <https://github.com/deepcurator/DCC/1805.07932v1_inputs>,
        <https://github.com/deepcurator/DCC/1805.07932v1_interaction>,
        <https://github.com/deepcurator/DCC/1805.07932v1_interactions>,
        <https://github.com/deepcurator/DCC/1805.07932v1_language>,
        <https://github.com/deepcurator/DCC/1805.07932v1_learn>,
        <https://github.com/deepcurator/DCC/1805.07932v1_maps>,
        <https://github.com/deepcurator/DCC/1805.07932v1_methods>,
        <https://github.com/deepcurator/DCC/1805.07932v1_modality>,
        <https://github.com/deepcurator/DCC/1805.07932v1_model>,
        <https://github.com/deepcurator/DCC/1805.07932v1_networks>,
        <https://github.com/deepcurator/DCC/1805.07932v1_pooling>,
        <https://github.com/deepcurator/DCC/1805.07932v1_problem>,
        <https://github.com/deepcurator/DCC/1805.07932v1_question_answering>,
        <https://github.com/deepcurator/DCC/1805.07932v1_rank>,
        <https://github.com/deepcurator/DCC/1805.07932v1_representations>,
        <https://github.com/deepcurator/DCC/1805.07932v1_state>,
        <https://github.com/deepcurator/DCC/1805.07932v1_that>,
        <https://github.com/deepcurator/DCC/1805.07932v1_this>,
        <https://github.com/deepcurator/DCC/1805.07932v1_two>,
        <https://github.com/deepcurator/DCC/1805.07932v1_vision>,
        <https://github.com/deepcurator/DCC/1805.07932v1_visual_information>,
        <https://github.com/deepcurator/DCC/1805.07932v1_way>,
        <https://github.com/deepcurator/DCC/1805.07932v1_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.08574> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.08574_drop-in_replacements>,
        <https://github.com/deepcurator/DCC/1805.08574_efficiency>,
        <https://github.com/deepcurator/DCC/1805.08574_function>,
        <https://github.com/deepcurator/DCC/1805.08574_input>,
        <https://github.com/deepcurator/DCC/1805.08574_learn>,
        <https://github.com/deepcurator/DCC/1805.08574_lstm>,
        <https://github.com/deepcurator/DCC/1805.08574_methods>,
        <https://github.com/deepcurator/DCC/1805.08574_modeling>,
        <https://github.com/deepcurator/DCC/1805.08574_neural_network_architectures>,
        <https://github.com/deepcurator/DCC/1805.08574_parameters>,
        <https://github.com/deepcurator/DCC/1805.08574_penn_treebank>,
        <https://github.com/deepcurator/DCC/1805.08574_state>,
        <https://github.com/deepcurator/DCC/1805.08574_structure>,
        <https://github.com/deepcurator/DCC/1805.08574_tasks>,
        <https://github.com/deepcurator/DCC/1805.08574_that>,
        <https://github.com/deepcurator/DCC/1805.08574_them>,
        <https://github.com/deepcurator/DCC/1805.08574_this>,
        <https://github.com/deepcurator/DCC/1805.08574_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.09112v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.09112v2_classification>,
        <https://github.com/deepcurator/DCC/1805.09112v2_data>,
        <https://github.com/deepcurator/DCC/1805.09112v2_deep_learning>,
        <https://github.com/deepcurator/DCC/1805.09112v2_embeddings>,
        <https://github.com/deepcurator/DCC/1805.09112v2_formalism>,
        <https://github.com/deepcurator/DCC/1805.09112v2_layers>,
        <https://github.com/deepcurator/DCC/1805.09112v2_machine_learning>,
        <https://github.com/deepcurator/DCC/1805.09112v2_model>,
        <https://github.com/deepcurator/DCC/1805.09112v2_neural_network>,
        <https://github.com/deepcurator/DCC/1805.09112v2_optimization>,
        <https://github.com/deepcurator/DCC/1805.09112v2_properties>,
        <https://github.com/deepcurator/DCC/1805.09112v2_recognition_tasks>,
        <https://github.com/deepcurator/DCC/1805.09112v2_recurrent_neural_networks>,
        <https://github.com/deepcurator/DCC/1805.09112v2_recurrent_units>,
        <https://github.com/deepcurator/DCC/1805.09112v2_sentence_embeddings>,
        <https://github.com/deepcurator/DCC/1805.09112v2_spaces>,
        <https://github.com/deepcurator/DCC/1805.09112v2_tasks>,
        <https://github.com/deepcurator/DCC/1805.09112v2_that>,
        <https://github.com/deepcurator/DCC/1805.09112v2_this>,
        <https://github.com/deepcurator/DCC/1805.09112v2_tools>,
        <https://github.com/deepcurator/DCC/1805.09112v2_tree>,
        <https://github.com/deepcurator/DCC/1805.09112v2_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.09298v4> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.09298v4_applications>,
        <https://github.com/deepcurator/DCC/1805.09298v4_complex>,
        <https://github.com/deepcurator/DCC/1805.09298v4_computation>,
        <https://github.com/deepcurator/DCC/1805.09298v4_effectiveness>,
        <https://github.com/deepcurator/DCC/1805.09298v4_end-to-end>,
        <https://github.com/deepcurator/DCC/1805.09298v4_experiments>,
        <https://github.com/deepcurator/DCC/1805.09298v4_functions>,
        <https://github.com/deepcurator/DCC/1805.09298v4_generalization>,
        <https://github.com/deepcurator/DCC/1805.09298v4_minimization>,
        <https://github.com/deepcurator/DCC/1805.09298v4_network>,
        <https://github.com/deepcurator/DCC/1805.09298v4_networks>,
        <https://github.com/deepcurator/DCC/1805.09298v4_neural_networks>,
        <https://github.com/deepcurator/DCC/1805.09298v4_one>,
        <https://github.com/deepcurator/DCC/1805.09298v4_our_method>,
        <https://github.com/deepcurator/DCC/1805.09298v4_problem>,
        <https://github.com/deepcurator/DCC/1805.09298v4_regularization>,
        <https://github.com/deepcurator/DCC/1805.09298v4_representation>,
        <https://github.com/deepcurator/DCC/1805.09298v4_sphere>,
        <https://github.com/deepcurator/DCC/1805.09298v4_state>,
        <https://github.com/deepcurator/DCC/1805.09298v4_tasks>,
        <https://github.com/deepcurator/DCC/1805.09298v4_that>,
        <https://github.com/deepcurator/DCC/1805.09298v4_this>,
        <https://github.com/deepcurator/DCC/1805.09298v4_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.12018v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.12018v1_adversarial_examples>,
        <https://github.com/deepcurator/DCC/1805.12018v1_data>,
        <https://github.com/deepcurator/DCC/1805.12018v1_data_augmentation>,
        <https://github.com/deepcurator/DCC/1805.12018v1_domains>,
        <https://github.com/deepcurator/DCC/1805.12018v1_examples>,
        <https://github.com/deepcurator/DCC/1805.12018v1_feature_space>,
        <https://github.com/deepcurator/DCC/1805.12018v1_formulation>,
        <https://github.com/deepcurator/DCC/1805.12018v1_iterative_procedure>,
        <https://github.com/deepcurator/DCC/1805.12018v1_learns>,
        <https://github.com/deepcurator/DCC/1805.12018v1_method>,
        <https://github.com/deepcurator/DCC/1805.12018v1_model>,
        <https://github.com/deepcurator/DCC/1805.12018v1_models>,
        <https://github.com/deepcurator/DCC/1805.12018v1_our_method>,
        <https://github.com/deepcurator/DCC/1805.12018v1_priori>,
        <https://github.com/deepcurator/DCC/1805.12018v1_regularization>,
        <https://github.com/deepcurator/DCC/1805.12018v1_regularizers>,
        <https://github.com/deepcurator/DCC/1805.12018v1_scheme>,
        <https://github.com/deepcurator/DCC/1805.12018v1_semantic_segmentation>,
        <https://github.com/deepcurator/DCC/1805.12018v1_source_domain>,
        <https://github.com/deepcurator/DCC/1805.12018v1_target_domain>,
        <https://github.com/deepcurator/DCC/1805.12018v1_tasks>,
        <https://github.com/deepcurator/DCC/1805.12018v1_that>,
        <https://github.com/deepcurator/DCC/1805.12018v1_training>,
        <https://github.com/deepcurator/DCC/1805.12018v1_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1805.12462v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1805.12462v1_approximating>,
        <https://github.com/deepcurator/DCC/1805.12462v1_automatic>,
        <https://github.com/deepcurator/DCC/1805.12462v1_computations>,
        <https://github.com/deepcurator/DCC/1805.12462v1_datasets>,
        <https://github.com/deepcurator/DCC/1805.12462v1_diversity>,
        <https://github.com/deepcurator/DCC/1805.12462v1_gans>,
        <https://github.com/deepcurator/DCC/1805.12462v1_gaussian_mixture_model>,
        <https://github.com/deepcurator/DCC/1805.12462v1_generative_models>,
        <https://github.com/deepcurator/DCC/1805.12462v1_gmm>,
        <https://github.com/deepcurator/DCC/1805.12462v1_gmms>,
        <https://github.com/deepcurator/DCC/1805.12462v1_images>,
        <https://github.com/deepcurator/DCC/1805.12462v1_learn>,
        <https://github.com/deepcurator/DCC/1805.12462v1_machine_learning>,
        <https://github.com/deepcurator/DCC/1805.12462v1_method>,
        <https://github.com/deepcurator/DCC/1805.12462v1_methods>,
        <https://github.com/deepcurator/DCC/1805.12462v1_model>,
        <https://github.com/deepcurator/DCC/1805.12462v1_modeling>,
        <https://github.com/deepcurator/DCC/1805.12462v1_models>,
        <https://github.com/deepcurator/DCC/1805.12462v1_network>,
        <https://github.com/deepcurator/DCC/1805.12462v1_neural_network>,
        <https://github.com/deepcurator/DCC/1805.12462v1_other>,
        <https://github.com/deepcurator/DCC/1805.12462v1_our_method>,
        <https://github.com/deepcurator/DCC/1805.12462v1_patches>,
        <https://github.com/deepcurator/DCC/1805.12462v1_problem>,
        <https://github.com/deepcurator/DCC/1805.12462v1_representation>,
        <https://github.com/deepcurator/DCC/1805.12462v1_resolution>,
        <https://github.com/deepcurator/DCC/1805.12462v1_results>,
        <https://github.com/deepcurator/DCC/1805.12462v1_signals>,
        <https://github.com/deepcurator/DCC/1805.12462v1_solution>,
        <https://github.com/deepcurator/DCC/1805.12462v1_statistical_model>,
        <https://github.com/deepcurator/DCC/1805.12462v1_statistical_models>,
        <https://github.com/deepcurator/DCC/1805.12462v1_structure>,
        <https://github.com/deepcurator/DCC/1805.12462v1_that>,
        <https://github.com/deepcurator/DCC/1805.12462v1_them>,
        <https://github.com/deepcurator/DCC/1805.12462v1_this>,
        <https://github.com/deepcurator/DCC/1805.12462v1_those>,
        <https://github.com/deepcurator/DCC/1805.12462v1_time>,
        <https://github.com/deepcurator/DCC/1805.12462v1_unsupervised_methods>,
        <https://github.com/deepcurator/DCC/1805.12462v1_utility>,
        <https://github.com/deepcurator/DCC/1805.12462v1_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.00035v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.00035v1_algorithm>,
        <https://github.com/deepcurator/DCC/1806.00035v1_approach>,
        <https://github.com/deepcurator/DCC/1806.00035v1_autoencoder>,
        <https://github.com/deepcurator/DCC/1806.00035v1_distance>,
        <https://github.com/deepcurator/DCC/1806.00035v1_divergence>,
        <https://github.com/deepcurator/DCC/1806.00035v1_divergences>,
        <https://github.com/deepcurator/DCC/1806.00035v1_evaluation_methods>,
        <https://github.com/deepcurator/DCC/1806.00035v1_evaluation_metrics>,
        <https://github.com/deepcurator/DCC/1806.00035v1_experiments>,
        <https://github.com/deepcurator/DCC/1806.00035v1_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/1806.00035v1_generative_models>,
        <https://github.com/deepcurator/DCC/1806.00035v1_inception>,
        <https://github.com/deepcurator/DCC/1806.00035v1_means>,
        <https://github.com/deepcurator/DCC/1806.00035v1_metric>,
        <https://github.com/deepcurator/DCC/1806.00035v1_metrics>,
        <https://github.com/deepcurator/DCC/1806.00035v1_model>,
        <https://github.com/deepcurator/DCC/1806.00035v1_modeling>,
        <https://github.com/deepcurator/DCC/1806.00035v1_one>,
        <https://github.com/deepcurator/DCC/1806.00035v1_precision>,
        <https://github.com/deepcurator/DCC/1806.00035v1_properties>,
        <https://github.com/deepcurator/DCC/1806.00035v1_quality>,
        <https://github.com/deepcurator/DCC/1806.00035v1_recall>,
        <https://github.com/deepcurator/DCC/1806.00035v1_target>,
        <https://github.com/deepcurator/DCC/1806.00035v1_that>,
        <https://github.com/deepcurator/DCC/1806.00035v1_these>,
        <https://github.com/deepcurator/DCC/1806.00035v1_they>,
        <https://github.com/deepcurator/DCC/1806.00035v1_this>,
        <https://github.com/deepcurator/DCC/1806.00035v1_two>,
        <https://github.com/deepcurator/DCC/1806.00035v1_utility>,
        <https://github.com/deepcurator/DCC/1806.00035v1_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.01822> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.01822_architectures>,
        <https://github.com/deepcurator/DCC/1806.01822_complex>,
        <https://github.com/deepcurator/DCC/1806.01822_data>,
        <https://github.com/deepcurator/DCC/1806.01822_datasets>,
        <https://github.com/deepcurator/DCC/1806.01822_domains>,
        <https://github.com/deepcurator/DCC/1806.01822_first>,
        <https://github.com/deepcurator/DCC/1806.01822_information>,
        <https://github.com/deepcurator/DCC/1806.01822_language_modeling>,
        <https://github.com/deepcurator/DCC/1806.01822_memory>,
        <https://github.com/deepcurator/DCC/1806.01822_model>,
        <https://github.com/deepcurator/DCC/1806.01822_module>,
        <https://github.com/deepcurator/DCC/1806.01822_neural_networks>,
        <https://github.com/deepcurator/DCC/1806.01822_program>,
        <https://github.com/deepcurator/DCC/1806.01822_results>,
        <https://github.com/deepcurator/DCC/1806.01822_rl>,
        <https://github.com/deepcurator/DCC/1806.01822_state>,
        <https://github.com/deepcurator/DCC/1806.01822_tasks>,
        <https://github.com/deepcurator/DCC/1806.01822_that>,
        <https://github.com/deepcurator/DCC/1806.01822_these>,
        <https://github.com/deepcurator/DCC/1806.01822_they>,
        <https://github.com/deepcurator/DCC/1806.01822_understanding>,
        <https://github.com/deepcurator/DCC/1806.01822_ways>,
        <https://github.com/deepcurator/DCC/1806.01822_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.02311> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.02311_algorithm>,
        <https://github.com/deepcurator/DCC/1806.02311_approach>,
        <https://github.com/deepcurator/DCC/1806.02311_approaches>,
        <https://github.com/deepcurator/DCC/1806.02311_attention_mechanisms>,
        <https://github.com/deepcurator/DCC/1806.02311_image>,
        <https://github.com/deepcurator/DCC/1806.02311_mappings>,
        <https://github.com/deepcurator/DCC/1806.02311_modeling>,
        <https://github.com/deepcurator/DCC/1806.02311_objects>,
        <https://github.com/deepcurator/DCC/1806.02311_regions>,
        <https://github.com/deepcurator/DCC/1806.02311_scene>,
        <https://github.com/deepcurator/DCC/1806.02311_supervision>,
        <https://github.com/deepcurator/DCC/1806.02311_tasks>,
        <https://github.com/deepcurator/DCC/1806.02311_techniques>,
        <https://github.com/deepcurator/DCC/1806.02311_that>,
        <https://github.com/deepcurator/DCC/1806.02311_this>,
        <https://github.com/deepcurator/DCC/1806.02311_those>,
        <https://github.com/deepcurator/DCC/1806.02311_time>,
        <https://github.com/deepcurator/DCC/1806.02311_way>,
        <https://github.com/deepcurator/DCC/1806.02311_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.02724> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.02724_action>,
        <https://github.com/deepcurator/DCC/1806.02724_annotated_data>,
        <https://github.com/deepcurator/DCC/1806.02724_approach>,
        <https://github.com/deepcurator/DCC/1806.02724_baseline>,
        <https://github.com/deepcurator/DCC/1806.02724_candidate>,
        <https://github.com/deepcurator/DCC/1806.02724_components>,
        <https://github.com/deepcurator/DCC/1806.02724_data_augmentation>,
        <https://github.com/deepcurator/DCC/1806.02724_decisions>,
        <https://github.com/deepcurator/DCC/1806.02724_granularity>,
        <https://github.com/deepcurator/DCC/1806.02724_information>,
        <https://github.com/deepcurator/DCC/1806.02724_language>,
        <https://github.com/deepcurator/DCC/1806.02724_machine_learning>,
        <https://github.com/deepcurator/DCC/1806.02724_model>,
        <https://github.com/deepcurator/DCC/1806.02724_models>,
        <https://github.com/deepcurator/DCC/1806.02724_natural_language>,
        <https://github.com/deepcurator/DCC/1806.02724_navigation>,
        <https://github.com/deepcurator/DCC/1806.02724_problem>,
        <https://github.com/deepcurator/DCC/1806.02724_that>,
        <https://github.com/deepcurator/DCC/1806.02724_these>,
        <https://github.com/deepcurator/DCC/1806.02724_this>,
        <https://github.com/deepcurator/DCC/1806.02724_vision>,
        <https://github.com/deepcurator/DCC/1806.02724_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.04090v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.04090v2_distributed>,
        <https://github.com/deepcurator/DCC/1806.04090v2_examples>,
        <https://github.com/deepcurator/DCC/1806.04090v2_framework>,
        <https://github.com/deepcurator/DCC/1806.04090v2_gradient>,
        <https://github.com/deepcurator/DCC/1806.04090v2_gradients>,
        <https://github.com/deepcurator/DCC/1806.04090v2_method>,
        <https://github.com/deepcurator/DCC/1806.04090v2_methods>,
        <https://github.com/deepcurator/DCC/1806.04090v2_model_training>,
        <https://github.com/deepcurator/DCC/1806.04090v2_nodes>,
        <https://github.com/deepcurator/DCC/1806.04090v2_one>,
        <https://github.com/deepcurator/DCC/1806.04090v2_overheads>,
        <https://github.com/deepcurator/DCC/1806.04090v2_sparsity>,
        <https://github.com/deepcurator/DCC/1806.04090v2_stochastic_gradients>,
        <https://github.com/deepcurator/DCC/1806.04090v2_studies>,
        <https://github.com/deepcurator/DCC/1806.04090v2_that>,
        <https://github.com/deepcurator/DCC/1806.04090v2_these>,
        <https://github.com/deepcurator/DCC/1806.04090v2_training>,
        <https://github.com/deepcurator/DCC/1806.04090v2_variance> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.05138v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.05138v1_architecture>,
        <https://github.com/deepcurator/DCC/1806.05138v1_bleu_scores>,
        <https://github.com/deepcurator/DCC/1806.05138v1_data>,
        <https://github.com/deepcurator/DCC/1806.05138v1_encoder-decoder>,
        <https://github.com/deepcurator/DCC/1806.05138v1_framework>,
        <https://github.com/deepcurator/DCC/1806.05138v1_language>,
        <https://github.com/deepcurator/DCC/1806.05138v1_languages>,
        <https://github.com/deepcurator/DCC/1806.05138v1_latent_variable>,
        <https://github.com/deepcurator/DCC/1806.05138v1_learn>,
        <https://github.com/deepcurator/DCC/1806.05138v1_model>,
        <https://github.com/deepcurator/DCC/1806.05138v1_neural_machine_translation>,
        <https://github.com/deepcurator/DCC/1806.05138v1_overfitting>,
        <https://github.com/deepcurator/DCC/1806.05138v1_parameters>,
        <https://github.com/deepcurator/DCC/1806.05138v1_representation>,
        <https://github.com/deepcurator/DCC/1806.05138v1_semantics>,
        <https://github.com/deepcurator/DCC/1806.05138v1_target>,
        <https://github.com/deepcurator/DCC/1806.05138v1_tasks>,
        <https://github.com/deepcurator/DCC/1806.05138v1_training>,
        <https://github.com/deepcurator/DCC/1806.05138v1_words> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.06029v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.06029v1_agents>,
        <https://github.com/deepcurator/DCC/1806.06029v1_autoencoder>,
        <https://github.com/deepcurator/DCC/1806.06029v1_domain>,
        <https://github.com/deepcurator/DCC/1806.06029v1_experiments>,
        <https://github.com/deepcurator/DCC/1806.06029v1_image>,
        <https://github.com/deepcurator/DCC/1806.06029v1_images>,
        <https://github.com/deepcurator/DCC/1806.06029v1_layers>,
        <https://github.com/deepcurator/DCC/1806.06029v1_method>,
        <https://github.com/deepcurator/DCC/1806.06029v1_methods>,
        <https://github.com/deepcurator/DCC/1806.06029v1_one>,
        <https://github.com/deepcurator/DCC/1806.06029v1_other>,
        <https://github.com/deepcurator/DCC/1806.06029v1_task>,
        <https://github.com/deepcurator/DCC/1806.06029v1_that>,
        <https://github.com/deepcurator/DCC/1806.06029v1_these>,
        <https://github.com/deepcurator/DCC/1806.06029v1_this>,
        <https://github.com/deepcurator/DCC/1806.06029v1_training>,
        <https://github.com/deepcurator/DCC/1806.06029v1_transfer>,
        <https://github.com/deepcurator/DCC/1806.06029v1_two>,
        <https://github.com/deepcurator/DCC/1806.06029v1_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.06621v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.06621v1_cifar-10>,
        <https://github.com/deepcurator/DCC/1806.06621v1_distance>,
        <https://github.com/deepcurator/DCC/1806.06621v1_features>,
        <https://github.com/deepcurator/DCC/1806.06621v1_gans>,
        <https://github.com/deepcurator/DCC/1806.06621v1_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/1806.06621v1_gradient_penalty>,
        <https://github.com/deepcurator/DCC/1806.06621v1_image>,
        <https://github.com/deepcurator/DCC/1806.06621v1_images>,
        <https://github.com/deepcurator/DCC/1806.06621v1_inception>,
        <https://github.com/deepcurator/DCC/1806.06621v1_metric>,
        <https://github.com/deepcurator/DCC/1806.06621v1_model>,
        <https://github.com/deepcurator/DCC/1806.06621v1_probability_distributions>,
        <https://github.com/deepcurator/DCC/1806.06621v1_spaces>,
        <https://github.com/deepcurator/DCC/1806.06621v1_state>,
        <https://github.com/deepcurator/DCC/1806.06621v1_theory>,
        <https://github.com/deepcurator/DCC/1806.06621v1_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1806.07336v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1806.07336v2_algorithm>,
        <https://github.com/deepcurator/DCC/1806.07336v2_analogies>,
        <https://github.com/deepcurator/DCC/1806.07336v2_approaches>,
        <https://github.com/deepcurator/DCC/1806.07336v2_architecture>,
        <https://github.com/deepcurator/DCC/1806.07336v2_classes>,
        <https://github.com/deepcurator/DCC/1806.07336v2_classification>,
        <https://github.com/deepcurator/DCC/1806.07336v2_data>,
        <https://github.com/deepcurator/DCC/1806.07336v2_device>,
        <https://github.com/deepcurator/DCC/1806.07336v2_distributional_hypothesis>,
        <https://github.com/deepcurator/DCC/1806.07336v2_embeddings>,
        <https://github.com/deepcurator/DCC/1806.07336v2_features>,
        <https://github.com/deepcurator/DCC/1806.07336v2_flow>,
        <https://github.com/deepcurator/DCC/1806.07336v2_function>,
        <https://github.com/deepcurator/DCC/1806.07336v2_ir>,
        <https://github.com/deepcurator/DCC/1806.07336v2_learn>,
        <https://github.com/deepcurator/DCC/1806.07336v2_learned_representation>,
        <https://github.com/deepcurator/DCC/1806.07336v2_methods>,
        <https://github.com/deepcurator/DCC/1806.07336v2_natural_language>,
        <https://github.com/deepcurator/DCC/1806.07336v2_natural_language_processing>,
        <https://github.com/deepcurator/DCC/1806.07336v2_prediction>,
        <https://github.com/deepcurator/DCC/1806.07336v2_processing>,
        <https://github.com/deepcurator/DCC/1806.07336v2_program>,
        <https://github.com/deepcurator/DCC/1806.07336v2_programming_language>,
        <https://github.com/deepcurator/DCC/1806.07336v2_programs>,
        <https://github.com/deepcurator/DCC/1806.07336v2_representation>,
        <https://github.com/deepcurator/DCC/1806.07336v2_research>,
        <https://github.com/deepcurator/DCC/1806.07336v2_rnn>,
        <https://github.com/deepcurator/DCC/1806.07336v2_semantics>,
        <https://github.com/deepcurator/DCC/1806.07336v2_state>,
        <https://github.com/deepcurator/DCC/1806.07336v2_tasks>,
        <https://github.com/deepcurator/DCC/1806.07336v2_technique>,
        <https://github.com/deepcurator/DCC/1806.07336v2_that>,
        <https://github.com/deepcurator/DCC/1806.07336v2_this>,
        <https://github.com/deepcurator/DCC/1806.07336v2_tree>,
        <https://github.com/deepcurator/DCC/1806.07336v2_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1807.02547> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1807.02547_classification>,
        <https://github.com/deepcurator/DCC/1807.02547_cnns>,
        <https://github.com/deepcurator/DCC/1807.02547_convolutions>,
        <https://github.com/deepcurator/DCC/1807.02547_data>,
        <https://github.com/deepcurator/DCC/1807.02547_effectiveness>,
        <https://github.com/deepcurator/DCC/1807.02547_euclidean_space>,
        <https://github.com/deepcurator/DCC/1807.02547_experimental_results>,
        <https://github.com/deepcurator/DCC/1807.02547_fields>,
        <https://github.com/deepcurator/DCC/1807.02547_kernels>,
        <https://github.com/deepcurator/DCC/1807.02547_linear_combination>,
        <https://github.com/deepcurator/DCC/1807.02547_map>,
        <https://github.com/deepcurator/DCC/1807.02547_maps>,
        <https://github.com/deepcurator/DCC/1807.02547_model>,
        <https://github.com/deepcurator/DCC/1807.02547_motions>,
        <https://github.com/deepcurator/DCC/1807.02547_network>,
        <https://github.com/deepcurator/DCC/1807.02547_prediction>,
        <https://github.com/deepcurator/DCC/1807.02547_problem>,
        <https://github.com/deepcurator/DCC/1807.02547_representations>,
        <https://github.com/deepcurator/DCC/1807.02547_structure>,
        <https://github.com/deepcurator/DCC/1807.02547_tensor>,
        <https://github.com/deepcurator/DCC/1807.02547_that>,
        <https://github.com/deepcurator/DCC/1807.02547_this> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1807.03039v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1807.03039v2_benchmarks>,
        <https://github.com/deepcurator/DCC/1807.03039v2_convolution>,
        <https://github.com/deepcurator/DCC/1807.03039v2_flow>,
        <https://github.com/deepcurator/DCC/1807.03039v2_generative_model>,
        <https://github.com/deepcurator/DCC/1807.03039v2_generative_models>,
        <https://github.com/deepcurator/DCC/1807.03039v2_images>,
        <https://github.com/deepcurator/DCC/1807.03039v2_improvement>,
        <https://github.com/deepcurator/DCC/1807.03039v2_log-likelihood>,
        <https://github.com/deepcurator/DCC/1807.03039v2_model>,
        <https://github.com/deepcurator/DCC/1807.03039v2_our_method>,
        <https://github.com/deepcurator/DCC/1807.03039v2_that>,
        <https://github.com/deepcurator/DCC/1807.03039v2_this>,
        <https://github.com/deepcurator/DCC/1807.03039v2_training>,
        <https://github.com/deepcurator/DCC/1807.03039v2_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_0> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_1> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_10> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_3> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_4> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_5> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_6> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_8> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_9> :partOf <https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp4> a :ConvBlock .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1_Comp5> a :NormBlock .

<https://github.com/deepcurator/DCC/1807.03146v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1807.03146v1_architecture>,
        <https://github.com/deepcurator/DCC/1807.03146v1_baseline>,
        <https://github.com/deepcurator/DCC/1807.03146v1_categories>,
        <https://github.com/deepcurator/DCC/1807.03146v1_end-to-end>,
        <https://github.com/deepcurator/DCC/1807.03146v1_framework>,
        <https://github.com/deepcurator/DCC/1807.03146v1_image>,
        <https://github.com/deepcurator/DCC/1807.03146v1_instances>,
        <https://github.com/deepcurator/DCC/1807.03146v1_keypoint_annotations>,
        <https://github.com/deepcurator/DCC/1807.03146v1_learn>,
        <https://github.com/deepcurator/DCC/1807.03146v1_model>,
        <https://github.com/deepcurator/DCC/1807.03146v1_neural_network>,
        <https://github.com/deepcurator/DCC/1807.03146v1_object_category>,
        <https://github.com/deepcurator/DCC/1807.03146v1_pose>,
        <https://github.com/deepcurator/DCC/1807.03146v1_pose_estimation>,
        <https://github.com/deepcurator/DCC/1807.03146v1_task>,
        <https://github.com/deepcurator/DCC/1807.03146v1_that>,
        <https://github.com/deepcurator/DCC/1807.03146v1_this>,
        <https://github.com/deepcurator/DCC/1807.03146v1_two>,
        <https://github.com/deepcurator/DCC/1807.03146v1_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1807.03247> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1807.03247_agents>,
        <https://github.com/deepcurator/DCC/1807.03247_cnn>,
        <https://github.com/deepcurator/DCC/1807.03247_convolution>,
        <https://github.com/deepcurator/DCC/1807.03247_convolutional_networks>,
        <https://github.com/deepcurator/DCC/1807.03247_convolutional_neural_networks>,
        <https://github.com/deepcurator/DCC/1807.03247_deep_learning>,
        <https://github.com/deepcurator/DCC/1807.03247_domain>,
        <https://github.com/deepcurator/DCC/1807.03247_efficiency>,
        <https://github.com/deepcurator/DCC/1807.03247_first>,
        <https://github.com/deepcurator/DCC/1807.03247_gan>,
        <https://github.com/deepcurator/DCC/1807.03247_generalization>,
        <https://github.com/deepcurator/DCC/1807.03247_input>,
        <https://github.com/deepcurator/DCC/1807.03247_layers>,
        <https://github.com/deepcurator/DCC/1807.03247_learn>,
        <https://github.com/deepcurator/DCC/1807.03247_mnist>,
        <https://github.com/deepcurator/DCC/1807.03247_model>,
        <https://github.com/deepcurator/DCC/1807.03247_models>,
        <https://github.com/deepcurator/DCC/1807.03247_networks>,
        <https://github.com/deepcurator/DCC/1807.03247_one>,
        <https://github.com/deepcurator/DCC/1807.03247_other>,
        <https://github.com/deepcurator/DCC/1807.03247_parameters>,
        <https://github.com/deepcurator/DCC/1807.03247_problem>,
        <https://github.com/deepcurator/DCC/1807.03247_reinforcement_learning_(rl)>,
        <https://github.com/deepcurator/DCC/1807.03247_representations>,
        <https://github.com/deepcurator/DCC/1807.03247_solution>,
        <https://github.com/deepcurator/DCC/1807.03247_task>,
        <https://github.com/deepcurator/DCC/1807.03247_tasks>,
        <https://github.com/deepcurator/DCC/1807.03247_that>,
        <https://github.com/deepcurator/DCC/1807.03247_they>,
        <https://github.com/deepcurator/DCC/1807.03247_this>,
        <https://github.com/deepcurator/DCC/1807.03247_times>,
        <https://github.com/deepcurator/DCC/1807.03247_translation_invariance>,
        <https://github.com/deepcurator/DCC/1807.03247_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1807.03756v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1807.03756v1_alignment>,
        <https://github.com/deepcurator/DCC/1807.03756v1_alignments>,
        <https://github.com/deepcurator/DCC/1807.03756v1_alternatives>,
        <https://github.com/deepcurator/DCC/1807.03756v1_approach>,
        <https://github.com/deepcurator/DCC/1807.03756v1_approaches>,
        <https://github.com/deepcurator/DCC/1807.03756v1_approximation>,
        <https://github.com/deepcurator/DCC/1807.03756v1_bounds>,
        <https://github.com/deepcurator/DCC/1807.03756v1_data>,
        <https://github.com/deepcurator/DCC/1807.03756v1_domains>,
        <https://github.com/deepcurator/DCC/1807.03756v1_fixes>,
        <https://github.com/deepcurator/DCC/1807.03756v1_gradients>,
        <https://github.com/deepcurator/DCC/1807.03756v1_latent_variable>,
        <https://github.com/deepcurator/DCC/1807.03756v1_latent_variable_models>,
        <https://github.com/deepcurator/DCC/1807.03756v1_machine_translation>,
        <https://github.com/deepcurator/DCC/1807.03756v1_method>,
        <https://github.com/deepcurator/DCC/1807.03756v1_methods>,
        <https://github.com/deepcurator/DCC/1807.03756v1_models>,
        <https://github.com/deepcurator/DCC/1807.03756v1_natural_language_processing>,
        <https://github.com/deepcurator/DCC/1807.03756v1_networks>,
        <https://github.com/deepcurator/DCC/1807.03756v1_other>,
        <https://github.com/deepcurator/DCC/1807.03756v1_posterior_inference>,
        <https://github.com/deepcurator/DCC/1807.03756v1_probabilistic_models>,
        <https://github.com/deepcurator/DCC/1807.03756v1_question_answering>,
        <https://github.com/deepcurator/DCC/1807.03756v1_state>,
        <https://github.com/deepcurator/DCC/1807.03756v1_that>,
        <https://github.com/deepcurator/DCC/1807.03756v1_these>,
        <https://github.com/deepcurator/DCC/1807.03756v1_training>,
        <https://github.com/deepcurator/DCC/1807.03756v1_variance>,
        <https://github.com/deepcurator/DCC/1807.03756v1_variational_inference> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1808.00508v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1808.00508v1_activations>,
        <https://github.com/deepcurator/DCC/1808.00508v1_analogy>,
        <https://github.com/deepcurator/DCC/1808.00508v1_architecture>,
        <https://github.com/deepcurator/DCC/1808.00508v1_architectures>,
        <https://github.com/deepcurator/DCC/1808.00508v1_generalization>,
        <https://github.com/deepcurator/DCC/1808.00508v1_images>,
        <https://github.com/deepcurator/DCC/1808.00508v1_information>,
        <https://github.com/deepcurator/DCC/1808.00508v1_language>,
        <https://github.com/deepcurator/DCC/1808.00508v1_learn>,
        <https://github.com/deepcurator/DCC/1808.00508v1_logic>,
        <https://github.com/deepcurator/DCC/1808.00508v1_magnitude>,
        <https://github.com/deepcurator/DCC/1808.00508v1_module>,
        <https://github.com/deepcurator/DCC/1808.00508v1_networks>,
        <https://github.com/deepcurator/DCC/1808.00508v1_neural_networks>,
        <https://github.com/deepcurator/DCC/1808.00508v1_numbers>,
        <https://github.com/deepcurator/DCC/1808.00508v1_objects>,
        <https://github.com/deepcurator/DCC/1808.00508v1_operators>,
        <https://github.com/deepcurator/DCC/1808.00508v1_that>,
        <https://github.com/deepcurator/DCC/1808.00508v1_they>,
        <https://github.com/deepcurator/DCC/1808.00508v1_this>,
        <https://github.com/deepcurator/DCC/1808.00508v1_time>,
        <https://github.com/deepcurator/DCC/1808.00508v1_training>,
        <https://github.com/deepcurator/DCC/1808.00508v1_translate>,
        <https://github.com/deepcurator/DCC/1808.00508v1_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1808.04768> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1808.04768_approach>,
        <https://github.com/deepcurator/DCC/1808.04768_computational_efficiency>,
        <https://github.com/deepcurator/DCC/1808.04768_dynamics>,
        <https://github.com/deepcurator/DCC/1808.04768_events>,
        <https://github.com/deepcurator/DCC/1808.04768_method>,
        <https://github.com/deepcurator/DCC/1808.04768_model>,
        <https://github.com/deepcurator/DCC/1808.04768_prediction>,
        <https://github.com/deepcurator/DCC/1808.04768_prediction_accuracy>,
        <https://github.com/deepcurator/DCC/1808.04768_predictions>,
        <https://github.com/deepcurator/DCC/1808.04768_sampling>,
        <https://github.com/deepcurator/DCC/1808.04768_tasks>,
        <https://github.com/deepcurator/DCC/1808.04768_that>,
        <https://github.com/deepcurator/DCC/1808.04768_time>,
        <https://github.com/deepcurator/DCC/1808.04768_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1808.06601> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1808.06601_adversarial>,
        <https://github.com/deepcurator/DCC/1808.06601_adversarial_learning>,
        <https://github.com/deepcurator/DCC/1808.06601_approach>,
        <https://github.com/deepcurator/DCC/1808.06601_approaches>,
        <https://github.com/deepcurator/DCC/1808.06601_baselines>,
        <https://github.com/deepcurator/DCC/1808.06601_benchmarks>,
        <https://github.com/deepcurator/DCC/1808.06601_counterpart>,
        <https://github.com/deepcurator/DCC/1808.06601_dynamics>,
        <https://github.com/deepcurator/DCC/1808.06601_framework>,
        <https://github.com/deepcurator/DCC/1808.06601_function>,
        <https://github.com/deepcurator/DCC/1808.06601_image>,
        <https://github.com/deepcurator/DCC/1808.06601_input>,
        <https://github.com/deepcurator/DCC/1808.06601_learn>,
        <https://github.com/deepcurator/DCC/1808.06601_model>,
        <https://github.com/deepcurator/DCC/1808.06601_modeling>,
        <https://github.com/deepcurator/DCC/1808.06601_models>,
        <https://github.com/deepcurator/DCC/1808.06601_our_method>,
        <https://github.com/deepcurator/DCC/1808.06601_poses>,
        <https://github.com/deepcurator/DCC/1808.06601_prediction>,
        <https://github.com/deepcurator/DCC/1808.06601_problem>,
        <https://github.com/deepcurator/DCC/1808.06601_resolution>,
        <https://github.com/deepcurator/DCC/1808.06601_results>,
        <https://github.com/deepcurator/DCC/1808.06601_scenes>,
        <https://github.com/deepcurator/DCC/1808.06601_segmentation>,
        <https://github.com/deepcurator/DCC/1808.06601_semantic_segmentation>,
        <https://github.com/deepcurator/DCC/1808.06601_state>,
        <https://github.com/deepcurator/DCC/1808.06601_systems>,
        <https://github.com/deepcurator/DCC/1808.06601_that>,
        <https://github.com/deepcurator/DCC/1808.06601_this>,
        <https://github.com/deepcurator/DCC/1808.06601_video>,
        <https://github.com/deepcurator/DCC/1808.06601_videos>,
        <https://github.com/deepcurator/DCC/1808.06601_visual_quality>,
        <https://github.com/deepcurator/DCC/1808.06601_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1808.08750v1> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1808.08750v1_classification>,
        <https://github.com/deepcurator/DCC/1808.08750v1_convolutional_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/1808.08750v1_deep_learning>,
        <https://github.com/deepcurator/DCC/1808.08750v1_dnns>,
        <https://github.com/deepcurator/DCC/1808.08750v1_generalisation>,
        <https://github.com/deepcurator/DCC/1808.08750v1_image>,
        <https://github.com/deepcurator/DCC/1808.08750v1_images>,
        <https://github.com/deepcurator/DCC/1808.08750v1_machine_learning_approach>,
        <https://github.com/deepcurator/DCC/1808.08750v1_noise>,
        <https://github.com/deepcurator/DCC/1808.08750v1_object_recognition>,
        <https://github.com/deepcurator/DCC/1808.08750v1_other>,
        <https://github.com/deepcurator/DCC/1808.08750v1_robustness>,
        <https://github.com/deepcurator/DCC/1808.08750v1_signal>,
        <https://github.com/deepcurator/DCC/1808.08750v1_system>,
        <https://github.com/deepcurator/DCC/1808.08750v1_that>,
        <https://github.com/deepcurator/DCC/1808.08750v1_they>,
        <https://github.com/deepcurator/DCC/1808.08750v1_training>,
        <https://github.com/deepcurator/DCC/1808.08750v1_vision_systems>,
        <https://github.com/deepcurator/DCC/1808.08750v1_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1809.01361> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1809.01361_adversarial>,
        <https://github.com/deepcurator/DCC/1809.01361_continuous>,
        <https://github.com/deepcurator/DCC/1809.01361_cross-domain>,
        <https://github.com/deepcurator/DCC/1809.01361_data>,
        <https://github.com/deepcurator/DCC/1809.01361_deep_learning>,
        <https://github.com/deepcurator/DCC/1809.01361_domain>,
        <https://github.com/deepcurator/DCC/1809.01361_domain_adaptation>,
        <https://github.com/deepcurator/DCC/1809.01361_domains>,
        <https://github.com/deepcurator/DCC/1809.01361_effectiveness>,
        <https://github.com/deepcurator/DCC/1809.01361_feature>,
        <https://github.com/deepcurator/DCC/1809.01361_features>,
        <https://github.com/deepcurator/DCC/1809.01361_framework>,
        <https://github.com/deepcurator/DCC/1809.01361_image>,
        <https://github.com/deepcurator/DCC/1809.01361_images>,
        <https://github.com/deepcurator/DCC/1809.01361_information>,
        <https://github.com/deepcurator/DCC/1809.01361_model>,
        <https://github.com/deepcurator/DCC/1809.01361_network>,
        <https://github.com/deepcurator/DCC/1809.01361_representation>,
        <https://github.com/deepcurator/DCC/1809.01361_training> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/1809.02840v2> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/1809.02840v2_approach>,
        <https://github.com/deepcurator/DCC/1809.02840v2_constraints>,
        <https://github.com/deepcurator/DCC/1809.02840v2_examples>,
        <https://github.com/deepcurator/DCC/1809.02840v2_graph>,
        <https://github.com/deepcurator/DCC/1809.02840v2_input>,
        <https://github.com/deepcurator/DCC/1809.02840v2_logic>,
        <https://github.com/deepcurator/DCC/1809.02840v2_method>,
        <https://github.com/deepcurator/DCC/1809.02840v2_model>,
        <https://github.com/deepcurator/DCC/1809.02840v2_neural_network_models>,
        <https://github.com/deepcurator/DCC/1809.02840v2_outputs>,
        <https://github.com/deepcurator/DCC/1809.02840v2_problem>,
        <https://github.com/deepcurator/DCC/1809.02840v2_problems>,
        <https://github.com/deepcurator/DCC/1809.02840v2_programs>,
        <https://github.com/deepcurator/DCC/1809.02840v2_recurrent_neural_network>,
        <https://github.com/deepcurator/DCC/1809.02840v2_representation>,
        <https://github.com/deepcurator/DCC/1809.02840v2_search>,
        <https://github.com/deepcurator/DCC/1809.02840v2_system>,
        <https://github.com/deepcurator/DCC/1809.02840v2_that> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_architecture>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_auto-encoders>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_backpropagation>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_clustering_techniques>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_complex>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_data>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_decoder>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_deep_neural_network>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_encoder>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_experiments>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_input>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_learn>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_map>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_method>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_network>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_networks>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_our_method>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_parameters>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_pre-training>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_procedure>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_state>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_strategies>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_structures>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_subspace>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_that>,
        <https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_way> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_architecture>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_architectures>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_complex>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_efficiency>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_gradients>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_long-term>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_matching>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_measure>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_measures>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_memory_capacity>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_other>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_our_method>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_parameters>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_recurrent_neural_networks>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_resolution>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_rnn>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_rnns>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_skip_connections>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_state>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_structure>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_task>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_tasks>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_theory>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_these>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_this>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_training>,
        <https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_abstracts>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_approach>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_data>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_embeddings>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_family>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_hierarchical>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_language>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_method>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_modeling>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_other>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_our_method>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_predicting>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_states>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_statistical_information>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_strategies>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_structured>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_studies>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_that>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_them>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_this>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_two>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_we>,
        <https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_words> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_adversarial>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_approximation>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_attention_mechanism>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_automatic_metrics>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_candidate>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_conversations>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_counterparts>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_cross-entropy>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_dialog>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_dialog_model>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_dialog_models>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_differentiability>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_discrete>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_diversity>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_domains>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_encoder>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_end-to-end>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_estimation>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_framework>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_generation>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_gradient>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_gradients>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_knowledge>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_maximum_likelihood>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_metric>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_model>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_models>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_paradigm>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_problem>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_rank>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_rnn>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_semantic>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_similarities>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_state>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_that>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_these>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_they>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_training>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_transfer>,
        <https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_3d_reconstruction>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_approach>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_approaches>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_end-to-end>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_feature>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_geometric_constraints>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_image>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_images>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_learn>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_methods>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_metric>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_operations>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_priors>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_problem>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_shape>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_surfaces>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_system>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_task>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_these>,
        <https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_algorithm>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_covariance>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_effectiveness>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_efficiency>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_feature>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_features>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_image>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_images>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matching>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matrix>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_method>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_methods>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_network>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_optimization>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_our_method>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_quality>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_textures>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_that>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_these>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_this>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_training>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_transfer>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_visual_quality>,
        <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_12> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_13> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_15> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_17> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_18> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_19> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_3> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_5> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_7> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_8> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_9> :partOf <https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp0> a :ActivationBlock .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp13> a :ActivationBlock .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp17> a :OutputBlock .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp5> a :ActivationBlock .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1_Comp7> a :ActivationBlock .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_activation_functions>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_approach>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_architectures>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_convergence>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_divergence>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_divergences>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_function>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_gan>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_generalization>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_improvements>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_kl>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_means>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_parameters>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_regular>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_results>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_that>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_these>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_this>,
        <https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_approach>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_architectures>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_diversity>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_image>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_input>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_learns>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_map>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_method>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_methods>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_model>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_modeling>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_network>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_one>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_other>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_our_method>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_outputs>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problem>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problems>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_results>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_this>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_time>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_training>,
        <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_0> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_1> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_10> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_11> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_12> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_13> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_15> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_16> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_2> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_3> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_4> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_5> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_6> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_7> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_8> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_9> :partOf <https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1_Comp4> a :InputBlock .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_approaches>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_datasets>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_distance>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_domain>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_experimental_results>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_learn>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_learner>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_mappings>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_method>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_one>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_results>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_that>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_this>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_translates>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_two>,
        <https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_approach>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_autoencoders>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_deblurring>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_field>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_formulation>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_gradient>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_gradient_descent>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_image>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_image_restoration>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_learn>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_mean-shift>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_minimization>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_noise>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_prior>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_problems>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_results>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_super-resolution>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_that>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_this>,
        <https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_architecture>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_audio>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_benchmarks>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_convolutional_neural_networks>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_correlations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_data>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_deep_learning>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_dynamics>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_first>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_images>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_information>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_interactions>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_layers>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_learn>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_locations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_model>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_modeling>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_principles>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_representations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_state>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structural_variations>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structured_data>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_that>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_those>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_video>,
        <https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_algorithm>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_approaches>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_baselines>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_benchmarks>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_classification>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_data>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_embeddings>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_feature>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_features>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_framework>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_function>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_functions>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_graph>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_graphs>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_information>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_interactions>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_learn>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_local_neighborhood>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_node>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_nodes>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_prediction>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_sampling>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_tasks>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_text>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_that>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_these>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_training>,
        <https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_accuracy>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_architecture>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_benchmarks>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_cifar-10>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_error_rate>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_imagenet>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_large_datasets>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_method>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_model>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_network>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_networks>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_per>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_predictions>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_problem>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_results>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_semi-supervised_learning>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_state>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_svhn>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_target>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_targets>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_that>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_this>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_training>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_we>,
        <https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_weights> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_accuracy>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_approach>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_bound>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_convergence>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_data>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_deep_learning>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_distributed>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_experiments>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_gradient>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_gradients>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_model>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_network>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_parallelism>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_parameters>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_scalability>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_source_code>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_that>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_this>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_time>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_training>,
        <https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_algorithm>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_approaches>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_continuous>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_correlation>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_data>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_datasets>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_discrete>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_end-to-end>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_features>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_function>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_hypotheses>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_interpretable>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_learns>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_location>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_neural_network>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_optimization>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_problem>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_procedure>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_real_datasets>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_studies>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_tests>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_that>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_these>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_this>,
        <https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_classifier>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_datasets>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_deep_convolutional_networks>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_deep_networks>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_feature>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_features>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_large-scale_data>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_layers>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_learn>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_multi-task_learning>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_networks>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_priors>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_problem>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_results>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_state>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_task>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_tasks>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_tensor>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_tensors>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_that>,
        <https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_transfer> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_algorithm>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_cifar-10>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_cifar-100>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_data>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_deep_learning_models>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_deep_models>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_distance>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_experiments>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_generalization>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_generalization_performance>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_gradient>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_imagenet>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_methods>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_mnist>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_models>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_one>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_problem>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_sizes>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_statistical_model>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_stochastic_gradient_descent>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_techniques>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_that>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_they>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_this>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_training>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_validate>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_we>,
        <https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_weights> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_algorithm>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_algorithms>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_architectures>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_convergence>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_current_algorithms>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_factors>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_field>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_formalism>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_gan>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_gans>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_gradient>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_properties>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_that>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_these>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_this>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_training>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_two>,
        <https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_approach>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_architecture>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_architectures>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_blocks>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_computational_cost>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_data>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_deep_learning>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_effectiveness>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_gan>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_gans>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_generation>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_generative_models>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_image>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_mismatch>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_model>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_models>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_parameters>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_procedure>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_quality>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_regularization>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_regularizer>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_tasks>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_that>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_they>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_this>,
        <https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_training> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_algorithm>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_algorithms>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_application>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_convergence>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_convex>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_deep_networks>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_functions>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_gradient>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_hyperparameters>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_learning_methods>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_methods>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_objective_function>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_one>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_optimization>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_procedure>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_results>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_state>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_stochastic_gradient_descent>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_that>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_these>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_this>,
        <https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_adversarial>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_approach>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_cifar-10>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_computational_efficiency>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_datasets>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_deep_generative_model>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_distance>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_gan>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_gradient_descent>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_matching>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_measure>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_mnist>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_model>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_network>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_other>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_sizes>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_techniques>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_that>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_this>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_topology>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_training>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_two>,
        <https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_approach>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_average_precision>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_benchmarks>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_classification>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_concepts>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_data>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_examples>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_framework>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_information>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_information_retrieval>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_method>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_model>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_ones>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_prediction>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_problem>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_rankings>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_ranks>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_relevance>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_results>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_retrieval>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_structured>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_that>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_them>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_these>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_this>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_training>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_understanding>,
        <https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_approach>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_continuous>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_datasets>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_discrete>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_end-to-end>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_first>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_framework>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_knowledge_base>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_learns>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_logic>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_model>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_operations>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_our_method>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_parameters>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_prior_work>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_problem>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_rules>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_structure>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_system>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_tasks>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_that>,
        <https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_these> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_activity>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_algorithm>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_algorithms>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_approach>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_data>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_datasets>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_dictionary_learning>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_experiments>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_extraction>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_framework>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_imaging>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_indicators>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_manually_annotated_data>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_methods>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_motion>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_processing>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_research>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_scale>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_that>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_them>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_time>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_two>,
        <https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6845-fisher-gan> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6845-fisher-gan_adversarial>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_algorithm>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_classification>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_complex>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_constraints>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_data>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_development>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_distance>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_divergence>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_divergences>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_extension>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_formulation>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_framework>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_function>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_gan>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_gans>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_generation>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_gradient_penalty>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_gradients>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_high_computational_cost>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_image>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_kl>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_learn>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_learns>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_measures>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_method>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_metrics>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_minimize>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_models>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_networks>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_neural_network>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_optimization>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_penalty>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_probability_distributions>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_real_data>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_that>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_these>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_this>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_time>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_training>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_validate>,
        <https://github.com/deepcurator/DCC/6845-fisher-gan_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_approach>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_architectures>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_codes>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_computations>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_concept>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_data>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_deep_neural_network>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_dnns>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_family>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_imaging>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_input>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_matrices>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_memory>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_model>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_modeling>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_models>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_networks>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_objects>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_operations>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_poses>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_processing>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_recognition_task>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_regions>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_results>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_shape>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_studies>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_system>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_systems>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_temporal_information>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_that>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_these>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_this>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_tool>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_ways>,
        <https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_accuracy>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_candidate>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_candidates>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_domain_experts>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_edges>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_efficiency>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_first>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_framework>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_graph>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_interactions>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_magnitude>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_model>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_models>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_network>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_nodes>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_prediction>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_problem>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_solution>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_template-based_approach>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_template-free_approach>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_templates>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_that>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_this>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_transformations>,
        <https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_approach>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_approaches>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_continuous>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_discrete>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_estimates>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_faster_convergence>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_gradient>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_hyperparameter>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_latent_variables>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_log-likelihood>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_modeling>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_models>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_relaxation>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_state>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_tasks>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_that>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_this>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_two>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_variables>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_variance>,
        <https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_agents>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_dynamics>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_environment>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_environments>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_learn>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_methods>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_network>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_one>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_other>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_parameters>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_per>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_platform>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_reinforcement_learning>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_reinforcement_learning_methods>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_relu>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_research>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_rl>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_rule>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_strategies>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_strategy>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_system>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_that>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_this>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_time>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_training>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_training_parameters>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_two>,
        <https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_adversarial>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_analogy>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_approach>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_baselines>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_cifar-10>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_data>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_database>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_datasets>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_divergences>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_evaluations>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experimental_results>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experiments>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_gan>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_imagenet>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kl>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kullback-leibler>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_mnist>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_network>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_novel_approach>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_objective_function>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_our_method>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_problem>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_properties>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_quality>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_rewards>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_scale>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_state>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_that>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_theoretical_analysis>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_these>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_this>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_two>,
        <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_3> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_6> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_7> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1_8> :partOf <https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_action>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_algorithm>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_baselines>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_benchmarks>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_entropy>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_minimizes>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_model>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_policy>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_q-learning>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_q-learning_algorithms>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_regularization>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_reinforcement_learning_(rl)>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_rl>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_state>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_that>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_this>,
        <https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_approximation>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_architectures>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_cifar-10>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_construction>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_domain>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_generalisation>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_images>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_improvements>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_inputs>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_kernels>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_mnist>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_models>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_posterior_inference>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_process_models>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_structure>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_that>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_them>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_way>,
        <https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_classification>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_constraints>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_deep_models>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_embeddings>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_ensembles>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_functions>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_graph>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_inputs>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_lattice>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_lattices>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_layers>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_network>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_networks>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_nodes>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_projections>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_real-world_datasets>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_state>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_stochastic_gradients>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_tensorflow>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_that>,
        <https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_training> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_activity_recognition>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_convergence>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_data>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_datasets>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_environments>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_features>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_first>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_framework>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_gradient>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_hierarchical>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_kitti>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_layers>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_loss_function>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_methods>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_model>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_models>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_priori>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_processing>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_region>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_target>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tasks>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_this>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tracking>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_training>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_two>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_videos>,
        <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1_0> :partOf <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1> .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1_1> :partOf <https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1> .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_accuracy>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_adversarial>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approach>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approaches>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_cifar-10>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data_augmentation>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_datasets>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain_experts>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_experiments>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_f1>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_functions>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_image>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_improvements>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_input>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_medical_imaging>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_method>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_model>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_operations>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_relation_extraction>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_results>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_state>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_task>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_technique>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_text>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_that>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_this>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_time>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_training>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformation_model>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformations>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_unlabeled_data>,
        <https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_accuracy>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_approaches>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_architectures>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_drop-in_replacement>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_experimental_results>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_flow>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_gradient>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_improvement>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_language_modeling>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_lstms>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_networks>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_penn_treebank>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_regularization>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_rnns>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_solutions>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_state>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_task>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_that>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_these>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_treatment>,
        <https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_compression>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_computational_efficiency>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_deep_learning>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_efficiency>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_factors>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_hierarchical>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_methods>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_network>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_nodes>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_posterior>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_precision>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_priors>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_problem>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_significance>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_sparsity>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_state>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_that>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_this>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_two>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_way>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_we>,
        <https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_weights> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_approaches>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_approximations>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_data>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_estimates>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_field>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_framework>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_function>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_heuristics>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_hyperparameter>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_hyperparameters>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_input>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_locations>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_method>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_methods>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_models>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_posterior>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_probabilistic_models>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_real-world_datasets>,
        <https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_that> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_architectures>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_concept>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_convolutions>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_data>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_domain>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_domains>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_environments>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_euclidean_spaces>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_feature>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_field>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_functions>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_image_analysis>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_knowledge>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_methods>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_model>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_models>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_networks>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_neural_networks>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_other>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_problem>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_representation>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_sphere>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_state>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_strategies>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_that>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_these>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_this>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_two>,
        <https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_architecture>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_cnn>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_computations>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_convolutional_neural_networks_(cnns)>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_data>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_end-to-end>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_estimation>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_experimental_techniques>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_experimental_time>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_feature>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_feature_spaces>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_field>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_locations>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_methods>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_model>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_models>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_network>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_one>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_problem>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_scales>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_state>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_surface>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_system>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_that>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_this>,
        <https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_data>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_dynamics>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_encoder>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_end-to-end>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_framework>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_methods>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_missing_data>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_model>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_objects>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representation>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representations>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_state>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_systems>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_tasks>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_that>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_time>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_two>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_unsupervised_learning>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_video>,
        <https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_videos> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6953-bayesian-gan> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6953-bayesian-gan_adversarial>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_approach>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_audio>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_benchmarks>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_candidate>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_cifar-10>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_data>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_ensembles>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_formulation>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_framework>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_gan>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_gans>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_gradient>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_images>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_interpretable>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_interventions>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_learn>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_model>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_networks>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_parameters>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_posterior>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_results>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_semi-supervised_learning>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_smoothing>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_state>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_svhn>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_this>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_we>,
        <https://github.com/deepcurator/DCC/6953-bayesian-gan_weights> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_architecture>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_convolutional_neural_network>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_datasets>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_deep_learning>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_formulations>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graph>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graphs>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_learn>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_matrix>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_models>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_neural_network>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_novel_approach>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_our_method>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_parameters>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_priors>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_recurrent_neural_network>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_smoothness>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_state>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structured>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structures>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_system>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_systems>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_techniques>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_that>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these>,
        <https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these_techniques> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_approximating>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_architecture>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_classification>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_conjunctions>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_convergence>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_data>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_datasets>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_deep_learning>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_eigenvectors>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_function>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_gradient_descent>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_improvements>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_information>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_kernel_methods>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_kernels>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_large_datasets>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_methods>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_optimization>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_regularization>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_results>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_scheme>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_sgd>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_significance>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_state>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_that>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_theoretical_analysis>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_this>,
        <https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_adversarial>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_algorithm>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_baselines>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_complex>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_data>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_domain>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_factors>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_human_actions>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_imitation_learning>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_inputs>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_interpretable>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_learn>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_method>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_model>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_our_method>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_representations>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_reward>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_signal>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_structure>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_that>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_this>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_way>,
        <https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_architectures>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_classification_accuracy>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_cnns>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_convolution>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_convolutional_networks>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_convolutional_neural_networks_(cnns)>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_counterparts>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_end-to-end>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_faster_convergence>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_formulation>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_framework>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_improvement>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_method>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_network>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_networks>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_optimization>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_parameter_space>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_representation>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_representations>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_that>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_training>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_visual_representation_learning>,
        <https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithm>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithms>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_architectures>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_complex>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_computations>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_data>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_flexibility>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_hardware>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_implementation>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_models>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_neural_network>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_operations>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_structure>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_task>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tasks>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tensorflow>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_that>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_them>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_this>,
        <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_3> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_5> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_6> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_7> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_8> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_9> :partOf <https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp0> a :RnnBlock .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp2> a :RnnBlock .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp3> a :RnnBlock .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp4> a :RnnBlock .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp5> a :RnnBlock .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp6> a :RnnBlock .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1_Comp7> a :RnnBlock .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_algorithm>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_approximation>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_classifier>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_classifiers>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_constraints>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_deep_neural_network>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_first>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_framework>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_functions>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_image>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_inception>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_interpretability>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_machine_learning_applications>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_objective_function>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_predictions>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_problem>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_state>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_that>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_this>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_times>,
        <https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_approaches>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_bias>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_classes>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_classification>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_classifier>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_data>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_decisions>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_distances>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_examples>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_few-shot_learning>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_improvements>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_learn>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_metric>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_networks>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_problem>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_representations>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_results>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_state>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_that>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_they>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_this>,
        <https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_training> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_algorithm>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_approaches>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_approximations>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_covariance>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_evaluations>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_function>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_information>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_information_sources>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_methods>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_objective_function>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_one>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_optimization>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_problem>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_quality>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_solution>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_state>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_technique>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_that>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_this>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_those>,
        <https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7033-dual-path-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7033-dual-path-networks_applications>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_architectures>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_computational_cost>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_datasets>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_experiments>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_feature>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_features>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_flexibility>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_framework>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_image_classification>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_memory>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_model>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_network>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_pascal_voc>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_representations>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_resnet>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_resnext>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_scale>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_scene>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_segmentation>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_state>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_that>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_this>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_times>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_topology>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_training>,
        <https://github.com/deepcurator/DCC/7033-dual-path-networks_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_10> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_11> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_12> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_15> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_3> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_5> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_6> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_8> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_9> :partOf <https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_Comp10> a :OutputBlock .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_Comp5> a :OutputBlock .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1_Comp8> a :DenseBlock .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_algorithm>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_approaches>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_approximation>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_classification>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_data>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_function>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_generalizations>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_independence>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_layers>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_model>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_models>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_our_method>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_overfitting>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_posteriors>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_scheme>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_that>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_these>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_they>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_uncertainty>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_variational_inference>,
        <https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_algorithm>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_architecture>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_encodes>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_end-to-end>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_experiments>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_model>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_model-based>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_model-free>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_network>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_neural_network>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_ones>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_other>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_planning>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_planning_algorithm>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_policy>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_solution>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_structure>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_task>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_tasks>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_that>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_training>,
        <https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_transfer> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_accuracy>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_applications>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_approaches>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_complex>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_components>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_datasets>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_deep_learning_models>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_ensemble>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_feature>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_interpretability>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_measures>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_method>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_methods>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_model>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_models>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_one>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_prediction>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_predictions>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_problem>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_properties>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_results>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_solution>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_that>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_these>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_this>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_unification>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_unified_framework>,
        <https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_data>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_embeddings>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_language>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_method>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_model>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_models>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_other>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_predictions>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_quality>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_systems>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_target>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_text>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_that>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_this>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_tool>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_variational_inference>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_we>,
        <https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_words> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_architecture>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_architectures>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_baseline>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_benchmarks>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_cnn>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_complex>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_convolution>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_criterion>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_distance>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_matching>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_methods>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_metric>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_network>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_regularization>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_results>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_retrieval>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_sift>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_state>,
        <https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_that> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_applications>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_baseline_methods>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_datasets>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_effectiveness>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_errors>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_experiments>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_function>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_linear_combination>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_method>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_methods>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_network>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_one>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_parameters>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_prediction>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_pruning>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_research>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_results>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_state>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_systems>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_that>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_this>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_those>,
        <https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_accuracy>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_approaches>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_architectures>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_benchmarks>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_input>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_interactions>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_model>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_modeling>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_models>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network_models>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_optimization>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_problem>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_research>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_results>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_scalability>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_state>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_systems>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_that>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_these>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_this>,
        <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_3> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_5> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_7> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1_8> :partOf <https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_benchmarks>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_complex>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_deep_learning>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_distances>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_features>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_hierarchical>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_input>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_layers>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_learn>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_local_features>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_metric>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_network>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_networks>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_neural_network>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_point_clouds>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_prior_works>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_results>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_scales>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_scenes>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_state>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_structures>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_that>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_this>,
        <https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_adversarial>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_approach>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_convergence>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_data>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_domain>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_flows>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_kernels>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_markov_chain>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_markov_chain_monte_carlo>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_markov_chains>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_mcmc>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_method>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_methods>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_model>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_posterior>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_problem>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_proposals>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_quality>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_results>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_schemes>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_that>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_this>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_training_method>,
        <https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_algorithm>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_architectures>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_bn>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_datasets>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_deep_neural_network>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_gradient>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_input>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_intrinsic_geometry>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_methods>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_network>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_regularization>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_rule>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_that>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_this>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_training>,
        <https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_weights> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_adversarial>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_adversarial_learning>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_approach>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_assumptions>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_correspondence>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_cross-domain>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_data>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_domain>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_domains>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_experiments>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_function>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_generation>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_image>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_image_classification>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_learn>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_matching>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_network>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_neural_networks>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_real_data>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_supervision>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_theory>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_training>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_two>,
        <https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_way> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_approach>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_approximation>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_continuous>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_deep_reinforcement_learning>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_discrete>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_domains>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_efficiency>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_environment>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_first>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_framework>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_gradient>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_improvement>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_inputs>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_knowledge>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_learns>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_method>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_methods>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_optimization>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_our_method>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_policies>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_policy>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_region>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_rewards>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_state>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_tasks>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_that>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_this>,
        <https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_approach>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_approximating>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_architecture>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_component>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_deep_rl>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_domains>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_features>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_function>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_generalisation>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_input>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_learns>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_low-dimensional_representation>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_method>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_methods>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_network>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_problem>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_reinforcement_learning_(rl)>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_reward>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_reward_function>,
        <https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_this> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_adversarial>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_algorithm>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_boosting_algorithms>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_complex>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_component>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_convergence>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_data>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_examples>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_gan>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_generative_models>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_iterative_procedure>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_method>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_model>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_natural_images>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_networks>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_problem>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_procedure>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_regions>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_that>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_they>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_this>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_training>,
        <https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_algorithms>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_applications>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_approach>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_approaches>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_criteria>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_end-to-end>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_evaluations>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_machine_learning_techniques>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_modeling>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_models>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_optimization>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_policy>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_popularity>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_prediction>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_probabilistic_machine_learning>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_problem>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_task>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_that>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_them>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_these>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_they>,
        <https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_conditions>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_convergence>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_convex>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_formulation>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gan>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gans>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gradient>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gradient_descent>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_optimization>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_other>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_parameters>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_procedure>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_regularization>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_that>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_this>,
        <https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_accuracy>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_convolution>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_deep_convolutional_neural_networks>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_dnns>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_extension>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_field>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_image>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_images>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_latter>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_layers>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_model>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_modeling>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_network>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_networks>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_object_recognition>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_objects>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_other>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_pooling>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_position>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_results>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_robustness>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_scale>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_target>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_targets>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_that>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_this>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_training>,
        <https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_adversarial>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_alternative>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_architectures>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_cifar-10>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_continuous>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_gan>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_gans>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_generations>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_generative_models>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_gradient>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_hyperparameter>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_input>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_language_models>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_method>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_networks>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_problems>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_quality>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_resnets>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_that>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_these>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_training>,
        <https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_weights> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_architectures>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_assumptions>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_autoencoders>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_data>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_datasets>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_decoder>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_encoder>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_features>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_flexibility>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_framework>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_graphical_model>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_importance_sampling>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_interpretable>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_learn>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_model>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_models>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_network>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_neural_networks>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_procedure>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_representations>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_semi-supervised_learning>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_structure>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_that>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_these>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_this>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_training>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_variables>,
        <https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_architecture>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanism>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanisms>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu_score>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_complex>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutional_neural_networks>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutions>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_decoder>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_encoder>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_ensembles>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_french>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_machine_translation_tasks>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_model>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_models>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_network>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_quality>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_results>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_state>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_task>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_that>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_these>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_time>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_training>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_two> ;
    :hasFigure <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1>,
        <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_3> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_5> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_6> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_7> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_8> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_9> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_Comp0> a :OutputBlock .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_Comp2> a :NormBlock .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_Comp4> a :NormBlock .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_Comp7> a :NormBlock .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_Comp8> a :NormBlock .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1_Comp9> a :NormBlock .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1_0> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1_2> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1_3> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1_4> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1_5> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1_6> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1_7> :partOf <https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7192-value-prediction-network_architecture>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_baselines>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_deep_reinforcement_learning_(rl)>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_dynamics>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_environment>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_experimental_results>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_learns>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_methods>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_model>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_model-based>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_model-free>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_network>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_neural_network>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_observations>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_planning>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction_model>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_predictions>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_representation>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_rewards>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_rl>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_state>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_states>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_that>,
        <https://github.com/deepcurator/DCC/7192-value-prediction-network_way> ;
    :hasFigure <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_3> :partOf <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_5> :partOf <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1_6> :partOf <https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_autoencoders>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_benchmarks>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_data>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_factors>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_latent_variable>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_measure>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_method>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_methods>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_modeling>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_one>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_our_method>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_policy>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_problem>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_state>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_structure>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_that>,
        <https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_treatment> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_classification_performance>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_datasets>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_feature_matching>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_formulation>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_gans>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_generative_adversarial_networks>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_results>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_state>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_supervised_learning_methods>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_that>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_time>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_training>,
        <https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_architecture>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_baselines>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_concepts>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_feature>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_input>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_inputs>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_language>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_maps>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_mechanism>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_models>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_network>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_processing>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_question_answering>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_representation>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_resnet>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_tasks>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_that>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_this>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_two>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_vision>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1>,
        <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_0> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_1> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_10> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_2> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_4> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_5> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_6> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_7> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_8> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_9> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1_Comp10> a :LSTMBlock .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_0> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_1> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_2> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_3> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_4> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_5> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_6> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_8> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_9> :partOf <https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_Comp0> a :ConvBlock .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_Comp3> a :ActivationBlock .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1_Comp9> a :LSTMBlock .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_adversarial>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_approximation>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_assumptions>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_cifar-10>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_complex>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_convergence>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_distance>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_dynamics>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_experiments>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_gan>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_gans>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_generation>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_image>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_images>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_inception>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_loss_functions>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_maximum_likelihood>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_models>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_networks>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_one>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_ones>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_optimization>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_rule>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_scale>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_stochastic_gradient_descent>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_svhn>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_that>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_theory>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_time>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_training>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_two>,
        <https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_clustering_method>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_component>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_distributed>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_first>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_framework>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_grouping>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_interaction>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_learned_representations>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_learns>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_model>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_neural_network>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_objects>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_our_method>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_prediction>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_problem>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_representations>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_task>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_tasks>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_that>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_these>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_this>,
        <https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_approach>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_architecture>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_complex>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_decisions>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_deep_learning>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_factors>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_feature>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_input>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_long_short-term_memory>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_lstm>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_maps>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_marks>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_model>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_modules>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_per>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_positions>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_predicting>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_prediction>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_scale>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_signals>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_state>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_structured>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_studies>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_target>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_tasks>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_technologies>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_that>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_they>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_two>,
        <https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_visualization_methods> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_alignment>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_effectiveness>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_family>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_machine_translation>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_method>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_one>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_other>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_parallel_text>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_problems>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_recovery>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_representations>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_sentiment>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_tasks>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_text>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_that>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_this>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_transfer>,
        <https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_word_order> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_approach>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_cifar-10>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_classifier>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_datasets>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_detection_method>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_image>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_imagenet>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_images>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_input>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_interpretable>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_maps>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_metric>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_model>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_other>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_our_method>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_results>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_supervised_methods>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_systems>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_task>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_that>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_this>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_time>,
        <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_we> ;
    :hasFigure <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_0> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_1> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_10> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_11> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_13> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_14> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_15> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_2> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_3> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_4> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_5> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_7> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_8> :partOf <https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_Comp0> a :InputBlock .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_Comp10> a :UnpoolingBlock .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_Comp3> a :OutputBlock .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_Comp5> a :UnpoolingBlock .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1_Comp7> a :UnpoolingBlock .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_bias>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_matrix>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_metric>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_observations>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_techniques>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_theory>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_this>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_understanding>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_variance>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_we>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_word_embedding>,
        <https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_word_embeddings> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_data>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_devices>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_efficiency>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_framework>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_learning_algorithm>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_linear_classification>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_machine_learning>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_methods>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_models>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_network>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_paradigm>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_privacy>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_scalability>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_training>,
        <https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_training_algorithm> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_adversarial>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_classes>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_classification>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_classifier>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_deep_models>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_deep_neural_networks>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_experiments>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_features>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_gaussian_distributions>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_machine_learning_applications>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_mahalanobis_distance>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_method>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_methods>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_posterior>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_prior>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_real-world>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_rule>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_state>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_that>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_this>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_training>,
        <https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_approaches>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_architectures>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_complex>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_components>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_deep_learning>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_framework>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_global_context>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_graph>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_images>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_interactions>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model_design>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_modeling>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_objects>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction_model>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_principles>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_results>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scene>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scenes>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_state>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_structured>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_task>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_that>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_this>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_understanding>,
        <https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_we> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_accuracy>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_approach>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_approaches>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_architecture>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_baseline>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_concept>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_concepts>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_convolutions>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_features>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_graph>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_image>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_input>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_interactions>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_interpretability>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_learn>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_learner>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_learns>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_method>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_module>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_natural_language_processing>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_problem>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_question_answering>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_representation>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_representations>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_results>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_semantic>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_spatial_relationships>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_strategy>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_techniques>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_that>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_this>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_two>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_vision>,
        <https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks> a :Publication ;
    :conferenceSeries "NIPS" ;
    :hasEntity <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_activations>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_backpropagation>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_data>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_decoder>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_encoder>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_first>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_flexibility>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_information>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_memory>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_method>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_models>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_neural_networks>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_processing>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_rnn>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_rnns>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_scheme>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_sequence-to-sequence>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_state>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_states>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_technique>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_that>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_they>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_training>,
        <https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_we> ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_complex,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_convolutions,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_domains,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_faces,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_image,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_information,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_map,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_mappings,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_objects,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_scale,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_semantic_segmentation,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_shape,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_techniques,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_texture,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_that,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_they,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_this,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_two,
        :Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_activations,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_classification_performance,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_datasets,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_end-to-end,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_feature,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_hyperparameter,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_neural_network,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_optimization,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_overfitting,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_overhead,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_prior_work,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_problem,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_procedure,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_regularization,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_segmentation_techniques,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_sizes,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_state,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_tasks,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_this,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_time,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_training,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_visual_classification,
        :Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:AbstractText a owl:Class ;
    rdfs:subClassOf :Text .

:Acuna_Efficient_Interactive_Annotation_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:AdadeltaOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:AdagradOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:AdamOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Ahmed_An_Improved_Deep_2015_CVPR_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Ahmed_An_Improved_Deep_2015_CVPR_paper_architecture,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_data,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_features,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_image,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_images,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_input,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_layers,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_method,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_metric,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_neighborhood,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_network,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_of_re-identification,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_outputs,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_overfitting,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_person_re-identification,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_problem,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_results,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_state,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_target,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_that,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_this,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_training,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_two,
        :Ahmed_An_Improved_Deep_2015_CVPR_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Ahn_Learning_Pixel-Level_Semantic_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Akhtar_Defense_Against_Universal_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_action,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_annotations,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_approach,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_architecture,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_attention_mechanisms,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_changing_backgrounds,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_conditions,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_continuous,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_discrete,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_domain,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_expressions,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_gan,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_gans,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_generation,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_generative_adversarial_networks,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_images,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_lighting,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_magnitude,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_model,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_network,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_results,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_scheme,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_strategy,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_task,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_that,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_them,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_this,
        :Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_0 :partOf :Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_1 :partOf :Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_2 :partOf :Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_3 :partOf :Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_4 :partOf :Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_6 :partOf :Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp0 a :InputBlock .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp1 a :NormBlock .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp2 a :ConvBlock .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp4 a :ActivationBlock .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1_Comp6 a :OutputBlock .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_accuracy,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_algorithms,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_approaches,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_architecture,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_architectures,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_cnn,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_cnns,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_connectivity,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_convolutions,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_efficiency,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_flow,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_graph,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_graphs,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_improvement,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_information,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_layers,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_model,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_models,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_network,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_networks,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_node,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_nodes,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_other,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_properties,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_pruning,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_resnet,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_resnets,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_results,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_sizes,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_sparsity,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_state,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_technique,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_techniques,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_that,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_theory,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_these,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_these_techniques,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_this,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_training,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_two,
        :Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper :hasFigure :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1,
        :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1_0 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1_1 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1_2 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1_4 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1_5 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1_7 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_0 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_1 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_2 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_3 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_4 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_5 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_7 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_8 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_9 :partOf :Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_Comp2 a :RnnSeqBlock .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_Comp5 a :DeconvBlock .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1_Comp9 a :PoolingBlock .

:Andreas_Neural_Module_Networks_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2016 .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1_0 :partOf :Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1_1 :partOf :Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1_2 :partOf :Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1_3 :partOf :Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1_4 :partOf :Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1_6 :partOf :Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1_Comp4 a :LSTMBlock .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_accuracy,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_approach,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_architecture,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_architectures,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_baselines,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_color,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_data,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_data_modalities,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_end-to-end,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_environments,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_feature,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_features,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_first,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_images,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_input,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_maps,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_mesh,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_method,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_methods,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_network,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_per,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_pooling,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_prediction,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_results,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_rgb,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_rgb_images,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_scene,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_segmentation,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_semantic,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_semantic_segmentation,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_target,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_task,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_that,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_this,
        :Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_approach,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_approaches,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_collection,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_collections,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_datasets,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_framework,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_image,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_learn,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_map,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_mechanism,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_mesh,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_model,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_object_category,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_objects,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_per,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_pose,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_prediction,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_representation,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_results,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_semantic,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_shape,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_shapes,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_supervision,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_texture,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_textures,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_that,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_training,
        :Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_algorithm,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_approach,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_approaches,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_categories,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_classes,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_classification,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_data,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_datasets,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_embeddings,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_first,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_methods,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_models,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_object_detection,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_other,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_prior_works,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_problem,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_problems,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_research,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_results,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_sampling,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_semantic,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_solution,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_these,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_training,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_two,
        :Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_visualgenome ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Antol_VQA_Visual_Question_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Antol_VQA_Visual_Question_ICCV_2015_paper_question_answering,
        :Antol_VQA_Visual_Question_ICCV_2015_paper_task ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_approach,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_approaches,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_bias,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_cnns,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_domain,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_encoder-decoder,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_image,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_imagenet,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_images,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_measure,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_network,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_patches,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_processing,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_quality,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_rank,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_research,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_resolution,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_results,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_rgb_images,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_that,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_these,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_this,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_those,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_time,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_transfer,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_videos,
        :Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_architectures,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_classification_tasks,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_deep_neural_network,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_domain,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_imagenet,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_learn,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_method,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_network,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_networks,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_overhead,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_per,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_pruning,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_quantization,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_task,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_tasks,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_that,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_those,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_we,
        :Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_weights ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Atapour-Abarghouei_Real-Time_Monocular_Depth_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Azadi_Multi-Content_GAN_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Bagherinezhad_LCNN_Lookup-Based_Convolutional_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Bai_Deep_Watershed_Transform_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Baradel_Glimpse_Clouds_Human_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_1 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_10 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_11 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_12 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_2 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_3 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_4 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_5 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_6 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_7 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_8 :partOf :Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_Comp11 a :LSTMBlock .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_Comp12 a :LSTMBlock .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_Comp3 a :LSTMBlock .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_Comp4 a :LSTMBlock .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1_Comp6 a :LSTMBlock .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_adversarial,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_adversarial_learning,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_algorithms,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_data,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_diversity,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_end-to-end,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_expressions,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_face,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_face_images,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_face_recognition,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_framework,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_illuminations,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_images,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_information,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_input,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_lighting,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_method,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_methods,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_model,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_networks,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_pose,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_poses,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_real_data,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_results,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_state,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_that,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_transfer,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_two,
        :Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_way ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_benchmarks,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_color,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_color_constancy,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_colors,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_images,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_model,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_modeling,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_object_detection,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_objects,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_prediction,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_problem,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_scene,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_structured,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_task,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_techniques,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_that,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_this,
        :Barron_Convolutional_Color_Constancy_ICCV_2015_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Baumgartner_Visual_Feature_Attribution_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Bautista_Deep_Unsupervised_Similarity_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Berman_The_LovaSz-Softmax_Loss_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_algorithm,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_baseline_methods,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_benchmarks,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_field,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_pose,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_pose.pytorch,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_pose_estimation,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_results,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_system,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_time,
        :Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_tracking ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_correspondences,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_experiments,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_generation,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_gmm,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_image,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_input,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_learns,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_logo,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_matching,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_module,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_network,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_prior_works,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_research,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_results,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_shape,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_smoothness,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_state,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_strategy,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_systems,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_target,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_task,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_that,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_these,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_they,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_this,
        :Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:BodyText a owl:Class ;
    rdfs:subClassOf :Text .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_classification,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_cnn,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_effectiveness,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_experiments,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_method,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_network,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_object_detectors,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_objects,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_optimization,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_procedure,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_state,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_this,
        :Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Brahmbhatt_Geometry-Aware_Learning_of_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_approach,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_baseline,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concept,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concepts,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_datasets,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_effectiveness,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_embeddings,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_end-to-end,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_experiments,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_grounding,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_images,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_improvement,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_layers,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_learns,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_model,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_phrases,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_prior_works,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_region,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representation,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representations,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_solution,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_subspaces,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_text,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_that,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_them,
        :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_we ;
    :hasFigure :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_0 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_1 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_10 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_11 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_12 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_13 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_15 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_16 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_17 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_2 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_4 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_5 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_6 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_7 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_8 :partOf :Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp0 a :InputBlock .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1_Comp5 a :NormBlock .

:Buch_SST_Single-Stream_Temporal_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Buch_SST_Single-Stream_Temporal_CVPR_2017_paper_approach ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper_novel_approach,
        :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper_this,
        :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper_we ;
    :hasFigure :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_0 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_1 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_10 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_11 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_2 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_3 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_4 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_5 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_7 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_8 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_9 :partOf :Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_Comp10 a :ConvBlock .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_Comp11 a :ConvBlock .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_Comp2 a :ActivationBlock .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_Comp3 a :ConvBlock .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_Comp4 a :ConvBlock .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_Comp7 a :ActivationBlock .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1_Comp9 a :ActivationBlock .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Busta_Deep_TextSpotter_An_ICCV_2017_paper_accuracy,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_cnn,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_data,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_datasets,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_end-to-end,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_input,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_magnitude,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_method,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_methods,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_per,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_resolution,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_scene,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_state,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_structure,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_text,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_text_recognition,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_that,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_training,
        :Busta_Deep_TextSpotter_An_ICCV_2017_paper_two ;
    :hasFigure :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_0 :partOf :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_1 :partOf :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_2 :partOf :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_3 :partOf :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_4 :partOf :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_5 :partOf :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_6 :partOf :Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp0 a :ConvBlock .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp2 a :PoolingBlock .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1_Comp6 a :RnnBlock .

:BytesList a owl:Class ;
    rdfs:subClassOf :train .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Caelles_One-Shot_Video_Object_CVPR_2017_paper_architecture,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_convolutional_neural_network,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_databases,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_experiments,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_first,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_imagenet,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_learn,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_model,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_one,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_results,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_segmentation,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_semantic_information,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_state,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_task,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_technique,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_that,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_transfer,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_two,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_video,
        :Caelles_One-Shot_Video_Object_CVPR_2017_paper_video_segmentation ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Camgoz_Neural_Sign_Language_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Camgoz_Neural_Sign_Language_CVPR_2018_paper_language ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_alignment,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_approach,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_deep_learning,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_problems,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_sequence-to-sequence ;
    :hasFigure :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1,
        :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_0 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_1 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_10 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_11 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_13 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_15 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_2 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_3 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_4 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_5 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_6 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_7 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_8 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_9 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_Comp0 a :LossBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_Comp1 a :LSTMBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1_Comp5 a :InputBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1_0 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1_2 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1_3 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1_4 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1_5 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1_0 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1_1 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1_2 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1_3 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1_4 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1_5 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1_Comp0 a :LossBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_0 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_1 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_2 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_3 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_4 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_5 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_Comp1 a :DenseBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1_Comp5 a :RnnSeqBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_0 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_1 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_10 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_11 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_12 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_13 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_14 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_15 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_16 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_17 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_2 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_3 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_4 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_5 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_6 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_7 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_8 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_9 :partOf :Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_Comp1 a :DenseBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_Comp16 a :RnnSeqBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_Comp17 a :RnnSeqBlock .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1_Comp6 a :LSTMBlock .

:Cao_HashGAN_Deep_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Cao_HashNet_Deep_Learning_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Cao_Partial_Transfer_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Cao_Realtime_Multi-Person_2D_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Cao_Realtime_Multi-Person_2D_CVPR_2017_paper_approach ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Cao_Visual_Question_Reasoning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Castrejon_Annotating_Object_Instances_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Castrejon_Annotating_Object_Instances_CVPR_2017_paper_approach ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Chang_Deep_Adaptive_Image_ICCV_2017_paper_cifar-10,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_classification,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_clustering_accuracy,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_clusters,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_datasets,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_distance,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_feature,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_features,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_framework,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_image,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_images,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_learning_algorithm,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_machine_learning,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_methods,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_mnist,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_network,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_one,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_problem,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_results,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_similarities,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_state,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_task,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_that,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_this,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_vision,
        :Chang_Deep_Adaptive_Image_ICCV_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_approach,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_architectures,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_cnn,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_codes,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_context_information,
        <https://github.com/deepcurator/DCC/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_convolutional_neural_networks_(cnns)>,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_correspondence,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_datasets,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_estimation,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_first,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_global_context,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_images,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_kitti,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_learns,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_locations,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_means,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_method,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_module,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_modules,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_network,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_networks,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_pooling,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_problem,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_regions,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_scales,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_stereo_matching,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_stereo_pair,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_supervised_learning,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_supervision,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_task,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_that,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_this,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_two,
        :Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_application,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_applications,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_crowdsourcing,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_distance,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_family,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_first,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_image,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_method,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_models,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_objects,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_one,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_research,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_retrieval,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_scale,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_scene,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_segmentation,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_semantic,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_semantic_segmentation,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_templates,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_that,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_training,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_trees,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_two,
        :Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_understanding ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:CheckpoingManager a owl:Class ;
    rdfs:subClassOf :train .

:Checkpoint a owl:Class ;
    rdfs:subClassOf :train .

:CheckpointSaverHook a owl:Class ;
    rdfs:subClassOf :train .

:CheckpointSaverListener a owl:Class ;
    rdfs:subClassOf :train .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper :hasFigure :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_0 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_1 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_10 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_11 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_12 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_13 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_14 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_15 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_2 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_3 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_4 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_5 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_6 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_7 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_8 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1_9 :partOf :Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Chen_Cascaded_Pyramid_Network_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_0 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_1 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_10 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_11 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_13 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_14 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_2 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_3 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_4 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_5 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_6 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_7 :partOf :Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp0 a :ConvBlock .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp14 a :ConvBlock .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp3 a :ConcatBlock .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp4 a :ConcatBlock .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1_Comp6 a :ConvBlock .

:Chen_Knowledge_Aided_Consistency_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_applications,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_approaches,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_benchmarks,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_cameras,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_data,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_experiments,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_first,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_information,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_learn,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_networks,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_one,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_point_clouds,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_policies,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_quality,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_research,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_scale,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_semantic,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_tasks,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_that,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_this,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_understanding,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_video,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_videos,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_vision,
        :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_we ;
    :hasFigure :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_0 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_1 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_12 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_13 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_15 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_16 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_21 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_23 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_26 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_3 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_4 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_5 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_6 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_8 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1_9 :partOf :Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_0 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_1 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_13 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_14 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_15 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_16 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_2 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_3 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_7 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1_8 :partOf :Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 .

:Chen_Photographic_Image_Synthesis_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Chen_Photographic_Image_Synthesis_ICCV_2017_paper_approach,
        :Chen_Photographic_Image_Synthesis_ICCV_2017_paper_images,
        :Chen_Photographic_Image_Synthesis_ICCV_2017_paper_scenes,
        :Chen_Photographic_Image_Synthesis_ICCV_2017_paper_semantic,
        :Chen_Photographic_Image_Synthesis_ICCV_2017_paper_that,
        :Chen_Photographic_Image_Synthesis_ICCV_2017_paper_training ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_0 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_1 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_10 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_11 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_12 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_13 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_14 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_15 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_2 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_3 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_4 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_5 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_6 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_7 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_8 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_9 :partOf :Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp1 a :InputBlock .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp14 a :LossBlock .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp2 a :InputBlock .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp8 a :ConcatBlock .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1_Comp9 a :LossBlock .

:Chen_Show_Adapt_and_ICCV_2017_paper :hasFigure :Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1_0 :partOf :Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1_1 :partOf :Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1_2 :partOf :Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1_4 :partOf :Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1_5 :partOf :Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1_6 :partOf :Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1_Comp1 a :LossBlock .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper :hasFigure :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_0 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_1 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_2 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_3 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_4 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_5 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_6 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_7 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_8 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_9 :partOf :Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_Comp0 a :InputBlock .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_Comp5 a :RnnBlock .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1_Comp6 a :LossBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_natural_images,
        :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_research,
        :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_text ;
    :hasFigure :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1,
        :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_0 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_10 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_11 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_12 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_13 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_14 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_15 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_17 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_18 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_19 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_2 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_20 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_21 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_25 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_27 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_3 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_4 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_5 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_6 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_7 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_8 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_9 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp0 a :ConvBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp12 a :FlattenBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp14 a :ConvBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp17 a :ConvBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp19 a :LSTMBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp20 a :ConvBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp27 a :LossBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp5 a :LossBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp6 a :ConvBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1_Comp9 a :ConvBlock .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_0 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_1 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_2 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_3 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_4 :partOf :Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1 .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1_Comp2 a :ConvBlock .

:Cheng_Deep_Colorization_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2015 .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_accuracies,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_algorithms,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_approach,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_cifar-10,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_classification,
        <https://github.com/deepcurator/DCC/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_convolutional_neural_networks_(cnns)>,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_imagenet,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_method,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_methods,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_model,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_models,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_optimization,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_our_method,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_reinforcement_learning,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_rl,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search_space,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_sequential_model-based,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_state,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_strategy,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structure,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structures,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_that,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_this,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_times,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_way,
        :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_we ;
    :hasFigure :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_1 :partOf :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1 .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_12 :partOf :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1 .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_3 :partOf :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1 .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_7 :partOf :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1 .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1_9 :partOf :Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1 .

:ChiefSessionCreator a owl:Class ;
    rdfs:subClassOf :train .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_domain,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_first,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_generator_network,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_image,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_images,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_input,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_results,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_that,
        :Choi_StarGAN_Unified_Generative_CVPR_2018_paper_transferring_knowledge ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Chollet_Xception_Deep_Learning_CVPR_2017_paper_inception,
        :Chollet_Xception_Deep_Learning_CVPR_2017_paper_modules ;
    :hasFigure :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1,
        :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1,
        :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1,
        :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1,
        :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_0 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_1 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_2 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_3 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_4 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_5 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_6 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_7 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_8 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_Comp0 a :ConcatBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_Comp1 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_Comp3 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1_Comp6 a :InputBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_0 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_1 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_2 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_3 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_4 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_5 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_6 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp0 a :ConcatBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_0 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_1 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_2 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_3 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_5 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_Comp0 a :ConcatBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_Comp1 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_Comp2 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1_Comp3 a :OutputBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_0 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_1 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_2 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_3 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_4 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_5 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_6 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_7 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_9 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_Comp0 a :ConcatBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_Comp6 a :OutputBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1_Comp9 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_0 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_1 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_10 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_11 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_12 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_13 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_14 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_15 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_16 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_17 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_2 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_20 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_21 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_22 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_24 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_26 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_27 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_3 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_31 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_32 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_33 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_36 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_37 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_4 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_5 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_6 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_7 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_8 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_9 :partOf :Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp1 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp13 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp14 a :PoolingBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp16 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp17 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp2 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp20 a :PoolingBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp22 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp24 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp26 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp27 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp3 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp31 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp33 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp36 a :PoolingBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp37 a :PoolingBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp7 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp8 a :ConvBlock .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1_Comp9 a :ConvBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1,
        :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_0 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_1 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_10 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_12 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_13 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_16 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_17 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_18 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_19 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_2 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_20 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_21 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_22 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_23 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_24 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_25 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_26 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_27 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_28 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_29 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_3 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_30 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_31 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_33 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_34 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_35 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_36 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_37 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_38 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_39 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_40 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_5 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_6 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_7 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_8 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_9 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp10 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp12 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp13 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp20 a :FlattenBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp21 a :OutputBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp22 a :OutputBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp25 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp27 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp29 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp30 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp31 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp33 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp34 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp35 a :DenseBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp36 a :DenseBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp37 a :DenseBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp38 a :ConvBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp39 a :ConvBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp40 a :DenseBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp8 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1_Comp9 a :LSTMBlock .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_0 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_1 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_10 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_2 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_3 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_4 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_5 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_6 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_7 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_8 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1_9 :partOf :Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 .

:Clark_VidLoc_A_Deep_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Clark_VidLoc_A_Deep_CVPR_2017_paper_approaches,
        <https://github.com/deepcurator/DCC/Clark_VidLoc_A_Deep_CVPR_2017_paper_convolutional_neural_networks_(cnn)>,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_datasets,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_estimates,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_forests,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_images,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_learning_techniques,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_means,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_model,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_motion,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_our_method,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_per,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_pose,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_smoothness,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_that,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_this,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_video,
        :Clark_VidLoc_A_Deep_CVPR_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:ClusterDef a owl:Class ;
    rdfs:subClassOf :train .

:ClusterSpec a owl:Class ;
    rdfs:subClassOf :train .

:Coordinator a owl:Class ;
    rdfs:subClassOf :train .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper :hasFigure :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_1 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_13 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_14 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_15 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_16 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_17 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_18 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_19 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_2 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_20 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_23 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_25 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_26 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_27 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_28 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_29 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_3 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_30 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_31 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_33 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_34 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_4 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_6 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_7 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1_8 :partOf :Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 .

:Cui_Large_Scale_Fine-Grained_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_datasets,
        :Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_imagenet,
        :Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_knowledge,
        :Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_scale ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Cui_Learning_to_Evaluate_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Cui_Learning_to_Evaluate_CVPR_2018_paper_approach,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_bleu,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_constructions,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_correlation,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_data_augmentation,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_examples,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_experiments,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_face,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_human_judgments,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_image,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_metric,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_metrics,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_other,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_robustness,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_rule,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_scheme,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_spots,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_syntactic_structure,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_system,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_tests,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_that,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_these,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_training,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_transformations,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_two,
        :Cui_Learning_to_Evaluate_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Dahl_Pixel_Recursive_Super_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Dai_Deformable_Convolutional_Networks_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Dai_Deformable_Convolutional_Networks_ICCV_2017_paper_cnns,
        <https://github.com/deepcurator/DCC/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper_neural_networks_(> ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_3d_reconstructions,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_accuracy,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_approach,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_approaches,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_convolutional_neural_networks,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_data,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_distance,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_environments,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_functions,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_occlusions,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_scene,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_semantic,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_sensor,
        :Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_that ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Demirel_Attributes2Classname_A_Discriminative_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Demirel_Attributes2Classname_A_Discriminative_ICCV_2017_paper_novel_approach ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_adversarial,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_adversarial_learning,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_approaches,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_architecture,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_baseline,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_classes,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_classification,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_datasets,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_deep_convolutional_networks,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_efficiency,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_framework,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_frameworks,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_image,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_network,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_robustness,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_state,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_tasks,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_that,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_these,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_this,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_two,
        :Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Dmytro_Mishkin_Repeatability_Is_Not_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Dolhansky_Eye_In-Painting_With_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Dolhansky_Eye_In-Painting_With_CVPR_2018_paper_novel_approach ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1,
        :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_0 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_1 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_10 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_11 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_13 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_14 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_16 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_17 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_18 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_19 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_2 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_20 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_21 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_22 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_23 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_24 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_25 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_26 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_3 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_4 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_5 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_6 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_7 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_8 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_9 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp13 a :OutputBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp16 a :ConvBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp18 a :ConcatBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp20 a :ConcatBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp22 a :OutputBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp24 a :ConcatBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp26 a :OutputBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp4 a :ConvBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp5 a :ConvBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp8 a :ConvBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1_Comp9 a :ConvBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_0 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_1 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_11 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_12 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_13 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_14 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_16 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_18 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_19 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_2 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_21 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_22 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_24 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_25 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_3 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_4 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_5 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_6 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_7 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_8 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_9 :partOf :Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_Comp1 a :ConvBlock .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1_Comp2 a :ConvBlock .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Dong_Su_Is_Robustness_the_ECCV_2018_paper_accuracy,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_adversarial,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_adversarial_examples,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_architecture,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_classification,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_classifiers,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_deep_neural_networks,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_evaluation_criterion,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_experimental_results,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_family,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_image,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_image_classification,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_imagenet,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_metrics,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_model,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_models,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_natural_images,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_network,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_one_model,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_prediction_accuracy,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_robustness,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_scale,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_studies,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_that,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_this,
        :Dong_Su_Is_Robustness_the_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_annotations,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_approach,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_flow,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_gradients,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_image,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_images,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_improvements,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_location,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_loss_function,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_our_method,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_precision,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_registration,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_supervision,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_that,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_this,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_tracking,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_training,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_video,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_videos,
        :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_we ;
    :hasFigure :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_0 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_1 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_10 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_11 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_2 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_3 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_4 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_6 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_7 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_8 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1_9 :partOf :Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_adversarial,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_algorithm,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_datasets,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_encoder-decoder,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_framework,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_generator_network,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_geometric_prior,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_image,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_images,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_information,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_locations,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_network,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_networks,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_objects,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_other,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_patches,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_predicting,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_problem,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_regions,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_results,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_scene,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_scenes,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_signals,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_target,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_that,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_this,
        :Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_time ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_accuracy,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_activations,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_approach,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_cifar-10,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_compression,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_datasets,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_deep_neural_network,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_experiments,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_imagenet,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_method,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_methods,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_model,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_network,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_operations,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_our_method,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_precision,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_prediction_accuracy,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_quantization,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_resnet,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_schemes,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_structures,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_that,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_this,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_we,
        :Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_weights ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Dorta_Structured_Uncertainty_Prediction_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Douze_Low-Shot_Learning_With_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Du_RPAN_An_End-To-End_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Du_RPAN_An_End-To-End_ICCV_2017_paper_action,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_action_recognition,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_action_videos,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_annotation,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_attention_mechanism,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_benchmarks,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_complex,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_datasets,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_effectiveness,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_features,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_human_pose,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_learn,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_learns,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_mechanism,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_methods,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_modeling,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_motion,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_network,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_one,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_parameters,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_pooling,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_pose,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_pose_estimation,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_prediction,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_recurrent_network,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_recurrent_neural_networks,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_representation,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_results,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_rnns,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_state,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_structures,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_studies,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_supervision,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_that,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_these,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_this,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_time,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_two,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_unified_framework,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_video,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_videos,
        :Du_RPAN_An_End-To-End_ICCV_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_average_improvement,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_cifar-10,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_codes,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_datasets,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_deep_reinforcement_learning,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_edges,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_graph,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_information,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_inputs,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_interaction,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_interactions,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_learn,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_maximizing,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_method,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_model,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_mutual_information,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_nodes,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_noise,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_one,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_prior_knowledge,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_representation_learning_methods,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_state,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_structure,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_that,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_this,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_uncertainty,
        :Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Durand_WILDCAT_Weakly_Supervised_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Dvornik_BlitzNet_A_Real-Time_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_approaches,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_convolution,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_convolutions,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_datasets,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_image,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_information,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_language,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_method,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_methods,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_natural_language,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_our_method,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_problem,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_segmentations,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_segmenting,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_state,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_task,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_techniques,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_that,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_these,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_this,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_two,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_visual_information,
        :Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Ehsani_Who_Let_the_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Ehsani_Who_Let_the_CVPR_2018_paper_actions,
        :Ehsani_Who_Let_the_CVPR_2018_paper_approach,
        :Ehsani_Who_Let_the_CVPR_2018_paper_data,
        :Ehsani_Who_Let_the_CVPR_2018_paper_domains,
        :Ehsani_Who_Let_the_CVPR_2018_paper_encodes,
        :Ehsani_Who_Let_the_CVPR_2018_paper_estimation,
        :Ehsani_Who_Let_the_CVPR_2018_paper_image_classification,
        :Ehsani_Who_Let_the_CVPR_2018_paper_information,
        :Ehsani_Who_Let_the_CVPR_2018_paper_input,
        :Ehsani_Who_Let_the_CVPR_2018_paper_learned_representation,
        :Ehsani_Who_Let_the_CVPR_2018_paper_metrics,
        :Ehsani_Who_Let_the_CVPR_2018_paper_model,
        :Ehsani_Who_Let_the_CVPR_2018_paper_modelling,
        :Ehsani_Who_Let_the_CVPR_2018_paper_other,
        :Ehsani_Who_Let_the_CVPR_2018_paper_representation,
        :Ehsani_Who_Let_the_CVPR_2018_paper_representation_learning,
        :Ehsani_Who_Let_the_CVPR_2018_paper_representations,
        :Ehsani_Who_Let_the_CVPR_2018_paper_results,
        :Ehsani_Who_Let_the_CVPR_2018_paper_scene_classification,
        :Ehsani_Who_Let_the_CVPR_2018_paper_surface,
        :Ehsani_Who_Let_the_CVPR_2018_paper_task,
        :Ehsani_Who_Let_the_CVPR_2018_paper_that,
        :Ehsani_Who_Let_the_CVPR_2018_paper_this,
        :Ehsani_Who_Let_the_CVPR_2018_paper_videos,
        :Ehsani_Who_Let_the_CVPR_2018_paper_vision,
        :Ehsani_Who_Let_the_CVPR_2018_paper_visual_information,
        :Ehsani_Who_Let_the_CVPR_2018_paper_we ;
    :hasFigure :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1,
        :Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1,
        :Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_0 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_1 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_10 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_12 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_13 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_14 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_2 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_5 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_6 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_9 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_Comp10 a :LSTMBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_Comp14 a :LSTMBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_Comp2 a :LSTMBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1_Comp6 a :LSTMBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_0 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_2 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_3 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_5 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_6 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_Comp2 a :LSTMBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_Comp5 a :LSTMBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1_Comp6 a :LSTMBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_0 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_1 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_2 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_3 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_4 :partOf :Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1 .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_Comp1 a :ConvBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_Comp2 a :ConvBlock .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1_Comp3 a :ConvBlock .

:Eigen_Predicting_Depth_Surface_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:EmbedBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper :hasFigure :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_0 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_1 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_10 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_11 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_12 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_13 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_14 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_2 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_3 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_4 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_5 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_6 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_7 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_8 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1_9 :partOf :Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 .

:Example a owl:Class ;
    rdfs:subClassOf :train .

:ExponentialMovingAverage a owl:Class ;
    rdfs:subClassOf :train .

:Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_deep_neural_networks,
        :Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_dnns,
        :Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_state,
        :Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_studies,
        :Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_that ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_0 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_1 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_10 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_2 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_3 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_4 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_5 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_6 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_8 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_9 :partOf :Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_Comp10 a :LSTMBlock .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_Comp2 a :LSTMBlock .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1_Comp8 a :LSTMBlock .

:Fan_A_Point_Set_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Fan_A_Point_Set_CVPR_2017_paper_data,
        :Fan_A_Point_Set_CVPR_2017_paper_deep_neural_networks ;
    :hasFigure :Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_0 :partOf :Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_1 :partOf :Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_2 :partOf :Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_3 :partOf :Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_4 :partOf :Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_5 :partOf :Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp0 a :ConvBlock .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1_Comp4 a :DropoutBlock .

:Fan_End-to-End_Learning_of_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Fan_End-to-End_Learning_of_CVPR_2018_paper_end-to-end,
        :Fan_End-to-End_Learning_of_CVPR_2018_paper_learned_representations ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_conditions,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_data,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_experiments,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_face,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_few-shot_learning,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_generation,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_grounding,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_image_classification,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_input,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_inputs,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_model,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_network,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_tasks,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_this,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_training,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_weights ;
    :hasFigure :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1,
        :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_0 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_1 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_17 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_2 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_3 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_4 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_5 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_6 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1_7 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_0 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_1 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_2 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_3 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_4 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_5 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_6 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1_Comp1 a :LSTMBlock .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_0 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_1 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_10 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_11 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_12 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_13 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_14 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_15 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_2 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_3 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_4 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_7 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_8 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_9 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1_Comp8 a :ConvBlock .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_0 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_1 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_10 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_2 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_3 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_4 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_5 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_6 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_7 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_8 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1_9 :partOf :Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 .

:Feature a owl:Class ;
    rdfs:subClassOf :train .

:FeatureList a owl:Class ;
    rdfs:subClassOf :train .

:FeatureLists a owl:Class ;
    rdfs:subClassOf :train .

:Features a owl:Class ;
    rdfs:subClassOf :train .

:FeedFnHook a owl:Class ;
    rdfs:subClassOf :train .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_annotations,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_classification,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_deep_neural_network,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_environment,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_image,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_interaction,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_map,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_methods,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_navigation,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_network,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_our_method,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_parameters,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_problem,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_results,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_rgb,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_robot,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_scale,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_segmentation,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_semantic_labels,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_structure,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_surfaces,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_tasks,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_that,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_this,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_vision,
        :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_we ;
    :hasFigure :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_0 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_1 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_11 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_14 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_15 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_16 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_17 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_18 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_19 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_2 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_20 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_21 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_3 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_4 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_5 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_6 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_7 :partOf :Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1_Comp4 a :InputBlock .

:Figurnov_Spatially_Adaptive_Computation_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:FinalOpsHook a owl:Class ;
    rdfs:subClassOf :train .

:FloatList a owl:Class ;
    rdfs:subClassOf :train .

:Fong_Interpretable_Explanations_of_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Fouhey_Single_Image_3D_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:FtrlOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper :hasFigure :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_0 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_1 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_10 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_11 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_12 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_13 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_2 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_3 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_4 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_5 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_6 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_7 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_8 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_9 :partOf :Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_Comp13 a :LSTMBlock .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1_Comp2 a :LSTMBlock .

:Gao_Compact_Bilinear_Pooling_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Gao_Learning_Generative_ConvNets_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:GenericTerm a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_0 :partOf :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_1 :partOf :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_2 :partOf :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_3 :partOf :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_4 :partOf :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_5 :partOf :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1_6 :partOf :Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 .

:GlobalStepWaiterHook a owl:Class ;
    rdfs:subClassOf :train .

:GradientDescentOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_3d_reconstruction,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_adversarial,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotation,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotations,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_data,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_images,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_measure,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_models,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_paradigms,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_pose,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_structure,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_supervision,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_task,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_that,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_these,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_this,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_training,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_transfer_learning,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_unified_framework,
        :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_we ;
    :hasFigure :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_0 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_1 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_10 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_11 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_12 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_13 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_14 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_15 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_16 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_17 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_18 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_19 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_2 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_21 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_3 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_4 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_5 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_6 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_7 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_8 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_9 :partOf :Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp10 a :ConvBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp12 a :ConvBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp14 a :LossBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp15 a :ActivationBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp17 a :ActivationBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp19 a :ActivationBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp4 a :ActivationBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp5 a :ConvBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp7 a :ConvBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp8 a :ActivationBlock .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1_Comp9 a :ConvBlock .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_approach,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_color,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_convolution,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_convolutions,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_deep_learning,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_image,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_image_inpainting,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_images,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_mechanism,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_methods,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_model,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_network,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_other,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_results,
        :Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_validate ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_0 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_1 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_10 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_11 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_2 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_3 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_4 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_5 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_7 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_8 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_9 :partOf :Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_Comp1 a :LSTMBlock .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_Comp11 a :LSTMBlock .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_Comp3 a :LSTMBlock .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1_Comp5 a :LSTMBlock .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_0 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_1 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_10 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_11 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_12 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_13 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_14 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_15 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_16 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_17 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_18 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_19 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_2 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_20 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_21 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_22 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_23 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_24 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_25 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_26 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_27 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_28 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_29 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_3 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_30 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_31 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_32 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_33 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_34 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_35 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_36 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_37 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_38 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_39 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_4 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_40 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_41 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_42 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_43 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_44 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_45 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_46 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_47 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_48 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_5 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_6 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_7 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_8 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_9 :partOf :Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1_Comp6 a :InputBlock .

:Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper_domain_adaptation,
        :Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper_end-to-end,
        :Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper_technique ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Haeusser_Learning_by_Association_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Haeusser_Learning_by_Association_CVPR_2017_paper_data,
        :Haeusser_Learning_by_Association_CVPR_2017_paper_machine_learning,
        :Haeusser_Learning_by_Association_CVPR_2017_paper_task ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Han_Automatic_Spatially-Aware_Fashion_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Han_Deep_Pyramidal_Residual_CVPR_2017_paper_convolutional_neural_networks ;
    :hasFigure :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1,
        :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1,
        :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_0 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_1 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_10 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_11 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_13 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_15 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_16 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_17 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_18 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_19 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_21 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_24 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_25 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_26 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_27 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_28 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_6 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_7 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_8 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp1 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp10 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp11 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp13 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp16 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp17 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp18 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp19 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp21 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp24 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp25 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp26 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp27 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp28 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1_Comp8 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_0 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_1 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_11 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_12 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_2 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_4 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_6 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_7 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_8 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_Comp0 a :InputBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_Comp1 a :InputBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_Comp12 a :OutputBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_Comp4 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1_Comp8 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_0 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_12 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_13 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_14 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_18 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_19 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_23 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_24 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_25 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_28 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_3 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_30 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_31 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_32 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_34 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_36 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_37 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_39 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_41 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_42 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_43 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_8 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_9 :partOf :Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp0 a :NormBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp12 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp14 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp19 a :NormBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp24 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp25 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp28 a :NormBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp3 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp36 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp37 a :ConvBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp39 a :OutputBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp41 a :NormBlock .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1_Comp43 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_0 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_1 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_12 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_13 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_14 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_16 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_17 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_2 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_22 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_24 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_25 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_32 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_35 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_37 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_4 :partOf :Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp12 a :ActivationBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp13 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp14 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp16 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp2 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp22 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp24 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp25 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp32 a :ConcatBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp35 a :ConvBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp37 a :ActivationBlock .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1_Comp4 a :ConvBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1,
        :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1,
        :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_0 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_1 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_2 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_3 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_4 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_5 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_6 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_7 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_Comp6 a :ActivationBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1_Comp7 a :ConvBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_0 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_1 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_2 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_3 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_4 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_5 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_6 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_7 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_Comp4 a :ConvBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_Comp5 a :ConvBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_Comp6 a :ConvBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1_Comp7 a :ActivationBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_0 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_10 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_11 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_13 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_2 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_3 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_4 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_6 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_7 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_8 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_9 :partOf :Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_Comp0 a :ConvBlock .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1_Comp9 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :He_Deep_Residual_Learning_CVPR_2016_paper_accuracy,
        :He_Deep_Residual_Learning_CVPR_2016_paper_cifar-10,
        :He_Deep_Residual_Learning_CVPR_2016_paper_classification_task,
        :He_Deep_Residual_Learning_CVPR_2016_paper_deep_representations,
        :He_Deep_Residual_Learning_CVPR_2016_paper_ensemble,
        :He_Deep_Residual_Learning_CVPR_2016_paper_framework,
        :He_Deep_Residual_Learning_CVPR_2016_paper_functions,
        :He_Deep_Residual_Learning_CVPR_2016_paper_imagenet,
        :He_Deep_Residual_Learning_CVPR_2016_paper_improvement,
        :He_Deep_Residual_Learning_CVPR_2016_paper_inputs,
        :He_Deep_Residual_Learning_CVPR_2016_paper_layers,
        :He_Deep_Residual_Learning_CVPR_2016_paper_networks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_neural_networks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_object_detection,
        :He_Deep_Residual_Learning_CVPR_2016_paper_recognition_tasks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_representations,
        :He_Deep_Residual_Learning_CVPR_2016_paper_segmentation,
        :He_Deep_Residual_Learning_CVPR_2016_paper_tasks,
        :He_Deep_Residual_Learning_CVPR_2016_paper_that,
        :He_Deep_Residual_Learning_CVPR_2016_paper_these,
        :He_Deep_Residual_Learning_CVPR_2016_paper_those,
        :He_Deep_Residual_Learning_CVPR_2016_paper_training,
        :He_Deep_Residual_Learning_CVPR_2016_paper_we ;
    :hasFigure :He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1,
        :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1,
        :He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1_0 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1_1 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1_Comp0 a :ActivationBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_0 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_2 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_22 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_23 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_24 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_25 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_26 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_27 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_29 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_3 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_30 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_31 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_32 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_35 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_38 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_39 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_40 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_42 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_43 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_44 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_46 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_47 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_5 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_57 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_6 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_63 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_7 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_70 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_73 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_74 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_75 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_77 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_81 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_82 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_83 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_84 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_85 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_86 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp0 a :OutputBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp2 a :OutputBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp22 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp24 a :OutputBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp26 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp29 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp31 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp32 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp35 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp38 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp39 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp40 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp5 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp57 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp6 a :OutputBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp70 a :PoolingBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp77 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp81 a :ConvBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp82 a :OutputBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp83 a :PoolingBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1_Comp85 a :PoolingBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_0 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_1 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_3 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_4 :partOf :He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1 .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_Comp3 a :ActivationBlock .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1_Comp4 a :ActivationBlock .

:He_Mask_R-CNN_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :He_Mask_R-CNN_ICCV_2017_paper_framework,
        :He_Mask_R-CNN_ICCV_2017_paper_segmentation ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_annotations,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_approach,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_data,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_deep_networks,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_encoder-decoder,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_encodes,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_experiments,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_human_pose,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_image,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_learn,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_methods,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_multi-view_images,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_one,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_other,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_poses,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_problem,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_representation,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_semi-supervised_methods,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_supervised_methods,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_supervision,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_techniques,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_that,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_they,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_this,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_training,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_viewpoint,
        :Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_approach,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_complex,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_convolutional_neural_networks,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_datasets,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_distributed,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_effectiveness,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_feature,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_flow,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_information,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_local_neighborhood,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_map,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_network,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_ones,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_other,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_parsing,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_pascal_voc,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_position,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_positions,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_prediction,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_regions,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_scene,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_scenes,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_this,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_understanding,
        :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_we ;
    :hasFigure :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_0 :partOf :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_1 :partOf :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_2 :partOf :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_3 :partOf :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_4 :partOf :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_5 :partOf :Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_Comp0 a :InputBlock .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_Comp1 a :UnpoolingBlock .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_Comp2 a :LossBlock .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_Comp3 a :LossBlock .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_Comp4 a :UnpoolingBlock .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1_Comp5 a :LossBlock .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_classification,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_ensemble,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_ensembles,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_family,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_feature_space,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_function,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_functions,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_image,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_information_retrieval_tasks,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_inputs,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_locations,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_map,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_method,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_metric,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_results,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_retrieval,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_state,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_that,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_these,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_this,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_training,
        :Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_algorithms,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_architecture,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_benchmarks,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_boundary_detection,
        <https://github.com/deepcurator/DCC/Hou_Deeply_Supervised_Salient_CVPR_2017_paper_convolutional_neural_networks_(cnns)>,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_development,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_edge,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_effectiveness,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_efficiency,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_fcn,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_fcns,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_feature,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_framework,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_fully_convolutional_neural_networks,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_image,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_improvement,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_maps,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_method,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_models,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_object_detection,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_per,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_problem,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_results,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_scale,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_segmentation,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_structure,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_structures,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_supervision,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_that,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_this,
        :Hou_Deeply_Supervised_Salient_CVPR_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Hu_Deep_360_Pilot_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Hu_Deep_360_Pilot_CVPR_2017_paper_dash,
        :Hu_Deep_360_Pilot_CVPR_2017_paper_recurrent_neural_network ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_accuracy,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_architecture,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_benchmarks,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_cnns,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_color_constancy,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_colors,
        <https://github.com/deepcurator/DCC/Hu_FC4_Fully_Convolutional_CVPR_2017_paper_convolutional_neural_networks_(cnns)>,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_datasets,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_efficiency,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_end-to-end,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_estimates,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_estimation,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_formulation,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_image,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_information,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_learn,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_network,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_patches,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_pooling,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_problem,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_quality,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_solution,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_state,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_supervision,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_that,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_they,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_this,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_training,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_we,
        :Hu_FC4_Fully_Convolutional_CVPR_2017_paper_weights ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Hu_Finding_Tiny_Faces_CVPR_2017_paper_errors,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_face,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_faces,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_models,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_objects,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_prior,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_resolution,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_results,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_scale,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_that,
        :Hu_Finding_Tiny_Faces_CVPR_2017_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Hu_Learning_Answer_Embeddings_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Hu_Learning_Answer_Embeddings_CVPR_2018_paper_probabilistic_model,
        :Hu_Learning_Answer_Embeddings_CVPR_2018_paper_question_answering ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Hu_Learning_to_Reason_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_1 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_10 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_11 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_12 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_13 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_2 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_3 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_4 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_6 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1_9 :partOf :Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 .

:Hu_Modeling_Relationships_in_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper :hasFigure :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1,
        :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_0 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_1 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_2 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_3 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_4 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_5 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_6 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_7 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_Comp2 a :PoolingBlock .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1_Comp6 a :LossBlock .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_0 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_1 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_2 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_3 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_4 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_6 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_7 :partOf :Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_Comp2 a :PoolingBlock .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1_Comp6 a :LossBlock .

:Hua_Pointwise_Convolutional_Neural_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Huang_Arbitrary_Style_Transfer_ICCV_2017_paper :hasFigure :Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1 .

:Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1_0 :partOf :Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1 .

:Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1_1 :partOf :Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1 .

:Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1_2 :partOf :Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1 .

:Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1_3 :partOf :Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1 .

:Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1_Comp2 a :RnnBlock .

:Huang_Beyond_Face_Rotation_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_0 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_1 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_10 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_11 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_12 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_13 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_14 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_15 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_16 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_17 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_2 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_4 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_5 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_6 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_7 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_8 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_9 :partOf :Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp12 a :ConvBlock .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp13 a :ConvBlock .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp14 a :ConvBlock .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp4 a :ConvBlock .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp5 a :ConvBlock .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp6 a :ConvBlock .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1_Comp9 a :ActivationBlock .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_convolutional_neural_network,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_map,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_maps,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_network,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_quality,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_regions,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_structures,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_that,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_these,
        :Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_computation,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_imagenet,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_improvements,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_models,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_state,
        :Huang_Densely_Connected_Convolutional_CVPR_2017_paper_them ;
    :hasFigure :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_0 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_1 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_2 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_3 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_4 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_5 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_6 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_7 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_8 :partOf :Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp1 a :DenseBlock .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp3 a :DenseBlock .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1_Comp5 a :DenseBlock .

:Huang_Learning_Deep_Representation_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Huang_Learning_Deep_Representation_CVPR_2016_paper_algorithm,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_approach,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_class_imbalance,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_classes,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_classification_methods,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_classification_tasks,
        <https://github.com/deepcurator/DCC/Huang_Learning_Deep_Representation_CVPR_2016_paper_convolutional_neural_network_(cnn)>,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_data,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_deep_learning,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_domain,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_effectiveness,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_experiments,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_framework,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_improvements,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_instances,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_k-nearest_neighbor,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_knn,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_methods,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_neighborhood,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_network,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_representation,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_representation_learning,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_sampling,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_schemes,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_strategies,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_that,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_these,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_this,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_training,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_validate,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_vision,
        :Huang_Learning_Deep_Representation_CVPR_2016_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2016 .

:Huang_Predicting_Gaze_in_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Huang_Predicting_Gaze_in_ECCV_2018_paper_activity,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_datasets,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_deep_neural_networks,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_dynamic_scenes,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_gaze,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_hybrid_model,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_learn,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_methods,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_model,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_prediction,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_recurrent_neural_network,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_state,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_task,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_tasks,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_that,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_videos,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_way,
        :Huang_Predicting_Gaze_in_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_0 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_1 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_10 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_12 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_13 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_15 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_16 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_17 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_2 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_3 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_4 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_5 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_6 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_7 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_8 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_9 :partOf :Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_Comp0 a :NormBlock .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_Comp12 a :LossBlock .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_Comp16 a :LossBlock .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_Comp6 a :OutputBlock .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1_Comp7 a :LossBlock .

:Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity <https://github.com/deepcurator/DCC/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_convolutional_neural_networks_(cnn)>,
        :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_face,
        :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_methods,
        :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_super-resolution ;
    :hasFigure :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_0 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_1 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_2 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_3 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_4 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_6 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_7 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_8 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1_9 :partOf :Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 .

:Hui_Fast_and_Accurate_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity <https://github.com/deepcurator/DCC/Hui_Fast_and_Accurate_CVPR_2018_paper_deep_convolutional_neural_networks_(cnns)> ;
    :hasFigure :Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_0 :partOf :Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_1 :partOf :Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_2 :partOf :Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_3 :partOf :Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_4 :partOf :Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_5 :partOf :Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Comp0 a :ConvBlock .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1_Comp1 a :ConvBlock .

:Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity <https://github.com/deepcurator/DCC/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper_convolutional_neural_network_(cnn)>,
        :Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper_state ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_approaches,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_color,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_conditional_generative_adversarial_networks,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_evaluation_results,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_former,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_generation,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_image,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_input,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_latter,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_model,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_networks,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_novel_approach,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_semantics,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_task,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_text,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_text_input,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_that,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_this,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_two,
        :Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_0 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_1 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_10 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_12 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_13 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_14 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_16 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_17 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_18 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_19 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_20 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_21 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_22 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_23 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_24 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_25 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_26 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_27 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_28 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_29 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_4 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_5 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_6 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_7 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_8 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_9 :partOf :Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp0 a :InputBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp10 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp12 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp19 a :NormBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp20 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp22 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp23 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp24 a :NormBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp25 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp27 a :DenseBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp28 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp6 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp7 a :ConvBlock .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1_Comp8 a :NormBlock .

:Int64List a owl:Class ;
    rdfs:subClassOf :train .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_algorithm,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_anatomy,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_benchmarks,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_data,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_image,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_images,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_imaging,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_input,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_learning_algorithm,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_learns,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_method,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_model,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_one,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_patches,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_problem,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_region,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_results,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_signal,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_state,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_supervised_learning,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_task,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_that,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_these,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_they,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_this,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_time,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_tool,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_tools,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_training,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_two,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_visual_information,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_ways,
        :Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Jas_Image_Specificity_2015_CVPR_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Jas_Image_Specificity_2015_CVPR_paper_application,
        :Jas_Image_Specificity_2015_CVPR_paper_descriptions,
        :Jas_Image_Specificity_2015_CVPR_paper_features,
        :Jas_Image_Specificity_2015_CVPR_paper_image,
        :Jas_Image_Specificity_2015_CVPR_paper_image_content,
        :Jas_Image_Specificity_2015_CVPR_paper_images,
        :Jas_Image_Specificity_2015_CVPR_paper_improvements,
        :Jas_Image_Specificity_2015_CVPR_paper_measure,
        :Jas_Image_Specificity_2015_CVPR_paper_mechanisms,
        :Jas_Image_Specificity_2015_CVPR_paper_modeling,
        :Jas_Image_Specificity_2015_CVPR_paper_models,
        :Jas_Image_Specificity_2015_CVPR_paper_ones,
        :Jas_Image_Specificity_2015_CVPR_paper_other,
        :Jas_Image_Specificity_2015_CVPR_paper_properties,
        :Jas_Image_Specificity_2015_CVPR_paper_retrieval,
        :Jas_Image_Specificity_2015_CVPR_paper_specificity,
        :Jas_Image_Specificity_2015_CVPR_paper_text,
        :Jas_Image_Specificity_2015_CVPR_paper_that,
        :Jas_Image_Specificity_2015_CVPR_paper_this,
        :Jas_Image_Specificity_2015_CVPR_paper_two,
        :Jas_Image_Specificity_2015_CVPR_paper_understanding,
        :Jas_Image_Specificity_2015_CVPR_paper_we,
        :Jas_Image_Specificity_2015_CVPR_paper_words ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_adversarial,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_adversarial_examples,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_algorithms,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_approximations,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_compression,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_data,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_decoder,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_deep_neural_networks,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_encoder,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_gaussian_blurring,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_image,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_images,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_information,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_input,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_learn,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_model,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_models,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_networks,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_neural_networks,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_noise,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_one,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_task,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_that,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_these,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_they,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_this,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_training,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_visual_quality,
        :Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:JobDef a owl:Class ;
    rdfs:subClassOf :train .

:Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Johnson_Image_Generation_From_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Joo_Total_Capture_A_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_imagenet,
        :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_mnist,
        :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_parameters,
        :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_pascal_voc ;
    :hasFigure :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_0 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_1 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_2 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_3 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_4 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_5 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_6 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_7 :partOf :Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_Comp3 a :NormBlock .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1_Comp7 a :NormBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_convolution,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_convolutional_neural_networks,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_deep_neural_networks,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_feature,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_features,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_hierarchical,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_image,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_image_information,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_image_super-resolution,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_kernels,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_methods,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_module,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_network,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_other,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_outputs,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_problems,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_quality,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_scale,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_scales,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_sizes,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_state,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_structure,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_studies,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_that,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_these,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_this,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_training,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_training_process,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_we ;
    :hasFigure :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1,
        :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_0 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_1 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_2 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_3 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_4 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_Comp0 a :ActivationBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_Comp1 a :DenseBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1_Comp3 a :ConcatBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_0 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_1 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_10 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_11 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_12 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_13 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_2 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_3 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_4 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_5 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_6 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_7 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_8 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_9 :partOf :Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp0 a :ConvBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp1 a :ConvBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp10 a :ActivationBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp11 a :ActivationBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp12 a :ConcatBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp13 a :ConvBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp2 a :ActivationBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp3 a :ActivationBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp6 a :ConcatBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp7 a :ConcatBlock .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1_Comp8 a :ConvBlock .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_adversarial_learning,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_alternative,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_approaches,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_classifiers,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_collection,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_concept,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_conditional_random_fields,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_crf,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_datasets,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_domains,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_evaluations,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_field,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_fields,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_gan,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_generalization,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_generative_adversarial_networks,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_learns,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_neural_network,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_pascal_voc,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_priors,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_problem,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_relations,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_segmentation,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_semantic,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_semantic_labels,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_semantic_relations,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_structure,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_structures,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_that,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_time,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_training,
        :Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_activations,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_approach,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_compression,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_datasets,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_deep_learning,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_deep_networks,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_distributed,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_experiments,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_magnitude,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_memory,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_models,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_network,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_networks,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_ones,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_operations,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_pooling,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_representation,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_representations,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_state,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_statistical_analysis,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_strategy,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_techniques,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_that,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_them,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_this,
        :Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_adversarial_learning,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_end-to-end,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_first,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_framework,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_human_pose,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_image,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_mesh,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_model,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_occlusions,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_orientations,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_recovery,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_results,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_rgb,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_shape,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_supervision,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_that,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_time,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_two,
        :Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_approach,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_cnn,
        <https://github.com/deepcurator/DCC/Kang_Convolutional_Neural_Networks_2014_CVPR_paper_convolutional_neural_network_(cnn)>,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_domain,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_estimation,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_experiments,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_feature,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_features,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_generalization,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_image,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_images,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_input,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_layers,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_methods,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_model,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_network,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_node,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_one,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_optimization,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_patches,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_pooling,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_quality,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_state,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_structure,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_that,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_this,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_two,
        :Kang_Convolutional_Neural_Networks_2014_CVPR_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2014 .

:Kendall_Geometric_Loss_Functions_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_convolutional_neural_network,
        :Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_image,
        :Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_input,
        :Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_pose,
        :Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_results,
        :Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_scenes,
        :Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_system ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Khoreva_Simple_Does_It_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Kim_Context_Embedding_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper_image_super-resolution,
        :Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper_method ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Klokov_Escape_From_Cells_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Klokov_Escape_From_Cells_ICCV_2017_paper_architecture,
        :Klokov_Escape_From_Cells_ICCV_2017_paper_deep_learning ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Koller_Deep_Hand_How_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_0 :partOf :Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_1 :partOf :Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_2 :partOf :Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_3 :partOf :Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_4 :partOf :Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1 .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp1 a :ConvBlock .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp2 a :ConvBlock .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1_Comp3 a :ConcatBlock .

:Kostrikov_Surface_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Kostrikov_Surface_Networks_CVPR_2018_paper_data ;
    :hasFigure :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_0 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_1 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_2 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_3 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_4 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_6 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_7 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_8 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1_9 :partOf :Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 .

:Krause_A_Hierarchical_Approach_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Kruse_Learning_to_Push_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_alignment,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_alignments,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_approach,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_datasets,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_image,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_interpretable,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_language,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_matching,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_methods,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_objects,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_other,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_problem,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_regions,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_results,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_retrieval,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_semantic,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_state,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_text,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_this,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_vision,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_we,
        :Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_words ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Kuen_Stochastic_Downsampling_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Kuen_Stochastic_Downsampling_for_CVPR_2018_paper_cnns,
        :Kuen_Stochastic_Downsampling_for_CVPR_2018_paper_convolutional_networks ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_adversarial,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_algorithms,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_approach,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_classes,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_domain,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_domain_adaptation,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_feature,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_features,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_matching,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_method,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_methods,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_ones,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_other,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_source_domain,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_target,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_target_domain,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_that,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_them,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_this,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_training,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_transferring_knowledge,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_two,
        :Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:LSTMSeqBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_accuracy,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_algorithm,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_applications,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_computational_complexity,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_convolutions,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_datasets,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_evaluations,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_feature,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_image,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_images,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_input,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_laplacian,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_loss_function,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_maps,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_method,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_methods,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_model,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_network,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_neural_networks,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_one,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_pre-processing_step,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_predictions,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_quality,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_resolution,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_resource,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_scale,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_state,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_super-resolution,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_superresolution,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_supervision,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_that,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_this,
        :Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_bias,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_cnn,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_convlstm,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_convolutional_neural_network,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_correlation,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_database,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_deep_learning,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_experimental_results,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_feature,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_features,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_hierarchical,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_input,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_long_short-term_memory,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_maps,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_method,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_motion,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_network,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_objectness,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_objects,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_prediction,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_scale,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_state,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_structured,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_that,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_this,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_tracking,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_video,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_videos,
        :Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Lambert_Deep_Learning_Under_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_deep_neural_network,
        :Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_pooling,
        :Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_that,
        :Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_this,
        :Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_topology,
        :Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Lavin_Fast_Algorithms_for_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Lavin_Fast_Algorithms_for_CVPR_2016_paper_convolutional_neural_networks ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_algorithm,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_applications,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_classification,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_classifiers,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_cnn,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_data,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_deep_convolutional_neural_networks,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_experiments,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_face,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_feature,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_generative_model,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_learns,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_model,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_modeling,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_properties,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_results,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_texture,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_training,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_unsupervised_learning,
        :Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_accuracy,
        :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_image_super-resolution ;
    :hasFigure :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_0 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_1 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_10 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_11 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_13 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_14 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_15 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_16 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_17 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_18 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_19 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_2 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_3 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_4 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_5 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_6 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_7 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_8 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_9 :partOf :Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp0 a :InputBlock .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp1 a :DeconvBlock .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp4 a :ConvBlock .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1_Comp7 a :DenseBlock .

:Lee_A_Memory_Network_CVPR_2018_paper :hasFigure :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_0 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_1 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_10 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_2 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_3 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_4 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_5 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_6 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_7 :partOf :Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1_Comp4 a :PoolingBlock .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_image_classification,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_models,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_noise,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_problem,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_this,
        :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_we ;
    :hasFigure :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_0 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_1 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_2 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_3 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_4 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_5 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_6 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_7 :partOf :Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp0 a :LossBlock .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp2 a :LossBlock .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp4 a :DenseBlock .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1_Comp5 a :ConvBlock .

:Lee_Hierarchical_Novelty_Detection_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Lee_RoomNet_End-To-End_Room_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper :hasFigure :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_0 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_1 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_10 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_11 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_13 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_2 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_3 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_4 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_5 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_6 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_7 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_8 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1_9 :partOf :Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_3d_reconstructions,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_component,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_deep_learning,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_distances,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edge,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edges,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_features,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_first,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_learn,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_loss_function,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_minimize,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_network,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_neural_network,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_our_method,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_patches,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_point_clouds,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_results,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_state,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_surface,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_technique,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_this,
        :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_we ;
    :hasFigure :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_0 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_1 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_10 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_11 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_12 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_2 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_3 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_4 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_5 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_6 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_7 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_8 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_9 :partOf :Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp3 a :InputBlock .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp7 a :OutputBlock .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1_Comp8 a :LossBlock .

:Li_CSRNet_Dilated_Convolutional_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Li_Convolutional_Sequence_to_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Li_Learning_Intrinsic_Image_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Li_MegaDepth_Learning_Single-View_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_0 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_1 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_10 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_11 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_12 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_13 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_14 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_15 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_16 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_17 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_2 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_3 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_4 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_5 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_6 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_7 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_8 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_9 :partOf :Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp11 a :ConvBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp12 a :ConvBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp13 a :ConvBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp14 a :ConvBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp15 a :ConvBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp17 a :LossBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp3 a :ConvBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp5 a :ConvBlock .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1_Comp8 a :ConvBlock .

:Li_Towards_Faster_Training_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Li_Towards_Faster_Training_CVPR_2018_paper_convolutional_neural_networks,
        :Li_Towards_Faster_Training_CVPR_2018_paper_covariance,
        :Li_Towards_Faster_Training_CVPR_2018_paper_improvement,
        :Li_Towards_Faster_Training_CVPR_2018_paper_pooling ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_contextual_information,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_convolution,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_datasets,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_decoder,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_deep_neural_networks,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_effectiveness,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_encoder-decoder,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_features,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_fields,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_former,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_implementation,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_latter,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_methods,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_model,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_models,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_module,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_modules,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_network,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_networks,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_object_boundaries,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_operations,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_pascal_voc,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_pooling,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_post-processing,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_research,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_scale,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_segmentation_results,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_semantic_segmentation,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_spatial_information,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_structure,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_task,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_tensorflow,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_this,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_tree,
        :Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_activities,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_activity,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_architecture,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_cifar-10,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_cifar-100,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_cnn,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_computer_vision_tasks,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_context_information,
        <https://github.com/deepcurator/DCC/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_convolutional_neural_network_(cnn)>,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_datasets,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_input,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_mnist,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_model,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_models,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_network,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_object_recognition,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_other,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_parameters,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_properties,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_recurrent_neural_networks,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_results,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_state,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_structure,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_svhn,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_system,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_that,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_these,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_this,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_time,
        :Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liao_Deep_Marching_Cubes_CVPR_2018_paper_algorithm,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_alternative,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_approach,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_convolutional_neural_network,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_encoder,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_end-to-end,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_experiments,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_first,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_formulation,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_learns,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_loss_functions,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_model,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_point_clouds,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_post-processing,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_predicting,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_prediction,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_problem,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_representations,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_shape,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_shapes,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_solutions,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_supervision,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_surface,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_task,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_techniques,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_that,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_they,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_this,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_topology,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_training,
        :Liao_Deep_Marching_Cubes_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Lin_Conditional_Image-to-Image_Translation_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Lin_Inverse_Compositional_Spatial_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Lin_Inverse_Compositional_Spatial_CVPR_2017_paper_this,
        :Lin_Inverse_Compositional_Spatial_CVPR_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_adversarial,
        :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_gan,
        :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_image,
        :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_network,
        :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_problem,
        :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_that,
        :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_this,
        :Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_algorithms,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_bound,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_bounds,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_cross-domain,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_domain,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_domains,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_experiments,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_gan,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_gans,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_generalization,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_hyperparameters,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_map,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_methods,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_one,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_per,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_predicting,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_stopping_criterion,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_supervised_learning,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_training,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_utility,
        :Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_way ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Liu_Decoupled_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liu_Decoupled_Networks_CVPR_2018_paper_component,
        :Liu_Decoupled_Networks_CVPR_2018_paper_convolution,
        <https://github.com/deepcurator/DCC/Liu_Decoupled_Networks_CVPR_2018_paper_convolutional_neural_networks_(cnns)> ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Liu_Deep_Supervised_Hashing_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Liu_FOTS_Fast_Oriented_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_10 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_11 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_13 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_16 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_17 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_2 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_4 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1_6 :partOf :Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity <https://github.com/deepcurator/DCC/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper_deep_convolutional_neural_networks_(cnns)> ;
    :hasFigure :Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1_0 :partOf :Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1_1 :partOf :Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1_2 :partOf :Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1_3 :partOf :Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1_4 :partOf :Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1_5 :partOf :Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1_Comp0 a :RnnBlock .

:Liu_PiCANet_Learning_Pixel-Wise_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_3d_model,
        :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_architecture,
        :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_image,
        :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_input,
        :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_rgb,
        :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_segmentation,
        :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_texture,
        :Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_this ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Liu_Predicting_Salient_Face_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity <https://github.com/deepcurator/DCC/Liu_Predicting_Salient_Face_CVPR_2017_paper_convolutional_neural_network_(cnn)> ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_3d_model,
        :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_image,
        :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_images,
        :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_input,
        :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_problem,
        :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_representation,
        :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_representations,
        :Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_this ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liu_Recognizing_Human_Actions_CVPR_2018_paper_action,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_action_recognition,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_actions,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_approaches,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_cues,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_datasets,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_deep_convolutional_neural_networks,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_effectiveness,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_features,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_human_action,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_human_pose,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_human_poses,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_image,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_images,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_maps,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_method,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_methods,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_modeling,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_motions,
        <https://github.com/deepcurator/DCC/Liu_Recognizing_Human_Actions_CVPR_2018_paper_ntu_rgb+d>,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_our_method,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_pooling,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_pose,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_pose_estimation,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_poses,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_properties,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_rank,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_sampling,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_shape,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_state,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_that,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_these,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_they,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_this,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_video,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_videos,
        :Liu_Recognizing_Human_Actions_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Liu_Richer_Convolutional_Features_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liu_Richer_Convolutional_Features_CVPR_2017_paper_edge,
        :Liu_Richer_Convolutional_Features_CVPR_2017_paper_features,
        :Liu_Richer_Convolutional_Features_CVPR_2017_paper_this,
        :Liu_Richer_Convolutional_Features_CVPR_2017_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper_face_recognition ;
    :hasFigure :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_0 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_1 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_2 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_3 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_4 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_5 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_6 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_7 :partOf :Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp1 a :ConvBlock .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1_Comp3 a :LossBlock .

:Liu_Structure_Inference_Net_CVPR_2018_paper :hasFigure :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_0 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_1 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_11 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_12 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_13 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_2 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_3 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_4 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_5 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_6 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_7 :partOf :Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_Comp2 a :PoolingBlock .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1_Comp5 a :LossBlock .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_architecture,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_cnn,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_database,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_experiments,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_image,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_images,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_information,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_model,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_nearest_neighbor,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_outputs,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_problem,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_processing,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_representation,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_retrieval,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_scene,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_search,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_semantic_information,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_state,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_system,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_task,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_text,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_that,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_them,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_this,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_time,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_understanding,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_way,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_we,
        :Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_words ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:LoggingTensorHook a owl:Class ;
    rdfs:subClassOf :train .

:Long_Attention_Clusters_Purely_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Long_Fully_Convolutional_Networks_2015_CVPR_paper_appearance_information,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_application,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_architecture,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_classification,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_convolutional_networks,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_end-to-end,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_features,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_flow,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_fully_convolutional_networks,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_image,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_improvement,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_input,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_learned_representations,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_models,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_network,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_networks,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_one,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_pascal_voc,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_prediction,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_prior,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_segmentation,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_segmentations,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_semantic_information,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_semantic_segmentation,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_sift,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_state,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_task,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_tasks,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_that,
        :Long_Fully_Convolutional_Networks_2015_CVPR_paper_transfer ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:LooperThread a owl:Class ;
    rdfs:subClassOf :train .

:Lu_Neural_Baby_Talk_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1_0 :partOf :Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1 .

:Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1_1 :partOf :Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1 .

:Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1_2 :partOf :Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1 .

:Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1_3 :partOf :Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1 .

:Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1_Comp0 a :LSTMBlock .

:Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1_Comp3 a :LSTMBlock .

:Luan_Deep_Photo_Style_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Luan_Deep_Photo_Style_CVPR_2017_paper_algorithm,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_color,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_colors,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_image,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_input,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_scene,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_that,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_transfer,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_transfers,
        :Luan_Deep_Photo_Style_CVPR_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Luo_Discriminability_Objective_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_0 :partOf :Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1 .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_1 :partOf :Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1 .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_2 :partOf :Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1 .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_7 :partOf :Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1 .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1_8 :partOf :Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1 .

:Luo_Non-Local_Deep_Features_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Ma_Disentangled_Person_Image_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1,
        :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_0 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_1 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_2 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_3 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_4 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_5 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_6 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1_7 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_0 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_1 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_2 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_3 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_4 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_5 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_6 :partOf :Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_Comp3 a :RnnBlock .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_Comp4 a :LSTMBlock .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1_Comp5 a :LSTMBlock .

:Mallya_PackNet_Adding_Multiple_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Marcos_Rotation_Equivariant_Vector_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_approach,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_data,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_image,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_improvement,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_learn,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_learned_representation,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_learns,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_method,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_model,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_one,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_prediction,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_representation,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_state,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_tasks,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_that,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_two,
        :Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Martinez_On_Human_Motion_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_0 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_1 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_10 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_11 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_12 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_13 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_14 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_15 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_2 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_3 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_4 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_5 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_6 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_7 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_9 :partOf :Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp0 a :ConvBlock .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp10 a :ConvBlock .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp13 a :NormBlock .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp14 a :LSTMBlock .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp2 a :LSTMBlock .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1_Comp9 a :LSTMBlock .

:Masana_Domain-Adaptive_Deep_Network_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_0 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_1 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_10 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_13 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_14 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_16 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_17 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_18 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_19 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_2 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_4 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_5 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_6 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1_9 :partOf :Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_approach,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_approaches,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_architectures,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_attention_mechanism,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_attention_mechanisms,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_criterion,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_datasets,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_feature,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_features,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_information,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_input,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_inputs,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_magnitudes,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_mechanisms,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_memory,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_operations,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_our_mechanism,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_processing,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_question_answering,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_relevance,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_semantic,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_signal,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_that,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_vision,
        :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_we ;
    :hasFigure :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1_0 :partOf :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1 .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1_1 :partOf :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1 .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1_2 :partOf :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1 .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1_3 :partOf :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1 .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1_4 :partOf :Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1 .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1_Comp1 a :LSTMBlock .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1_Comp3 a :ConvBlock .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_algorithm,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_benchmarks,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_clustering_method,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_convolutional_neural_networks,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_datasets,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_end-to-end,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_features,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_imagenet,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_large_datasets,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_learns,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_model,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_network,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_neural_network,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_parameters,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_scale,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_state,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_supervision,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_that,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_this,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_training,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_unsupervised_learning_methods,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_vision,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_we,
        :Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_weights ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Meinhardt_Learning_Proximal_Operators_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1,
        :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_0 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_1 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_10 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_11 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_12 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_13 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_14 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_15 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_16 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_18 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_19 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_2 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_3 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_5 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_6 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_7 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_8 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_9 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp0 a :OutputBlock .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp1 a :InputBlock .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp2 a :NormBlock .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1_Comp3 a :NormBlock .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_0 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_1 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_3 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_4 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_5 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_6 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_7 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_8 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1_9 :partOf :Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 .

:Metric a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_approach,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_cameras,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_collections,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_correspondences,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_datasets,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_first,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_information,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_matching,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_method,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_one,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_problem,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_problems,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_scene,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_segmentation,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_spaces,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_that,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_them,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_these,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_this,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_two,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_video,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_videos,
        :Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_activations,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_approach,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_approaches,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_architectures,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_baseline_methods,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_categories,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_convolutional_networks,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_end-to-end,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_field,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_learn,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_method,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_network,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_networks,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_representations,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_task,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_tasks,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_that,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_this,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_training_examples,
        :Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_action_classification,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_actions,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_activities,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_approach,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_architecture,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_classification,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_datasets,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_local_methods,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_long-term,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_methods,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_network,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_per,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_problems,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_processing,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_quality,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_retrieval,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_sampling,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_state,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_strategy,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_that,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_this,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_time,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_two,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_understanding,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_video,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_videos,
        :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_we ;
    :hasFigure :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1_0 :partOf :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1_1 :partOf :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1_2 :partOf :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1_3 :partOf :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1_4 :partOf :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1_5 :partOf :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1_6 :partOf :Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_approach,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_datasets,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_descriptions,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_diversity,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_effectiveness,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_encoder,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_formulation,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_generation,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_image,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_images,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_methods,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_modeling,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_popularity,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_state,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_structure,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_summarization,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_systems,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_task,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_techniques,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_they,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_this,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_those,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_two,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_video,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_we ;
    :hasFigure :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1,
        :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_0 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_1 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_2 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_3 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_4 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_5 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_6 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_7 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_8 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_9 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_Comp4 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_Comp5 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_Comp7 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1_Comp9 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_0 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_1 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_2 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_3 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_4 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_5 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_6 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_7 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_8 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_Comp0 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_Comp2 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_Comp3 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_Comp5 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_Comp6 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_Comp7 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1_Comp8 a :RnnBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1_0 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1_1 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1_2 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1_3 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1_Comp2 a :OutputBlock .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1_0 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1_1 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1_2 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1_3 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1_5 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1_6 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1_7 :partOf :Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 .

:MomentumOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:MonitoredSession a owl:Class ;
    rdfs:subClassOf :train .

:Mopuri_NAG_Network_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Morerio_Curriculum_Dropout_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_accuracy,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_aerial_images,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_applications,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_approach,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_approaches,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_architectures,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_automatic,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_classifier,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_cross-entropy,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_data,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_deep_learning,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_features,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_methods,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_model,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_parameters,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_prediction,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_predictions,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_problem,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_quality,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_state,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_structures,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_that,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_this,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_vision,
        :Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Mousavian_3D_Bounding_Box_CVPR_2017_paper_image,
        :Mousavian_3D_Bounding_Box_CVPR_2017_paper_method,
        :Mousavian_3D_Bounding_Box_CVPR_2017_paper_object_detection,
        :Mousavian_3D_Bounding_Box_CVPR_2017_paper_pose_estimation ;
    :hasFigure :Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_0 :partOf :Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_10 :partOf :Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_3 :partOf :Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_7 :partOf :Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_8 :partOf :Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_9 :partOf :Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp0 a :ConvBlock .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1_Comp8 a :NormBlock .

:Movshovitz-Attias_No_Fuss_Distance_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Movshovitz-Attias_No_Fuss_Distance_ICCV_2017_paper_distance,
        :Movshovitz-Attias_No_Fuss_Distance_ICCV_2017_paper_metric,
        :Movshovitz-Attias_No_Fuss_Distance_ICCV_2017_paper_problem ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_accuracy,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_architecture,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_instances,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_map,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_method,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_methods,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_model,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_network,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_our_pose,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_pose,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_pose_estimation,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_poses,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_segmentation,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_system,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_task,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_that,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_this,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_time,
        :Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:NanLossDuringTrainingError a owl:Class ;
    rdfs:subClassOf :train .

:NanTensorHook a owl:Class ;
    rdfs:subClassOf :train .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper :hasFigure :Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1 .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_0 :partOf :Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1 .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_1 :partOf :Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1 .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_2 :partOf :Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1 .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_3 :partOf :Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1 .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_5 :partOf :Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1 .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_Comp0 a :InputBlock .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_Comp1 a :PoolingBlock .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_Comp2 a :PoolingBlock .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1_Comp3 a :PoolingBlock .

:Nguyen_Improved_Fusion_of_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Nguyen_Improved_Fusion_of_CVPR_2018_paper_question_answering,
        :Nguyen_Improved_Fusion_of_CVPR_2018_paper_solution ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1,
        :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_0 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_1 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_2 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_3 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_5 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_8 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_9 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_Comp0 a :DeconvBlock .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1_Comp1 a :DeconvBlock .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1_0 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1_1 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1_3 :partOf :Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1 .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1_Comp0 a :ConvBlock .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1_Comp1 a :PoolingBlock .

:Niklaus_Video_Frame_Interpolation_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Niklaus_Video_Frame_Interpolation_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Noroozi_Representation_Learning_by_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Oh_Fast_Video_Object_CVPR_2018_paper :hasFigure :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_0 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_1 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_10 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_11 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_12 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_13 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_14 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_16 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_18 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_19 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_2 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_20 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_21 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_22 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_23 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_24 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_25 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_26 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_27 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_28 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_29 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_3 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_30 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_31 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_4 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_5 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_6 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_7 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_8 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_9 :partOf :Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp11 a :ActivationBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp12 a :ConcatBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp14 a :ConvBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp16 a :ConvBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp18 a :ConvBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp2 a :ConvBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp23 a :ConvBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp24 a :ConvBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp26 a :UnpoolingBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp29 a :ActivationBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp30 a :ActivationBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp7 a :ActivationBlock .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1_Comp9 a :ActivationBlock .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_agents,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_approach,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_classifiers,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_complex,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_data,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_model,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_prediction,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_principles,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_semantics,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_stack,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_stacks,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_state,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_structures,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_task,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_tasks,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_this,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_training,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_understanding,
        :Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Optimizer a owl:Class ;
    rdfs:subClassOf :train .

:Osokin_GANs_for_Biological_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Osokin_GANs_for_Biological_ICCV_2017_paper_application,
        :Osokin_GANs_for_Biological_ICCV_2017_paper_gan,
        :Osokin_GANs_for_Biological_ICCV_2017_paper_generative_adversarial_networks,
        :Osokin_GANs_for_Biological_ICCV_2017_paper_this,
        :Osokin_GANs_for_Biological_ICCV_2017_paper_we ;
    :hasFigure :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_0 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_1 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_10 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_12 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_14 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_15 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_2 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_20 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_21 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_23 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_3 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_7 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_8 :partOf :Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_Comp10 a :ConvBlock .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_Comp14 a :NormBlock .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_Comp21 a :ConvBlock .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1_Comp7 a :ActivationBlock .

:OtherScientificTerm a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_algorithm,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_errors,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_image,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_images,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_learn,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_learning_algorithms,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_nearest_neighbor,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_results,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_signal,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_structure,
        :Pan_Learning_Dual_Convolutional_CVPR_2018_paper_structures ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Papandreou_Towards_Accurate_Multi-Person_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Patrini_Making_Deep_Neural_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_adversarial,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_architecture,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_classification,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_classifier,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_classifiers,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_effectiveness,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_entropy,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_examples,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_implementation,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_inputs,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_learn,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_learns,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_network,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_networks,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_ones,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_outputs,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_prediction,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_predictions,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_problem,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_results,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_supervision,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_that,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_these,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_they,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_this,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_those,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_two,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_way,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_we,
        :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_weights ;
    :hasFigure :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_0 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_1 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_2 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_3 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_4 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_5 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_7 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_9 :partOf :Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1_Comp9 a :ActivationBlock .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Peng_Jointly_Optimize_Data_CVPR_2018_paper_adversarial,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_approach,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_data,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_data_augmentation,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_deep_neural_network_models,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_effectiveness,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_human_pose,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_latter,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_learns,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_models,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_network,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_operations,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_our_method,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_overfitting,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_penalty,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_problem,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_reward,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_state,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_strategy,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_target,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_technique,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_that,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_this,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_training,
        :Peng_Jointly_Optimize_Data_CVPR_2018_paper_two ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_activity,
        :Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_concept,
        :Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_events,
        :Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_this,
        :Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_videos,
        :Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Pohlen_Full-Resolution_Residual_Networks_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Poier_Learning_Pose_Specific_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Poier_Learning_Pose_Specific_CVPR_2018_paper_accuracy,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_counterpart,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_data,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_experiments,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_input,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_learn,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_learned_representations,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_magnitude,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_method,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_model,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_objects,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_one,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_parameters,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_pose,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_pose_estimation,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_pose_information,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_poses,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_representation,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_representations,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_shape,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_supervision,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_that,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_this,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_time,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_training,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_training_process,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_unlabeled_data,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_viewpoint,
        :Poier_Learning_Pose_Specific_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1,
        :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_0 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_1 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_2 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_3 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_4 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_5 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_6 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_7 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1_Comp5 a :LossBlock .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_2 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_4 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_5 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_6 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_7 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1_9 :partOf :Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 .

:ProfilerHook a owl:Class ;
    rdfs:subClassOf :train .

:ProximalAdagradOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:ProximalGradientDescentOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Qi_Frustum_PointNets_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Qi_Frustum_PointNets_for_CVPR_2018_paper_object_detection,
        :Qi_Frustum_PointNets_for_CVPR_2018_paper_rgb,
        :Qi_Frustum_PointNets_for_CVPR_2018_paper_this,
        :Qi_Frustum_PointNets_for_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_accuracy,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_cnns,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_constraints,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_estimation,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_experiments,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_image,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_learn,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_map,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_maps,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_methods,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_model,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_module,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_network,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_networks,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_neural_network,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_quality,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_solution,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_state,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_surface,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_that,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_this,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_two,
        :Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_0 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_1 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_10 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_11 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_2 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_3 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_5 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_6 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_7 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_8 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_9 :partOf :Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1_Comp2 a :ConvBlock .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_0 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_1 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_12 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_14 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_16 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_18 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_19 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_2 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_20 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_21 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_22 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_3 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_4 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_5 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_6 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_7 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_8 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_9 :partOf :Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1_Comp7 a :OutputBlock .

:Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper_approach,
        :Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper_image,
        :Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper_semantic ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1,
        :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_0 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_1 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_10 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_11 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_12 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_13 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_14 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_15 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_16 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_17 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_18 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_19 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_2 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_20 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_21 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_22 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_23 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_24 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_25 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_29 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_3 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_6 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_7 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_9 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_Comp10 a :ConvBlock .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_Comp15 a :ConvBlock .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_Comp16 a :ConvBlock .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_Comp17 a :ConvBlock .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1_Comp6 a :ConvBlock .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_0 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_1 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_10 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_11 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_2 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_3 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_4 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_5 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_6 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_7 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_8 :partOf :Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_Comp2 a :ConvBlock .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_Comp4 a :ConvBlock .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1_Comp5 a :ConvBlock .

:Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper_this ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:QueueRunner a owl:Class ;
    rdfs:subClassOf :train .

:RMSPropOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:RNNBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:RNNSeqBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Ramanishka_Top-Down_Visual_Saliency_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_approach,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_approaches,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_benchmarks,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_classes,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_classification_tasks,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_classifiers,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_concepts,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_convolutional_neural_networks,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_deep_networks,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_domain_knowledge,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_image,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_improvements,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_inputs,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_learn,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_learns,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_map,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_mappings,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_names,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_natural_language,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_network,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_objects,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_parameters,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_predictions,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_semantic,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_shapes,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_textures,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_that,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_these,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_this,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_transfer,
        :Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Ranjan_Optical_Flow_Estimation_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Recasens_Following_Gaze_in_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Recasens_Following_Gaze_in_ICCV_2017_paper_actions,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_approach,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_gaze,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_knowledge,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_models,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_objects,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_predicting,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_problem,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_semantic,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_signal,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_system,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_targets,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_that,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_this,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_times,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_understanding,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_video,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_videos,
        :Recasens_Following_Gaze_in_ICCV_2017_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Ren_Cross-Domain_Self-Supervised_Multi-Task_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_applications,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_architecture,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_attention_mechanism,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_convolutional_neural_networks,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_end-to-end,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_graphical_models,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_image,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_instances,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_model,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_network,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_prediction,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_problem,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_problems,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_question_answering,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_recurrent_neural_network,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_region,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_regions,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_rnn,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_scene,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_segmentation,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_segmentations,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_semantic_segmentation,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_structured,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_that,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_this,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_vision,
        :Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper_policy,
        :Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper_that ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Richard_Action_Sets_Weakly_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Rocco_End-to-End_Weakly-Supervised_Semantic_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Ronghang_Hu_Explainable_Neural_Computation_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Runia_Real-World_Repetition_Estimation_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Runia_Real-World_Repetition_Estimation_CVPR_2018_paper_problem,
        :Runia_Real-World_Repetition_Estimation_CVPR_2018_paper_video ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_approach,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_audio,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_events,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_first,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_framework,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_learn,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_map,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_modeling,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_object_models,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_objects,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_per,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_results,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_scale,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_scene,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_scenes,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_source_separation,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_state,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_that,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_those,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_video,
        :Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_videos ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_applications,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_architecture,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_baseline,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_concept,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_data,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_datasets,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_deep_models,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_deep_networks,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_end-to-end,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_framework,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_generative_adversarial_networks,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_image,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_images,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_learns,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_method,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_methods,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_mnist,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_model,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_network,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_observations,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_one-class_classification,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_one-class_classifiers,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_other,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_outlier_detection,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_outliers,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_problems,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_results,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_state,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_target,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_task,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_that,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_this,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_training,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_two,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_video,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_videos,
        :Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_accuracy,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_cnn,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_computation,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_constraints,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_convolutional_neural_network,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_datasets,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_device,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_devices,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_edge,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_efficiency,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_image,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_images,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_measure,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_memory,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_metrics,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_module,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_network,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_networks,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_pascal_voc,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_per,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_resolution,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_resource,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_semantic_segmentation,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_state,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_that,
        :Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_times ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Sage_Logo_Synthesis_and_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Sage_Logo_Synthesis_and_CVPR_2018_paper_logo ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_image,
        :Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_image_super-resolution,
        :Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_resolution,
        :Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_task ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_accuracy,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_approaches,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_architecture,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_complex,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_dialog,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_effectiveness,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_first,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_granularity,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_grounding,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_image,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_images,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_interpretable,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_memory,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_mnist,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_model,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_models,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_module,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_modules,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_network,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_one,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_other,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_phrases,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_problem,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_pronouns,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_question_answering,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_resolution,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_that,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_this,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_two,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_we,
        :Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_words ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Saver a owl:Class ;
    rdfs:subClassOf :train .

:SaverDef a owl:Class ;
    rdfs:subClassOf :train .

:Scaffold a owl:Class ;
    rdfs:subClassOf :train .

:SecondOrStepTimer a owl:Class ;
    rdfs:subClassOf :train .

:Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper_technique ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:SequenceExample a owl:Class ;
    rdfs:subClassOf :train .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_adaptation,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_approaches,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_automatic,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_commercial_systems,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_conditions,
        <https://github.com/deepcurator/DCC/Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_convolutional_neural_network_(cnn)>,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_datasets,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_experimental_results,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_image,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_images,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_manual_annotations,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_method,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_methods,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_ones,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_region,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_regions,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_state,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_system,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_that,
        :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_we ;
    :hasFigure :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_0 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_1 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_10 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_11 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_13 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_14 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_15 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_16 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_17 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_2 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_3 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_4 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_6 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_8 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_9 :partOf :Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp0 a :PoolingBlock .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp1 a :ConvBlock .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp10 a :ConvBlock .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp15 a :ConvBlock .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp17 a :ConvBlock .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp3 a :ConvBlock .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1_Comp6 a :LossBlock .

:Server a owl:Class ;
    rdfs:subClassOf :train .

:ServerDef a owl:Class ;
    rdfs:subClassOf :train .

:SessionCreator a owl:Class ;
    rdfs:subClassOf :train .

:SessionManager a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunArgs a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunContext a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunHook a owl:Class ;
    rdfs:subClassOf :train .

:SessionRunValues a owl:Class ;
    rdfs:subClassOf :train .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_approach,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_approaches,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_color,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_dynamic_scenes,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_first,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_flow,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_flows,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_framework,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_image,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_images,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_imaging,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_input,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_inputs,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_methods,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_motions,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_network,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_occlusion,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_optimization,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_problem,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_results,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_scale,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_state,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_that,
        :Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_flow,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_framework,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_generation,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_image,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_images,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_information,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_learns,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_mechanism,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_model,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_module,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_modules,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_object_models,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_pose,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_prediction,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_predictions,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_priors,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_results,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_scenes,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_state,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_supervision,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_target,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_task,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_that,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_this,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_two,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_viewpoints,
        :Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Shen_DSOD_Learning_Deeply_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Shen_Weakly_Supervised_Dense_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_0 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_1 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_10 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_11 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_12 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_13 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_14 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_15 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_16 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_17 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_18 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_19 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_2 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_20 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_21 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_22 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_23 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_24 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_25 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_26 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_27 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_28 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_29 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_3 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_30 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_31 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_32 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_33 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_34 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_5 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_6 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_7 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_8 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1_9 :partOf :Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 .

:Shi_Real-Time_Single_Image_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2016 .

:Shihao_Wu_Specular-to-Diffuse_Translation_for_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Shin_Pixels_Voxels_and_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Shin_Pixels_Voxels_and_CVPR_2018_paper_algorithm,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_classes,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_image,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_images,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_input,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_maps,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_methods,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_models,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_network,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_object_category,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_object_shape,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_objects,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_outputs,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_predicting,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_prediction,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_representations,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_resolution,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_rgb,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_shape,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_shape_representation,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_shape_representations,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_surface,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_surfaces,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_that,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_this,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_viewpoints,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_way,
        :Shin_Pixels_Voxels_and_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Shmelkov_Incremental_Learning_of_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Shrivastava_Learning_From_Simulated_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_approach,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_framework,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_image,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_input,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_learn,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_representation,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_scene,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_signal,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_structured,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_supervision,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_task,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_texture,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_that,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_this,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_two,
        :Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_actions,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_applications,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_data,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_domain,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_effectiveness,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_first,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_human_action_recognition,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_human_actions,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_interactions,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_knowledge,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_learn,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_learns,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_modalities,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_model,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_models,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_one,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_representation,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_research,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_scale,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_supervision,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_that,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_these,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_they,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_this,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_transfer,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_transferring_knowledge,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_two,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_understanding,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_videos,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_vision,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_we,
        :Sigurdsson_Actor_and_Observer_CVPR_2018_paper_web ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Singh_Hide-And-Seek_Forcing_a_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:SingularMonitoredSession a owl:Class ;
    rdfs:subClassOf :train .

:Snape_Face_Flow_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Snape_Face_Flow_ICCV_2015_paper_algorithm,
        :Snape_Face_Flow_ICCV_2015_paper_compositional,
        :Snape_Face_Flow_ICCV_2015_paper_computation,
        :Snape_Face_Flow_ICCV_2015_paper_correlation,
        :Snape_Face_Flow_ICCV_2015_paper_correspondences,
        :Snape_Face_Flow_ICCV_2015_paper_data,
        :Snape_Face_Flow_ICCV_2015_paper_experiments,
        :Snape_Face_Flow_ICCV_2015_paper_face,
        :Snape_Face_Flow_ICCV_2015_paper_flow,
        :Snape_Face_Flow_ICCV_2015_paper_formulation,
        :Snape_Face_Flow_ICCV_2015_paper_human_expressions,
        :Snape_Face_Flow_ICCV_2015_paper_images,
        :Snape_Face_Flow_ICCV_2015_paper_input,
        :Snape_Face_Flow_ICCV_2015_paper_magnitude,
        :Snape_Face_Flow_ICCV_2015_paper_matrix,
        :Snape_Face_Flow_ICCV_2015_paper_method,
        :Snape_Face_Flow_ICCV_2015_paper_model-based,
        :Snape_Face_Flow_ICCV_2015_paper_motion,
        :Snape_Face_Flow_ICCV_2015_paper_occlusions,
        :Snape_Face_Flow_ICCV_2015_paper_prior,
        :Snape_Face_Flow_ICCV_2015_paper_problem,
        :Snape_Face_Flow_ICCV_2015_paper_rank,
        :Snape_Face_Flow_ICCV_2015_paper_registration,
        :Snape_Face_Flow_ICCV_2015_paper_results,
        :Snape_Face_Flow_ICCV_2015_paper_state,
        :Snape_Face_Flow_ICCV_2015_paper_strategy,
        :Snape_Face_Flow_ICCV_2015_paper_techniques,
        :Snape_Face_Flow_ICCV_2015_paper_that,
        :Snape_Face_Flow_ICCV_2015_paper_this,
        :Snape_Face_Flow_ICCV_2015_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Song_Deep_Metric_Learning_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_adversarial_learning,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_algorithm,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_appearance_variations,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_approaches,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_class_imbalance,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_classification,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_datasets,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_experiments,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_features,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_first,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_framework,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_input,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_network,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_networks,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_objects,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_problems,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_target,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_that,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_these,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_they,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_tracker,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_trackers,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_tracking,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_training,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_two,
        :Song_VITAL_VIsual_Tracking_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:SourceCodeFile a owl:Class .

:Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Srinivasan_Light_Field_Blind_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:StepCounterHook a owl:Class ;
    rdfs:subClassOf :train .

:Stewart_End-To-End_People_Detection_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:StopAtStepHook a owl:Class ;
    rdfs:subClassOf :train .

:Su_Multi-View_Convolutional_Neural_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Su_Reasoning_About_Fine-Grained_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Su_Reasoning_About_Fine-Grained_ICCV_2017_paper_framework,
        :Su_Reasoning_About_Fine-Grained_ICCV_2017_paper_instances,
        :Su_Reasoning_About_Fine-Grained_ICCV_2017_paper_phrases ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:SummarySaverHook a owl:Class ;
    rdfs:subClassOf :train .

:Sun_PWC-Net_CNNs_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Sun_Pix3D_Dataset_and_CVPR_2018_paper_3d_models,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_alignment,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_annotation,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_annotations,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_datasets,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_image,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_images,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_objects,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_pose,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_scale,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_shape,
        :Sun_Pix3D_Dataset_and_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Sung_Learning_to_Compare_CVPR_2018_paper :hasFigure :Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_0 :partOf :Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_1 :partOf :Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_2 :partOf :Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_3 :partOf :Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_4 :partOf :Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_5 :partOf :Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_Comp0 a :LossBlock .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_Comp1 a :ActivationBlock .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_Comp2 a :ConcatBlock .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1_Comp5 a :ActivationBlock .

:Supervisor a owl:Class ;
    rdfs:subClassOf :train .

:SyncReplicasOptimizer a owl:Class ;
    rdfs:subClassOf :train .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1,
        :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1,
        :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1,
        :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1,
        :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1,
        :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1,
        :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_0 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_1 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_10 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_11 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_3 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_4 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_5 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_7 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_8 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_9 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp3 a :ConcatBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp7 a :PoolingBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp8 a :ConvBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1_Comp9 a :ConcatBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_0 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_3 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_5 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_6 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_7 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_8 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp5 a :ConcatBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1_Comp7 a :PoolingBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_0 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_1 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_3 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_4 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_6 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_8 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_9 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp6 a :ConcatBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1_Comp8 a :PoolingBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_0 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_11 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_12 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_4 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_5 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_6 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_7 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_9 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp11 a :PoolingBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1_Comp9 a :ConcatBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_0 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_1 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_10 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_11 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_2 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_3 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_4 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_5 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_6 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_7 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_8 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp0 a :ConcatBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1_Comp7 a :PoolingBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_0 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_1 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_2 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_3 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_4 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp2 a :DenseBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp3 a :ConvBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1_Comp4 a :PoolingBlock .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_0 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_1 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_2 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_3 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_4 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_5 :partOf :Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1_Comp3 a :PoolingBlock .

:Szeto_Click_Here_Human-Localized_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tabernik_Spatially-Adaptive_Filter_Units_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1,
        :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1,
        :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1,
        :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_0 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_1 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_10 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_11 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_12 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_13 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_14 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_15 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_16 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_17 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_2 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_3 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_4 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_5 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_6 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_7 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_8 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_9 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp1 a :InputBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp10 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp12 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp14 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp15 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp16 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp2 a :InputBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp3 a :InputBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp4 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp5 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp6 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp7 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1_Comp8 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_0 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_1 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_3 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_4 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1_Comp0 a :ActivationBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_0 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_1 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_10 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_11 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_2 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_3 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_4 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_5 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_6 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_8 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_9 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp0 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp11 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp2 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp3 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp4 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp5 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp6 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp8 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1_Comp9 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_0 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_1 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_10 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_11 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_2 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_3 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_4 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_5 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_6 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_7 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_8 :partOf :Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp1 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp10 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp11 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp3 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp5 a :ConvBlock .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1_Comp7 a :ConvBlock .

:Tai_MemNet_A_Persistent_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity <https://github.com/deepcurator/DCC/Tai_MemNet_A_Persistent_ICCV_2017_paper_deep_convolutional_neural_networks_(cnns)> ;
    :hasFigure :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1,
        :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_0 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_1 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_10 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_2 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_21 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_3 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_4 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_5 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_6 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_7 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_8 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_9 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1_Comp2 a :InputBlock .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_0 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_1 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_10 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_14 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_2 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_3 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_5 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_7 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_8 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_9 :partOf :Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp10 a :OutputBlock .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1_Comp8 a :OutputBlock .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tang_Multiple_Instance_Detection_CVPR_2017_paper_algorithm,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_benchmarks,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_classifier,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_classifiers,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_deep_learning,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_end-to-end,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_experiments,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_image,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_information,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_instances,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_latter,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_location,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_map,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_network,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_nodes,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_object_detection,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_object_detectors,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_object_recognition,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_pascal_voc,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_problem,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_procedure,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_results,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_state,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_supervised_detectors,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_supervision,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_that,
        :Tang_Multiple_Instance_Detection_CVPR_2017_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Tang_Multiple_People_Tracking_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_image_deblurring,
        :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_scheme ;
    :hasFigure :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_1 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_12 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_13 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_15 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_16 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_17 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_18 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_19 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_2 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_21 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_22 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_4 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_6 :partOf :Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp19 a :ConvBlock .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1_Comp4 a :OutputBlock .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_0 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_1 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_2 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_3 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_4 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_5 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_6 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_7 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_8 :partOf :Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp1 a :PoolingBlock .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1_Comp6 a :UnpoolingBlock .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_accuracy,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_algorithm,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_approach,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_approaches,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_cnn,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_datasets,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_image,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_locations,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_methods,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_object_pose_estimation,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_occlusion,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_other,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_our_method,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_pose,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_post-processing,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_that,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_these,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_they,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_two,
        :Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_vertices ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_0 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_1 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_10 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_11 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_14 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_15 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_2 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_3 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_5 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_6 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_7 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_8 :partOf :Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp10 a :ConvBlock .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp15 a :ConvBlock .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1_Comp6 a :LossBlock .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_accuracy,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_approach,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_correspondences,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_datasets,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_deep_neural_network,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_face,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_image,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_instances,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_learn,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_method,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_object_categories,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_problem,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_structure,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_that,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_these,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_this,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_viewpoint,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_vision,
        :Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_approach,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_correspondence,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_correspondences,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_deep_learning,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_distance,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_feature,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_input,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_matching,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_networks,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_our_method,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_predicting,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_representation,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_results,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_robustness,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_shape,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_shapes,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_surface,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_that,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_these,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_this,
        :Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_architecture,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_branches,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_cifar-10,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_classifiers,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_data,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_datasets,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_encoder-decoder,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_extraction,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_first,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_generalization_performances,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_image,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_information,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_input,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_learned_representations,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_model,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_representations,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_results,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_signal,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_state,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_studies,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_supervision,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_svhn,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_this,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_training,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_two,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_unlabeled_data,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_validate,
        :Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_conditions,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_factors,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_image,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_interpretable,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_latent_variables,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_layers,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_learn,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_model,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_models,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_properties,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_scenes,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_shapes,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_spheres,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_system,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_that,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_these,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_they,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_this,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_variables,
        :Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_accuracy,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_appearance_variations,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_attention_mechanism,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_detection_methods,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_experiments,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_feature,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_first,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_information,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_input,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_location,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_long-term,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_lstm,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_map,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_matching,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_memory,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_methods,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_model,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_network,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_neural_networks,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_other,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_outputs,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_parameters,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_popularity,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_search,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_signals,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_state,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_target,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_task,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_that,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_they,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_this,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_time,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_tracker,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_tracking,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_tracking_methods,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_ways,
        :Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:TitleText a owl:Class ;
    rdfs:subClassOf :Text .

:Toderici_Full_Resolution_Image_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1,
        :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_0 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_1 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_10 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_11 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_12 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_13 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_14 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_2 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_3 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_4 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_5 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_6 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_7 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_8 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_9 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp0 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp1 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp11 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp12 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp14 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp2 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp5 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp6 a :RnnBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp8 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1_Comp9 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_0 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_1 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_2 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_4 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_5 :partOf :Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1 .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp0 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp1 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp2 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp4 a :ConvBlock .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1_Comp5 a :ConvBlock .

:Tome_Lifting_From_the_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tome_Lifting_From_the_CVPR_2017_paper_errors ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tong_Image_Super-Resolution_Using_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_adaptation,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_algorithm,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_approach,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_confidence_measures,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_corpus,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_data,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_datasets,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_deep_learning,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_deep_neural_networks,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_effectiveness,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_end-to-end,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_environments,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_former,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_function,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_gather,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_image,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_images,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_information,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_kitti,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_latter,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_maps,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_model,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_network,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_networks,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_other,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_predictions,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_proposal,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_smoothness,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_state,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_stereo_algorithms,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_that,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_this,
        :Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tran_A_Closer_Look_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_0 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_1 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_10 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_11 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_12 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_13 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_15 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_16 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_17 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_18 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_19 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_2 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_20 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_21 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_23 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_24 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_25 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_26 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_27 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_28 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_29 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_3 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_31 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_32 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_33 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_34 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_35 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_36 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_37 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_39 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_4 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_5 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_6 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_8 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_9 :partOf :Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp0 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp1 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp10 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp11 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp12 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp13 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp16 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp17 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp19 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp20 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp21 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp24 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp26 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp27 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp28 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp29 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp3 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp33 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp34 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp35 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp36 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp37 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp4 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp5 a :ConvBlock .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1_Comp8 a :ConvBlock .

:Trigeorgis_Face_Normals_In-The-Wild_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Tsai_Learning_to_Adapt_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tsai_Learning_to_Adapt_CVPR_2018_paper_accuracy,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_adversarial,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_adversarial_learning,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_algorithms,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_approaches,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_domain_adaptation,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_domains,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_experiments,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_feature,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_image,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_method,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_methods,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_model,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_network,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_neural_network,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_outputs,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_semantic_segmentation,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_semantic_segmentations,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_similarities,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_structured,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_supervision,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_target_domain,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_target_domains,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_that,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_this,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_visual_quality,
        :Tsai_Learning_to_Adapt_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tu_Learning_Superpixels_With_CVPR_2018_paper_algorithm,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_algorithms,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_approach,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_computer_vision_tasks,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_deep_neural_networks,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_experimental_results,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_features,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_images,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_improvements,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_loss_function,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_methods,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_model,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_object_boundaries,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_prediction,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_segmentation,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_state,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_that,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_these,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_this,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_vision_applications,
        :Tu_Learning_Superpixels_With_CVPR_2018_paper_we ;
    :hasFigure :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_0 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_1 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_10 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_11 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_12 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_13 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_14 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_2 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_3 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_4 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_5 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_6 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_7 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_8 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_9 :partOf :Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_Comp11 a :LossBlock .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_Comp7 a :ConcatBlock .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1_Comp9 a :RnnBlock .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_applications,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_collection,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_complex,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_data,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_examples,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_framework,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_image,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_instances,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_interpretable,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_objects,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_our_method,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_parsing,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_predicting,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_prediction,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_representation,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_shape,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_shape_representations,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_shapes,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_similarity_measure,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_structure,
        :Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_that ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_0 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_1 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_2 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_3 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_4 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_5 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_6 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_7 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_8 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_9 :partOf :Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_Comp0 a :InputBlock .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1_Comp6 a :InputBlock .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2015 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_0 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_1 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_10 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_11 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_12 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_13 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_14 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_15 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_16 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_2 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_3 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_4 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_5 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_6 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_7 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_8 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_9 :partOf :Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_Comp10 a :LossBlock .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_Comp11 a :LossBlock .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_Comp14 a :LossBlock .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_Comp2 a :ConvBlock .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1_Comp3 a :ConvBlock .

:Ulyanov_Deep_Image_Prior_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Upchurch_Deep_Feature_Interpolation_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:UserDefined a owl:Class ;
    rdfs:subClassOf :CodeEntity .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_0 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_1 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_10 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_11 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_12 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_15 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_16 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_17 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_18 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_19 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_2 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_20 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_3 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_5 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_6 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_7 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_8 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_9 :partOf :Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp15 a :NormBlock .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp16 a :NormBlock .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp18 a :NormBlock .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp7 a :FlattenBlock .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1_Comp8 a :RnnSeqBlock .

:Veit_Conditional_Similarity_Networks_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_0 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_1 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_10 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_11 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_12 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_13 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_15 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_16 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_17 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_2 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_3 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_4 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_5 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_6 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_7 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_8 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_9 :partOf :Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp0 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp1 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp10 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp11 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp13 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp15 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp16 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp17 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp2 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp4 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp5 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp6 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp7 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp8 a :LSTMBlock .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1_Comp9 a :LSTMBlock .

:Villegas_Neural_Kinematic_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_end-to-end,
        :Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_input,
        :Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_method,
        :Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_motion,
        :Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_target,
        :Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_training ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Vinyals_Show_and_Tell_2015_CVPR_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Vinyals_Show_and_Tell_2015_CVPR_paper_accuracy,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_approach,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_architecture,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_datasets,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_generative_model,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_image,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_image_descriptions,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_improvements,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_language,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_learns,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_machine_translation,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_model,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_natural_language_processing,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_problem,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_state,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_target,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_that,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_this,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_training,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_vision,
        :Vinyals_Show_and_Tell_2015_CVPR_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_adversarial,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_evaluation_method,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_gradient,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_inputs,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_large_datasets,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_learn,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_machine_learning,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_mechanism,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_methods,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_model,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_models,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_our_method,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_policy,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_robustness,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_scale,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_systems,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_that,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_these,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_this,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_training,
        :Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1_0 :partOf :Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1 .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1_1 :partOf :Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1 .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1_2 :partOf :Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1 .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1_3 :partOf :Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1 .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1_Comp0 a :InputBlock .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1_Comp1 a :OutputBlock .

:VocabInfo a owl:Class ;
    rdfs:subClassOf :train .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_benchmarks,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_classifiers,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_data_augmentation,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_deep_learning,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_domain,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_domain_adaptation,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_feature,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_feature_space,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_features,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_framework,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_gan,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_gans,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_generative_adversarial_networks,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_image,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_learn,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_means,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_objective_function,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_ones,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_results,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_state,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_target,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_technique,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_that,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_this,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_training,
        :Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_approaches,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_entropy,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_feature,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_image,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_image_classification,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_instances,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_learn,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_learning_algorithm,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_locations,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_measure,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_metric,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_model,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_models,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_object_detection,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_object_detectors,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_objects,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_state,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_supervision,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_task,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_that,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_this,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_time,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_transfers,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_two,
        :Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_variance ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_adversarial_examples,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_benchmarks,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_classification,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_classification_performance,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_classification_tasks,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_cross-entropy,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_deep_neural_networks,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_effectiveness,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_experiments,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_feature,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_features,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_gaussian_mixture,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_imagenet,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_inputs,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_mnist,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_modeling,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_proposal,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_regularization,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_that,
        :Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_training ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Wang_Face_Aging_With_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_color,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_conditional_gans,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_conditional_generative_adversarial_networks,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_framework,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_images,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_map,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_maps,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_method,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_objects,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_scene,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_scenes,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_semantic,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_texture,
        :Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_trees ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Wang_Learning_a_Discriminative_CVPR_2018_paper_annotations,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_approach,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_approaches,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_architecture,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_classification,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_cnn,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_cnns,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_datasets,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_discriminative_patches,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_end-to-end,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_feature,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_features,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_framework,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_frameworks,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_information,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_method,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_network,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_representation_learning,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_results,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_state,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_statistics,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_structured,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_studies,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_supervision,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_that,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper_this ;
    :hasFigure :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1,
        :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_0 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_1 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_2 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_3 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_4 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_5 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_Comp0 a :InputBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_Comp3 a :ConvBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1_Comp5 a :LossBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_0 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_1 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_10 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_11 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_2 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_20 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_21 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_26 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_27 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_29 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_3 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_30 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_4 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_5 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_7 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_Comp1 a :ConvBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_Comp2 a :ConvBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_Comp3 a :ConvBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1_Comp5 a :LossBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_0 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_1 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_2 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_3 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_4 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_5 :partOf :Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_Comp0 a :PoolingBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_Comp2 a :LossBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_Comp3 a :OutputBlock .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1_Comp4 a :OutputBlock .

:Wang_Multi-Label_Image_Recognition_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Wang_Multimodal_Transfer_A_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper :hasFigure :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_1 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_10 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_11 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_12 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_13 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_14 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_15 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_16 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_2 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_3 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_4 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_5 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_7 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_8 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_9 :partOf :Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_Comp11 a :ConvBlock .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_Comp12 a :ConvBlock .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_Comp13 a :LossBlock .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_Comp14 a :ConvBlock .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_Comp16 a :LossBlock .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_Comp2 a :ConvBlock .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1_Comp9 a :ConvBlock .

:Wang_Repulsion_Loss_Detecting_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Wang_Revisiting_Video_Saliency_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Wang_Revisiting_Video_Saliency_CVPR_2018_paper_research,
        :Wang_Revisiting_Video_Saliency_CVPR_2018_paper_this,
        :Wang_Revisiting_Video_Saliency_CVPR_2018_paper_two,
        :Wang_Revisiting_Video_Saliency_CVPR_2018_paper_video,
        :Wang_Revisiting_Video_Saliency_CVPR_2018_paper_ways,
        :Wang_Revisiting_Video_Saliency_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Wang_SGPN_Similarity_Group_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Wang_Skeleton_Key_Image_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_0 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_1 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_10 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_11 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_12 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_13 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_14 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_15 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_16 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_17 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_2 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_3 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_4 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_5 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_6 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_7 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_8 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_9 :partOf :Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp0 a :ConcatBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp10 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp11 a :PoolingBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp12 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp13 a :ConcatBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp14 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp15 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp16 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp17 a :InputBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp2 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp5 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp6 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp7 a :ConvBlock .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1_Comp9 a :ConvBlock .

:Wei_Convolutional_Pose_Machines_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Wei_Dense_Human_Body_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Wei_Dense_Human_Body_CVPR_2016_paper_3d_models,
        :Wei_Dense_Human_Body_CVPR_2016_paper_approach,
        :Wei_Dense_Human_Body_CVPR_2016_paper_classification,
        :Wei_Dense_Human_Body_CVPR_2016_paper_complex,
        :Wei_Dense_Human_Body_CVPR_2016_paper_convolutional_neural_network,
        :Wei_Dense_Human_Body_CVPR_2016_paper_correspondence,
        :Wei_Dense_Human_Body_CVPR_2016_paper_correspondences,
        :Wei_Dense_Human_Body_CVPR_2016_paper_deep_learning,
        :Wei_Dense_Human_Body_CVPR_2016_paper_feature,
        :Wei_Dense_Human_Body_CVPR_2016_paper_feature_space,
        :Wei_Dense_Human_Body_CVPR_2016_paper_framework,
        :Wei_Dense_Human_Body_CVPR_2016_paper_information,
        :Wei_Dense_Human_Body_CVPR_2016_paper_map,
        :Wei_Dense_Human_Body_CVPR_2016_paper_maps,
        :Wei_Dense_Human_Body_CVPR_2016_paper_method,
        :Wei_Dense_Human_Body_CVPR_2016_paper_methods,
        :Wei_Dense_Human_Body_CVPR_2016_paper_network,
        :Wei_Dense_Human_Body_CVPR_2016_paper_our_method,
        :Wei_Dense_Human_Body_CVPR_2016_paper_poses,
        :Wei_Dense_Human_Body_CVPR_2016_paper_problem,
        :Wei_Dense_Human_Body_CVPR_2016_paper_region,
        :Wei_Dense_Human_Body_CVPR_2016_paper_shape,
        :Wei_Dense_Human_Body_CVPR_2016_paper_shapes,
        :Wei_Dense_Human_Body_CVPR_2016_paper_smoothness,
        :Wei_Dense_Human_Body_CVPR_2016_paper_state,
        :Wei_Dense_Human_Body_CVPR_2016_paper_surfaces,
        :Wei_Dense_Human_Body_CVPR_2016_paper_synthetic_data,
        :Wei_Dense_Human_Body_CVPR_2016_paper_target,
        :Wei_Dense_Human_Body_CVPR_2016_paper_that,
        :Wei_Dense_Human_Body_CVPR_2016_paper_those,
        :Wei_Dense_Human_Body_CVPR_2016_paper_time,
        :Wei_Dense_Human_Body_CVPR_2016_paper_training,
        :Wei_Dense_Human_Body_CVPR_2016_paper_two,
        :Wei_Dense_Human_Body_CVPR_2016_paper_unsupervised_methods,
        :Wei_Dense_Human_Body_CVPR_2016_paper_validate,
        :Wei_Dense_Human_Body_CVPR_2016_paper_viewpoints,
        :Wei_Dense_Human_Body_CVPR_2016_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_accuracy,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_applications,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_architecture,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_benchmarks,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_cnn,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_detection_accuracy,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_flow,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_latter,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_module,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_quality,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_results,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_solution,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_stacks,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_this,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_training,
        :Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_two ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_approach,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_benchmarks,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_cnn,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_cnns,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_computation,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_convolution,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_data,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_effectiveness,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_experiments,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_flexibility,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_images,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_information,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_memory,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_methods,
        <https://github.com/deepcurator/DCC/Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_neural_networks_(>,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_operations,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_operators,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_parameters,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_point_clouds,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_pooling,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_rgb,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_semantic_segmentation,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_spatial_information,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_structure,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_studies,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_these,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_two,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_validate,
        :Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_deblurring,
        :Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_feature,
        :Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_inputs,
        :Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_learning-based_approach,
        :Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_methods,
        :Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_state,
        :Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_time ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:WorkerSessionCreator a owl:Class ;
    rdfs:subClassOf :train .

:Wu_Compressed_Video_Action_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Wu_Exploit_the_Unknown_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Wu_Exploit_the_Unknown_CVPR_2018_paper_one,
        :Wu_Exploit_the_Unknown_CVPR_2018_paper_person_re-identification,
        :Wu_Exploit_the_Unknown_CVPR_2018_paper_video ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Wu_Quantized_Convolutional_Neural_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Wu_Self-Organized_Text_Detection_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Wu_Unsupervised_Feature_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Xia_Gibson_Env_Real-World_CVPR_2018_paper_agents,
        :Xia_Gibson_Env_Real-World_CVPR_2018_paper_board,
        :Xia_Gibson_Env_Real-World_CVPR_2018_paper_constraints,
        :Xia_Gibson_Env_Real-World_CVPR_2018_paper_environment,
        :Xia_Gibson_Env_Real-World_CVPR_2018_paper_modalities,
        :Xia_Gibson_Env_Real-World_CVPR_2018_paper_observations,
        :Xia_Gibson_Env_Real-World_CVPR_2018_paper_semantic_labels,
        :Xia_Gibson_Env_Real-World_CVPR_2018_paper_two ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_algorithms,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_approaches,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_architecture,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_contextual_information,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_convolutional_neural_network,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_evaluation_metrics,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_experiments,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_field,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_first,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_image,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_images,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_information,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_layers,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_method,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_network,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_neural_network,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_one,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_other,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_real-world_datasets,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_recurrent_neural_networks,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_shapes,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_state,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_task,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_vision,
        :Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Xian_Feature_Generating_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_accuracy,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_approach,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_architecture,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computation_efficiency,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computer_vision_tasks,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_convolutional_neural_network,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_efficiency,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_end-to-end,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_face,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_feature,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_gradient,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_interference,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_module,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_multi-task_learning_methods,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_optimum,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_other,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_parameters,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_problem,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_quality,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_relevance,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_retrieval,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_scales,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_task,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_tasks,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_that,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_they,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_this,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_those,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_time,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_training,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_two,
        :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_we ;
    :hasFigure :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_0 :partOf :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_2 :partOf :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_3 :partOf :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_4 :partOf :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_5 :partOf :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_6 :partOf :Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp2 a :ConvBlock .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1_Comp4 a :ConvBlock .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_effectiveness,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_experiments,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_first,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_human_pose,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_map,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_methods,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_pose_estimation,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_quantization,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_representation,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_that,
        :Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_time ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_classification,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_color,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_conditions,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_datasets,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_detection_method,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_fcn,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_focal_loss_function,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_gaussian_mixture_model,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_gmm,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_images,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_method,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_metrics,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_network,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_pose,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_precision,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_problem,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_recall,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_reflection,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_scene,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_stereo_images,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_surface,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_task,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_that,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_this,
        :Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1,
        :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1,
        :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_0 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_1 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_10 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_11 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_2 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_3 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_4 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_5 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_6 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_7 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_8 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_9 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1_Comp11 a :PoolingBlock .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_0 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_1 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_10 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_11 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_12 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_13 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_14 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_15 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_16 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_17 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_18 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_2 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_3 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_4 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_5 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_6 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_7 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_8 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_9 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1_Comp8 a :ConcatBlock .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_0 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_1 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_2 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_3 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_4 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_5 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_6 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1_7 :partOf :Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 .

:Xie_Genetic_CNN_ICCV_2017_paper :hasFigure :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1,
        :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_0 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_1 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_10 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_11 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_12 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_13 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_14 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_15 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_16 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_17 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_2 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_3 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_4 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_5 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_6 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_7 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_8 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_9 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp0 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp1 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp10 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp11 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp12 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp13 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp14 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp15 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp16 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp17 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp2 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp3 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp4 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp5 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp6 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp7 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp8 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1_Comp9 a :ConvBlock .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_0 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_10 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_11 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_2 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_3 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_4 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_5 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_6 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_7 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_8 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1_9 :partOf :Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 .

:Xie_Learning_Descriptor_Networks_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_adversarial,
        :Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_network,
        :Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_this,
        :Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Xu_Deep_Image_Matting_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_0 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_1 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_10 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_11 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_12 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_13 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_2 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_3 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_4 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_5 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_6 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_8 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_9 :partOf :Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp3 a :PoolingBlock .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1_Comp4 a :PoolingBlock .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_applications,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_cnns,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_color,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_convolution,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_convolutional_neural_network,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_convolutional_neural_networks,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_deep_learning,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_domain,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_extensions,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_features,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_fields,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_image_classification,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_images,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_matrices,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_model,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_models,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_modules,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_networks,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_neural_network_models,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_other,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_real_domain,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_results,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_structures,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_tasks,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_techniques,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_that,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_these,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_they,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_time,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_vision,
        :Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Xue_Deep_Texture_Manifold_CVPR_2018_paper_network,
        :Xue_Deep_Texture_Manifold_CVPR_2018_paper_pooling,
        :Xue_Deep_Texture_Manifold_CVPR_2018_paper_texture ;
    :hasFigure :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure3-1,
        :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure3-1_0 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure3-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure3-1_2 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure3-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1_0 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1_1 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1_2 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1_3 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1_4 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1_5 :partOf :Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1_Comp5 a :NormBlock .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_baselines,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_benchmarks,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_convolutional_neural_networks,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_counterparts,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_effectiveness,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_end-to-end,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_experiments,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_information,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_methods,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_model,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_model_parameters,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_models,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_multi-task_learning,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_parsing,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_pose,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_pose_estimation,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_post-processing,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_representations,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_results,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_task,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_tasks ;
    :hasFigure :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1,
        :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_1 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_10 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_2 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_3 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_4 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_5 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_6 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_7 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_8 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_9 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_Comp3 a :LossBlock .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_Comp6 a :LossBlock .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1_Comp8 a :LossBlock .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_0 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_1 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_12 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_13 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_2 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_3 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_4 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_8 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_9 :partOf :Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_Comp0 a :DeconvBlock .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_Comp3 a :ConvBlock .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1_Comp4 a :LossBlock .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_accuracy,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_architecture,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_benchmarks,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_candidates,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_computation,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_cues,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_efficiency,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_experiments,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_graph,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_human_pose,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_learn,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_network,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_pose,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_pose_estimation,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_state,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_this,
        :Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_way ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper :hasFigure :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_0 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_1 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_2 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_3 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_4 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_5 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_6 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_7 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_8 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_9 :partOf :Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1_Comp0 a :RnnBlock .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_0 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_1 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_10 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_11 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_12 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_2 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_3 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_4 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_5 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_6 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_7 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1_9 :partOf :Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_applications,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_baseline_methods,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_classification,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_concept,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_correspondence,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_datasets,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_experimental_results,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_first,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_information,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_matching,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_method,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_methods,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_model,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_prediction,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_problem,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_problems,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_research,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_results,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_segments,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_semantic,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_semantic_coherence,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_task,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_technology,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_that,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_they,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_this,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_time,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_video,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_video_surveillance,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_videos,
        :Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yang_Learning_Face_Age_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Yang_Learning_Face_Age_CVPR_2018_paper_face,
        :Yang_Learning_Face_Age_CVPR_2018_paper_two ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Yang_Neural_Aggregation_Network_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Yang_Object_Contour_Detection_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Yang_Object_Contour_Detection_CVPR_2016_paper_deep_learning_algorithm ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Yang_Robust_Classification_With_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Yang_Robust_Classification_With_CVPR_2018_paper_accuracies,
        :Yang_Robust_Classification_With_CVPR_2018_paper_adversarial_examples,
        :Yang_Robust_Classification_With_CVPR_2018_paper_categories,
        :Yang_Robust_Classification_With_CVPR_2018_paper_classes,
        :Yang_Robust_Classification_With_CVPR_2018_paper_classification,
        :Yang_Robust_Classification_With_CVPR_2018_paper_cnn,
        :Yang_Robust_Classification_With_CVPR_2018_paper_cnns,
        :Yang_Robust_Classification_With_CVPR_2018_paper_criteria,
        :Yang_Robust_Classification_With_CVPR_2018_paper_datasets,
        :Yang_Robust_Classification_With_CVPR_2018_paper_feature,
        :Yang_Robust_Classification_With_CVPR_2018_paper_framework,
        :Yang_Robust_Classification_With_CVPR_2018_paper_generative_model,
        :Yang_Robust_Classification_With_CVPR_2018_paper_image_classification,
        :Yang_Robust_Classification_With_CVPR_2018_paper_model,
        :Yang_Robust_Classification_With_CVPR_2018_paper_network,
        <https://github.com/deepcurator/DCC/Yang_Robust_Classification_With_CVPR_2018_paper_neural_networks_(>,
        :Yang_Robust_Classification_With_CVPR_2018_paper_problem,
        :Yang_Robust_Classification_With_CVPR_2018_paper_regularization,
        :Yang_Robust_Classification_With_CVPR_2018_paper_representation,
        :Yang_Robust_Classification_With_CVPR_2018_paper_results,
        :Yang_Robust_Classification_With_CVPR_2018_paper_robustness,
        :Yang_Robust_Classification_With_CVPR_2018_paper_tasks,
        :Yang_Robust_Classification_With_CVPR_2018_paper_that,
        :Yang_Robust_Classification_With_CVPR_2018_paper_this,
        :Yang_Robust_Classification_With_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yang_Semantic_Filtering_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_alignment,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_convolutional_neural_network,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_datasets,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_face,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_image,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_loss_function,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_map,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_method,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_methods,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_model,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_network,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_other,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_our_method,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_position,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_prior,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_records,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_representation,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_semantic,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_shape,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_state,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_structure,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_tasks,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_that,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_this,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_training,
        :Yao_Feng_Joint_3D_Face_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_architecture,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_complex,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_convolutions,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_deep_learning,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_end-to-end,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_feature,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_features,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_first,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_framework,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_generalization,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_homography,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_image,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_images,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_inputs,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_map,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_maps,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_metric,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_network,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_one,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_our_method,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_post-processing,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_ranks,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_scale,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_state,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_that,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_times,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_variance,
        :Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_alignment,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_attention_mechanism,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_audio,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_correlations,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_distance,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_event,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_experiments,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_features,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_information,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modalities,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modality,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modeling,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_network,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_objects,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_problem,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_semantics,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_tasks,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_that,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_this,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_two,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_unconstrained_videos,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_video,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_we ;
    :hasFigure :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1,
        :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_1 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_10 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_11 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_12 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_13 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_3 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_5 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_6 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_7 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_8 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_9 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp10 a :DeconvBlock .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp13 a :FlattenBlock .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1_Comp7 a :LSTMBlock .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_0 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_1 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_2 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_3 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_4 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_5 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_6 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_7 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_8 :partOf :Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp2 a :ActivationBlock .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp3 a :ActivationBlock .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp7 a :ActivationBlock .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1_Comp8 a :ActivationBlock .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_adversarial,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_blur,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_classification,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_convergence,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_framework,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_generalization,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_images,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_map,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_mechanisms,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_methods,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_model,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_network,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_networks,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_other,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_parsing,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_parsing_accuracy,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_patches,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_problem,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_problems,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_resolution,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_semantic,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_state,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_that,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_them,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_this,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_two,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_validate,
        :Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_accuracy,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_architecture,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_benchmarks,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_cnns,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_domains,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_family,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_features,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_function,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_hierarchical,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_information,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_neural_networks,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_operations,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_point_clouds,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_regular,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_scale,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_segmentation,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_semantic,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_structures,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_task,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_tasks,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_that,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_this,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_vision,
        :Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_alternatives,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_benchmarks,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_constraints,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_convolutional_neural_network,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_formulation,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_frameworks,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_image,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_images,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_information,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_matching,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_methods,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_network,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_other,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_our_method,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_outputs,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_patches,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_results,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_rgb,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_scenes,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_state,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_surfaces,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_task,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_that,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_this,
        :Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_accuracy,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_clustering_method,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_computation,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_data,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_datasets,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_efficiency,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_features,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_first,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_generation,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_graph,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_image,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_interactions,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_method,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_methods,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_model,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_module,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_objects,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_our_method,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_representations,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_scene,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_spatial_information,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_state,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_structure,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_structures,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_subgraphs,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_these,
        :Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_ambiguities,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_components,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_dynamic_scene,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_end-to-end,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_estimation,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_flow,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_framework,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_image,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_kitti,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_modules,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_occlusions,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_ones,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_outliers,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_predictions,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_regions,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_results,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_robustness,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_scene_geometry,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_scheme,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_state,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_tasks,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_texture,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_that,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_unsupervised_learning,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_unsupervised_methods,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_videos,
        :Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yu_Crafting_a_Toolchain_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_0 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_1 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_10 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_11 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_12 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_13 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_14 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_15 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_16 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_18 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_19 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_2 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_20 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_22 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_23 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_24 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_26 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_27 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_28 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_29 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_3 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_30 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_32 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_34 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_35 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_36 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_39 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_4 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_40 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_42 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_43 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_44 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_46 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_47 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_48 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_5 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_51 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_52 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_53 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_54 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_55 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_56 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_57 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_58 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_59 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_6 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_61 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_62 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_63 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_64 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_65 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_66 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_67 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_68 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_7 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_8 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_9 :partOf :Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_Comp5 a :PoolingBlock .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1_Comp65 a :DenseBlock .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Yu_Generative_Image_Inpainting_CVPR_2018_paper_face,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_image,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_images,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_input,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_neural_networks,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_our_method,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_post-processing,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_regions,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_results,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_scene,
        :Yu_Generative_Image_Inpainting_CVPR_2018_paper_texture ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_0 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_1 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_2 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_3 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_4 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_5 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_6 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_7 :partOf :Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1_Comp2 a :LSTMBlock .

:Yu_PU-Net_Point_Cloud_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yu_Video_Paragraph_Captioning_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Yu_Video_Paragraph_Captioning_CVPR_2016_paper_approach,
        :Yu_Video_Paragraph_Captioning_CVPR_2016_paper_hierarchical,
        :Yu_Video_Paragraph_Captioning_CVPR_2016_paper_recurrent_neural_networks,
        :Yu_Video_Paragraph_Captioning_CVPR_2016_paper_rnns,
        :Yu_Video_Paragraph_Captioning_CVPR_2016_paper_that ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasFigure :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_0 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_1 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_12 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_13 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_2 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_3 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_4 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_5 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_6 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_7 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_8 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_9 :partOf :Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp1 a :RnnBlock .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp2 a :RnnBlock .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp3 a :DeconvBlock .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp4 a :RnnBlock .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp7 a :DeconvBlock .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1_Comp8 a :LossBlock .

:Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_accuracy,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_alternative,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_bn,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_classification,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_computation,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_computer_vision_tasks,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_counterpart,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_counterparts,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_deep_learning,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_development,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_estimation,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_features,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_imagenet,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_memory,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_models,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_networks,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_object_detection,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_other,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_pre-training,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_problems,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_resnet-50,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_segmentation,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_sizes,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_statistics,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_tasks,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_technique,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_that,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_this,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_training,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_variance,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_video,
        :Yuxin_Wu_Group_Normalization_ECCV_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_agents,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_annotations,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_classification,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_datasets,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_discriminative_features,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_end-to-end,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_mechanism,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_model,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_network,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_other,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_paradigm,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_predictions,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_regions,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_state,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_supervision,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_that,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_this,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_those,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_training,
        :Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zeng_Learning_to_Promote_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhai_S3Pool_Pooling_With_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhai_S3Pool_Pooling_With_CVPR_2017_paper_layers,
        :Zhai_S3Pool_Pooling_With_CVPR_2017_paper_pooling ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_adversarial,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_adversarial_learning,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_approach,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_architecture,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_classification,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_classifier,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_classifiers,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_counterpart,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_end-to-end,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_error_rate,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_experiments,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_feature,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_first,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_maps,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_network,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_object_regions,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_objects,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_one,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_regions,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_semantic,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_state,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_supervision,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_target,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_that,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_this,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_two,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_way,
        :Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasFigure :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_0 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_1 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_10 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_11 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_2 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_4 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_5 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_6 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_7 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_9 :partOf :Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1_Comp2 a :RnnBlock .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_0 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_1 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_10 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_11 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_2 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_3 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_4 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_5 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_6 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_7 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_8 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_9 :partOf :Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1_Comp2 a :RnnBlock .

:Zhang_Densely_Connected_Pyramid_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhang_Densely_Connected_Pyramid_CVPR_2018_paper_end-to-end ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1,
        :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_0 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_1 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_11 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_2 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_3 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_4 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_5 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_6 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_7 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_8 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_9 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_Comp11 a :OutputBlock .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_Comp3 a :ConcatBlock .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_Comp4 a :DenseBlock .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_Comp5 a :ConcatBlock .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1_Comp6 a :DenseBlock .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1_0 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1_2 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1_5 :partOf :Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1 .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1_Comp0 a :DenseBlock .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1_Comp2 a :DenseBlock .

:Zhang_Grounding_Referring_Expressions_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhang_Grounding_Referring_Expressions_CVPR_2018_paper_grounding ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_ISTA-Net_Interpretable_Optimization-Inspired_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_Joint_Pose_and_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_Learning_Spread-Out_Local_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zhang_Learning_a_Deep_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zhang_Residual_Dense_Network_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhang_Residual_Dense_Network_CVPR_2018_paper_cnn,
        <https://github.com/deepcurator/DCC/Zhang_Residual_Dense_Network_CVPR_2018_paper_convolutional_neural_network_(cnn)>,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_datasets,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_feature,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_features,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_hierarchical,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_image,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_image_super-resolution,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_images,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_layers,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_learn,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_local_features,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_mechanism,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_memory,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_methods,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_models,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_network,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_problem,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_resolution,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_state,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_that,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_this,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_training,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_way,
        :Zhang_Residual_Dense_Network_CVPR_2018_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_Unsupervised_Discovery_of_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_datasets,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_effectiveness,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_feature_extraction,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_first,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_image,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_knowledge,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_knowledge_bases,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_language,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_methods,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_model,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_modeling,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_natural_language,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_network,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_networks,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_object_detection,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_object_relation,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_objects,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_other,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_priors,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_relations,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_representation_learning,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_scale,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_scene,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_state,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_that,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_training,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_transfer,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_two,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_understanding,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_utility,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_vision,
        :Zhang_Visual_Translation_Embedding_CVPR_2017_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_adversarial,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_adversarial_learning,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_classes,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_classifier,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_data,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_datasets,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_deep_models,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_domain,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_domain_adaptation,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_domains,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_feature,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_matching,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_methods,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_mismatch,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_networks,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_problem,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_results,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_source_domain,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_spaces,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_state,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_target,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_target_domain,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_target_domains,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_tasks,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_that,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_training,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_transfer,
        :Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_two ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper :hasFigure :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_0 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_1 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_10 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_11 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_12 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_13 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_14 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_15 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_16 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_17 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_18 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_19 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_2 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_4 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_7 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_8 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_9 :partOf :Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_Comp0 a :InputBlock .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_Comp15 a :ConvBlock .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_Comp16 a :PoolingBlock .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_Comp17 a :ConvBlock .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1_Comp19 a :ConvBlock .

:Zhaoyi_Yan_Shift-Net_Image_Inpainting_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zheng_Conditional_Random_Fields_ICCV_2015_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2015 .

:Zheng_Ring_Loss_Convex_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper a :Publication ;
    :conferenceSeries "ICCV" ;
    :hasEntity :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_adversarial,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_baseline,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cameras,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cnn_embeddings,
        <https://github.com/deepcurator/DCC/Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_convolutional_neural_network_(cnn)>,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_data,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_datasets,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_gan,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_generation,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_images,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_improvement,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_method,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_model,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_network,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_other,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_outliers,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_person_re-identification,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_problem,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_regularization,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_representation_learning,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_scale,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_smoothing,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_task,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_that,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_this,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_training,
        :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_we ;
    :hasFigure :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_0 :partOf :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1 .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_1 :partOf :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1 .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_2 :partOf :Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1 .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Comp0 a :RnnBlock .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Comp1 a :ConvBlock .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1_Comp2 a :LossBlock .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_accuracy,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_approach,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_approaches,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_benchmarks,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_data,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_domain,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_experiments,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_features,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_first,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_learned_features,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_long-term,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_model,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_module,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_networks,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_region,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_robustness,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_sampling,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_search,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_semantic,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_short-term,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_state,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_strategy,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_that,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_this,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_tracker,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_trackers,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_tracking,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_training,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_transfer,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_video,
        :Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_approach,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_approaches,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_categories,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_classification,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_classifier,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_classifiers,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_component,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_criterion,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_data,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_deep_networks,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_deep_neural_network,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_embeddings,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_end-to-end,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_examples,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_experiments,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_feature,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_formulation,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_image,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_imagenet,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_input,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_large_datasets,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_learn,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_mechanism,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_memory,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_method,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_nca,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_neighborhood,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_nonparametric,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_one,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_representation,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_scale,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_semantic,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_structure,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_that,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_this,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_training,
        :Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasFigure :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1,
        :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_0 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_1 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_10 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_11 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_12 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_13 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_14 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_15 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_16 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_2 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_3 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_4 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_5 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_6 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_7 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_8 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_9 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1_Comp1 a :RnnBlock .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_0 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_1 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_10 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_11 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_12 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_2 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_3 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_4 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_5 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_6 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_7 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_8 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_9 :partOf :Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_Comp1 a :OutputBlock .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_Comp10 a :ConcatBlock .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_Comp12 a :ConvBlock .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_Comp3 a :ConvBlock .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_Comp4 a :ConcatBlock .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_Comp6 a :ConvBlock .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1_Comp7 a :ConcatBlock .

:Zhou_Learning_Deep_Features_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhou_Learning_Deep_Features_CVPR_2016_paper_pooling,
        :Zhou_Learning_Deep_Features_CVPR_2016_paper_this,
        :Zhou_Learning_Deep_Features_CVPR_2016_paper_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2016 .

:Zhou_Learning_Rich_Features_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zhou_Weakly_Supervised_Instance_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhu_Learning_Spatial_Regularization_CVPR_2017_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:Zhuang_Fast_Training_of_CVPR_2016_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :hasEntity :Zhuang_Fast_Training_of_CVPR_2016_paper_learn,
        :Zhuang_Fast_Training_of_CVPR_2016_paper_this,
        :Zhuang_Fast_Training_of_CVPR_2016_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2016 .

:Zhuang_Towards_Effective_Low-Bitwidth_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper :hasFigure :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_0 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_1 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_10 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_11 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_12 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_13 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_14 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_15 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_16 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_17 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_2 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_3 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_4 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_5 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_6 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_7 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_8 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_9 :partOf :Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1_Comp0 a :LossBlock .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :hasEntity :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_approach,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_convolution,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_effectiveness,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_experiments,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_location,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_neural_network,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_parameters,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_rotation,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_scale,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_scheme,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_sphere,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_spherical_convolutional_neural_network,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_that,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_u-net,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_validate,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_video,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_videos,
        :Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zixin_Luo_Learning_Local_Descriptors_ECCV_2018_paper a :Publication ;
    :conferenceSeries "ECCV" ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:Zoph_Learning_Transferable_Architectures_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:Zou_LayoutNet_Reconstructing_the_CVPR_2018_paper a :Publication ;
    :conferenceSeries "CVPR" ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:amos17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :amos17a_application,
        :amos17a_approaches,
        :amos17a_architecture,
        :amos17a_architectures,
        :amos17a_backpropagation,
        :amos17a_complex,
        :amos17a_constraints,
        :amos17a_deep_networks,
        :amos17a_end-to-end,
        :amos17a_gradients,
        :amos17a_information,
        :amos17a_input,
        :amos17a_layers,
        :amos17a_learn,
        :amos17a_method,
        :amos17a_network,
        :amos17a_one,
        :amos17a_optimization,
        :amos17a_optimization_problems,
        :amos17a_other,
        :amos17a_parameters,
        :amos17a_priori,
        :amos17a_problems,
        :amos17a_programs,
        :amos17a_rules,
        :amos17a_states,
        :amos17a_techniques,
        :amos17a_that,
        :amos17a_these,
        :amos17a_this,
        :amos17a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:amos17b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :amos17b_architecture,
        :amos17b_constraints,
        :amos17b_convex,
        :amos17b_data,
        :amos17b_function,
        :amos17b_image_completion,
        :amos17b_improvement,
        :amos17b_input,
        :amos17b_inputs,
        :amos17b_methods,
        :amos17b_models,
        :amos17b_network,
        :amos17b_networks,
        :amos17b_neural_network,
        :amos17b_neural_network_architectures,
        :amos17b_neural_networks,
        :amos17b_optimization,
        :amos17b_optimization_algorithms,
        :amos17b_others,
        :amos17b_parameters,
        :amos17b_prediction,
        :amos17b_problems,
        :amos17b_reinforcement_learning,
        :amos17b_state,
        :amos17b_structured,
        :amos17b_that,
        :amos17b_these,
        :amos17b_this,
        :amos17b_we ;
    :hasFigure :amos17b-Figure1-1,
        :amos17b-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:amos17b-Figure1-1_0 :partOf :amos17b-Figure1-1 .

:amos17b-Figure1-1_1 :partOf :amos17b-Figure1-1 .

:amos17b-Figure1-1_2 :partOf :amos17b-Figure1-1 .

:amos17b-Figure1-1_3 :partOf :amos17b-Figure1-1 .

:amos17b-Figure1-1_4 :partOf :amos17b-Figure1-1 .

:amos17b-Figure1-1_5 :partOf :amos17b-Figure1-1 .

:amos17b-Figure2-1_3 :partOf :amos17b-Figure2-1 .

:amos17b-Figure2-1_4 :partOf :amos17b-Figure2-1 .

:amos17b-Figure2-1_5 :partOf :amos17b-Figure2-1 .

:amos17b-Figure2-1_6 :partOf :amos17b-Figure2-1 .

:amos17b-Figure2-1_7 :partOf :amos17b-Figure2-1 .

:amos17b-Figure2-1_8 :partOf :amos17b-Figure2-1 .

:amos17b-Figure2-1_9 :partOf :amos17b-Figure2-1 .

:and a owl:ObjectProperty .

:arjovsky17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :arjovsky17a_algorithm,
        :arjovsky17a_alternative,
        :arjovsky17a_distances,
        :arjovsky17a_gan,
        :arjovsky17a_hyperparameter,
        :arjovsky17a_model,
        :arjovsky17a_optimization_problem,
        :arjovsky17a_problems,
        :arjovsky17a_that,
        :arjovsky17a_theoretical_work,
        :arjovsky17a_this,
        :arjovsky17a_training,
        :arjovsky17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:arxivId a owl:DatatypeProperty .

:athalye18b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :athalye18b_adversarial,
        :athalye18b_adversarial_examples,
        :athalye18b_algorithm,
        :athalye18b_complex,
        :athalye18b_examples,
        :athalye18b_first,
        :athalye18b_images,
        :athalye18b_methods,
        :athalye18b_neural_network_classifiers,
        :athalye18b_neural_networks,
        :athalye18b_noise,
        :athalye18b_objects,
        :athalye18b_other,
        :athalye18b_relevance,
        :athalye18b_results,
        :athalye18b_systems,
        :athalye18b_that,
        :athalye18b_transformations,
        :athalye18b_two,
        :athalye18b_viewpoint,
        :athalye18b_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:author a owl:ObjectProperty .

:authorName a owl:DatatypeProperty .

:authorOf a owl:ObjectProperty ;
    rdfs:domain :PublicationAuthor ;
    rdfs:range :Publication .

:balog18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :balog18a_accuracy,
        :balog18a_algorithm,
        :balog18a_algorithms,
        :balog18a_data,
        :balog18a_database,
        :balog18a_framework,
        :balog18a_hilbert_space,
        :balog18a_mechanisms,
        :balog18a_metric,
        :balog18a_outputs,
        :balog18a_privacy,
        :balog18a_results,
        :balog18a_statistics,
        :balog18a_synthetic_data,
        :balog18a_that,
        :balog18a_two ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:bojanowski18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :bojanowski18a_adversarial,
        :bojanowski18a_applications,
        :bojanowski18a_deep_convolutional_neural_networks,
        :bojanowski18a_experiments,
        :bojanowski18a_factors,
        :bojanowski18a_framework,
        :bojanowski18a_functions,
        :bojanowski18a_gan,
        :bojanowski18a_gans,
        :bojanowski18a_models,
        :bojanowski18a_natural_images,
        :bojanowski18a_networks,
        :bojanowski18a_noise,
        :bojanowski18a_optimization,
        :bojanowski18a_optimization_problem,
        :bojanowski18a_optimization_scheme,
        :bojanowski18a_properties,
        :bojanowski18a_results,
        :bojanowski18a_task,
        :bojanowski18a_that,
        :bojanowski18a_these,
        :bojanowski18a_this,
        :bojanowski18a_two,
        :bojanowski18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:bojchevski18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :bojchevski18a_approach,
        :bojchevski18a_discrete,
        :bojchevski18a_first,
        :bojchevski18a_gan,
        :bojchevski18a_generalization,
        :bojchevski18a_generation,
        :bojchevski18a_generative_model,
        :bojchevski18a_graph,
        :bojchevski18a_graphs,
        :bojchevski18a_input,
        :bojchevski18a_model,
        :bojchevski18a_network,
        :bojchevski18a_neural_network,
        :bojchevski18a_pose,
        :bojchevski18a_prediction,
        :bojchevski18a_problem,
        :bojchevski18a_properties,
        :bojchevski18a_real-world_networks,
        :bojchevski18a_research,
        :bojchevski18a_task,
        :bojchevski18a_that,
        :bojchevski18a_them,
        :bojchevski18a_these,
        :bojchevski18a_this,
        :bojchevski18a_time ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:bora17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :bora17a_accuracy,
        :bora17a_autoencoder,
        :bora17a_domain,
        :bora17a_generative_adversarial_networks,
        :bora17a_generative_model,
        :bora17a_generative_models,
        :bora17a_method,
        :bora17a_prior_knowledge,
        :bora17a_recovery,
        :bora17a_results,
        :bora17a_sparsity,
        :bora17a_structure,
        :bora17a_system,
        :bora17a_that,
        :bora17a_this,
        :bora17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:brukhim18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :brukhim18a_approach,
        :brukhim18a_approaches,
        :brukhim18a_architecture,
        :brukhim18a_baselines,
        :brukhim18a_benchmarks,
        :brukhim18a_classification,
        :brukhim18a_constraints,
        :brukhim18a_deep_learning,
        :brukhim18a_framework,
        :brukhim18a_machine_learning,
        :brukhim18a_model,
        :brukhim18a_modeling,
        :brukhim18a_models,
        :brukhim18a_outputs,
        :brukhim18a_prediction,
        :brukhim18a_prediction_models,
        :brukhim18a_problems,
        :brukhim18a_results,
        :brukhim18a_state,
        :brukhim18a_structured,
        :brukhim18a_that,
        :brukhim18a_them,
        :brukhim18a_this,
        :brukhim18a_we ;
    :hasFigure :brukhim18a-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:brukhim18a-Figure1-1_1 :partOf :brukhim18a-Figure1-1 .

:brukhim18a-Figure1-1_3 :partOf :brukhim18a-Figure1-1 .

:brukhim18a-Figure1-1_4 :partOf :brukhim18a-Figure1-1 .

:brukhim18a-Figure1-1_5 :partOf :brukhim18a-Figure1-1 .

:brukhim18a-Figure1-1_6 :partOf :brukhim18a-Figure1-1 .

:brukhim18a-Figure1-1_7 :partOf :brukhim18a-Figure1-1 .

:brukhim18a-Figure1-1_Comp6 a :RnnBlock .

:cai18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :cai18a_accuracy,
        :cai18a_architecture,
        :cai18a_architectures,
        :cai18a_cifar-10,
        :cai18a_complex,
        :cai18a_datasets,
        :cai18a_effectiveness,
        :cai18a_efficiency,
        :cai18a_function,
        :cai18a_generalization,
        :cai18a_image_classification,
        :cai18a_imagenet,
        :cai18a_inception,
        :cai18a_limited_computational_resources,
        :cai18a_models,
        :cai18a_network,
        :cai18a_networks,
        :cai18a_neural_architecture_search,
        :cai18a_operations,
        :cai18a_parameters,
        :cai18a_pruning,
        :cai18a_reinforcement_learning,
        :cai18a_results,
        :cai18a_structures,
        :cai18a_that,
        :cai18a_topology,
        :cai18a_we,
        :cai18a_weights ;
    :hasFigure :cai18a-Figure2-1,
        :cai18a-Figure3-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:cai18a-Figure2-1_0 :partOf :cai18a-Figure2-1 .

:cai18a-Figure2-1_1 :partOf :cai18a-Figure2-1 .

:cai18a-Figure2-1_2 :partOf :cai18a-Figure2-1 .

:cai18a-Figure2-1_4 :partOf :cai18a-Figure2-1 .

:cai18a-Figure2-1_Comp1 a :ConcatBlock .

:cai18a-Figure3-1_0 :partOf :cai18a-Figure3-1 .

:cai18a-Figure3-1_1 :partOf :cai18a-Figure3-1 .

:cai18a-Figure3-1_10 :partOf :cai18a-Figure3-1 .

:cai18a-Figure3-1_8 :partOf :cai18a-Figure3-1 .

:calls a owl:ObjectProperty ;
    rdfs:domain :Function ;
    rdfs:range :Function .

:cao18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :cao18a_adversarial,
        :cao18a_bound,
        :cao18a_data,
        :cao18a_effectiveness,
        :cao18a_experiments,
        :cao18a_gans,
        :cao18a_generalization,
        :cao18a_generalization_performance,
        :cao18a_images,
        :cao18a_input,
        :cao18a_method,
        :cao18a_networks,
        :cao18a_noises,
        :cao18a_prior,
        :cao18a_real-world_datasets,
        :cao18a_real_data,
        :cao18a_sampling,
        :cao18a_semantic_information,
        :cao18a_structure,
        :cao18a_that,
        :cao18a_this,
        :cao18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:chapfuwa18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :chapfuwa18a_adversarial_learning,
        :chapfuwa18a_alternative,
        :chapfuwa18a_applications,
        :chapfuwa18a_approach,
        :chapfuwa18a_clinical,
        :chapfuwa18a_cost_function,
        :chapfuwa18a_data,
        :chapfuwa18a_estimation,
        :chapfuwa18a_event,
        :chapfuwa18a_events,
        :chapfuwa18a_examples,
        :chapfuwa18a_formulation,
        :chapfuwa18a_information,
        :chapfuwa18a_machine_learning,
        :chapfuwa18a_model,
        :chapfuwa18a_modeling,
        :chapfuwa18a_models,
        :chapfuwa18a_network,
        :chapfuwa18a_nonparametric,
        :chapfuwa18a_one,
        :chapfuwa18a_real_datasets,
        :chapfuwa18a_statistical_models,
        :chapfuwa18a_that,
        :chapfuwa18a_time,
        :chapfuwa18a_validate,
        :chapfuwa18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:chen18g a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :chen18g_applications,
        :chen18g_approach,
        :chen18g_codes,
        :chen18g_continuous,
        :chen18g_convolutional_networks,
        :chen18g_discrete,
        :chen18g_embedding_methods,
        :chen18g_end-to-end,
        :chen18g_experiments,
        :chen18g_graph,
        :chen18g_learn,
        :chen18g_natural_language_processing,
        :chen18g_one,
        :chen18g_optimization,
        :chen18g_overfitting,
        :chen18g_parameters,
        :chen18g_scheme,
        :chen18g_stochastic_gradient_descent,
        :chen18g_that,
        :chen18g_this,
        :chen18g_way,
        :chen18g_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:chen18l a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :chen18l_adversarial,
        :chen18l_approaches,
        :chen18l_assumptions,
        :chen18l_computational_overhead,
        :chen18l_convergence_guarantees,
        :chen18l_distributed,
        :chen18l_experiments,
        :chen18l_framework,
        :chen18l_gradient,
        :chen18l_gradients,
        :chen18l_magnitude,
        :chen18l_model,
        :chen18l_model_training,
        :chen18l_models,
        :chen18l_node,
        :chen18l_nodes,
        :chen18l_one,
        :chen18l_real_datasets,
        :chen18l_robustness,
        :chen18l_rule,
        :chen18l_rules,
        :chen18l_scale,
        :chen18l_system,
        :chen18l_that,
        :chen18l_theory,
        :chen18l_this,
        :chen18l_times,
        :chen18l_training,
        :chen18l_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:cites a owl:ObjectProperty .

:compare a owl:ObjectProperty .

:conferenceSeries a owl:DatatypeProperty .

:conjunction a owl:ObjectProperty .

:cortes17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :cortes17a_accuracies,
        :cortes17a_algorithm,
        :cortes17a_algorithms,
        :cortes17a_approaches,
        :cortes17a_artificial_neural_networks,
        :cortes17a_binary_classification_tasks,
        :cortes17a_cifar-10,
        :cortes17a_data,
        :cortes17a_experiments,
        :cortes17a_generalization_guarantees,
        :cortes17a_learn,
        :cortes17a_network,
        :cortes17a_neural_networks,
        :cortes17a_one,
        :cortes17a_results,
        :cortes17a_scale,
        :cortes17a_structure,
        :cortes17a_structures,
        :cortes17a_that,
        :cortes17a_theoretical_analysis,
        :cortes17a_those,
        :cortes17a_we,
        :cortes17a_weights ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:cremer18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :cremer18a_approximating,
        :cremer18a_approximation,
        :cremer18a_autoencoders,
        :cremer18a_divergence,
        :cremer18a_factors,
        :cremer18a_large_datasets,
        :cremer18a_models,
        :cremer18a_parameters,
        :cremer18a_posterior,
        :cremer18a_quality,
        :cremer18a_recognition_network,
        :cremer18a_recognition_networks,
        :cremer18a_scale,
        :cremer18a_that,
        :cremer18a_these,
        :cremer18a_this,
        :cremer18a_two,
        :cremer18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:dai17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :dai17a_approach,
        :dai17a_codes,
        :dai17a_constraints,
        :dai17a_databases,
        :dai17a_datasets,
        :dai17a_discrete,
        :dai17a_experiments,
        :dai17a_function,
        :dai17a_functions,
        :dai17a_generative_model,
        :dai17a_gradient,
        :dai17a_inputs,
        :dai17a_learn,
        :dai17a_learning_algorithm,
        :dai17a_method,
        :dai17a_methods,
        :dai17a_minimum_description_length_principle,
        :dai17a_objective_functions,
        :dai17a_outputs,
        :dai17a_paradigm,
        :dai17a_parameters,
        :dai17a_results,
        :dai17a_retrieval,
        :dai17a_scale,
        :dai17a_search,
        :dai17a_state,
        :dai17a_techniques,
        :dai17a_that,
        :dai17a_this,
        :dai17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:dai18b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :dai18b_adversarial,
        :dai18b_applications,
        :dai18b_classification_tasks,
        :dai18b_classifier,
        :dai18b_classifiers,
        :dai18b_data,
        :dai18b_deep_learning_models,
        :dai18b_family,
        :dai18b_first,
        :dai18b_genetic_algorithms,
        :dai18b_gradient_descent,
        :dai18b_gradients,
        :dai18b_graph,
        :dai18b_graph_structures,
        :dai18b_image,
        :dai18b_learns,
        :dai18b_method,
        :dai18b_methods,
        :dai18b_models,
        :dai18b_neural_network_models,
        :dai18b_node,
        :dai18b_policy,
        :dai18b_prediction,
        :dai18b_reinforcement_learning,
        :dai18b_research,
        :dai18b_results,
        :dai18b_robustness,
        :dai18b_structure,
        :dai18b_target,
        :dai18b_text,
        :dai18b_that,
        :dai18b_these,
        :dai18b_this,
        :dai18b_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:dauphin17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :dauphin17a_approach,
        :dauphin17a_baseline,
        :dauphin17a_convolutions,
        :dauphin17a_decisions,
        :dauphin17a_features,
        :dauphin17a_first,
        :dauphin17a_knowledge,
        :dauphin17a_language,
        :dauphin17a_language_modeling,
        :dauphin17a_magnitude,
        :dauphin17a_mechanism,
        :dauphin17a_model,
        :dauphin17a_models,
        :dauphin17a_recurrent_neural_networks,
        :dauphin17a_results,
        :dauphin17a_scale,
        :dauphin17a_state,
        :dauphin17a_task,
        :dauphin17a_tasks,
        :dauphin17a_that,
        :dauphin17a_these,
        :dauphin17a_they,
        :dauphin17a_this,
        :dauphin17a_time,
        :dauphin17a_we,
        :dauphin17a_words ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:denton18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :denton18a_approach,
        :denton18a_approaches,
        :denton18a_datasets,
        :denton18a_end-to-end,
        :denton18a_generation,
        :denton18a_generations,
        :denton18a_latent_variables,
        :denton18a_learned_prior,
        :denton18a_model,
        :denton18a_prior,
        :denton18a_states,
        :denton18a_that,
        :denton18a_them,
        :denton18a_this,
        :denton18a_those,
        :denton18a_time,
        :denton18a_video,
        :denton18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:donahue17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :donahue17a_approaches,
        :donahue17a_audio,
        :donahue17a_chart,
        :donahue17a_charts,
        :donahue17a_convolutional_neural_networks,
        :donahue17a_features,
        :donahue17a_generative_model,
        :donahue17a_lstm,
        :donahue17a_music,
        :donahue17a_platform,
        :donahue17a_rhythm,
        :donahue17a_task,
        :donahue17a_that,
        :donahue17a_two,
        :donahue17a_video,
        :donahue17a_we ;
    :hasFigure :donahue17a-Figure5-1,
        :donahue17a-Figure7-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:donahue17a-Figure5-1_0 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_1 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_2 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_3 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_4 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_5 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_6 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_7 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_8 :partOf :donahue17a-Figure5-1 .

:donahue17a-Figure5-1_Comp0 a :DenseBlock .

:donahue17a-Figure5-1_Comp1 a :DenseBlock .

:donahue17a-Figure5-1_Comp2 a :LSTMBlock .

:donahue17a-Figure5-1_Comp3 a :LSTMBlock .

:donahue17a-Figure5-1_Comp4 a :LSTMBlock .

:donahue17a-Figure5-1_Comp5 a :ConvBlock .

:donahue17a-Figure7-1_0 :partOf :donahue17a-Figure7-1 .

:donahue17a-Figure7-1_1 :partOf :donahue17a-Figure7-1 .

:donahue17a-Figure7-1_2 :partOf :donahue17a-Figure7-1 .

:donahue17a-Figure7-1_3 :partOf :donahue17a-Figure7-1 .

:donahue17a-Figure7-1_4 :partOf :donahue17a-Figure7-1 .

:donahue17a-Figure7-1_5 :partOf :donahue17a-Figure7-1 .

:donahue17a-Figure7-1_Comp0 a :LSTMBlock .

:donahue17a-Figure7-1_Comp1 a :LSTMBlock .

:donahue17a-Figure7-1_Comp2 a :RnnBlock .

:donahue17a-Figure7-1_Comp3 a :LSTMBlock .

:feature-of a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:featureOf a owl:ObjectProperty .

:finn17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :finn17a_algorithm,
        :finn17a_approach,
        :finn17a_benchmarks,
        :finn17a_classification,
        :finn17a_data,
        :finn17a_generalization_performance,
        :finn17a_gradient,
        :finn17a_gradient_descent,
        :finn17a_image_classification,
        :finn17a_model,
        :finn17a_neural_network,
        :finn17a_our_method,
        :finn17a_parameters,
        :finn17a_policies,
        :finn17a_policy,
        :finn17a_problems,
        :finn17a_reinforcement_learning,
        :finn17a_results,
        :finn17a_state,
        :finn17a_task,
        :finn17a_tasks,
        :finn17a_that,
        :finn17a_this,
        :finn17a_training,
        :finn17a_two ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:foerster17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :foerster17a_architecture,
        :foerster17a_computational_efficiency,
        :foerster17a_domains,
        :foerster17a_input,
        :foerster17a_interpretability,
        :foerster17a_linear_methods,
        :foerster17a_model,
        :foerster17a_modeling,
        :foerster17a_network,
        :foerster17a_networks,
        :foerster17a_neural_network_models,
        :foerster17a_nonlinearities,
        :foerster17a_other,
        :foerster17a_predictions,
        :foerster17a_problem,
        :foerster17a_rnn,
        :foerster17a_solution,
        :foerster17a_subspaces,
        :foerster17a_task,
        :foerster17a_tasks,
        :foerster17a_text,
        :foerster17a_this,
        :foerster17a_transformations,
        :foerster17a_we,
        :foerster17a_weights,
        :foerster17a_words ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:followedBy a owl:ObjectProperty .

:franceschi18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :franceschi18a_approach,
        :franceschi18a_approaches,
        :franceschi18a_conditions,
        :franceschi18a_deep_learning,
        :franceschi18a_dynamics,
        :franceschi18a_experiments,
        :franceschi18a_few-shot_learning,
        :franceschi18a_framework,
        :franceschi18a_gradient,
        :franceschi18a_hyperparameter,
        :franceschi18a_hyperparameters,
        :franceschi18a_layers,
        :franceschi18a_learn,
        :franceschi18a_learner,
        :franceschi18a_optimization,
        :franceschi18a_parameters,
        :franceschi18a_problem,
        :franceschi18a_representation,
        :franceschi18a_results,
        :franceschi18a_solutions,
        :franceschi18a_supervised_learning,
        :franceschi18a_that,
        :franceschi18a_those,
        :franceschi18a_training,
        :franceschi18a_variables,
        :franceschi18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:gehring17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :gehring17a_accuracy,
        :gehring17a_approach,
        :gehring17a_architecture,
        :gehring17a_computations,
        :gehring17a_convolutional_neural_networks,
        :gehring17a_decoder,
        :gehring17a_french,
        :gehring17a_gradient,
        :gehring17a_hardware,
        :gehring17a_input,
        :gehring17a_lstm,
        :gehring17a_magnitude,
        :gehring17a_maps,
        :gehring17a_models,
        :gehring17a_module,
        :gehring17a_non-linearities,
        :gehring17a_optimization,
        :gehring17a_recurrent_neural_networks,
        :gehring17a_training,
        :gehring17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:githubrepo a owl:DatatypeProperty .

:greydanus18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :greydanus18a_agents,
        :greydanus18a_decisions,
        :greydanus18a_deep_reinforcement_learning,
        :greydanus18a_deep_rl,
        :greydanus18a_environments,
        :greydanus18a_information,
        :greydanus18a_learns,
        :greydanus18a_maps,
        :greydanus18a_maximizing,
        :greydanus18a_method,
        :greydanus18a_our_method,
        :greydanus18a_policy,
        :greydanus18a_results,
        :greydanus18a_rewards,
        :greydanus18a_rl,
        :greydanus18a_strategies,
        :greydanus18a_that,
        :greydanus18a_these,
        :greydanus18a_they,
        :greydanus18a_this,
        :greydanus18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:guo17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :guo17a_applications,
        :guo17a_architectures,
        :guo17a_classification,
        :guo17a_datasets,
        :guo17a_document_classification,
        :guo17a_estimates,
        :guo17a_experiments,
        :guo17a_factors,
        :guo17a_image,
        :guo17a_methods,
        :guo17a_models,
        :guo17a_neural_network,
        :guo17a_neural_networks,
        :guo17a_post-processing,
        :guo17a_predicting,
        :guo17a_predictions,
        :guo17a_problem,
        :guo17a_state,
        :guo17a_that,
        :guo17a_those,
        :guo17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:hadjeres17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :hadjeres17a_approaches,
        :hadjeres17a_automatic,
        :hadjeres17a_constraints,
        :hadjeres17a_data,
        :hadjeres17a_editor,
        :hadjeres17a_generation,
        :hadjeres17a_graphical_model,
        :hadjeres17a_interaction,
        :hadjeres17a_model,
        :hadjeres17a_modeling,
        :hadjeres17a_music,
        :hadjeres17a_representation,
        :hadjeres17a_rhythms,
        :hadjeres17a_sampling,
        :hadjeres17a_that ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:hasAbstractText a owl:DatatypeProperty ;
    rdfs:subPropertyOf :hasText .

:hasBodyText a owl:DatatypeProperty ;
    rdfs:subPropertyOf :hasText .

:hasCSOEquivalent a owl:ObjectProperty ;
    rdfs:comment "This property provides the mapping between DeepSciKG and the CSO terms wherever found." .

:hasCaptionText a owl:DatatypeProperty ;
    rdfs:domain :CaptionText ;
    rdfs:subPropertyOf :hasText .

:hasComponent a owl:ObjectProperty .

:hasDataFlow a owl:DatatypeProperty .

:hasEntity a owl:ObjectProperty ;
    rdfs:domain :Modality ;
    rdfs:range :TextEntity .

:hasFigure a owl:ObjectProperty .

:hasFigureId a owl:DatatypeProperty ;
    rdfs:domain :ImageComponent .

:hasFile a owl:DatatypeProperty .

:hasFunction a owl:ObjectProperty ;
    rdfs:domain :app,
        :audio,
        :strings,
        :summary,
        :sysconfig,
        :tf ;
    rdfs:range :decode_wav,
        :encode_wav,
        :run .

:hasModality a owl:ObjectProperty ;
    rdfs:domain :Publication ;
    rdfs:range :Code,
        :Figure,
        :Modality,
        :Text .

:hasPublicationId a owl:DatatypeProperty ;
    rdfs:domain :Publication .

:hasRepository a owl:ObjectProperty ;
    rdfs:domain :Publication ;
    rdfs:range :Repository .

:hasTitle a owl:DatatypeProperty .

:hasTitleText a owl:DatatypeProperty ;
    rdfs:subPropertyOf :hasText .

:heinonen18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :heinonen18a_complex,
        :heinonen18a_continuous,
        :heinonen18a_dynamics,
        :heinonen18a_fields,
        :heinonen18a_formalism,
        :heinonen18a_functions,
        :heinonen18a_interactions,
        :heinonen18a_learn,
        :heinonen18a_model,
        :heinonen18a_nonparametric,
        :heinonen18a_observations,
        :heinonen18a_paradigm,
        :heinonen18a_prior_knowledge,
        :heinonen18a_sparse_data,
        :heinonen18a_state,
        :heinonen18a_system,
        :heinonen18a_systems,
        :heinonen18a_that,
        :heinonen18a_these,
        :heinonen18a_this,
        :heinonen18a_time,
        :heinonen18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:hu17e a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :hu17e_accuracy,
        :hu17e_algorithm,
        :hu17e_annotations,
        :hu17e_approximation,
        :hu17e_auto-encoders,
        :hu17e_classifiers,
        :hu17e_constraints,
        :hu17e_data,
        :hu17e_discrete,
        :hu17e_domain,
        :hu17e_experiments,
        :hu17e_generation,
        :hu17e_generative_model,
        :hu17e_interpretable,
        :hu17e_learns,
        :hu17e_model,
        :hu17e_modeling,
        :hu17e_representations,
        :hu17e_semantic,
        :hu17e_semantics,
        :hu17e_sentiment,
        :hu17e_structures,
        :hu17e_text,
        :hu17e_training,
        :hu17e_validate ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:huang18d a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :huang18d_approaches,
        :huang18d_autoencoders,
        :huang18d_autoregressive_models,
        :huang18d_continuous,
        :huang18d_density_estimation,
        :huang18d_flows,
        :huang18d_mnist,
        :huang18d_neural_networks,
        :huang18d_probability_distributions,
        :huang18d_results,
        :huang18d_state,
        :huang18d_target,
        :huang18d_tasks,
        :huang18d_that,
        :huang18d_them,
        :huang18d_these,
        :huang18d_time,
        :huang18d_transformations,
        :huang18d_wavenet ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:huo18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :huo18a_accuracy,
        :huo18a_algorithm,
        :huo18a_algorithms,
        :huo18a_backpropagation,
        :huo18a_backpropagation_algorithm,
        :huo18a_convergence,
        :huo18a_convex_problem,
        :huo18a_datasets,
        :huo18a_deep_convolutional_neural_networks,
        :huo18a_deep_learning,
        :huo18a_experimental_results,
        :huo18a_experiments,
        :huo18a_gradients,
        :huo18a_input,
        :huo18a_layers,
        :huo18a_method,
        :huo18a_methods,
        :huo18a_modules,
        :huo18a_network,
        :huo18a_networks,
        :huo18a_neural_networks,
        :huo18a_optimization,
        :huo18a_our_method,
        :huo18a_resources,
        :huo18a_that,
        :huo18a_theoretical_analysis,
        :huo18a_this,
        :huo18a_training,
        :huo18a_two,
        :huo18a_way,
        :huo18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:ilse18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :ilse18a_application,
        :ilse18a_approach,
        :ilse18a_attention_mechanism,
        :ilse18a_datasets,
        :ilse18a_instances,
        :ilse18a_interpretability,
        :ilse18a_methods,
        :ilse18a_mnist,
        :ilse18a_neural_network,
        :ilse18a_neural_networks,
        :ilse18a_other,
        :ilse18a_problem,
        :ilse18a_state,
        :ilse18a_supervised_learning,
        :ilse18a_that,
        :ilse18a_this,
        :ilse18a_two,
        :ilse18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:ilyas18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :ilyas18a_adversarial_examples,
        :ilyas18a_classifier,
        :ilyas18a_classifiers,
        :ilyas18a_imagenet,
        :ilyas18a_information,
        :ilyas18a_inputs,
        :ilyas18a_methods,
        :ilyas18a_model,
        :ilyas18a_models,
        :ilyas18a_network,
        :ilyas18a_neural_network,
        :ilyas18a_other,
        :ilyas18a_our_methods,
        :ilyas18a_systems,
        :ilyas18a_that,
        :ilyas18a_these,
        :ilyas18a_vision ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:isDeprecated a owl:DatatypeProperty .

:isExperimental a owl:DatatypeProperty .

:jaderberg17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :jaderberg17a_computation,
        :jaderberg17a_data,
        :jaderberg17a_framework,
        :jaderberg17a_gradient,
        :jaderberg17a_gradients,
        :jaderberg17a_graph,
        :jaderberg17a_hierarchical,
        :jaderberg17a_information,
        :jaderberg17a_inputs,
        :jaderberg17a_interfaces,
        :jaderberg17a_layers,
        :jaderberg17a_learn,
        :jaderberg17a_model,
        :jaderberg17a_models,
        :jaderberg17a_modules,
        :jaderberg17a_network,
        :jaderberg17a_networks,
        :jaderberg17a_neural_networks,
        :jaderberg17a_one,
        :jaderberg17a_predicting,
        :jaderberg17a_recurrent_neural_networks,
        :jaderberg17a_results,
        :jaderberg17a_rnn,
        :jaderberg17a_rnns,
        :jaderberg17a_signal,
        :jaderberg17a_subgraphs,
        :jaderberg17a_system,
        :jaderberg17a_that,
        :jaderberg17a_them,
        :jaderberg17a_they,
        :jaderberg17a_this,
        :jaderberg17a_time,
        :jaderberg17a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:jeong18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :jeong18a_accuracy,
        :jeong18a_algorithms,
        :jeong18a_cifar-100,
        :jeong18a_data,
        :jeong18a_datasets,
        :jeong18a_efficiency,
        :jeong18a_end-to-end,
        :jeong18a_flow,
        :jeong18a_imagenet,
        :jeong18a_learning_methods,
        :jeong18a_metric,
        :jeong18a_metrics,
        :jeong18a_neural_networks,
        :jeong18a_per,
        :jeong18a_polynomial_time,
        :jeong18a_problem,
        :jeong18a_representation,
        :jeong18a_representation_learning,
        :jeong18a_representations,
        :jeong18a_results,
        :jeong18a_search,
        :jeong18a_source_code,
        :jeong18a_state,
        :jeong18a_that,
        :jeong18a_this,
        :jeong18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:johnson18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :johnson18a_adversarial,
        :johnson18a_data,
        :johnson18a_divergence,
        :johnson18a_effectiveness,
        :johnson18a_experiments,
        :johnson18a_first,
        :johnson18a_formulation,
        :johnson18a_gan,
        :johnson18a_generation,
        :johnson18a_gradient,
        :johnson18a_image,
        :johnson18a_kl,
        :johnson18a_method,
        :johnson18a_methods,
        :johnson18a_real_data,
        :johnson18a_that,
        :johnson18a_theory,
        :johnson18a_this,
        :johnson18a_viewpoint,
        :johnson18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:kalchbrenner17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kalchbrenner17a_action,
        :kalchbrenner17a_approaches,
        :kalchbrenner17a_architecture,
        :kalchbrenner17a_color,
        :kalchbrenner17a_discrete,
        :kalchbrenner17a_estimates,
        :kalchbrenner17a_model,
        :kalchbrenner17a_motion,
        :kalchbrenner17a_moving_mnist,
        :kalchbrenner17a_network,
        :kalchbrenner17a_objects,
        :kalchbrenner17a_state,
        :kalchbrenner17a_structure,
        :kalchbrenner17a_tensors,
        :kalchbrenner17a_that,
        :kalchbrenner17a_time,
        :kalchbrenner17a_video,
        :kalchbrenner17a_videos ;
    :hasFigure :kalchbrenner17a-Figure2-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:kalchbrenner17a-Figure2-1_0 :partOf :kalchbrenner17a-Figure2-1 .

:kalchbrenner17a-Figure2-1_1 :partOf :kalchbrenner17a-Figure2-1 .

:kalchbrenner17a-Figure2-1_2 :partOf :kalchbrenner17a-Figure2-1 .

:kalchbrenner17a-Figure2-1_3 :partOf :kalchbrenner17a-Figure2-1 .

:kalchbrenner17a-Figure2-1_4 :partOf :kalchbrenner17a-Figure2-1 .

:kalchbrenner17a-Figure2-1_Comp4 a :ConvBlock .

:kalchbrenner18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kalchbrenner18a_audio,
        :kalchbrenner18a_data,
        :kalchbrenner18a_efficiency,
        :kalchbrenner18a_first,
        :kalchbrenner18a_generation,
        :kalchbrenner18a_method,
        :kalchbrenner18a_model,
        :kalchbrenner18a_models,
        :kalchbrenner18a_network,
        :kalchbrenner18a_networks,
        :kalchbrenner18a_one,
        :kalchbrenner18a_parameters,
        :kalchbrenner18a_per,
        :kalchbrenner18a_problem,
        :kalchbrenner18a_pruning,
        :kalchbrenner18a_quality,
        :kalchbrenner18a_recurrent_neural_network,
        :kalchbrenner18a_results,
        :kalchbrenner18a_sampling,
        :kalchbrenner18a_scheme,
        :kalchbrenner18a_sparsity,
        :kalchbrenner18a_state,
        :kalchbrenner18a_technique,
        :kalchbrenner18a_techniques,
        :kalchbrenner18a_text,
        :kalchbrenner18a_textual_domains,
        :kalchbrenner18a_that,
        :kalchbrenner18a_this,
        :kalchbrenner18a_time,
        :kalchbrenner18a_wavenet,
        :kalchbrenner18a_we,
        :kalchbrenner18a_weights ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:kim17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kim17a_cross-domain,
        :kim17a_data,
        :kim17a_domain,
        :kim17a_domains,
        :kim17a_face,
        :kim17a_generative_adversarial_networks,
        :kim17a_learns,
        :kim17a_method,
        :kim17a_network,
        :kim17a_one,
        :kim17a_orientation,
        :kim17a_relations,
        :kim17a_supervision,
        :kim17a_task,
        :kim17a_that,
        :kim17a_them,
        :kim17a_transfers,
        :kim17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:kim17b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kim17b_accuracies,
        :kim17b_cifar-100,
        :kim17b_computations,
        :kim17b_datasets,
        :kim17b_deep_networks,
        :kim17b_deep_neural_network,
        :kim17b_feature,
        :kim17b_features,
        :kim17b_image_classification,
        :kim17b_learns,
        :kim17b_matrices,
        :kim17b_model,
        :kim17b_models,
        :kim17b_network,
        :kim17b_networks,
        :kim17b_our_method,
        :kim17b_parameters,
        :kim17b_resnet,
        :kim17b_structured,
        :kim17b_that,
        :kim17b_time,
        :kim17b_two,
        :kim17b_validate,
        :kim17b_we,
        :kim17b_weights ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:kim18b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kim18b_data,
        :kim18b_factors,
        :kim18b_method,
        :kim18b_metric,
        :kim18b_problem,
        :kim18b_problems,
        :kim18b_quality,
        :kim18b_representations,
        :kim18b_that,
        :kim18b_them,
        :kim18b_unsupervised_learning,
        :kim18b_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:kipf18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kipf18a_complex,
        :kipf18a_components,
        :kipf18a_data,
        :kipf18a_dynamics,
        :kipf18a_encoder,
        :kipf18a_experiments,
        :kipf18a_graph,
        :kipf18a_interaction,
        :kipf18a_interactions,
        :kipf18a_interpretable,
        :kipf18a_learns,
        :kipf18a_model,
        :kipf18a_motion,
        :kipf18a_neural_networks,
        :kipf18a_structure,
        :kipf18a_system,
        :kipf18a_systems,
        :kipf18a_that,
        :kipf18a_this,
        :kipf18a_tracking,
        :kipf18a_unsupervised_manner,
        :kipf18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:kusner17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :kusner17a_audio,
        :kusner17a_autoencoder,
        :kusner17a_continuous,
        :kusner17a_data,
        :kusner17a_discrete,
        :kusner17a_discrete_data,
        :kusner17a_effectiveness,
        :kusner17a_encodes,
        :kusner17a_expressions,
        :kusner17a_generation,
        :kusner17a_generative_models,
        :kusner17a_grammar,
        :kusner17a_learns,
        :kusner17a_methods,
        :kusner17a_model,
        :kusner17a_modeling,
        :kusner17a_models,
        :kusner17a_natural_images,
        :kusner17a_optimization,
        :kusner17a_outputs,
        :kusner17a_parse_tree,
        :kusner17a_parse_trees,
        :kusner17a_poses,
        :kusner17a_representations,
        :kusner17a_state,
        :kusner17a_structures,
        :kusner17a_that,
        :kusner17a_these,
        :kusner17a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:law17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :law17a_approach,
        :law17a_approaches,
        :law17a_clusters,
        :law17a_data,
        :law17a_data_representation,
        :law17a_datasets,
        :law17a_examples,
        :law17a_factors,
        :law17a_gradient,
        :law17a_gradients,
        :law17a_grouping,
        :law17a_learn,
        :law17a_loss_function,
        :law17a_method,
        :law17a_methods,
        :law17a_metric,
        :law17a_problem,
        :law17a_quality,
        :law17a_real-world_datasets,
        :law17a_representation,
        :law17a_spectral_clustering,
        :law17a_state,
        :law17a_task,
        :law17a_that,
        :law17a_these,
        :law17a_they,
        :law17a_this,
        :law17a_training,
        :law17a_two,
        :law17a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:le18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :le18a_approach,
        :le18a_benchmarks,
        :le18a_decision-making,
        :le18a_feedback,
        :le18a_framework,
        :le18a_hierarchical,
        :le18a_hierarchical_structure,
        :le18a_imitation_learning,
        :le18a_interaction,
        :le18a_learn,
        :le18a_policies,
        :le18a_pose,
        :le18a_problem,
        :le18a_problems,
        :le18a_reinforcement_learning,
        <https://github.com/deepcurator/DCC/le18a_reinforcement_learning_(rl)>,
        :le18a_rewards,
        :le18a_rl,
        :le18a_that,
        :le18a_time,
        :le18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:lee17b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :lee17b_applications,
        :lee17b_architecture,
        :lee17b_boosting,
        :lee17b_classification_task,
        :lee17b_components,
        :lee17b_deep_neural_networks,
        :lee17b_ensemble,
        :lee17b_ensemble_methods,
        :lee17b_ensembles,
        :lee17b_error_rates,
        :lee17b_experiments,
        :lee17b_feature,
        :lee17b_ie,
        :lee17b_image_classification,
        :lee17b_machine_learning,
        :lee17b_methods,
        :lee17b_models,
        :lee17b_networks,
        :lee17b_scheme,
        :lee17b_segmentation,
        :lee17b_svhn,
        :lee17b_techniques,
        :lee17b_they,
        :lee17b_this,
        :lee17b_training_method,
        :lee17b_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:lee18c a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :lee18c_agents,
        :lee18c_architecture,
        :lee18c_convolutional_networks,
        :lee18c_convolutions,
        :lee18c_differentiability,
        :lee18c_effectiveness,
        :lee18c_end-to-end,
        :lee18c_environment,
        :lee18c_first,
        :lee18c_generalization,
        :lee18c_hyperparameter,
        :lee18c_metrics,
        :lee18c_modules,
        :lee18c_navigation,
        :lee18c_network,
        :lee18c_networks,
        :lee18c_optimization,
        :lee18c_optimization_problems,
        :lee18c_other,
        :lee18c_planning,
        :lee18c_pooling,
        :lee18c_rgb_images,
        :lee18c_sizes,
        :lee18c_that,
        :lee18c_they,
        :lee18c_this,
        :lee18c_training,
        :lee18c_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:lehtinen18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :lehtinen18a_data,
        :lehtinen18a_examples,
        :lehtinen18a_image,
        :lehtinen18a_images,
        :lehtinen18a_learn,
        :lehtinen18a_learns,
        :lehtinen18a_machine_learning,
        :lehtinen18a_map,
        :lehtinen18a_model,
        :lehtinen18a_models,
        :lehtinen18a_mri,
        :lehtinen18a_noise,
        :lehtinen18a_observations,
        :lehtinen18a_priors,
        :lehtinen18a_signal,
        :lehtinen18a_signals,
        :lehtinen18a_that,
        :lehtinen18a_training,
        :lehtinen18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:li17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :li17a_accuracy,
        :li17a_adversarial,
        :li17a_alternative,
        :li17a_applications,
        :li17a_approximating,
        :li17a_approximations,
        :li17a_data,
        :li17a_deep_learning_models,
        :li17a_divergence,
        :li17a_divergences,
        :li17a_estimates,
        :li17a_images,
        :li17a_kl,
        :li17a_model,
        :li17a_models,
        :li17a_networks,
        :li17a_technique,
        :li17a_techniques,
        :li17a_that,
        :li17a_these,
        :li17a_uncertainty,
        :li17a_variational_inference,
        :li17a_vision ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:li18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :li18a_baseline,
        :li18a_bias,
        :li18a_convolutional_networks,
        :li18a_data,
        :li18a_features,
        :li18a_finetuning,
        :li18a_mechanism,
        :li18a_model,
        :li18a_penalty,
        :li18a_regularization,
        :li18a_schemes,
        :li18a_solution,
        :li18a_target,
        :li18a_task,
        :li18a_tasks,
        :li18a_that,
        :li18a_this,
        :li18a_training,
        :li18a_transfer_learning,
        :li18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:li18c a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :li18c_approximation,
        :li18c_baseline,
        :li18c_compression,
        :li18c_computations,
        :li18c_flow,
        :li18c_generalization,
        :li18c_implementation,
        :li18c_information,
        :li18c_inputs,
        :li18c_interpretable,
        :li18c_lstm,
        :li18c_memory,
        :li18c_model,
        :li18c_modeling,
        :li18c_models,
        :li18c_one,
        :li18c_outputs,
        :li18c_precision,
        :li18c_rank,
        :li18c_results,
        :li18c_short-term,
        :li18c_state,
        :li18c_structures,
        :li18c_studies,
        :li18c_that,
        :li18c_this,
        :li18c_training,
        :li18c_way,
        :li18c_ways,
        :li18c_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:liu17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :liu17a_approaches,
        :liu17a_data,
        :liu17a_embedding_methods,
        :liu17a_embeddings,
        :liu17a_family,
        :liu17a_interaction,
        :liu17a_language,
        :liu17a_learn,
        :liu17a_method,
        :liu17a_model,
        :liu17a_observations,
        :liu17a_other,
        :liu17a_representation,
        :liu17a_sparse_data,
        :liu17a_technique,
        :liu17a_term_matrix,
        :liu17a_that,
        :liu17a_this,
        :liu17a_tool,
        :liu17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:ma18d a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :ma18d_approach,
        :ma18d_data,
        :ma18d_datasets,
        :ma18d_deep_neural_networks,
        :ma18d_dnns,
        :ma18d_generalization,
        :ma18d_learn,
        :ma18d_loss_function,
        :ma18d_representation,
        :ma18d_strategy,
        :ma18d_subspace,
        :ma18d_subspaces,
        :ma18d_that,
        :ma18d_this,
        :ma18d_training,
        :ma18d_understanding,
        :ma18d_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:mescheder17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :mescheder17a_adversarial,
        :mescheder17a_approach,
        :mescheder17a_approaches,
        :mescheder17a_autoencoders,
        :mescheder17a_complex,
        :mescheder17a_data,
        :mescheder17a_gans,
        :mescheder17a_generative_adversarial_networks,
        :mescheder17a_generative_model,
        :mescheder17a_latent_variable_models,
        :mescheder17a_latent_variables,
        :mescheder17a_learn,
        :mescheder17a_maximum-likelihood,
        :mescheder17a_model,
        :mescheder17a_models,
        :mescheder17a_network,
        :mescheder17a_nonparametric,
        :mescheder17a_our_method,
        :mescheder17a_parameters,
        :mescheder17a_posterior,
        :mescheder17a_probability_distributions,
        :mescheder17a_quality,
        :mescheder17a_technique,
        :mescheder17a_that,
        :mescheder17a_this,
        :mescheder17a_training,
        :mescheder17a_two ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:mirhoseini17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :mirhoseini17a_approach,
        :mirhoseini17a_classification,
        :mirhoseini17a_device,
        :mirhoseini17a_devices,
        :mirhoseini17a_distributed,
        :mirhoseini17a_environment,
        :mirhoseini17a_graph,
        :mirhoseini17a_graphs,
        :mirhoseini17a_hardware,
        :mirhoseini17a_heuristics,
        :mirhoseini17a_imagenet,
        :mirhoseini17a_inception,
        :mirhoseini17a_language_modeling,
        :mirhoseini17a_learns,
        :mirhoseini17a_lstm,
        :mirhoseini17a_method,
        :mirhoseini17a_methods,
        :mirhoseini17a_model,
        :mirhoseini17a_models,
        :mirhoseini17a_neural_machine_translation,
        :mirhoseini17a_neural_networks,
        :mirhoseini17a_operations,
        :mirhoseini17a_our_method,
        :mirhoseini17a_parameters,
        :mirhoseini17a_reward,
        :mirhoseini17a_rnn,
        :mirhoseini17a_sequence-to-sequence,
        :mirhoseini17a_signal,
        :mirhoseini17a_tensorflow,
        :mirhoseini17a_that,
        :mirhoseini17a_these,
        :mirhoseini17a_this,
        :mirhoseini17a_time,
        :mirhoseini17a_training,
        :mirhoseini17a_we ;
    :hasFigure :mirhoseini17a-Figure1-1,
        :mirhoseini17a-Figure5-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:mirhoseini17a-Figure1-1_0 :partOf :mirhoseini17a-Figure1-1 .

:mirhoseini17a-Figure1-1_1 :partOf :mirhoseini17a-Figure1-1 .

:mirhoseini17a-Figure5-1_0 :partOf :mirhoseini17a-Figure5-1 .

:mirhoseini17a-Figure5-1_1 :partOf :mirhoseini17a-Figure5-1 .

:mirhoseini17a-Figure5-1_2 :partOf :mirhoseini17a-Figure5-1 .

:mirhoseini17a-Figure5-1_3 :partOf :mirhoseini17a-Figure5-1 .

:niculae18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :niculae18a_accuracy,
        :niculae18a_ambiguities,
        :niculae18a_backpropagation,
        :niculae18a_deep_neural_networks,
        :niculae18a_dependency_parsing,
        :niculae18a_gradient,
        :niculae18a_hidden_layers,
        :niculae18a_interpretability,
        :niculae18a_loss_function,
        :niculae18a_map,
        :niculae18a_method,
        :niculae18a_natural_language,
        :niculae18a_ones,
        :niculae18a_prediction,
        :niculae18a_problems,
        :niculae18a_structure,
        :niculae18a_structured,
        :niculae18a_structures,
        :niculae18a_systems,
        :niculae18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:nie18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :nie18a_backpropagation,
        :nie18a_cnns,
        <https://github.com/deepcurator/DCC/nie18a_convolutional_neural_networks_(cnns)>,
        :nie18a_decisions,
        :nie18a_experiments,
        :nie18a_image,
        :nie18a_interpretable,
        :nie18a_map,
        :nie18a_network,
        :nie18a_recovery,
        :nie18a_relu,
        :nie18a_that,
        :nie18a_theoretical_analysis,
        :nie18a_theory,
        :nie18a_this,
        :nie18a_two,
        :nie18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:oliva17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :oliva17a_alternatives,
        :oliva17a_applications,
        :oliva17a_architecture,
        :oliva17a_architectures,
        :oliva17a_data,
        :oliva17a_gru,
        :oliva17a_grus,
        :oliva17a_hyperparameters,
        :oliva17a_learn,
        :oliva17a_lstm,
        :oliva17a_lstms,
        :oliva17a_one,
        :oliva17a_parameters,
        :oliva17a_real-world_tasks,
        :oliva17a_recurrent_neural_network_architectures,
        :oliva17a_statistics,
        :oliva17a_tasks,
        :oliva17a_that ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:ostrovski17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :ostrovski17a_algorithm,
        :ostrovski17a_approach,
        :ostrovski17a_architectures,
        :ostrovski17a_assumptions,
        :ostrovski17a_first,
        :ostrovski17a_images,
        :ostrovski17a_model,
        :ostrovski17a_quality,
        :ostrovski17a_reinforcement_learning,
        :ostrovski17a_state,
        :ostrovski17a_that,
        :ostrovski17a_two,
        :ostrovski17a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:part-of a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:partOf a owl:ObjectProperty .

:pham18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :pham18a_approach,
        :pham18a_approaches,
        :pham18a_architecture,
        :pham18a_architectures,
        :pham18a_automatic,
        :pham18a_cifar-10,
        :pham18a_enas,
        :pham18a_entropy,
        :pham18a_gradient,
        :pham18a_graph,
        :pham18a_methods,
        :pham18a_minimize,
        :pham18a_model,
        :pham18a_model_design,
        :pham18a_models,
        :pham18a_neural_architecture_search,
        :pham18a_neural_network,
        :pham18a_parameters,
        :pham18a_penn_treebank,
        :pham18a_policy,
        :pham18a_processing,
        :pham18a_reward,
        :pham18a_search,
        :pham18a_state,
        :pham18a_that,
        :pham18a_training ;
    :hasFigure :pham18a-Figure3-1,
        :pham18a-Figure4-1,
        :pham18a-Figure5-1,
        :pham18a-Figure6-1,
        :pham18a-Figure7-1,
        :pham18a-Figure8-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:pham18a-Figure3-1_0 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_1 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_10 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_11 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_12 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_2 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_3 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_4 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_5 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_6 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_7 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_9 :partOf :pham18a-Figure3-1 .

:pham18a-Figure3-1_Comp0 a :ConvBlock .

:pham18a-Figure3-1_Comp1 a :ConvBlock .

:pham18a-Figure3-1_Comp11 a :ConvBlock .

:pham18a-Figure3-1_Comp2 a :RnnBlock .

:pham18a-Figure3-1_Comp5 a :RnnBlock .

:pham18a-Figure4-1_0 :partOf :pham18a-Figure4-1 .

:pham18a-Figure4-1_1 :partOf :pham18a-Figure4-1 .

:pham18a-Figure4-1_3 :partOf :pham18a-Figure4-1 .

:pham18a-Figure4-1_4 :partOf :pham18a-Figure4-1 .

:pham18a-Figure4-1_5 :partOf :pham18a-Figure4-1 .

:pham18a-Figure4-1_6 :partOf :pham18a-Figure4-1 .

:pham18a-Figure4-1_7 :partOf :pham18a-Figure4-1 .

:pham18a-Figure4-1_8 :partOf :pham18a-Figure4-1 .

:pham18a-Figure5-1_0 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_1 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_10 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_12 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_13 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_14 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_15 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_16 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_2 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_3 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_4 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_5 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_6 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_7 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_8 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_9 :partOf :pham18a-Figure5-1 .

:pham18a-Figure5-1_Comp2 a :NormBlock .

:pham18a-Figure5-1_Comp4 a :PoolingBlock .

:pham18a-Figure5-1_Comp6 a :NormBlock .

:pham18a-Figure5-1_Comp7 a :PoolingBlock .

:pham18a-Figure7-1_0 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_1 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_10 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_11 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_12 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_13 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_14 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_15 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_2 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_3 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_4 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_5 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_6 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_7 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_8 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_9 :partOf :pham18a-Figure7-1 .

:pham18a-Figure7-1_Comp1 a :PoolingBlock .

:pham18a-Figure7-1_Comp10 a :PoolingBlock .

:pham18a-Figure7-1_Comp4 a :LossBlock .

:pham18a-Figure7-1_Comp5 a :PoolingBlock .

:pham18a-Figure8-1_0 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_1 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_10 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_12 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_13 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_14 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_16 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_19 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_2 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_5 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_9 :partOf :pham18a-Figure8-1 .

:pham18a-Figure8-1_Comp13 a :PoolingBlock .

:pinto17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :pinto17a_adversarial,
        :pinto17a_approaches,
        :pinto17a_baseline,
        :pinto17a_computation,
        :pinto17a_conditions,
        :pinto17a_data,
        :pinto17a_environments,
        :pinto17a_errors,
        :pinto17a_experiments,
        :pinto17a_field,
        :pinto17a_generalization,
        :pinto17a_learns,
        :pinto17a_methods,
        :pinto17a_modeling,
        :pinto17a_neural_networks,
        :pinto17a_objective_function,
        :pinto17a_our_method,
        :pinto17a_policy,
        :pinto17a_reinforcement_learning,
        <https://github.com/deepcurator/DCC/pinto17a_reinforcement_learning_(rl)>,
        :pinto17a_rl,
        :pinto17a_system,
        :pinto17a_that,
        :pinto17a_training,
        :pinto17a_transfer,
        :pinto17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:platform a owl:DatatypeProperty .

:pritzel17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :pritzel17a_agents,
        :pritzel17a_data,
        :pritzel17a_deep_reinforcement_learning,
        :pritzel17a_environments,
        :pritzel17a_estimates,
        :pritzel17a_function,
        :pritzel17a_learns,
        :pritzel17a_magnitudes,
        :pritzel17a_methods,
        :pritzel17a_other,
        :pritzel17a_reinforcement_learning_methods,
        :pritzel17a_representation,
        :pritzel17a_representations,
        :pritzel17a_state,
        :pritzel17a_that,
        :pritzel17a_them ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:raffel17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :raffel17a_alignment,
        :raffel17a_alignments,
        :raffel17a_approach,
        :raffel17a_attention_mechanism,
        :raffel17a_attention_mechanisms,
        :raffel17a_end-to-end,
        :raffel17a_input,
        :raffel17a_machine_translation,
        :raffel17a_method,
        :raffel17a_models,
        :raffel17a_neural_network_models,
        :raffel17a_problems,
        :raffel17a_quadratic_time,
        :raffel17a_results,
        :raffel17a_speech_recognition,
        :raffel17a_summarization,
        :raffel17a_that,
        :raffel17a_time,
        :raffel17a_validate,
        :raffel17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:ren18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :ren18a_algorithms,
        :ren18a_class_imbalance,
        :ren18a_complex,
        :ren18a_data,
        :ren18a_functions,
        :ren18a_gradient,
        :ren18a_gradient_descent,
        :ren18a_hyperparameter,
        :ren18a_hyperparameters,
        :ren18a_input,
        :ren18a_learning_algorithm,
        :ren18a_learns,
        :ren18a_method,
        :ren18a_methods,
        :ren18a_minimize,
        :ren18a_modeling,
        :ren18a_network,
        :ren18a_neural_networks,
        :ren18a_noises,
        :ren18a_our_method,
        :ren18a_problems,
        :ren18a_regularization,
        :ren18a_regularizers,
        :ren18a_solutions,
        :ren18a_supervised_learning,
        :ren18a_tasks,
        :ren18a_that,
        :ren18a_these,
        :ren18a_they,
        :ren18a_this,
        :ren18a_tools,
        :ren18a_training,
        :ren18a_training_examples,
        :ren18a_we,
        :ren18a_weights ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:ruff18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :ruff18a_adaptation,
        :ruff18a_adversarial_examples,
        :ruff18a_approaches,
        :ruff18a_cifar-10,
        :ruff18a_compression,
        :ruff18a_data,
        :ruff18a_datasets,
        :ruff18a_deep_learning,
        :ruff18a_detection_method,
        :ruff18a_effectiveness,
        :ruff18a_generative_models,
        :ruff18a_image,
        :ruff18a_machine_learning,
        :ruff18a_mnist,
        :ruff18a_networks,
        :ruff18a_neural_network,
        :ruff18a_other,
        :ruff18a_our_method,
        :ruff18a_problems,
        :ruff18a_procedure,
        :ruff18a_properties,
        :ruff18a_task,
        :ruff18a_that,
        :ruff18a_they,
        :ruff18a_this,
        :ruff18a_training,
        :ruff18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:sameAs a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:seward18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :seward18a_cifar-10,
        :seward18a_derivation,
        :seward18a_distance,
        :seward18a_divergence,
        :seward18a_examples,
        :seward18a_first,
        :seward18a_framework,
        :seward18a_gan,
        :seward18a_generation,
        :seward18a_image,
        :seward18a_information,
        :seward18a_language,
        :seward18a_method,
        :seward18a_one,
        :seward18a_our_method,
        :seward18a_parameters,
        :seward18a_state,
        :seward18a_task,
        :seward18a_that,
        :seward18a_these,
        :seward18a_they,
        :seward18a_this,
        :seward18a_those,
        :seward18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:sharchilev18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :sharchilev18a_approach,
        :sharchilev18a_approaches,
        :sharchilev18a_approximations,
        :sharchilev18a_baselines,
        :sharchilev18a_computational_complexity,
        :sharchilev18a_computational_efficiency,
        :sharchilev18a_decision_trees,
        :sharchilev18a_ensemble,
        :sharchilev18a_ensembles,
        :sharchilev18a_framework,
        :sharchilev18a_gradient,
        :sharchilev18a_model,
        :sharchilev18a_models,
        :sharchilev18a_one,
        :sharchilev18a_our_method,
        :sharchilev18a_parametric_models,
        :sharchilev18a_predictions,
        :sharchilev18a_problem,
        :sharchilev18a_quality,
        :sharchilev18a_scheme,
        :sharchilev18a_that,
        :sharchilev18a_this,
        :sharchilev18a_training,
        :sharchilev18a_tree,
        :sharchilev18a_tree_structures,
        :sharchilev18a_way,
        :sharchilev18a_ways,
        :sharchilev18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:shi18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :shi18a_applications,
        :shi18a_approach,
        :shi18a_bias,
        :shi18a_effectiveness,
        :shi18a_error_bound,
        :shi18a_estimates,
        :shi18a_extension,
        :shi18a_function,
        :shi18a_gradient,
        :shi18a_method,
        :shi18a_operators,
        :shi18a_our_method,
        :shi18a_pca,
        :shi18a_results,
        :shi18a_spectral_decomposition,
        :shi18a_that,
        :shi18a_this,
        :shi18a_variance,
        :shi18a_variational_inference,
        :shi18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:shu17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :shu17a_autoencoder,
        :shu17a_benchmarks,
        :shu17a_deep_generative_models,
        :shu17a_density_estimation,
        :shu17a_framework,
        :shu17a_generative_model,
        :shu17a_input,
        :shu17a_mechanism,
        :shu17a_mnist,
        :shu17a_models,
        :shu17a_overfitting,
        :shu17a_prediction,
        :shu17a_procedure,
        :shu17a_results,
        :shu17a_stochastic_variables,
        :shu17a_supervised_regime,
        :shu17a_svhn,
        :shu17a_target,
        :shu17a_task,
        :shu17a_that,
        :shu17a_training,
        :shu17a_training_method,
        :shu17a_unlabeled_data,
        :shu17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:shyam17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :shyam17a_approximating,
        :shyam17a_classification,
        :shyam17a_comparators,
        :shyam17a_dynamic_representation,
        :shyam17a_error_rate,
        :shyam17a_first,
        :shyam17a_information,
        :shyam17a_model,
        :shyam17a_models,
        :shyam17a_objects,
        :shyam17a_observations,
        :shyam17a_one,
        :shyam17a_representations,
        :shyam17a_state,
        :shyam17a_task,
        :shyam17a_that,
        :shyam17a_them,
        :shyam17a_this,
        :shyam17a_way,
        :shyam17a_we ;
    :hasFigure :shyam17a-Figure1-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2017 .

:shyam17a-Figure1-1_1 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_10 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_11 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_12 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_2 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_4 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_5 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_6 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_8 :partOf :shyam17a-Figure1-1 .

:shyam17a-Figure1-1_Comp2 a :RnnBlock .

:silver17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :silver17a_architecture,
        :silver17a_deep_neural_network_architectures,
        :silver17a_fully_model,
        :silver17a_function,
        :silver17a_learn,
        :silver17a_models,
        :silver17a_planning,
        :silver17a_predictions,
        :silver17a_reward,
        :silver17a_rewards,
        :silver17a_that,
        :silver17a_these,
        :silver17a_this,
        :silver17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:song18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :song18a_convergence,
        :song18a_deep_neural_network,
        :song18a_gradient,
        :song18a_implementations,
        :song18a_method,
        :song18a_model,
        :song18a_optimization,
        :song18a_policy,
        :song18a_properties,
        :song18a_reinforcement_learning,
        :song18a_sizes,
        :song18a_solution,
        :song18a_techniques,
        :song18a_that,
        :song18a_they,
        :song18a_this,
        :song18a_training,
        :song18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:suganuma18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :suganuma18a_adversarial,
        :suganuma18a_algorithm,
        :suganuma18a_approach,
        :suganuma18a_architectures,
        :suganuma18a_autoencoders,
        :suganuma18a_caes,
        :suganuma18a_components,
        :suganuma18a_deep_neural_networks,
        :suganuma18a_experimental_results,
        :suganuma18a_former,
        :suganuma18a_image_restoration,
        :suganuma18a_images,
        :suganuma18a_layers,
        :suganuma18a_loss_functions,
        :suganuma18a_methods,
        :suganuma18a_network,
        :suganuma18a_noise,
        :suganuma18a_search,
        :suganuma18a_signal,
        :suganuma18a_skip_connections,
        :suganuma18a_state,
        :suganuma18a_studies,
        :suganuma18a_svhn,
        :suganuma18a_tasks,
        :suganuma18a_that,
        :suganuma18a_they,
        :suganuma18a_this,
        :suganuma18a_training,
        :suganuma18a_training_methods,
        :suganuma18a_we ;
    :hasFigure :suganuma18a-Figure1-1,
        :suganuma18a-Figure5-1 ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:suganuma18a-Figure1-1_0 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_1 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_13 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_17 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_18 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_2 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_5 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_8 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_9 :partOf :suganuma18a-Figure1-1 .

:suganuma18a-Figure1-1_Comp0 a :ConvBlock .

:suganuma18a-Figure1-1_Comp1 a :OutputBlock .

:suganuma18a-Figure1-1_Comp2 a :NormBlock .

:suganuma18a-Figure5-1_0 :partOf :suganuma18a-Figure5-1 .

:suganuma18a-Figure5-1_1 :partOf :suganuma18a-Figure5-1 .

:suganuma18a-Figure5-1_Comp1 a :ConvBlock .

:sun18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :sun18a_end-to-end,
        :sun18a_learn,
        :sun18a_logic,
        :sun18a_model,
        :sun18a_module,
        :sun18a_network,
        :sun18a_program,
        :sun18a_programs,
        :sun18a_representations,
        :sun18a_task,
        :sun18a_that,
        :sun18a_this,
        :sun18a_training,
        :sun18a_videos,
        :sun18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:sun18e a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :sun18e_architecture,
        :sun18e_automatic,
        :sun18e_compositional,
        :sun18e_end-to-end,
        :sun18e_family,
        :sun18e_generalization,
        :sun18e_kernels,
        :sun18e_network,
        :sun18e_neural_network,
        :sun18e_optimization,
        :sun18e_properties,
        :sun18e_rules,
        :sun18e_structure,
        :sun18e_structures,
        :sun18e_tasks,
        :sun18e_texture,
        :sun18e_that,
        :sun18e_this,
        :sun18e_those,
        :sun18e_time_series,
        :sun18e_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:sundararajan17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :sundararajan17a_applications,
        :sundararajan17a_approach,
        :sundararajan17a_baseline,
        :sundararajan17a_deep_networks,
        :sundararajan17a_errors,
        :sundararajan17a_extraction,
        :sundararajan17a_features,
        :sundararajan17a_function,
        :sundararajan17a_gradient,
        :sundararajan17a_gradients,
        :sundararajan17a_image,
        :sundararajan17a_images,
        :sundararajan17a_imaging,
        :sundararajan17a_implementation,
        :sundararajan17a_input,
        :sundararajan17a_machine_learning,
        :sundararajan17a_method,
        :sundararajan17a_methods,
        :sundararajan17a_model,
        :sundararajan17a_models,
        :sundararajan17a_network,
        :sundararajan17a_networks,
        :sundararajan17a_object_recognition,
        :sundararajan17a_one,
        :sundararajan17a_original_network,
        :sundararajan17a_other,
        :sundararajan17a_prediction,
        :sundararajan17a_problem,
        :sundararajan17a_programs,
        :sundararajan17a_results,
        :sundararajan17a_rule,
        :sundararajan17a_rules,
        :sundararajan17a_system,
        :sundararajan17a_technique,
        :sundararajan17a_text_models,
        :sundararajan17a_text_processing,
        :sundararajan17a_that,
        :sundararajan17a_these,
        :sundararajan17a_they,
        :sundararajan17a_this,
        :sundararajan17a_those,
        :sundararajan17a_two,
        :sundararajan17a_understanding,
        :sundararajan17a_way,
        :sundararajan17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:tansey18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :tansey18a_benchmarks,
        :tansey18a_deep_neural_network_prior,
        :tansey18a_experiments,
        :tansey18a_features,
        :tansey18a_learns,
        :tansey18a_method,
        :tansey18a_methods,
        :tansey18a_model,
        :tansey18a_per,
        :tansey18a_predictive_models,
        :tansey18a_results,
        :tansey18a_scale,
        :tansey18a_significance,
        :tansey18a_state,
        :tansey18a_studies,
        :tansey18a_that,
        :tansey18a_two,
        :tansey18a_variables ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:trinh18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :trinh18a_approach,
        :trinh18a_approaches,
        :trinh18a_backpropagation,
        :trinh18a_baselines,
        :trinh18a_document_classification,
        :trinh18a_efficiency,
        :trinh18a_events,
        :trinh18a_image_classification,
        :trinh18a_long-term,
        :trinh18a_method,
        :trinh18a_models,
        :trinh18a_optimization,
        :trinh18a_other,
        :trinh18a_our_method,
        :trinh18a_recurrent_neural_networks,
        :trinh18a_regularization,
        :trinh18a_resource,
        :trinh18a_results,
        :trinh18a_rnns,
        :trinh18a_scale,
        :trinh18a_that,
        :trinh18a_this,
        :trinh18a_time,
        :trinh18a_training ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:tucker18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :tucker18a_action,
        :tucker18a_baseline,
        :tucker18a_baselines,
        :tucker18a_bias,
        :tucker18a_decisions,
        :tucker18a_development,
        :tucker18a_domains,
        :tucker18a_efficiency,
        :tucker18a_estimates,
        :tucker18a_function,
        :tucker18a_gradient,
        :tucker18a_implementation,
        :tucker18a_improvement,
        :tucker18a_methods,
        :tucker18a_model-free,
        :tucker18a_policy,
        :tucker18a_prior,
        :tucker18a_reinforcement_learning_algorithms,
        :tucker18a_source_code,
        :tucker18a_state,
        :tucker18a_that,
        :tucker18a_these,
        :tucker18a_this,
        :tucker18a_variance,
        :tucker18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:used-for a owl:ObjectProperty ;
    rdfs:subPropertyOf owl:topObjectProperty .

:usedFor a owl:ObjectProperty .

:venueOfPublication a owl:ObjectProperty .

:villegas17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :villegas17a_actions,
        :villegas17a_analogy,
        :villegas17a_approach,
        :villegas17a_convolutional_neural_networks,
        :villegas17a_datasets,
        :villegas17a_encoder-decoder,
        :villegas17a_errors,
        :villegas17a_experiments,
        :villegas17a_first,
        :villegas17a_hierarchical,
        :villegas17a_input,
        :villegas17a_long-term,
        :villegas17a_lstm,
        :villegas17a_model,
        :villegas17a_penn_action,
        :villegas17a_prediction,
        :villegas17a_predictions,
        :villegas17a_results,
        :villegas17a_state,
        :villegas17a_structure,
        :villegas17a_task,
        :villegas17a_that,
        :villegas17a_video,
        :villegas17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:wang18b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wang18b_alternative,
        :wang18b_datasets,
        :wang18b_deep_predictive_models,
        :wang18b_dynamics,
        :wang18b_flows,
        :wang18b_gradient,
        :wang18b_inputs,
        :wang18b_long-term,
        :wang18b_lstm,
        :wang18b_lstms,
        :wang18b_model,
        :wang18b_modeling,
        :wang18b_motions,
        :wang18b_network,
        :wang18b_outputs,
        :wang18b_prediction,
        :wang18b_recurrent_network,
        :wang18b_results,
        :wang18b_short-term,
        :wang18b_state,
        :wang18b_structure,
        :wang18b_time,
        :wang18b_video,
        :wang18b_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:wang18h a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wang18h_audio,
        :wang18h_corpus,
        :wang18h_data,
        :wang18h_embeddings,
        :wang18h_interpretable,
        :wang18h_learn,
        :wang18h_model,
        :wang18h_noise,
        :wang18h_results,
        :wang18h_state,
        :wang18h_system,
        :wang18h_text,
        :wang18h_that,
        :wang18h_they,
        :wang18h_this,
        :wang18h_transfer,
        :wang18h_ways,
        :wang18h_we ;
    :hasFigure :wang18h-Figure1-1 ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:wang18h-Figure1-1_0 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_1 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_10 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_11 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_12 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_13 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_14 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_15 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_16 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_17 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_18 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_19 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_2 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_20 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_3 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_4 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_5 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_6 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_7 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_8 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_9 :partOf :wang18h-Figure1-1 .

:wang18h-Figure1-1_Comp0 a :InputBlock .

:wang18h-Figure1-1_Comp1 a :InputBlock .

:wang18h-Figure1-1_Comp14 a :InputBlock .

:wei18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wei18a_bound,
        :wei18a_domains,
        :wei18a_educational_psychology,
        :wei18a_framework,
        :wei18a_function,
        :wei18a_generalization,
        :wei18a_knowledge,
        :wei18a_learn,
        :wei18a_one,
        :wei18a_performance_improvement,
        :wei18a_reflection,
        :wei18a_state,
        :wei18a_target_domain,
        :wei18a_that,
        :wei18a_this,
        :wei18a_transfer,
        :wei18a_transfer_learning,
        :wei18a_transfer_learning_algorithms,
        :wei18a_two,
        :wei18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:wichers18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wichers18a_adversarial,
        :wichers18a_annotation,
        :wichers18a_decoder,
        :wichers18a_encoder,
        :wichers18a_encodes,
        :wichers18a_feature_space,
        :wichers18a_first,
        :wichers18a_generation,
        :wichers18a_hierarchical,
        :wichers18a_image,
        :wichers18a_input,
        :wichers18a_long-term,
        :wichers18a_method,
        :wichers18a_network,
        :wichers18a_prediction,
        :wichers18a_research,
        :wichers18a_results,
        :wichers18a_short-term,
        :wichers18a_state,
        :wichers18a_structures,
        :wichers18a_supervision,
        :wichers18a_that,
        :wichers18a_this,
        :wichers18a_training_method,
        :wichers18a_training_time,
        :wichers18a_video,
        :wichers18a_videos,
        :wichers18a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:wu18g a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wu18g_algorithms,
        :wu18g_baseline,
        :wu18g_bounds,
        :wu18g_data,
        :wu18g_divergence,
        :wu18g_end-to-end,
        :wu18g_feedbacks,
        :wu18g_generalization,
        :wu18g_importance_sampling,
        :wu18g_improvement,
        :wu18g_method,
        :wu18g_minimization,
        :wu18g_neural_network,
        :wu18g_policies,
        :wu18g_policy,
        :wu18g_prior_work,
        :wu18g_problems,
        :wu18g_regularization,
        :wu18g_results,
        :wu18g_task,
        :wu18g_that,
        :wu18g_this,
        :wu18g_training,
        :wu18g_training_algorithms,
        :wu18g_variance,
        :wu18g_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:wu18h a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :wu18h_accuracy,
        :wu18h_cnn,
        :wu18h_cnn_models,
        :wu18h_cnns,
        :wu18h_compression,
        :wu18h_compression_ratio,
        :wu18h_computation,
        :wu18h_convolutions,
        :wu18h_devices,
        :wu18h_estimation,
        :wu18h_hardware,
        :wu18h_implementations,
        :wu18h_k-means,
        :wu18h_k-means_clustering,
        :wu18h_metrics,
        :wu18h_regularization,
        :wu18h_resnet,
        :wu18h_results,
        :wu18h_scheme,
        :wu18h_this,
        :wu18h_tool,
        :wu18h_training,
        :wu18h_we,
        :wu18h_weights ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:xie18c a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :xie18c_alignment,
        :xie18c_classification_accuracy,
        :xie18c_datasets,
        :xie18c_domain_adaptation,
        :xie18c_domains,
        :xie18c_features,
        :xie18c_information,
        :xie18c_knowledge,
        :xie18c_learn,
        :xie18c_methods,
        :xie18c_model,
        :xie18c_network,
        :xie18c_prior,
        :xie18c_problem,
        :xie18c_results,
        :xie18c_semantic,
        :xie18c_semantic_information,
        :xie18c_semantic_representations,
        :xie18c_source_domain,
        :xie18c_state,
        :xie18c_statistics,
        :xie18c_target,
        :xie18c_target_domain,
        :xie18c_that,
        :xie18c_they,
        :xie18c_this,
        :xie18c_transfer,
        :xie18c_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2018 .

:yang17d a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :yang17d_architecture,
        :yang17d_autoencoders,
        :yang17d_baselines,
        :yang17d_cnn,
        :yang17d_datasets,
        :yang17d_decoder,
        :yang17d_decoders,
        :yang17d_encoder,
        :yang17d_experiments,
        :yang17d_first,
        :yang17d_information,
        :yang17d_language_modeling,
        :yang17d_language_models,
        :yang17d_lstm,
        :yang17d_modeling,
        :yang17d_tasks,
        :yang17d_text,
        :yang17d_that,
        :yang17d_this,
        :yang17d_two,
        :yang17d_we,
        :yang17d_words ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:yang18d a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :yang18d_agents,
        :yang18d_algorithms,
        :yang18d_approaches,
        :yang18d_convergence,
        :yang18d_dynamics,
        :yang18d_effectiveness,
        :yang18d_field,
        :yang18d_first,
        :yang18d_interactions,
        :yang18d_model,
        :yang18d_model-free,
        :yang18d_policies,
        :yang18d_policy,
        :yang18d_q-learning,
        :yang18d_reinforcement_learning,
        :yang18d_reinforcement_learning_methods,
        :yang18d_solution,
        :yang18d_this,
        :yang18d_those,
        :yang18d_two,
        :yang18d_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:yearOfPublication a owl:DatatypeProperty .

:yoon17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :yoon17a_correlations,
        :yoon17a_datasets,
        :yoon17a_deep_neural_network,
        :yoon17a_efficiency,
        :yoon17a_features,
        :yoon17a_memory,
        :yoon17a_method,
        :yoon17a_network,
        :yoon17a_networks,
        :yoon17a_our_method,
        :yoon17a_overfitting,
        :yoon17a_parameters,
        :yoon17a_prediction_accuracy,
        :yoon17a_regularization,
        :yoon17a_regularizations,
        :yoon17a_results,
        :yoon17a_scalability,
        :yoon17a_sparsity,
        :yoon17a_that,
        :yoon17a_them,
        :yoon17a_this,
        :yoon17a_time,
        :yoon17a_training,
        :yoon17a_validate,
        :yoon17a_we,
        :yoon17a_weights ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:you18a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :you18a_assumptions,
        :you18a_baselines,
        :you18a_biology,
        :you18a_complex,
        :you18a_datasets,
        :you18a_deep_models,
        :you18a_distances,
        :you18a_edge,
        :you18a_edges,
        :you18a_evaluation_metrics,
        :you18a_experiments,
        :you18a_generation,
        :you18a_graph,
        :you18a_graph_structure,
        :you18a_graphs,
        :you18a_learns,
        :you18a_measure,
        :you18a_model,
        :you18a_modeling,
        :you18a_networks,
        :you18a_node,
        :you18a_sampling,
        :you18a_structure,
        :you18a_target,
        :you18a_that,
        :you18a_these,
        :you18a_training,
        :you18a_we ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:zenke17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :zenke17a_applications,
        :zenke17a_approach,
        :zenke17a_artificial_neural_networks,
        :zenke17a_classification_tasks,
        :zenke17a_complex,
        :zenke17a_computational_efficiency,
        :zenke17a_data,
        :zenke17a_deep_learning,
        :zenke17a_domains,
        :zenke17a_information,
        :zenke17a_neural_networks,
        :zenke17a_ones,
        :zenke17a_relevant_information,
        :zenke17a_task,
        :zenke17a_tasks,
        :zenke17a_that,
        :zenke17a_this,
        :zenke17a_time,
        :zenke17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:zhang17b a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :zhang17b_adversarial,
        :zhang17b_convergence,
        :zhang17b_discrete_data,
        :zhang17b_experiments,
        :zhang17b_feature,
        :zhang17b_framework,
        :zhang17b_gan,
        :zhang17b_matching,
        :zhang17b_memory,
        :zhang17b_metric,
        :zhang17b_model,
        :zhang17b_network,
        :zhang17b_problem,
        :zhang17b_synthetic_data,
        :zhang17b_text,
        :zhang17b_that,
        :zhang17b_training,
        :zhang17b_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:zhang17f a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :zhang17f_auto-encoders,
        :zhang17f_backpropagation,
        :zhang17f_baseline_methods,
        :zhang17f_cnn,
        :zhang17f_cnns,
        :zhang17f_convex,
        :zhang17f_convex_optimization_problem,
        :zhang17f_convolutional_neural_networks,
        :zhang17f_generalization,
        :zhang17f_hilbert_space,
        :zhang17f_matrix,
        :zhang17f_networks,
        :zhang17f_neural_networks,
        :zhang17f_other,
        :zhang17f_parameters,
        :zhang17f_rank,
        :zhang17f_svms,
        :zhang17f_that,
        :zhang17f_two,
        :zhang17f_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:zhang18l a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :zhang18l_active_learning,
        :zhang18l_algorithm,
        :zhang18l_benchmarks,
        :zhang18l_covariance,
        :zhang18l_deep_learning,
        :zhang18l_estimates,
        :zhang18l_estimation,
        :zhang18l_flexibility,
        :zhang18l_gradient,
        :zhang18l_lower_bound,
        :zhang18l_matrix,
        :zhang18l_methods,
        :zhang18l_noise,
        :zhang18l_posterior,
        :zhang18l_posteriors,
        :zhang18l_predictions,
        :zhang18l_procedures,
        :zhang18l_reinforcement_learning,
        :zhang18l_scale,
        :zhang18l_that,
        :zhang18l_uncertainty,
        :zhang18l_variances ;
    :platform "pytorch" ;
    :yearOfPublication 2018 .

:zheng17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :zheng17a_algorithm,
        :zheng17a_algorithms,
        :zheng17a_data,
        :zheng17a_deep_learning_models,
        :zheng17a_family,
        :zheng17a_networks,
        :zheng17a_one,
        :zheng17a_optimization,
        :zheng17a_other,
        :zheng17a_properties,
        :zheng17a_results,
        :zheng17a_state,
        :zheng17a_tasks,
        :zheng17a_that,
        :zheng17a_training,
        :zheng17a_we ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

:zilly17a a :Publication ;
    :conferenceSeries "ICML" ;
    :hasEntity :zilly17a_architecture,
        :zilly17a_complex,
        :zilly17a_corpus,
        :zilly17a_datasets,
        :zilly17a_entropy,
        :zilly17a_experiments,
        :zilly17a_functions,
        :zilly17a_language_modeling,
        :zilly17a_long_short-term_memory,
        :zilly17a_lstm,
        :zilly17a_modeling,
        :zilly17a_models,
        :zilly17a_networks,
        :zilly17a_one,
        :zilly17a_optimization,
        :zilly17a_parameters,
        :zilly17a_penn_treebank,
        :zilly17a_per,
        :zilly17a_prediction,
        :zilly17a_processing,
        :zilly17a_recurrent_networks,
        :zilly17a_recurrent_neural_networks,
        :zilly17a_results,
        :zilly17a_tasks,
        :zilly17a_that,
        :zilly17a_theoretical_analysis,
        :zilly17a_this,
        :zilly17a_understanding,
        :zilly17a_we,
        :zilly17a_wikipedia ;
    :platform "tensorflow" ;
    :yearOfPublication 2017 .

<https://github.com/deepcurator/DCC/1712.09913v2_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/1712.09913v2_convex> a :Other ;
    :hasText "convex" .

<https://github.com/deepcurator/DCC/1712.09913v2_generalization> a :Generic ;
    :hasText "generalization" .

<https://github.com/deepcurator/DCC/1712.09913v2_loss_function> a :Method ;
    :hasText "loss_function" .

<https://github.com/deepcurator/DCC/1712.09913v2_loss_functions> a :Other ;
    :hasText "loss_functions" .

<https://github.com/deepcurator/DCC/1712.09913v2_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1712.09913v2_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1712.09913v2_shape> a :Other ;
    :hasText "shape" .

<https://github.com/deepcurator/DCC/1712.09913v2_skip_connections> a :Other ;
    :hasText "skip_connections" .

<https://github.com/deepcurator/DCC/1712.09913v2_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/1712.09913v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1712.09913v2_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1712.09913v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1712.09913v2_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1712.09913v2_training_parameters> a :Eval ;
    :hasText "training_parameters" .

<https://github.com/deepcurator/DCC/1712.09913v2_visualization_methods> a :Method ;
    :hasText "visualization_methods" .

<https://github.com/deepcurator/DCC/1712.09913v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1801.07791v3_cnns> a :Method ;
    :hasText "cnns" .

<https://github.com/deepcurator/DCC/1801.07791v3_convolution> a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/1801.07791v3_correlation> a :Other ;
    :hasText "correlation" .

<https://github.com/deepcurator/DCC/1801.07791v3_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1801.07791v3_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1801.07791v3_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/1801.07791v3_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/1801.07791v3_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/1801.07791v3_generalization> a :Generic ;
    :hasText "generalization" .

<https://github.com/deepcurator/DCC/1801.07791v3_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1801.07791v3_kernels> a :Method ;
    :hasText "kernels" .

<https://github.com/deepcurator/DCC/1801.07791v3_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1801.07791v3_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1801.07791v3_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1801.07791v3_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/1801.07791v3_point_cloud> a :Other ;
    :hasText "point_cloud" .

<https://github.com/deepcurator/DCC/1801.07791v3_problems> a :Generic ;
    :hasText "problems" .

<https://github.com/deepcurator/DCC/1801.07791v3_shape_information> a :Other ;
    :hasText "shape_information" .

<https://github.com/deepcurator/DCC/1801.07791v3_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1801.07791v3_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1801.07791v3_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1801.07791v3_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/1801.07791v3_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1801.07791v3_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1802.05074v4_adaptation> a :Task ;
    :hasText "adaptation" .

<https://github.com/deepcurator/DCC/1802.05074v4_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/1802.05074v4_cnns> a :Method ;
    :hasText "cnns" .

<https://github.com/deepcurator/DCC/1802.05074v4_computational_cost> a :Eval ;
    :hasText "computational_cost" .

<https://github.com/deepcurator/DCC/1802.05074v4_counterparts> a :Generic ;
    :hasText "counterparts" .

<https://github.com/deepcurator/DCC/1802.05074v4_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1802.05074v4_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/1802.05074v4_hyperparameters> a :Generic ;
    :hasText "hyperparameters" .

<https://github.com/deepcurator/DCC/1802.05074v4_loss_function> a :Method ;
    :hasText "loss_function" .

<https://github.com/deepcurator/DCC/1802.05074v4_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/1802.05074v4_ones> a :Generic ;
    :hasText "ones" .

<https://github.com/deepcurator/DCC/1802.05074v4_others> a :Generic ;
    :hasText "others" .

<https://github.com/deepcurator/DCC/1802.05074v4_resnets> a :Method ;
    :hasText "resnets" .

<https://github.com/deepcurator/DCC/1802.05074v4_scheme> a :Generic ;
    :hasText "scheme" .

<https://github.com/deepcurator/DCC/1802.05074v4_stochastic_gradient_descent> a :Method ;
    :hasText "stochastic_gradient_descent" .

<https://github.com/deepcurator/DCC/1802.05335v2_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/1802.05335v2_autoencoder> a :Method ;
    :hasText "autoencoder" .

<https://github.com/deepcurator/DCC/1802.05335v2_computation> a :Other ;
    :hasText "computation" .

<https://github.com/deepcurator/DCC/1802.05335v2_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1802.05335v2_edge> a :Other ;
    :hasText "edge" .

<https://github.com/deepcurator/DCC/1802.05335v2_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1802.05335v2_inference_problem> a :Task ;
    :hasText "inference_problem" .

<https://github.com/deepcurator/DCC/1802.05335v2_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1802.05335v2_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1802.05335v2_missing_data> a :Material ;
    :hasText "missing_data" .

<https://github.com/deepcurator/DCC/1802.05335v2_modalities> a :Generic ;
    :hasText "modalities" .

<https://github.com/deepcurator/DCC/1802.05335v2_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1802.05335v2_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1802.05335v2_paradigm> a :Generic ;
    :hasText "paradigm" .

<https://github.com/deepcurator/DCC/1802.05335v2_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/1802.05335v2_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/1802.05335v2_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/1802.05335v2_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/1802.05335v2_segmentation> a :Task ;
    :hasText "segmentation" .

<https://github.com/deepcurator/DCC/1802.05335v2_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1802.05335v2_supervised_learning> a :Method ;
    :hasText "supervised_learning" .

<https://github.com/deepcurator/DCC/1802.05335v2_supervision> a :Other ;
    :hasText "supervision" .

<https://github.com/deepcurator/DCC/1802.05335v2_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1802.05335v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1802.05335v2_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1802.05335v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1802.05335v2_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1802.05335v2_transformations> a :Generic ;
    :hasText "transformations" .

<https://github.com/deepcurator/DCC/1802.05335v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1802.06006v2_adaptation> a :Task ;
    :hasText "adaptation" .

<https://github.com/deepcurator/DCC/1802.06006v2_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1802.06006v2_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/1802.06006v2_audio_samples> a :Material ;
    :hasText "audio_samples" .

<https://github.com/deepcurator/DCC/1802.06006v2_audios> a :Material ;
    :hasText "audios" .

<https://github.com/deepcurator/DCC/1802.06006v2_generative_model> a :Method ;
    :hasText "generative_model" .

<https://github.com/deepcurator/DCC/1802.06006v2_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1802.06006v2_interfaces> a :Other ;
    :hasText "interfaces" .

<https://github.com/deepcurator/DCC/1802.06006v2_memory> a :Other ;
    :hasText "memory" .

<https://github.com/deepcurator/DCC/1802.06006v2_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1802.06006v2_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1802.06006v2_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/1802.06006v2_resource> a :Generic ;
    :hasText "resource" .

<https://github.com/deepcurator/DCC/1802.06006v2_speaker_adaptation> a :Task ;
    :hasText "speaker_adaptation" .

<https://github.com/deepcurator/DCC/1802.06006v2_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/1802.06006v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1802.06006v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1802.06006v2_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/1802.06006v2_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1802.06006v2_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/1802.06006v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1802.07044_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1802.07044_bounds> a :Other ;
    :hasText "bounds" .

<https://github.com/deepcurator/DCC/1802.07044_compression> a :Task ;
    :hasText "compression" .

<https://github.com/deepcurator/DCC/1802.07044_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1802.07044_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/1802.07044_deep_networks> a :Method ;
    :hasText "deep_networks" .

<https://github.com/deepcurator/DCC/1802.07044_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/1802.07044_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1802.07044_minimize> a :Task ;
    :hasText "minimize" .

<https://github.com/deepcurator/DCC/1802.07044_minimum_description_length_principle> a :Method ;
    :hasText "minimum_description_length_principle" .

<https://github.com/deepcurator/DCC/1802.07044_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1802.07044_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/1802.07044_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/1802.07044_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/1802.07044_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1802.07044_theory> a :Generic ;
    :hasText "theory" .

<https://github.com/deepcurator/DCC/1802.07044_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1802.07044_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1802.07044_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1802.07044_viewpoint> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/viewpoint> ;
    :hasText "viewpoint" .

<https://github.com/deepcurator/DCC/1802.07044_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1803.00404v2_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/1803.00404v2_adversarial_examples> a :Task ;
    :hasText "adversarial_examples" .

<https://github.com/deepcurator/DCC/1803.00404v2_applications> a :Generic ;
    :hasText "applications" .

<https://github.com/deepcurator/DCC/1803.00404v2_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/1803.00404v2_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/1803.00404v2_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/1803.00404v2_classifiers> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

<https://github.com/deepcurator/DCC/1803.00404v2_computer_vision_tasks> a :Task ;
    :hasText "computer_vision_tasks" .

<https://github.com/deepcurator/DCC/1803.00404v2_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1803.00404v2_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/1803.00404v2_dnns> a :Method ;
    :hasText "dnns" .

<https://github.com/deepcurator/DCC/1803.00404v2_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1803.00404v2_imagenet> a :Material ;
    :hasText "imagenet" .

<https://github.com/deepcurator/DCC/1803.00404v2_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/1803.00404v2_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1803.00404v2_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/1803.00404v2_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/1803.00404v2_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1803.00404v2_optimization_problem> a :Task ;
    :hasText "optimization_problem" .

<https://github.com/deepcurator/DCC/1803.00404v2_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1803.00404v2_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/1803.00404v2_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1803.00404v2_regularizations> a :Other ;
    :hasText "regularizations" .

<https://github.com/deepcurator/DCC/1803.00404v2_regularizer> a :Method ;
    :hasText "regularizer" .

<https://github.com/deepcurator/DCC/1803.00404v2_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/1803.00404v2_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/1803.00404v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1803.00404v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1803.00404v2_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1803.00404v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1803.06373v1_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/1803.06373v1_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/1803.06373v1_adversarial_examples> a :Task ;
    :hasText "adversarial_examples" .

<https://github.com/deepcurator/DCC/1803.06373v1_counterparts> a :Generic ;
    :hasText "counterparts" .

<https://github.com/deepcurator/DCC/1803.06373v1_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1803.06373v1_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/1803.06373v1_imagenet> a :Material ;
    :hasText "imagenet" .

<https://github.com/deepcurator/DCC/1803.06373v1_improvement> a :Eval ;
    :hasText "improvement" .

<https://github.com/deepcurator/DCC/1803.06373v1_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1803.06373v1_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/1803.06373v1_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1803.06373v1_technique> a :Generic ;
    :hasText "technique" .

<https://github.com/deepcurator/DCC/1803.06373v1_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/1803.06373v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1803.06373v1_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1803.06373v1_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1803.06373v1_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/1803.06373v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1804.09170v2_algorithms> a :Generic ;
    :hasText "algorithms" .

<https://github.com/deepcurator/DCC/1804.09170v2_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/1804.09170v2_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/1804.09170v2_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/1804.09170v2_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/1804.09170v2_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/1804.09170v2_face> a :Other ;
    :hasText "face" .

<https://github.com/deepcurator/DCC/1804.09170v2_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/1804.09170v2_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1804.09170v2_platform> a :Generic ;
    :hasText "platform" .

<https://github.com/deepcurator/DCC/1804.09170v2_real-world_applications> a :Generic ;
    :hasText "real-world_applications" .

<https://github.com/deepcurator/DCC/1804.09170v2_research> a :Generic ;
    :hasText "research" .

<https://github.com/deepcurator/DCC/1804.09170v2_supervised_learning> a :Method ;
    :hasText "supervised_learning" .

<https://github.com/deepcurator/DCC/1804.09170v2_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1804.09170v2_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/1804.09170v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1804.09170v2_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/1804.09170v2_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1804.09170v2_unlabeled_data> a :Other ;
    :hasText "unlabeled_data" .

<https://github.com/deepcurator/DCC/1804.09170v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1805.07445v3_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/1805.07445v3_autoencoders> a :Method ;
    :hasText "autoencoders" .

<https://github.com/deepcurator/DCC/1805.07445v3_bound> a :Generic ;
    :hasText "bound" .

<https://github.com/deepcurator/DCC/1805.07445v3_bounds> a :Other ;
    :hasText "bounds" .

<https://github.com/deepcurator/DCC/1805.07445v3_continuous> a :Generic ;
    :hasText "continuous" .

<https://github.com/deepcurator/DCC/1805.07445v3_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1805.07445v3_discrete> a :Generic ;
    :hasText "discrete" .

<https://github.com/deepcurator/DCC/1805.07445v3_implementation> a :Material ;
    :hasText "implementation" .

<https://github.com/deepcurator/DCC/1805.07445v3_latent_variables> a :Other ;
    :hasText "latent_variables" .

<https://github.com/deepcurator/DCC/1805.07445v3_lower_bound> a :Material ;
    :hasText "lower_bound" .

<https://github.com/deepcurator/DCC/1805.07445v3_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1805.07445v3_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/1805.07445v3_prior> a :Generic ;
    :hasText "prior" .

<https://github.com/deepcurator/DCC/1805.07445v3_priors> a :Other ;
    :hasText "priors" .

<https://github.com/deepcurator/DCC/1805.07445v3_relaxations> a :Generic ;
    :hasText "relaxations" .

<https://github.com/deepcurator/DCC/1805.07445v3_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/1805.07445v3_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.07445v3_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1805.07445v3_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1805.07445v3_transformations> a :Generic ;
    :hasText "transformations" .

<https://github.com/deepcurator/DCC/1805.07445v3_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/1805.07674_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1805.07674_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/1805.07674_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/1805.07674_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/1805.07674_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/1805.07674_gaussian_mixture_model> a :Method ;
    :hasText "gaussian_mixture_model" .

<https://github.com/deepcurator/DCC/1805.07674_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/1805.07674_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/1805.07674_metrics> a :Generic ;
    :hasText "metrics" .

<https://github.com/deepcurator/DCC/1805.07674_objective_function> a :Other ;
    :hasText "objective_function" .

<https://github.com/deepcurator/DCC/1805.07674_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1805.07674_pairwise_distance> a :Generic ;
    :hasText "pairwise_distance" .

<https://github.com/deepcurator/DCC/1805.07674_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/1805.07674_synthetic_data> a :Material ;
    :hasText "synthetic_data" .

<https://github.com/deepcurator/DCC/1805.07674_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.07674_theoretical_analysis> a :Generic ;
    :hasText "theoretical_analysis" .

<https://github.com/deepcurator/DCC/1805.07674_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1805.07674_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1805.07932v1_computational_cost> a :Eval ;
    :hasText "computational_cost" .

<https://github.com/deepcurator/DCC/1805.07932v1_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1805.07932v1_flickr30k_entities_datasets> a :Material ;
    :hasText "flickr30k_entities_datasets" .

<https://github.com/deepcurator/DCC/1805.07932v1_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/1805.07932v1_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1805.07932v1_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/1805.07932v1_interaction> a :Generic ;
    :hasText "interaction" .

<https://github.com/deepcurator/DCC/1805.07932v1_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/1805.07932v1_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/1805.07932v1_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1805.07932v1_maps> a :Eval ;
    :hasText "maps" .

<https://github.com/deepcurator/DCC/1805.07932v1_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1805.07932v1_modality> a :Other ;
    :hasText "modality" .

<https://github.com/deepcurator/DCC/1805.07932v1_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1805.07932v1_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/1805.07932v1_pooling> a :Method ;
    :hasText "pooling" .

<https://github.com/deepcurator/DCC/1805.07932v1_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1805.07932v1_question_answering> a :Task ;
    :hasText "question_answering" .

<https://github.com/deepcurator/DCC/1805.07932v1_rank> a :Other ;
    :hasText "rank" .

<https://github.com/deepcurator/DCC/1805.07932v1_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/1805.07932v1_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1805.07932v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.07932v1_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1805.07932v1_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/1805.07932v1_vision> a :Generic ;
    :hasText "vision" .

<https://github.com/deepcurator/DCC/1805.07932v1_visual_information> a :Generic ;
    :hasText "visual_information" .

<https://github.com/deepcurator/DCC/1805.07932v1_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/1805.07932v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1805.08574_drop-in_replacements> a :Other ;
    :hasText "drop-in_replacements" .

<https://github.com/deepcurator/DCC/1805.08574_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/1805.08574_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/1805.08574_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1805.08574_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1805.08574_lstm> a :Method ;
    :hasText "lstm" .

<https://github.com/deepcurator/DCC/1805.08574_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1805.08574_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/1805.08574_neural_network_architectures> a :Generic ;
    :hasText "neural_network_architectures" .

<https://github.com/deepcurator/DCC/1805.08574_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/1805.08574_penn_treebank> a :Material ;
    :hasText "penn_treebank" .

<https://github.com/deepcurator/DCC/1805.08574_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1805.08574_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/1805.08574_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1805.08574_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.08574_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/1805.08574_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1805.08574_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1805.09112v2_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/1805.09112v2_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1805.09112v2_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/1805.09112v2_embeddings> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

<https://github.com/deepcurator/DCC/1805.09112v2_formalism> a :Generic ;
    :hasText "formalism" .

<https://github.com/deepcurator/DCC/1805.09112v2_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/1805.09112v2_machine_learning> a :Task ;
    :hasText "machine_learning" .

<https://github.com/deepcurator/DCC/1805.09112v2_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1805.09112v2_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/1805.09112v2_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/1805.09112v2_properties> a :Generic ;
    :hasText "properties" .

<https://github.com/deepcurator/DCC/1805.09112v2_recognition_tasks> a :Task ;
    :hasText "recognition_tasks" .

<https://github.com/deepcurator/DCC/1805.09112v2_recurrent_neural_networks> a :Method ;
    :hasText "recurrent_neural_networks" .

<https://github.com/deepcurator/DCC/1805.09112v2_recurrent_units> a :Generic ;
    :hasText "recurrent_units" .

<https://github.com/deepcurator/DCC/1805.09112v2_sentence_embeddings> a :Method ;
    :hasText "sentence_embeddings" .

<https://github.com/deepcurator/DCC/1805.09112v2_spaces> a :Generic ;
    :hasText "spaces" .

<https://github.com/deepcurator/DCC/1805.09112v2_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1805.09112v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.09112v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1805.09112v2_tools> a :Generic ;
    :hasText "tools" .

<https://github.com/deepcurator/DCC/1805.09112v2_tree> a :Other ;
    :hasText "tree" .

<https://github.com/deepcurator/DCC/1805.09112v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1805.09298v4_applications> a :Generic ;
    :hasText "applications" .

<https://github.com/deepcurator/DCC/1805.09298v4_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/1805.09298v4_computation> a :Other ;
    :hasText "computation" .

<https://github.com/deepcurator/DCC/1805.09298v4_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/1805.09298v4_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/1805.09298v4_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/1805.09298v4_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/1805.09298v4_generalization> a :Generic ;
    :hasText "generalization" .

<https://github.com/deepcurator/DCC/1805.09298v4_minimization> a :Task ;
    :hasText "minimization" .

<https://github.com/deepcurator/DCC/1805.09298v4_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1805.09298v4_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/1805.09298v4_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/1805.09298v4_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/1805.09298v4_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1805.09298v4_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1805.09298v4_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/1805.09298v4_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/1805.09298v4_sphere> a :Generic ;
    :hasText "sphere" .

<https://github.com/deepcurator/DCC/1805.09298v4_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1805.09298v4_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1805.09298v4_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.09298v4_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1805.09298v4_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1805.12018v1_adversarial_examples> a :Task ;
    :hasText "adversarial_examples" .

<https://github.com/deepcurator/DCC/1805.12018v1_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1805.12018v1_data_augmentation> a :Method ;
    :hasText "data_augmentation" .

<https://github.com/deepcurator/DCC/1805.12018v1_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/1805.12018v1_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/1805.12018v1_feature_space> a :Other ;
    :hasText "feature_space" .

<https://github.com/deepcurator/DCC/1805.12018v1_formulation> a :Generic ;
    :hasText "formulation" .

<https://github.com/deepcurator/DCC/1805.12018v1_iterative_procedure> a :Method ;
    :hasText "iterative_procedure" .

<https://github.com/deepcurator/DCC/1805.12018v1_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/1805.12018v1_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1805.12018v1_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1805.12018v1_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/1805.12018v1_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1805.12018v1_priori> a :Other ;
    :hasText "priori" .

<https://github.com/deepcurator/DCC/1805.12018v1_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/1805.12018v1_regularizers> a :Method ;
    :hasText "regularizers" .

<https://github.com/deepcurator/DCC/1805.12018v1_scheme> a :Generic ;
    :hasText "scheme" .

<https://github.com/deepcurator/DCC/1805.12018v1_semantic_segmentation> a :Task ;
    :hasText "semantic_segmentation" .

<https://github.com/deepcurator/DCC/1805.12018v1_source_domain> a :Material ;
    :hasText "source_domain" .

<https://github.com/deepcurator/DCC/1805.12018v1_target_domain> a :Material ;
    :hasText "target_domain" .

<https://github.com/deepcurator/DCC/1805.12018v1_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1805.12018v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.12018v1_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1805.12018v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1805.12462v1_approximating> a :Task ;
    :hasText "approximating" .

<https://github.com/deepcurator/DCC/1805.12462v1_automatic> a :Task ;
    :hasText "automatic" .

<https://github.com/deepcurator/DCC/1805.12462v1_computations> a :Generic ;
    :hasText "computations" .

<https://github.com/deepcurator/DCC/1805.12462v1_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1805.12462v1_diversity> a :Other ;
    :hasText "diversity" .

<https://github.com/deepcurator/DCC/1805.12462v1_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/1805.12462v1_gaussian_mixture_model> a :Method ;
    :hasText "gaussian_mixture_model" .

<https://github.com/deepcurator/DCC/1805.12462v1_generative_models> a :Method ;
    :hasText "generative_models" .

<https://github.com/deepcurator/DCC/1805.12462v1_gmm> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gaussian_mixture_model> ;
    :hasText "gmm" .

<https://github.com/deepcurator/DCC/1805.12462v1_gmms> a :Method ;
    :hasText "gmms" .

<https://github.com/deepcurator/DCC/1805.12462v1_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1805.12462v1_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1805.12462v1_machine_learning> a :Task ;
    :hasText "machine_learning" .

<https://github.com/deepcurator/DCC/1805.12462v1_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1805.12462v1_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1805.12462v1_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1805.12462v1_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/1805.12462v1_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/1805.12462v1_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1805.12462v1_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/1805.12462v1_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/1805.12462v1_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1805.12462v1_patches> a :Generic ;
    :hasText "patches" .

<https://github.com/deepcurator/DCC/1805.12462v1_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1805.12462v1_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/1805.12462v1_resolution> a :Eval ;
    :hasText "resolution" .

<https://github.com/deepcurator/DCC/1805.12462v1_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/1805.12462v1_signals> a :Generic ;
    :hasText "signals" .

<https://github.com/deepcurator/DCC/1805.12462v1_solution> a :Generic ;
    :hasText "solution" .

<https://github.com/deepcurator/DCC/1805.12462v1_statistical_model> a :Method ;
    :hasText "statistical_model" .

<https://github.com/deepcurator/DCC/1805.12462v1_statistical_models> a :Method ;
    :hasText "statistical_models" .

<https://github.com/deepcurator/DCC/1805.12462v1_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/1805.12462v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1805.12462v1_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/1805.12462v1_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1805.12462v1_those> a :Generic ;
    :hasText "those" .

<https://github.com/deepcurator/DCC/1805.12462v1_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/1805.12462v1_unsupervised_methods> a :Method ;
    :hasText "unsupervised_methods" .

<https://github.com/deepcurator/DCC/1805.12462v1_utility> a :Eval ;
    :hasText "utility" .

<https://github.com/deepcurator/DCC/1805.12462v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1806.00035v1_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/1806.00035v1_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1806.00035v1_autoencoder> a :Method ;
    :hasText "autoencoder" .

<https://github.com/deepcurator/DCC/1806.00035v1_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/1806.00035v1_divergence> a :Generic ;
    :hasText "divergence" .

<https://github.com/deepcurator/DCC/1806.00035v1_divergences> a :Generic ;
    :hasText "divergences" .

<https://github.com/deepcurator/DCC/1806.00035v1_evaluation_methods> a :Generic ;
    :hasText "evaluation_methods" .

<https://github.com/deepcurator/DCC/1806.00035v1_evaluation_metrics> a :Eval ;
    :hasText "evaluation_metrics" .

<https://github.com/deepcurator/DCC/1806.00035v1_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/1806.00035v1_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/1806.00035v1_generative_models> a :Method ;
    :hasText "generative_models" .

<https://github.com/deepcurator/DCC/1806.00035v1_inception> a :Method ;
    :hasText "inception" .

<https://github.com/deepcurator/DCC/1806.00035v1_means> a :Generic ;
    :hasText "means" .

<https://github.com/deepcurator/DCC/1806.00035v1_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/1806.00035v1_metrics> a :Generic ;
    :hasText "metrics" .

<https://github.com/deepcurator/DCC/1806.00035v1_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1806.00035v1_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/1806.00035v1_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/1806.00035v1_precision> a :Eval ;
    :hasText "precision" .

<https://github.com/deepcurator/DCC/1806.00035v1_properties> a :Generic ;
    :hasText "properties" .

<https://github.com/deepcurator/DCC/1806.00035v1_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/1806.00035v1_recall> a :Eval ;
    :hasText "recall" .

<https://github.com/deepcurator/DCC/1806.00035v1_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/1806.00035v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1806.00035v1_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1806.00035v1_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/1806.00035v1_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1806.00035v1_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/1806.00035v1_utility> a :Eval ;
    :hasText "utility" .

<https://github.com/deepcurator/DCC/1806.00035v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1806.01822_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/1806.01822_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/1806.01822_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1806.01822_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/1806.01822_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/1806.01822_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/1806.01822_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/1806.01822_language_modeling> a :Task ;
    :hasText "language_modeling" .

<https://github.com/deepcurator/DCC/1806.01822_memory> a :Other ;
    :hasText "memory" .

<https://github.com/deepcurator/DCC/1806.01822_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1806.01822_module> a :Generic ;
    :hasText "module" .

<https://github.com/deepcurator/DCC/1806.01822_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/1806.01822_program> a :Generic ;
    :hasText "program" .

<https://github.com/deepcurator/DCC/1806.01822_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/1806.01822_rl> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

<https://github.com/deepcurator/DCC/1806.01822_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1806.01822_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1806.01822_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1806.01822_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1806.01822_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/1806.01822_understanding> a :Task ;
    :hasText "understanding" .

<https://github.com/deepcurator/DCC/1806.01822_ways> a :Generic ;
    :hasText "ways" .

<https://github.com/deepcurator/DCC/1806.01822_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1806.02311_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/1806.02311_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1806.02311_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/1806.02311_attention_mechanisms> a :Method ;
    :hasText "attention_mechanisms" .

<https://github.com/deepcurator/DCC/1806.02311_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1806.02311_mappings> a :Other ;
    :hasText "mappings" .

<https://github.com/deepcurator/DCC/1806.02311_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/1806.02311_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/1806.02311_regions> a :Generic ;
    :hasText "regions" .

<https://github.com/deepcurator/DCC/1806.02311_scene> a :Generic ;
    :hasText "scene" .

<https://github.com/deepcurator/DCC/1806.02311_supervision> a :Other ;
    :hasText "supervision" .

<https://github.com/deepcurator/DCC/1806.02311_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1806.02311_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/1806.02311_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1806.02311_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1806.02311_those> a :Generic ;
    :hasText "those" .

<https://github.com/deepcurator/DCC/1806.02311_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/1806.02311_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/1806.02311_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1806.02724_action> a :Generic ;
    :hasText "action" .

<https://github.com/deepcurator/DCC/1806.02724_annotated_data> a :Material ;
    :hasText "annotated_data" .

<https://github.com/deepcurator/DCC/1806.02724_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1806.02724_baseline> a :Generic ;
    :hasText "baseline" .

<https://github.com/deepcurator/DCC/1806.02724_candidate> a :Generic ;
    :hasText "candidate" .

<https://github.com/deepcurator/DCC/1806.02724_components> a :Generic ;
    :hasText "components" .

<https://github.com/deepcurator/DCC/1806.02724_data_augmentation> a :Method ;
    :hasText "data_augmentation" .

<https://github.com/deepcurator/DCC/1806.02724_decisions> a :Generic ;
    :hasText "decisions" .

<https://github.com/deepcurator/DCC/1806.02724_granularity> a :Eval ;
    :hasText "granularity" .

<https://github.com/deepcurator/DCC/1806.02724_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/1806.02724_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/1806.02724_machine_learning> a :Task ;
    :hasText "machine_learning" .

<https://github.com/deepcurator/DCC/1806.02724_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1806.02724_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/1806.02724_natural_language> a :Material ;
    :hasText "natural_language" .

<https://github.com/deepcurator/DCC/1806.02724_navigation> a :Task ;
    :hasText "navigation" .

<https://github.com/deepcurator/DCC/1806.02724_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1806.02724_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1806.02724_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1806.02724_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1806.02724_vision> a :Generic ;
    :hasText "vision" .

<https://github.com/deepcurator/DCC/1806.02724_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1806.04090v2_distributed> a :Generic ;
    :hasText "distributed" .

<https://github.com/deepcurator/DCC/1806.04090v2_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/1806.04090v2_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/1806.04090v2_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/1806.04090v2_gradients> a :Other ;
    :hasText "gradients" .

<https://github.com/deepcurator/DCC/1806.04090v2_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1806.04090v2_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1806.04090v2_model_training> a :Task ;
    :hasText "model_training" .

<https://github.com/deepcurator/DCC/1806.04090v2_nodes> a :Other ;
    :hasText "nodes" .

<https://github.com/deepcurator/DCC/1806.04090v2_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/1806.04090v2_overheads> a :Other ;
    :hasText "overheads" .

<https://github.com/deepcurator/DCC/1806.04090v2_sparsity> a :Other ;
    :hasText "sparsity" .

<https://github.com/deepcurator/DCC/1806.04090v2_stochastic_gradients> a :Other ;
    :hasText "stochastic_gradients" .

<https://github.com/deepcurator/DCC/1806.04090v2_studies> a :Generic ;
    :hasText "studies" .

<https://github.com/deepcurator/DCC/1806.04090v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1806.04090v2_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1806.04090v2_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1806.04090v2_variance> a :Generic ;
    :hasText "variance" .

<https://github.com/deepcurator/DCC/1806.05138v1_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/1806.05138v1_bleu_scores> a :Eval ;
    :hasText "bleu_scores" .

<https://github.com/deepcurator/DCC/1806.05138v1_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1806.05138v1_encoder-decoder> a :Task ;
    :hasText "encoder-decoder" .

<https://github.com/deepcurator/DCC/1806.05138v1_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/1806.05138v1_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/1806.05138v1_languages> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/languages> ;
    :hasText "languages" .

<https://github.com/deepcurator/DCC/1806.05138v1_latent_variable> a :Other ;
    :hasText "latent_variable" .

<https://github.com/deepcurator/DCC/1806.05138v1_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1806.05138v1_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1806.05138v1_neural_machine_translation> a :Task ;
    :hasText "neural_machine_translation" .

<https://github.com/deepcurator/DCC/1806.05138v1_overfitting> a :Other ;
    :hasText "overfitting" .

<https://github.com/deepcurator/DCC/1806.05138v1_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/1806.05138v1_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/1806.05138v1_semantics> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/semantics> ;
    :hasText "semantics" .

<https://github.com/deepcurator/DCC/1806.05138v1_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/1806.05138v1_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1806.05138v1_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1806.05138v1_words> a :Generic ;
    :hasText "words" .

<https://github.com/deepcurator/DCC/1806.06029v1_agents> a :Generic ;
    :hasText "agents" .

<https://github.com/deepcurator/DCC/1806.06029v1_autoencoder> a :Method ;
    :hasText "autoencoder" .

<https://github.com/deepcurator/DCC/1806.06029v1_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/1806.06029v1_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/1806.06029v1_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1806.06029v1_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1806.06029v1_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/1806.06029v1_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1806.06029v1_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1806.06029v1_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/1806.06029v1_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/1806.06029v1_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/1806.06029v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1806.06029v1_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1806.06029v1_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1806.06029v1_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1806.06029v1_transfer> a :Other ;
    :hasText "transfer" .

<https://github.com/deepcurator/DCC/1806.06029v1_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/1806.06029v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1806.06621v1_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/1806.06621v1_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/1806.06621v1_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/1806.06621v1_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/1806.06621v1_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/1806.06621v1_gradient_penalty> a :Generic ;
    :hasText "gradient_penalty" .

<https://github.com/deepcurator/DCC/1806.06621v1_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1806.06621v1_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1806.06621v1_inception> a :Method ;
    :hasText "inception" .

<https://github.com/deepcurator/DCC/1806.06621v1_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/1806.06621v1_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1806.06621v1_probability_distributions> a :Other ;
    :hasText "probability_distributions" .

<https://github.com/deepcurator/DCC/1806.06621v1_spaces> a :Generic ;
    :hasText "spaces" .

<https://github.com/deepcurator/DCC/1806.06621v1_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1806.06621v1_theory> a :Generic ;
    :hasText "theory" .

<https://github.com/deepcurator/DCC/1806.06621v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1806.07336v2_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/1806.07336v2_analogies> a :Other ;
    :hasText "analogies" .

<https://github.com/deepcurator/DCC/1806.07336v2_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/1806.07336v2_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/1806.07336v2_classes> a :Generic ;
    :hasText "classes" .

<https://github.com/deepcurator/DCC/1806.07336v2_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/1806.07336v2_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1806.07336v2_device> a :Generic ;
    :hasText "device" .

<https://github.com/deepcurator/DCC/1806.07336v2_distributional_hypothesis> a :Other ;
    :hasText "distributional_hypothesis" .

<https://github.com/deepcurator/DCC/1806.07336v2_embeddings> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

<https://github.com/deepcurator/DCC/1806.07336v2_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/1806.07336v2_flow> a :Other ;
    :hasText "flow" .

<https://github.com/deepcurator/DCC/1806.07336v2_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/1806.07336v2_ir> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/information_retrieval> ;
    :hasText "ir" .

<https://github.com/deepcurator/DCC/1806.07336v2_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1806.07336v2_learned_representation> a :Generic ;
    :hasText "learned_representation" .

<https://github.com/deepcurator/DCC/1806.07336v2_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1806.07336v2_natural_language> a :Material ;
    :hasText "natural_language" .

<https://github.com/deepcurator/DCC/1806.07336v2_natural_language_processing> a :Method ;
    :hasText "natural_language_processing" .

<https://github.com/deepcurator/DCC/1806.07336v2_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/1806.07336v2_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/1806.07336v2_program> a :Generic ;
    :hasText "program" .

<https://github.com/deepcurator/DCC/1806.07336v2_programming_language> a :Other ;
    :hasText "programming_language" .

<https://github.com/deepcurator/DCC/1806.07336v2_programs> a :Generic ;
    :hasText "programs" .

<https://github.com/deepcurator/DCC/1806.07336v2_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/1806.07336v2_research> a :Generic ;
    :hasText "research" .

<https://github.com/deepcurator/DCC/1806.07336v2_rnn> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

<https://github.com/deepcurator/DCC/1806.07336v2_semantics> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/semantics> ;
    :hasText "semantics" .

<https://github.com/deepcurator/DCC/1806.07336v2_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1806.07336v2_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1806.07336v2_technique> a :Generic ;
    :hasText "technique" .

<https://github.com/deepcurator/DCC/1806.07336v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1806.07336v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1806.07336v2_tree> a :Other ;
    :hasText "tree" .

<https://github.com/deepcurator/DCC/1806.07336v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1807.02547_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/1807.02547_cnns> a :Method ;
    :hasText "cnns" .

<https://github.com/deepcurator/DCC/1807.02547_convolutions> a :Method ;
    :hasText "convolutions" .

<https://github.com/deepcurator/DCC/1807.02547_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1807.02547_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/1807.02547_euclidean_space> a :Other ;
    :hasText "euclidean_space" .

<https://github.com/deepcurator/DCC/1807.02547_experimental_results> a :Generic ;
    :hasText "experimental_results" .

<https://github.com/deepcurator/DCC/1807.02547_fields> a :Generic ;
    :hasText "fields" .

<https://github.com/deepcurator/DCC/1807.02547_kernels> a :Method ;
    :hasText "kernels" .

<https://github.com/deepcurator/DCC/1807.02547_linear_combination> a :Method ;
    :hasText "linear_combination" .

<https://github.com/deepcurator/DCC/1807.02547_map> a :Eval ;
    :hasText "map" .

<https://github.com/deepcurator/DCC/1807.02547_maps> a :Eval ;
    :hasText "maps" .

<https://github.com/deepcurator/DCC/1807.02547_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1807.02547_motions> a :Generic ;
    :hasText "motions" .

<https://github.com/deepcurator/DCC/1807.02547_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1807.02547_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/1807.02547_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1807.02547_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/1807.02547_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/1807.02547_tensor> a :Generic ;
    :hasText "tensor" .

<https://github.com/deepcurator/DCC/1807.02547_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1807.02547_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1807.03039v2_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/1807.03039v2_convolution> a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/1807.03039v2_flow> a :Other ;
    :hasText "flow" .

<https://github.com/deepcurator/DCC/1807.03039v2_generative_model> a :Method ;
    :hasText "generative_model" .

<https://github.com/deepcurator/DCC/1807.03039v2_generative_models> a :Method ;
    :hasText "generative_models" .

<https://github.com/deepcurator/DCC/1807.03039v2_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1807.03039v2_improvement> a :Eval ;
    :hasText "improvement" .

<https://github.com/deepcurator/DCC/1807.03039v2_log-likelihood> a :Other ;
    :hasText "log-likelihood" .

<https://github.com/deepcurator/DCC/1807.03039v2_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1807.03039v2_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1807.03039v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1807.03039v2_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1807.03039v2_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1807.03039v2_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1807.03146v1_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/1807.03146v1_baseline> a :Generic ;
    :hasText "baseline" .

<https://github.com/deepcurator/DCC/1807.03146v1_categories> a :Generic ;
    :hasText "categories" .

<https://github.com/deepcurator/DCC/1807.03146v1_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/1807.03146v1_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/1807.03146v1_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1807.03146v1_instances> a :Generic ;
    :hasText "instances" .

<https://github.com/deepcurator/DCC/1807.03146v1_keypoint_annotations> a :Method ;
    :hasText "keypoint_annotations" .

<https://github.com/deepcurator/DCC/1807.03146v1_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1807.03146v1_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1807.03146v1_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/1807.03146v1_object_category> a :Other ;
    :hasText "object_category" .

<https://github.com/deepcurator/DCC/1807.03146v1_pose> a :Other ;
    :hasText "pose" .

<https://github.com/deepcurator/DCC/1807.03146v1_pose_estimation> a :Task ;
    :hasText "pose_estimation" .

<https://github.com/deepcurator/DCC/1807.03146v1_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/1807.03146v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1807.03146v1_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1807.03146v1_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/1807.03146v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1807.03247_agents> a :Generic ;
    :hasText "agents" .

<https://github.com/deepcurator/DCC/1807.03247_cnn> a :Method ;
    :hasText "cnn" .

<https://github.com/deepcurator/DCC/1807.03247_convolution> a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/1807.03247_convolutional_networks> a :Method ;
    :hasText "convolutional_networks" .

<https://github.com/deepcurator/DCC/1807.03247_convolutional_neural_networks> a :Method ;
    :hasText "convolutional_neural_networks" .

<https://github.com/deepcurator/DCC/1807.03247_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/1807.03247_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/1807.03247_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/1807.03247_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/1807.03247_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/1807.03247_generalization> a :Generic ;
    :hasText "generalization" .

<https://github.com/deepcurator/DCC/1807.03247_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1807.03247_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/1807.03247_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1807.03247_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/1807.03247_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1807.03247_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/1807.03247_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/1807.03247_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/1807.03247_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/1807.03247_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/1807.03247_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1807.03247_reinforcement_learning_(rl)> a :Task ;
    :hasText "reinforcement_learning_(rl)" .

<https://github.com/deepcurator/DCC/1807.03247_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/1807.03247_solution> a :Generic ;
    :hasText "solution" .

<https://github.com/deepcurator/DCC/1807.03247_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/1807.03247_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1807.03247_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1807.03247_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/1807.03247_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1807.03247_times> a :Generic ;
    :hasText "times" .

<https://github.com/deepcurator/DCC/1807.03247_translation_invariance> a :Other ;
    :hasText "translation_invariance" .

<https://github.com/deepcurator/DCC/1807.03247_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1807.03756v1_alignment> a :Task ;
    :hasText "alignment" .

<https://github.com/deepcurator/DCC/1807.03756v1_alignments> a :Other ;
    :hasText "alignments" .

<https://github.com/deepcurator/DCC/1807.03756v1_alternatives> a :Generic ;
    :hasText "alternatives" .

<https://github.com/deepcurator/DCC/1807.03756v1_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1807.03756v1_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/1807.03756v1_approximation> a :Generic ;
    :hasText "approximation" .

<https://github.com/deepcurator/DCC/1807.03756v1_bounds> a :Other ;
    :hasText "bounds" .

<https://github.com/deepcurator/DCC/1807.03756v1_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1807.03756v1_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/1807.03756v1_fixes> a :Other ;
    :hasText "fixes" .

<https://github.com/deepcurator/DCC/1807.03756v1_gradients> a :Other ;
    :hasText "gradients" .

<https://github.com/deepcurator/DCC/1807.03756v1_latent_variable> a :Other ;
    :hasText "latent_variable" .

<https://github.com/deepcurator/DCC/1807.03756v1_latent_variable_models> a :Method ;
    :hasText "latent_variable_models" .

<https://github.com/deepcurator/DCC/1807.03756v1_machine_translation> a :Task ;
    :hasText "machine_translation" .

<https://github.com/deepcurator/DCC/1807.03756v1_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1807.03756v1_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/1807.03756v1_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/1807.03756v1_natural_language_processing> a :Method ;
    :hasText "natural_language_processing" .

<https://github.com/deepcurator/DCC/1807.03756v1_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/1807.03756v1_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/1807.03756v1_posterior_inference> a :Task ;
    :hasText "posterior_inference" .

<https://github.com/deepcurator/DCC/1807.03756v1_probabilistic_models> a :Method ;
    :hasText "probabilistic_models" .

<https://github.com/deepcurator/DCC/1807.03756v1_question_answering> a :Task ;
    :hasText "question_answering" .

<https://github.com/deepcurator/DCC/1807.03756v1_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1807.03756v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1807.03756v1_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/1807.03756v1_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1807.03756v1_variance> a :Generic ;
    :hasText "variance" .

<https://github.com/deepcurator/DCC/1807.03756v1_variational_inference> a :Method ;
    :hasText "variational_inference" .

<https://github.com/deepcurator/DCC/1808.00508v1_activations> a :Other ;
    :hasText "activations" .

<https://github.com/deepcurator/DCC/1808.00508v1_analogy> a :Other ;
    :hasText "analogy" .

<https://github.com/deepcurator/DCC/1808.00508v1_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/1808.00508v1_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/1808.00508v1_generalization> a :Generic ;
    :hasText "generalization" .

<https://github.com/deepcurator/DCC/1808.00508v1_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1808.00508v1_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/1808.00508v1_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/1808.00508v1_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1808.00508v1_logic> a :Other ;
    :hasText "logic" .

<https://github.com/deepcurator/DCC/1808.00508v1_magnitude> a :Other ;
    :hasText "magnitude" .

<https://github.com/deepcurator/DCC/1808.00508v1_module> a :Generic ;
    :hasText "module" .

<https://github.com/deepcurator/DCC/1808.00508v1_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/1808.00508v1_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/1808.00508v1_numbers> a :Generic ;
    :hasText "numbers" .

<https://github.com/deepcurator/DCC/1808.00508v1_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/1808.00508v1_operators> a :Generic ;
    :hasText "operators" .

<https://github.com/deepcurator/DCC/1808.00508v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1808.00508v1_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/1808.00508v1_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1808.00508v1_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/1808.00508v1_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1808.00508v1_translate> a :Task ;
    :hasText "translate" .

<https://github.com/deepcurator/DCC/1808.00508v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1808.04768_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1808.04768_computational_efficiency> a :Eval ;
    :hasText "computational_efficiency" .

<https://github.com/deepcurator/DCC/1808.04768_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/1808.04768_events> a :Other ;
    :hasText "events" .

<https://github.com/deepcurator/DCC/1808.04768_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1808.04768_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1808.04768_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/1808.04768_prediction_accuracy> a :Eval ;
    :hasText "prediction_accuracy" .

<https://github.com/deepcurator/DCC/1808.04768_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/1808.04768_sampling> a :Method ;
    :hasText "sampling" .

<https://github.com/deepcurator/DCC/1808.04768_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/1808.04768_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1808.04768_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/1808.04768_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1808.06601_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/1808.06601_adversarial_learning> a :Method ;
    :hasText "adversarial_learning" .

<https://github.com/deepcurator/DCC/1808.06601_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1808.06601_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/1808.06601_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/1808.06601_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/1808.06601_counterpart> a :Generic ;
    :hasText "counterpart" .

<https://github.com/deepcurator/DCC/1808.06601_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/1808.06601_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/1808.06601_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/1808.06601_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1808.06601_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1808.06601_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/1808.06601_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1808.06601_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/1808.06601_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/1808.06601_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/1808.06601_poses> a :Other ;
    :hasText "poses" .

<https://github.com/deepcurator/DCC/1808.06601_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/1808.06601_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1808.06601_resolution> a :Eval ;
    :hasText "resolution" .

<https://github.com/deepcurator/DCC/1808.06601_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/1808.06601_scenes> a :Generic ;
    :hasText "scenes" .

<https://github.com/deepcurator/DCC/1808.06601_segmentation> a :Task ;
    :hasText "segmentation" .

<https://github.com/deepcurator/DCC/1808.06601_semantic_segmentation> a :Task ;
    :hasText "semantic_segmentation" .

<https://github.com/deepcurator/DCC/1808.06601_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/1808.06601_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/1808.06601_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1808.06601_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/1808.06601_video> a :Material ;
    :hasText "video" .

<https://github.com/deepcurator/DCC/1808.06601_videos> a :Material ;
    :hasText "videos" .

<https://github.com/deepcurator/DCC/1808.06601_visual_quality> a :Eval ;
    :hasText "visual_quality" .

<https://github.com/deepcurator/DCC/1808.06601_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1808.08750v1_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/1808.08750v1_convolutional_deep_neural_networks> a :Method ;
    :hasText "convolutional_deep_neural_networks" .

<https://github.com/deepcurator/DCC/1808.08750v1_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/1808.08750v1_dnns> a :Method ;
    :hasText "dnns" .

<https://github.com/deepcurator/DCC/1808.08750v1_generalisation> a :Generic ;
    :hasText "generalisation" .

<https://github.com/deepcurator/DCC/1808.08750v1_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1808.08750v1_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1808.08750v1_machine_learning_approach> a :Task ;
    :hasText "machine_learning_approach" .

<https://github.com/deepcurator/DCC/1808.08750v1_noise> a :Other ;
    :hasText "noise" .

<https://github.com/deepcurator/DCC/1808.08750v1_object_recognition> a :Task ;
    :hasText "object_recognition" .

<https://github.com/deepcurator/DCC/1808.08750v1_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/1808.08750v1_robustness> a :Eval ;
    :hasText "robustness" .

<https://github.com/deepcurator/DCC/1808.08750v1_signal> a :Generic ;
    :hasText "signal" .

<https://github.com/deepcurator/DCC/1808.08750v1_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/1808.08750v1_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/1808.08750v1_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/1808.08750v1_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1808.08750v1_vision_systems> a :Task ;
    :hasText "vision_systems" .

<https://github.com/deepcurator/DCC/1808.08750v1_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/1809.01361_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/1809.01361_continuous> a :Generic ;
    :hasText "continuous" .

<https://github.com/deepcurator/DCC/1809.01361_cross-domain> a :Task ;
    :hasText "cross-domain" .

<https://github.com/deepcurator/DCC/1809.01361_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/1809.01361_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/1809.01361_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/1809.01361_domain_adaptation> a :Method ;
    :hasText "domain_adaptation" .

<https://github.com/deepcurator/DCC/1809.01361_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/1809.01361_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/1809.01361_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/1809.01361_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/1809.01361_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/1809.01361_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/1809.01361_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/1809.01361_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/1809.01361_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1809.01361_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/1809.01361_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/1809.01361_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/1809.02840v2_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/1809.02840v2_constraints> a :Generic ;
    :hasText "constraints" .

<https://github.com/deepcurator/DCC/1809.02840v2_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/1809.02840v2_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/1809.02840v2_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/1809.02840v2_logic> a :Other ;
    :hasText "logic" .

<https://github.com/deepcurator/DCC/1809.02840v2_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/1809.02840v2_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/1809.02840v2_neural_network_models> a :Method ;
    :hasText "neural_network_models" .

<https://github.com/deepcurator/DCC/1809.02840v2_outputs> a :Generic ;
    :hasText "outputs" .

<https://github.com/deepcurator/DCC/1809.02840v2_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/1809.02840v2_problems> a :Generic ;
    :hasText "problems" .

<https://github.com/deepcurator/DCC/1809.02840v2_programs> a :Generic ;
    :hasText "programs" .

<https://github.com/deepcurator/DCC/1809.02840v2_recurrent_neural_network> a :Method ;
    :hasText "recurrent_neural_network" .

<https://github.com/deepcurator/DCC/1809.02840v2_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/1809.02840v2_search> a :Task ;
    :hasText "search" .

<https://github.com/deepcurator/DCC/1809.02840v2_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/1809.02840v2_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_auto-encoders> a :Method ;
    :hasText "auto-encoders" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_backpropagation> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_clustering_techniques> a :Method ;
    :hasText "clustering_techniques" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_decoder> a :Method ;
    :hasText "decoder" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_deep_neural_network> a :Method ;
    :hasText "deep_neural_network" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_map> a :Eval ;
    :hasText "map" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_pre-training> a :Method ;
    :hasText "pre-training" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_procedure> a :Generic ;
    :hasText "procedure" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_strategies> a :Generic ;
    :hasText "strategies" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_structures> a :Generic ;
    :hasText "structures" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_subspace> a :Other ;
    :hasText "subspace" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6608-deep-subspace-clustering-networks_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_gradients> a :Other ;
    :hasText "gradients" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_long-term> a :Generic ;
    :hasText "long-term" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_matching> a :Task ;
    :hasText "matching" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_measure> a :Generic ;
    :hasText "measure" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_measures> a :Generic ;
    :hasText "measures" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_memory_capacity> a :Other ;
    :hasText "memory_capacity" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_recurrent_neural_networks> a :Method ;
    :hasText "recurrent_neural_networks" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_resolution> a :Eval ;
    :hasText "resolution" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_rnn> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_rnns> a :Method ;
    :hasText "rnns" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_skip_connections> a :Other ;
    :hasText "skip_connections" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_theory> a :Generic ;
    :hasText "theory" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6613-dilated-recurrent-neural-networks_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_abstracts> a :Material ;
    :hasText "abstracts" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_embeddings> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_family> a :Generic ;
    :hasText "family" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_hierarchical> a :Generic ;
    :hasText "hierarchical" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_predicting> a :Task ;
    :hasText "predicting" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_states> a :Generic ;
    :hasText "states" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_statistical_information> a :Other ;
    :hasText "statistical_information" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_strategies> a :Generic ;
    :hasText "strategies" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_structured> a :Generic ;
    :hasText "structured" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_studies> a :Generic ;
    :hasText "studies" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6629-structured-embedding-models-for-grouped-data_words> a :Generic ;
    :hasText "words" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_approximation> a :Generic ;
    :hasText "approximation" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_attention_mechanism> a :Method ;
    :hasText "attention_mechanism" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_automatic_metrics> a :Eval ;
    :hasText "automatic_metrics" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_candidate> a :Generic ;
    :hasText "candidate" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_conversations> a :Material ;
    :hasText "conversations" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_counterparts> a :Generic ;
    :hasText "counterparts" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_cross-entropy> a :Method ;
    :hasText "cross-entropy" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_dialog> a :Other ;
    :hasText "dialog" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_dialog_model> a :Method ;
    :hasText "dialog_model" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_dialog_models> a :Method ;
    :hasText "dialog_models" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_differentiability> a :Generic ;
    :hasText "differentiability" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_discrete> a :Generic ;
    :hasText "discrete" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_diversity> a :Other ;
    :hasText "diversity" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_estimation> a :Generic ;
    :hasText "estimation" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_generation> a :Task ;
    :hasText "generation" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_gradients> a :Other ;
    :hasText "gradients" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_knowledge> a :Generic ;
    :hasText "knowledge" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_maximum_likelihood> a :Other ;
    :hasText "maximum_likelihood" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_paradigm> a :Generic ;
    :hasText "paradigm" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_rank> a :Other ;
    :hasText "rank" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_rnn> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_semantic> a :Other ;
    :hasText "semantic" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_similarities> a :Generic ;
    :hasText "similarities" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_transfer> a :Other ;
    :hasText "transfer" .

<https://github.com/deepcurator/DCC/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_3d_reconstruction> a :Method ;
    :hasText "3d_reconstruction" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_geometric_constraints> a :Other ;
    :hasText "geometric_constraints" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_priors> a :Other ;
    :hasText "priors" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_shape> a :Other ;
    :hasText "shape" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_surfaces> a :Other ;
    :hasText "surfaces" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6640-learning-a-multi-view-stereo-machine_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_covariance> a :Other ;
    :hasText "covariance" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matching> a :Task ;
    :hasText "matching" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_matrix> a :Generic ;
    :hasText "matrix" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_textures> a :Other ;
    :hasText "textures" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_transfer> a :Other ;
    :hasText "transfer" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_visual_quality> a :Eval ;
    :hasText "visual_quality" .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_activation_functions> a :Generic ;
    :hasText "activation_functions" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_divergence> a :Generic ;
    :hasText "divergence" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_divergences> a :Generic ;
    :hasText "divergences" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_generalization> a :Generic ;
    :hasText "generalization" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_improvements> a :Eval ;
    :hasText "improvements" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_kl> a :Generic ;
    :hasText "kl" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_means> a :Generic ;
    :hasText "means" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_regular> a :Generic ;
    :hasText "regular" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6649-f-gans-in-an-information-geometric-nutshell_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_diversity> a :Other ;
    :hasText "diversity" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_map> a :Eval ;
    :hasText "map" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_outputs> a :Generic ;
    :hasText "outputs" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_problems> a :Generic ;
    :hasText "problems" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_experimental_results> a :Generic ;
    :hasText "experimental_results" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_learner> a :Generic ;
    :hasText "learner" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_mappings> a :Other ;
    :hasText "mappings" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_translates> a :Task ;
    :hasText "translates" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6677-one-sided-unsupervised-domain-mapping_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_autoencoders> a :Method ;
    :hasText "autoencoders" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_deblurring> a :Task ;
    :hasText "deblurring" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_field> a :Generic ;
    :hasText "field" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_formulation> a :Generic ;
    :hasText "formulation" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_gradient_descent> a :Method ;
    :hasText "gradient_descent" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_image_restoration> a :Task ;
    :hasText "image_restoration" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_mean-shift> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/mean_shift> ;
    :hasText "mean-shift" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_minimization> a :Task ;
    :hasText "minimization" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_noise> a :Other ;
    :hasText "noise" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_prior> a :Generic ;
    :hasText "prior" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_problems> a :Generic ;
    :hasText "problems" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_super-resolution> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/superresolution> ;
    :hasText "super-resolution" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6678-deep-mean-shift-priors-for-image-restoration_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_audio> a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_convolutional_neural_networks> a :Method ;
    :hasText "convolutional_neural_networks" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_correlations> a :Other ;
    :hasText "correlations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_locations> a :Generic ;
    :hasText "locations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_principles> a :Generic ;
    :hasText "principles" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structural_variations> a :Other ;
    :hasText "structural_variations" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_structured_data> a :Material ;
    :hasText "structured_data" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_those> a :Generic ;
    :hasText "those" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_video> a :Material ;
    :hasText "video" .

<https://github.com/deepcurator/DCC/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_embeddings> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_graphs> a :Other ;
    :hasText "graphs" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_local_neighborhood> a :Generic ;
    :hasText "local_neighborhood" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_node> a :Other ;
    :hasText "node" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_nodes> a :Other ;
    :hasText "nodes" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_sampling> a :Method ;
    :hasText "sampling" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_text> a :Material ;
    :hasText "text" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6703-inductive-representation-learning-on-large-graphs_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_error_rate> a :Eval ;
    :hasText "error_rate" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_imagenet> a :Material ;
    :hasText "imagenet" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_large_datasets> a :Material ;
    :hasText "large_datasets" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_per> a :Eval ;
    :hasText "per" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_semi-supervised_learning> a :Task ;
    :hasText "semi-supervised_learning" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_svhn> a :Material ;
    :hasText "svhn" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_targets> a :Generic ;
    :hasText "targets" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results_weights> a :Generic ;
    :hasText "weights" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_bound> a :Generic ;
    :hasText "bound" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_distributed> a :Generic ;
    :hasText "distributed" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_gradients> a :Other ;
    :hasText "gradients" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_parallelism> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/parallelism> ;
    :hasText "parallelism" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_scalability> a :Eval ;
    :hasText "scalability" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_source_code> a :Generic ;
    :hasText "source_code" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_continuous> a :Generic ;
    :hasText "continuous" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_correlation> a :Other ;
    :hasText "correlation" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_discrete> a :Generic ;
    :hasText "discrete" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_hypotheses> a :Generic ;
    :hasText "hypotheses" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_interpretable> a :Generic ;
    :hasText "interpretable" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_location> a :Generic ;
    :hasText "location" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_procedure> a :Generic ;
    :hasText "procedure" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_real_datasets> a :Generic ;
    :hasText "real_datasets" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_studies> a :Generic ;
    :hasText "studies" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_tests> a :Generic ;
    :hasText "tests" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6752-neuralfdr-learning-discovery-thresholds-from-hypothesis-features_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_classifier> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_deep_convolutional_networks> a :Method ;
    :hasText "deep_convolutional_networks" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_deep_networks> a :Method ;
    :hasText "deep_networks" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_large-scale_data> a :Generic ;
    :hasText "large-scale_data" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_multi-task_learning> a :Task ;
    :hasText "multi-task_learning" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_priors> a :Other ;
    :hasText "priors" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_tensor> a :Generic ;
    :hasText "tensor" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_tensors> a :Generic ;
    :hasText "tensors" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6757-learning-multiple-tasks-with-multilinear-relationship-networks_transfer> a :Other ;
    :hasText "transfer" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_cifar-100> a :Material ;
    :hasText "cifar-100" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_deep_learning_models> a :Method ;
    :hasText "deep_learning_models" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_deep_models> a :Method ;
    :hasText "deep_models" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_generalization> a :Generic ;
    :hasText "generalization" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_generalization_performance> a :Eval ;
    :hasText "generalization_performance" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_imagenet> a :Material ;
    :hasText "imagenet" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_sizes> a :Other ;
    :hasText "sizes" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_statistical_model> a :Method ;
    :hasText "statistical_model" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_stochastic_gradient_descent> a :Method ;
    :hasText "stochastic_gradient_descent" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_validate> a :Task ;
    :hasText "validate" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks_weights> a :Generic ;
    :hasText "weights" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_algorithms> a :Generic ;
    :hasText "algorithms" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_current_algorithms> a :Method ;
    :hasText "current_algorithms" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_factors> a :Generic ;
    :hasText "factors" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_field> a :Generic ;
    :hasText "field" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_formalism> a :Generic ;
    :hasText "formalism" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_properties> a :Generic ;
    :hasText "properties" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6779-the-numerics-of-gans_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_blocks> a :Other ;
    :hasText "blocks" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_computational_cost> a :Eval ;
    :hasText "computational_cost" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_generation> a :Task ;
    :hasText "generation" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_generative_models> a :Method ;
    :hasText "generative_models" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_mismatch> a :Other ;
    :hasText "mismatch" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_procedure> a :Generic ;
    :hasText "procedure" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_regularizer> a :Method ;
    :hasText "regularizer" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6797-stabilizing-training-of-generative-adversarial-networks-through-regularization_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_algorithms> a :Generic ;
    :hasText "algorithms" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_application> a :Generic ;
    :hasText "application" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_convex> a :Other ;
    :hasText "convex" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_deep_networks> a :Method ;
    :hasText "deep_networks" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_hyperparameters> a :Generic ;
    :hasText "hyperparameters" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_learning_methods> a :Method ;
    :hasText "learning_methods" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_objective_function> a :Other ;
    :hasText "objective_function" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_procedure> a :Generic ;
    :hasText "procedure" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_stochastic_gradient_descent> a :Method ;
    :hasText "stochastic_gradient_descent" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6811-training-deep-networks-without-learning-rates-through-coin-betting_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_computational_efficiency> a :Eval ;
    :hasText "computational_efficiency" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_deep_generative_model> a :Method ;
    :hasText "deep_generative_model" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_gradient_descent> a :Method ;
    :hasText "gradient_descent" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_matching> a :Task ;
    :hasText "matching" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_measure> a :Generic ;
    :hasText "measure" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_sizes> a :Other ;
    :hasText "sizes" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_topology> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/topology> ;
    :hasText "topology" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6815-mmd-gan-towards-deeper-understanding-of-moment-matching-network_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_average_precision> a :Eval ;
    :hasText "average_precision" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_concepts> a :Other ;
    :hasText "concepts" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_information_retrieval> a :Task ;
    :hasText "information_retrieval" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_ones> a :Generic ;
    :hasText "ones" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_rankings> a :Other ;
    :hasText "rankings" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_ranks> a :Other ;
    :hasText "ranks" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_relevance> a :Eval ;
    :hasText "relevance" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_retrieval> a :Task ;
    :hasText "retrieval" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_structured> a :Generic ;
    :hasText "structured" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_understanding> a :Task ;
    :hasText "understanding" .

<https://github.com/deepcurator/DCC/6820-few-shot-learning-through-an-information-retrieval-lens_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_continuous> a :Generic ;
    :hasText "continuous" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_discrete> a :Generic ;
    :hasText "discrete" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_knowledge_base> a :Material ;
    :hasText "knowledge_base" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_logic> a :Other ;
    :hasText "logic" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_prior_work> a :Generic ;
    :hasText "prior_work" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_rules> a :Other ;
    :hasText "rules" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_activity> a :Other ;
    :hasText "activity" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_algorithms> a :Generic ;
    :hasText "algorithms" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_dictionary_learning> a :Task ;
    :hasText "dictionary_learning" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_extraction> a :Task ;
    :hasText "extraction" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_imaging> a :Generic ;
    :hasText "imaging" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_indicators> a :Generic ;
    :hasText "indicators" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_manually_annotated_data> a :Material ;
    :hasText "manually_annotated_data" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_motion> a :Generic ;
    :hasText "motion" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_research> a :Generic ;
    :hasText "research" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_constraints> a :Generic ;
    :hasText "constraints" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_development> a :Material ;
    :hasText "development" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_divergence> a :Generic ;
    :hasText "divergence" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_divergences> a :Generic ;
    :hasText "divergences" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_extension> a :Generic ;
    :hasText "extension" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_formulation> a :Generic ;
    :hasText "formulation" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_generation> a :Task ;
    :hasText "generation" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_gradient_penalty> a :Generic ;
    :hasText "gradient_penalty" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_gradients> a :Other ;
    :hasText "gradients" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_high_computational_cost> a :Eval ;
    :hasText "high_computational_cost" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_kl> a :Generic ;
    :hasText "kl" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_measures> a :Generic ;
    :hasText "measures" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_metrics> a :Generic ;
    :hasText "metrics" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_minimize> a :Task ;
    :hasText "minimize" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_penalty> a :Eval ;
    :hasText "penalty" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_probability_distributions> a :Other ;
    :hasText "probability_distributions" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_real_data> a :Material ;
    :hasText "real_data" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_validate> a :Task ;
    :hasText "validate" .

<https://github.com/deepcurator/DCC/6845-fisher-gan_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_codes> a :Material ;
    :hasText "codes" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_computations> a :Generic ;
    :hasText "computations" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_concept> a :Other ;
    :hasText "concept" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_deep_neural_network> a :Method ;
    :hasText "deep_neural_network" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_dnns> a :Method ;
    :hasText "dnns" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_family> a :Generic ;
    :hasText "family" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_imaging> a :Generic ;
    :hasText "imaging" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_matrices> a :Generic ;
    :hasText "matrices" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_memory> a :Other ;
    :hasText "memory" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_poses> a :Other ;
    :hasText "poses" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_recognition_task> a :Task ;
    :hasText "recognition_task" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_regions> a :Generic ;
    :hasText "regions" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_shape> a :Other ;
    :hasText "shape" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_studies> a :Generic ;
    :hasText "studies" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_temporal_information> a :Other ;
    :hasText "temporal_information" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_tool> a :Generic ;
    :hasText "tool" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_ways> a :Generic ;
    :hasText "ways" .

<https://github.com/deepcurator/DCC/6849-toward-goal-driven-neural-network-models-for-the-rodent-whisker-trigeminal-system_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_candidate> a :Generic ;
    :hasText "candidate" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_candidates> a :Generic ;
    :hasText "candidates" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_domain_experts> a :Other ;
    :hasText "domain_experts" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_edges> a :Other ;
    :hasText "edges" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_magnitude> a :Other ;
    :hasText "magnitude" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_nodes> a :Other ;
    :hasText "nodes" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_solution> a :Generic ;
    :hasText "solution" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_template-based_approach> a :Method ;
    :hasText "template-based_approach" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_template-free_approach> a :Method ;
    :hasText "template-free_approach" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_templates> a :Other ;
    :hasText "templates" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_transformations> a :Generic ;
    :hasText "transformations" .

<https://github.com/deepcurator/DCC/6854-predicting-organic-reaction-outcomes-with-weisfeiler-lehman-network_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_continuous> a :Generic ;
    :hasText "continuous" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_discrete> a :Generic ;
    :hasText "discrete" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_estimates> a :Generic ;
    :hasText "estimates" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_faster_convergence> a :Eval ;
    :hasText "faster_convergence" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_hyperparameter> a :Generic ;
    :hasText "hyperparameter" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_latent_variables> a :Other ;
    :hasText "latent_variables" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_log-likelihood> a :Other ;
    :hasText "log-likelihood" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_relaxation> a :Generic ;
    :hasText "relaxation" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_variables> a :Other ;
    :hasText "variables" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_variance> a :Generic ;
    :hasText "variance" .

<https://github.com/deepcurator/DCC/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_agents> a :Generic ;
    :hasText "agents" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_environment> a :Generic ;
    :hasText "environment" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_environments> a :Generic ;
    :hasText "environments" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_per> a :Eval ;
    :hasText "per" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_platform> a :Generic ;
    :hasText "platform" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_reinforcement_learning> a :Task ;
    :hasText "reinforcement_learning" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_reinforcement_learning_methods> a :Task ;
    :hasText "reinforcement_learning_methods" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_relu> a :Other ;
    :hasText "relu" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_research> a :Generic ;
    :hasText "research" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_rl> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_rule> a :Other ;
    :hasText "rule" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_strategies> a :Generic ;
    :hasText "strategies" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_strategy> a :Generic ;
    :hasText "strategy" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_training_parameters> a :Eval ;
    :hasText "training_parameters" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_analogy> a :Other ;
    :hasText "analogy" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_database> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/database_systems> ;
    :hasText "database" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_divergences> a :Generic ;
    :hasText "divergences" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_evaluations> a :Generic ;
    :hasText "evaluations" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experimental_results> a :Generic ;
    :hasText "experimental_results" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_imagenet> a :Material ;
    :hasText "imagenet" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kl> a :Generic ;
    :hasText "kl" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_kullback-leibler> a :Generic ;
    :hasText "kullback-leibler" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_novel_approach> a :Generic ;
    :hasText "novel_approach" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_objective_function> a :Other ;
    :hasText "objective_function" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_properties> a :Generic ;
    :hasText "properties" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_rewards> a :Eval ;
    :hasText "rewards" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_theoretical_analysis> a :Generic ;
    :hasText "theoretical_analysis" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_action> a :Generic ;
    :hasText "action" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_entropy> a :Other ;
    :hasText "entropy" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_minimizes> a :Task ;
    :hasText "minimizes" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_policy> a :Other ;
    :hasText "policy" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_q-learning> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/q-learning> ;
    :hasText "q-learning" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_q-learning_algorithms> a :Method ;
    :hasText "q-learning_algorithms" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_reinforcement_learning_(rl)> a :Task ;
    :hasText "reinforcement_learning_(rl)" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_rl> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_approximation> a :Generic ;
    :hasText "approximation" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_construction> a :Task ;
    :hasText "construction" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_generalisation> a :Generic ;
    :hasText "generalisation" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_improvements> a :Eval ;
    :hasText "improvements" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_kernels> a :Method ;
    :hasText "kernels" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_mnist> a :Material ;
    :hasText "mnist" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_posterior_inference> a :Task ;
    :hasText "posterior_inference" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_process_models> a :Method ;
    :hasText "process_models" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/6877-convolutional-gaussian-processes_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_constraints> a :Generic ;
    :hasText "constraints" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_deep_models> a :Method ;
    :hasText "deep_models" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_embeddings> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_ensembles> a :Task ;
    :hasText "ensembles" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_lattice> a :Other ;
    :hasText "lattice" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_lattices> a :Other ;
    :hasText "lattices" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_nodes> a :Other ;
    :hasText "nodes" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_projections> a :Other ;
    :hasText "projections" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_real-world_datasets> a :Material ;
    :hasText "real-world_datasets" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_stochastic_gradients> a :Other ;
    :hasText "stochastic_gradients" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_tensorflow> a :Other ;
    :hasText "tensorflow" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6891-deep-lattice-networks-and-partial-monotonic-functions_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_activity_recognition> a :Task ;
    :hasText "activity_recognition" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_environments> a :Generic ;
    :hasText "environments" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_hierarchical> a :Generic ;
    :hasText "hierarchical" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_kitti> a :Material ;
    :hasText "kitti" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_loss_function> a :Method ;
    :hasText "loss_function" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_priori> a :Other ;
    :hasText "priori" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_region> a :Generic ;
    :hasText "region" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_tracking> a :Task ;
    :hasText "tracking" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_videos> a :Material ;
    :hasText "videos" .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_data_augmentation> a :Method ;
    :hasText "data_augmentation" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_domain_experts> a :Other ;
    :hasText "domain_experts" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_f1> a :Eval ;
    :hasText "f1" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_improvements> a :Eval ;
    :hasText "improvements" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_medical_imaging> a :Material ;
    :hasText "medical_imaging" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_relation_extraction> a :Task ;
    :hasText "relation_extraction" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_technique> a :Generic ;
    :hasText "technique" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_text> a :Material ;
    :hasText "text" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformation_model> a :Method ;
    :hasText "transformation_model" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_transformations> a :Generic ;
    :hasText "transformations" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_unlabeled_data> a :Other ;
    :hasText "unlabeled_data" .

<https://github.com/deepcurator/DCC/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_drop-in_replacement> a :Other ;
    :hasText "drop-in_replacement" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_experimental_results> a :Generic ;
    :hasText "experimental_results" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_flow> a :Other ;
    :hasText "flow" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_improvement> a :Eval ;
    :hasText "improvement" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_language_modeling> a :Task ;
    :hasText "language_modeling" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_lstms> a :Method ;
    :hasText "lstms" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_penn_treebank> a :Material ;
    :hasText "penn_treebank" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_rnns> a :Method ;
    :hasText "rnns" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_solutions> a :Generic ;
    :hasText "solutions" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_treatment> a :Generic ;
    :hasText "treatment" .

<https://github.com/deepcurator/DCC/6919-language-modeling-with-recurrent-highway-hypernetworks_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_compression> a :Task ;
    :hasText "compression" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_computational_efficiency> a :Eval ;
    :hasText "computational_efficiency" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_factors> a :Generic ;
    :hasText "factors" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_hierarchical> a :Generic ;
    :hasText "hierarchical" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_nodes> a :Other ;
    :hasText "nodes" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_posterior> a :Generic ;
    :hasText "posterior" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_precision> a :Eval ;
    :hasText "precision" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_priors> a :Other ;
    :hasText "priors" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_significance> a :Other ;
    :hasText "significance" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_sparsity> a :Other ;
    :hasText "sparsity" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6921-bayesian-compression-for-deep-learning_weights> a :Generic ;
    :hasText "weights" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_approximations> a :Generic ;
    :hasText "approximations" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_estimates> a :Generic ;
    :hasText "estimates" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_field> a :Generic ;
    :hasText "field" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_heuristics> a :Method ;
    :hasText "heuristics" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_hyperparameter> a :Generic ;
    :hasText "hyperparameter" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_hyperparameters> a :Generic ;
    :hasText "hyperparameters" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_locations> a :Generic ;
    :hasText "locations" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_posterior> a :Generic ;
    :hasText "posterior" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_probabilistic_models> a :Method ;
    :hasText "probabilistic_models" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_real-world_datasets> a :Material ;
    :hasText "real-world_datasets" .

<https://github.com/deepcurator/DCC/6922-streaming-sparse-gaussian-process-approximations_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_concept> a :Other ;
    :hasText "concept" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_convolutions> a :Method ;
    :hasText "convolutions" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_environments> a :Generic ;
    :hasText "environments" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_euclidean_spaces> a :Other ;
    :hasText "euclidean_spaces" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_field> a :Generic ;
    :hasText "field" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_image_analysis> a :Task ;
    :hasText "image_analysis" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_knowledge> a :Generic ;
    :hasText "knowledge" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_sphere> a :Generic ;
    :hasText "sphere" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_strategies> a :Generic ;
    :hasText "strategies" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6935-spherical-convolutions-and-their-application-in-molecular-modelling_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_cnn> a :Method ;
    :hasText "cnn" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_computations> a :Generic ;
    :hasText "computations" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_estimation> a :Generic ;
    :hasText "estimation" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_experimental_techniques> a :Generic ;
    :hasText "experimental_techniques" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_experimental_time> a :Generic ;
    :hasText "experimental_time" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_feature_spaces> a :Other ;
    :hasText "feature_spaces" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_field> a :Generic ;
    :hasText "field" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_locations> a :Generic ;
    :hasText "locations" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_scales> a :Other ;
    :hasText "scales" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_surface> a :Other ;
    :hasText "surface" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6942-neural-system-identification-for-large-populations-separating-what-and-where_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_missing_data> a :Material ;
    :hasText "missing_data" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_unsupervised_learning> a :Method ;
    :hasText "unsupervised_learning" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_video> a :Material ;
    :hasText "video" .

<https://github.com/deepcurator/DCC/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning_videos> a :Material ;
    :hasText "videos" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_audio> a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_candidate> a :Generic ;
    :hasText "candidate" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_ensembles> a :Task ;
    :hasText "ensembles" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_formulation> a :Generic ;
    :hasText "formulation" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_interpretable> a :Generic ;
    :hasText "interpretable" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_interventions> a :Other ;
    :hasText "interventions" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_posterior> a :Generic ;
    :hasText "posterior" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_semi-supervised_learning> a :Task ;
    :hasText "semi-supervised_learning" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_smoothing> a :Method ;
    :hasText "smoothing" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_svhn> a :Material ;
    :hasText "svhn" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6953-bayesian-gan_weights> a :Generic ;
    :hasText "weights" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_convolutional_neural_network> a :Method ;
    :hasText "convolutional_neural_network" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_formulations> a :Generic ;
    :hasText "formulations" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_graphs> a :Other ;
    :hasText "graphs" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_matrix> a :Generic ;
    :hasText "matrix" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_novel_approach> a :Generic ;
    :hasText "novel_approach" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_priors> a :Other ;
    :hasText "priors" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_recurrent_neural_network> a :Method ;
    :hasText "recurrent_neural_network" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_smoothness> a :Other ;
    :hasText "smoothness" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structured> a :Generic ;
    :hasText "structured" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_structures> a :Generic ;
    :hasText "structures" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_system> a :Generic ;
    :hasText "system" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks_these_techniques> a :Generic ;
    :hasText "these_techniques" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_approximating> a :Task ;
    :hasText "approximating" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_conjunctions> a :Other ;
    :hasText "conjunctions" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_eigenvectors> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/eigenvectors> ;
    :hasText "eigenvectors" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_gradient_descent> a :Method ;
    :hasText "gradient_descent" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_improvements> a :Eval ;
    :hasText "improvements" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_kernel_methods> a :Method ;
    :hasText "kernel_methods" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_kernels> a :Method ;
    :hasText "kernels" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_large_datasets> a :Material ;
    :hasText "large_datasets" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_scheme> a :Generic ;
    :hasText "scheme" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_sgd> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/stochastic_gradient_descent> ;
    :hasText "sgd" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_significance> a :Other ;
    :hasText "significance" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_theoretical_analysis> a :Generic ;
    :hasText "theoretical_analysis" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6968-diving-into-the-shallows-a-computational-perspective-on-large-scale-shallow-learning_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_factors> a :Generic ;
    :hasText "factors" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_human_actions> a :Other ;
    :hasText "human_actions" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_imitation_learning> a :Method ;
    :hasText "imitation_learning" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_interpretable> a :Generic ;
    :hasText "interpretable" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_reward> a :Eval ;
    :hasText "reward" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_signal> a :Generic ;
    :hasText "signal" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_classification_accuracy> a :Eval ;
    :hasText "classification_accuracy" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_cnns> a :Method ;
    :hasText "cnns" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_convolution> a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_convolutional_networks> a :Method ;
    :hasText "convolutional_networks" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_counterparts> a :Generic ;
    :hasText "counterparts" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_faster_convergence> a :Eval ;
    :hasText "faster_convergence" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_formulation> a :Generic ;
    :hasText "formulation" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_improvement> a :Eval ;
    :hasText "improvement" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_parameter_space> a :Other ;
    :hasText "parameter_space" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_visual_representation_learning> a :Method ;
    :hasText "visual_representation_learning" .

<https://github.com/deepcurator/DCC/6984-deep-hyperspherical-learning_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_algorithms> a :Generic ;
    :hasText "algorithms" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_computations> a :Generic ;
    :hasText "computations" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_flexibility> a :Eval ;
    :hasText "flexibility" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_hardware> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/computer_hardware> ;
    :hasText "hardware" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_implementation> a :Material ;
    :hasText "implementation" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_operations> a :Generic ;
    :hasText "operations" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_tensorflow> a :Other ;
    :hasText "tensorflow" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_approximation> a :Generic ;
    :hasText "approximation" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_classifier> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_classifiers> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_constraints> a :Generic ;
    :hasText "constraints" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_deep_neural_network> a :Method ;
    :hasText "deep_neural_network" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_functions> a :Generic ;
    :hasText "functions" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_inception> a :Method ;
    :hasText "inception" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_interpretability> a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/interpretability> ;
    :hasText "interpretability" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_machine_learning_applications> a :Task ;
    :hasText "machine_learning_applications" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_objective_function> a :Other ;
    :hasText "objective_function" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_times> a :Generic ;
    :hasText "times" .

<https://github.com/deepcurator/DCC/6993-streaming-weak-submodularity-interpreting-neural-networks-on-the-fly_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_bias> a :Method ;
    :hasText "bias" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_classes> a :Generic ;
    :hasText "classes" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_classifier> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_decisions> a :Generic ;
    :hasText "decisions" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_distances> a :Other ;
    :hasText "distances" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_few-shot_learning> a :Task ;
    :hasText "few-shot_learning" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_improvements> a :Eval ;
    :hasText "improvements" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/6996-prototypical-networks-for-few-shot-learning_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_approximations> a :Generic ;
    :hasText "approximations" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_covariance> a :Other ;
    :hasText "covariance" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_evaluations> a :Generic ;
    :hasText "evaluations" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_information_sources> a :Material ;
    :hasText "information_sources" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_objective_function> a :Other ;
    :hasText "objective_function" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_solution> a :Generic ;
    :hasText "solution" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_technique> a :Generic ;
    :hasText "technique" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_those> a :Generic ;
    :hasText "those" .

<https://github.com/deepcurator/DCC/7016-multi-information-source-optimization_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_applications> a :Generic ;
    :hasText "applications" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_computational_cost> a :Eval ;
    :hasText "computational_cost" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_flexibility> a :Eval ;
    :hasText "flexibility" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_image_classification> a :Task ;
    :hasText "image_classification" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_memory> a :Other ;
    :hasText "memory" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_pascal_voc> a :Material ;
    :hasText "pascal_voc" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_resnet> a :Method ;
    :hasText "resnet" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_resnext> a :Method ;
    :hasText "resnext" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_scene> a :Generic ;
    :hasText "scene" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_segmentation> a :Task ;
    :hasText "segmentation" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_times> a :Generic ;
    :hasText "times" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_topology> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/topology> ;
    :hasText "topology" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7033-dual-path-networks_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_approximation> a :Generic ;
    :hasText "approximation" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_generalizations> a :Generic ;
    :hasText "generalizations" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_independence> a :Other ;
    :hasText "independence" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_overfitting> a :Other ;
    :hasText "overfitting" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_posteriors> a :Generic ;
    :hasText "posteriors" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_scheme> a :Generic ;
    :hasText "scheme" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_uncertainty> a :Generic ;
    :hasText "uncertainty" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_variational_inference> a :Method ;
    :hasText "variational_inference" .

<https://github.com/deepcurator/DCC/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_encodes> a :Generic ;
    :hasText "encodes" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_model-based> a :Generic ;
    :hasText "model-based" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_model-free> a :Generic ;
    :hasText "model-free" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_ones> a :Generic ;
    :hasText "ones" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_planning> a :Other ;
    :hasText "planning" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_planning_algorithm> a :Method ;
    :hasText "planning_algorithm" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_policy> a :Other ;
    :hasText "policy" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_solution> a :Generic ;
    :hasText "solution" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7055-qmdp-net-deep-learning-for-planning-under-partial-observability_transfer> a :Other ;
    :hasText "transfer" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_applications> a :Generic ;
    :hasText "applications" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_components> a :Generic ;
    :hasText "components" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_deep_learning_models> a :Method ;
    :hasText "deep_learning_models" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_ensemble> a :Task ;
    :hasText "ensemble" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_interpretability> a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/interpretability> ;
    :hasText "interpretability" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_measures> a :Generic ;
    :hasText "measures" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_properties> a :Generic ;
    :hasText "properties" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_solution> a :Generic ;
    :hasText "solution" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_unification> a :Method ;
    :hasText "unification" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_unified_framework> a :Method ;
    :hasText "unified_framework" .

<https://github.com/deepcurator/DCC/7062-a-unified-approach-to-interpreting-model-predictions_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_embeddings> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_text> a :Material ;
    :hasText "text" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_tool> a :Generic ;
    :hasText "tool" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_variational_inference> a :Method ;
    :hasText "variational_inference" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7067-context-selection-for-embedding-models_words> a :Generic ;
    :hasText "words" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_baseline> a :Generic ;
    :hasText "baseline" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_cnn> a :Method ;
    :hasText "cnn" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_convolution> a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_criterion> a :Generic ;
    :hasText "criterion" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_matching> a :Task ;
    :hasText "matching" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_retrieval> a :Task ;
    :hasText "retrieval" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_sift> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/sift> ;
    :hasText "sift" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7068-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_applications> a :Generic ;
    :hasText "applications" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_baseline_methods> a :Generic ;
    :hasText "baseline_methods" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_errors> a :Other ;
    :hasText "errors" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_linear_combination> a :Method ;
    :hasText "linear_combination" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_pruning> a :Method ;
    :hasText "pruning" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_research> a :Generic ;
    :hasText "research" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_those> a :Generic ;
    :hasText "those" .

<https://github.com/deepcurator/DCC/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_neural_network_models> a :Method ;
    :hasText "neural_network_models" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_research> a :Generic ;
    :hasText "research" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_scalability> a :Eval ;
    :hasText "scalability" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_distances> a :Other ;
    :hasText "distances" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_hierarchical> a :Generic ;
    :hasText "hierarchical" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_local_features> a :Other ;
    :hasText "local_features" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_point_clouds> a :Other ;
    :hasText "point_clouds" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_prior_works> a :Generic ;
    :hasText "prior_works" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_scales> a :Other ;
    :hasText "scales" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_scenes> a :Generic ;
    :hasText "scenes" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_structures> a :Generic ;
    :hasText "structures" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_flows> a :Other ;
    :hasText "flows" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_kernels> a :Method ;
    :hasText "kernels" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_markov_chain> a :Other ;
    :hasText "markov_chain" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_markov_chain_monte_carlo> a :Method ;
    :hasText "markov_chain_monte_carlo" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_markov_chains> a :Other ;
    :hasText "markov_chains" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_mcmc> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/markov_chain_monte_carlo> ;
    :hasText "mcmc" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_posterior> a :Generic ;
    :hasText "posterior" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_proposals> a :Generic ;
    :hasText "proposals" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_schemes> a :Generic ;
    :hasText "schemes" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_training_method> a :Method ;
    :hasText "training_method" .

<https://github.com/deepcurator/DCC/7099-a-nice-mc-adversarial-training-for-mcmc_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_bn> a :Method ;
    :hasText "bn" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_deep_neural_network> a :Method ;
    :hasText "deep_neural_network" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_intrinsic_geometry> a :Other ;
    :hasText "intrinsic_geometry" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_rule> a :Other ;
    :hasText "rule" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7107-riemannian-approach-to-batch-normalization_weights> a :Generic ;
    :hasText "weights" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_adversarial_learning> a :Method ;
    :hasText "adversarial_learning" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_assumptions> a :Generic ;
    :hasText "assumptions" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_correspondence> a :Generic ;
    :hasText "correspondence" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_cross-domain> a :Task ;
    :hasText "cross-domain" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_domain> a :Generic ;
    :hasText "domain" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_generation> a :Task ;
    :hasText "generation" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_image_classification> a :Task ;
    :hasText "image_classification" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_matching> a :Task ;
    :hasText "matching" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_real_data> a :Material ;
    :hasText "real_data" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_supervision> a :Other ;
    :hasText "supervision" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_theory> a :Generic ;
    :hasText "theory" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/7109-triangle-generative-adversarial-networks_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_approximation> a :Generic ;
    :hasText "approximation" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_continuous> a :Generic ;
    :hasText "continuous" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_deep_reinforcement_learning> a :Method ;
    :hasText "deep_reinforcement_learning" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_discrete> a :Generic ;
    :hasText "discrete" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_environment> a :Generic ;
    :hasText "environment" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_improvement> a :Eval ;
    :hasText "improvement" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_knowledge> a :Generic ;
    :hasText "knowledge" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_policies> a :Other ;
    :hasText "policies" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_policy> a :Other ;
    :hasText "policy" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_region> a :Generic ;
    :hasText "region" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_rewards> a :Eval ;
    :hasText "rewards" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_approximating> a :Task ;
    :hasText "approximating" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_component> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/component> ;
    :hasText "component" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_deep_rl> a :Method ;
    :hasText "deep_rl" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_domains> a :Generic ;
    :hasText "domains" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_function> a :Generic ;
    :hasText "function" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_generalisation> a :Generic ;
    :hasText "generalisation" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_low-dimensional_representation> a :Method ;
    :hasText "low-dimensional_representation" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_reinforcement_learning_(rl)> a :Task ;
    :hasText "reinforcement_learning_(rl)" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_reward> a :Eval ;
    :hasText "reward" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_reward_function> a :Other ;
    :hasText "reward_function" .

<https://github.com/deepcurator/DCC/7123-hybrid-reward-architecture-for-reinforcement-learning_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_algorithm> a :Generic ;
    :hasText "algorithm" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_boosting_algorithms> a :Method ;
    :hasText "boosting_algorithms" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_component> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/component> ;
    :hasText "component" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_examples> a :Generic ;
    :hasText "examples" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_generative_models> a :Method ;
    :hasText "generative_models" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_iterative_procedure> a :Method ;
    :hasText "iterative_procedure" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_natural_images> a :Material ;
    :hasText "natural_images" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_procedure> a :Generic ;
    :hasText "procedure" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_regions> a :Generic ;
    :hasText "regions" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7126-adagan-boosting-generative-models_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_algorithms> a :Generic ;
    :hasText "algorithms" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_applications> a :Generic ;
    :hasText "applications" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_criteria> a :Generic ;
    :hasText "criteria" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_end-to-end> a :Generic ;
    :hasText "end-to-end" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_evaluations> a :Generic ;
    :hasText "evaluations" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_machine_learning_techniques> a :Task ;
    :hasText "machine_learning_techniques" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_policy> a :Other ;
    :hasText "policy" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_popularity> a :Other ;
    :hasText "popularity" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_probabilistic_machine_learning> a :Method ;
    :hasText "probabilistic_machine_learning" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_them> a :Generic ;
    :hasText "them" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/7132-task-based-end-to-end-model-learning-in-stochastic-optimization_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_conditions> a :Generic ;
    :hasText "conditions" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_convex> a :Other ;
    :hasText "convex" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_formulation> a :Generic ;
    :hasText "formulation" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_gradient_descent> a :Method ;
    :hasText "gradient_descent" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_parameters> a :Generic ;
    :hasText "parameters" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_procedure> a :Generic ;
    :hasText "procedure" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_regularization> a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7142-gradient-descent-gan-optimization-is-locally-stable_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_convolution> a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_deep_convolutional_neural_networks> a :Method ;
    :hasText "deep_convolutional_neural_networks" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_dnns> a :Method ;
    :hasText "dnns" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_extension> a :Generic ;
    :hasText "extension" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_field> a :Generic ;
    :hasText "field" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_latter> a :Generic ;
    :hasText "latter" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_layers> a :Other ;
    :hasText "layers" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_object_recognition> a :Task ;
    :hasText "object_recognition" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_pooling> a :Method ;
    :hasText "pooling" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_position> a :Other ;
    :hasText "position" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_robustness> a :Eval ;
    :hasText "robustness" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_targets> a :Generic ;
    :hasText "targets" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7146-do-deep-neural-networks-suffer-from-crowding_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_alternative> a :Generic ;
    :hasText "alternative" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_continuous> a :Generic ;
    :hasText "continuous" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_generations> a :Task ;
    :hasText "generations" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_generative_models> a :Method ;
    :hasText "generative_models" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_gradient> a :Other ;
    :hasText "gradient" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_hyperparameter> a :Generic ;
    :hasText "hyperparameter" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_language_models> a :Task ;
    :hasText "language_models" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_problems> a :Generic ;
    :hasText "problems" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_resnets> a :Method ;
    :hasText "resnets" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7159-improved-training-of-wasserstein-gans_weights> a :Generic ;
    :hasText "weights" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_assumptions> a :Generic ;
    :hasText "assumptions" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_autoencoders> a :Method ;
    :hasText "autoencoders" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_decoder> a :Method ;
    :hasText "decoder" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_flexibility> a :Eval ;
    :hasText "flexibility" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_graphical_model> a :Method ;
    :hasText "graphical_model" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_importance_sampling> a :Method ;
    :hasText "importance_sampling" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_interpretable> a :Generic ;
    :hasText "interpretable" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_procedure> a :Generic ;
    :hasText "procedure" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_semi-supervised_learning> a :Task ;
    :hasText "semi-supervised_learning" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_variables> a :Other ;
    :hasText "variables" .

<https://github.com/deepcurator/DCC/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanism> a :Method ;
    :hasText "attention_mechanism" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_attention_mechanisms> a :Method ;
    :hasText "attention_mechanisms" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu> a :Eval ;
    :hasText "bleu" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_bleu_score> a :Eval ;
    :hasText "bleu_score" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutional_neural_networks> a :Method ;
    :hasText "convolutional_neural_networks" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_convolutions> a :Method ;
    :hasText "convolutions" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_decoder> a :Method ;
    :hasText "decoder" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_ensembles> a :Task ;
    :hasText "ensembles" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_french> a :Material ;
    :hasText "french" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_machine_translation_tasks> a :Task ;
    :hasText "machine_translation_tasks" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_quality> a :Eval ;
    :hasText "quality" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_deep_reinforcement_learning_(rl)> a :Method ;
    :hasText "deep_reinforcement_learning_(rl)" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_environment> a :Generic ;
    :hasText "environment" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_experimental_results> a :Generic ;
    :hasText "experimental_results" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_model-based> a :Generic ;
    :hasText "model-based" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_model-free> a :Generic ;
    :hasText "model-free" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_observations> a :Generic ;
    :hasText "observations" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_planning> a :Other ;
    :hasText "planning" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_prediction_model> a :Method ;
    :hasText "prediction_model" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_predictions> a :Task ;
    :hasText "predictions" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_rewards> a :Eval ;
    :hasText "rewards" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_rl> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_states> a :Generic ;
    :hasText "states" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7192-value-prediction-network_way> a :Generic ;
    :hasText "way" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_autoencoders> a :Method ;
    :hasText "autoencoders" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_benchmarks> a :Material ;
    :hasText "benchmarks" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_factors> a :Generic ;
    :hasText "factors" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_latent_variable> a :Other ;
    :hasText "latent_variable" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_measure> a :Generic ;
    :hasText "measure" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_policy> a :Other ;
    :hasText "policy" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_structure> a :Generic ;
    :hasText "structure" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7223-causal-effect-inference-with-deep-latent-variable-models_treatment> a :Generic ;
    :hasText "treatment" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_classification_performance> a :Method ;
    :hasText "classification_performance" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_feature_matching> a :Method ;
    :hasText "feature_matching" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_formulation> a :Generic ;
    :hasText "formulation" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_generative_adversarial_networks> a :Method ;
    :hasText "generative_adversarial_networks" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_supervised_learning_methods> a :Method ;
    :hasText "supervised_learning_methods" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7229-good-semi-supervised-learning-that-requires-a-bad-gan_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_baselines> a :Generic ;
    :hasText "baselines" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_concepts> a :Other ;
    :hasText "concepts" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_inputs> a :Generic ;
    :hasText "inputs" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_language> a :Generic ;
    :hasText "language" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_maps> a :Eval ;
    :hasText "maps" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_mechanism> a :Generic ;
    :hasText "mechanism" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_question_answering> a :Task ;
    :hasText "question_answering" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_resnet> a :Method ;
    :hasText "resnet" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_vision> a :Generic ;
    :hasText "vision" .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_approximation> a :Generic ;
    :hasText "approximation" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_assumptions> a :Generic ;
    :hasText "assumptions" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_convergence> a :Generic ;
    :hasText "convergence" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_distance> a :Other ;
    :hasText "distance" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_dynamics> a :Generic ;
    :hasText "dynamics" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_gan> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_gans> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_generation> a :Task ;
    :hasText "generation" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_inception> a :Method ;
    :hasText "inception" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_loss_functions> a :Other ;
    :hasText "loss_functions" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_maximum_likelihood> a :Other ;
    :hasText "maximum_likelihood" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_networks> a :Generic ;
    :hasText "networks" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_ones> a :Generic ;
    :hasText "ones" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_optimization> a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_rule> a :Other ;
    :hasText "rule" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_stochastic_gradient_descent> a :Method ;
    :hasText "stochastic_gradient_descent" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_svhn> a :Material ;
    :hasText "svhn" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_theory> a :Generic ;
    :hasText "theory" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_clustering_method> a :Method ;
    :hasText "clustering_method" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_component> a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/component> ;
    :hasText "component" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_distributed> a :Generic ;
    :hasText "distributed" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_grouping> a :Generic ;
    :hasText "grouping" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_interaction> a :Generic ;
    :hasText "interaction" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_learned_representations> a :Generic ;
    :hasText "learned_representations" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_neural_network> a :Method ;
    :hasText "neural_network" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_these> a :Generic ;
    :hasText "these" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7246-neural-expectation-maximization_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_decisions> a :Generic ;
    :hasText "decisions" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_factors> a :Generic ;
    :hasText "factors" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_feature> a :Other ;
    :hasText "feature" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_long_short-term_memory> a :Method ;
    :hasText "long_short-term_memory" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_lstm> a :Method ;
    :hasText "lstm" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_maps> a :Eval ;
    :hasText "maps" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_marks> a :Generic ;
    :hasText "marks" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_modules> a :Generic ;
    :hasText "modules" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_per> a :Eval ;
    :hasText "per" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_positions> a :Other ;
    :hasText "positions" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_predicting> a :Task ;
    :hasText "predicting" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_scale> a :Other ;
    :hasText "scale" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_signals> a :Generic ;
    :hasText "signals" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_structured> a :Generic ;
    :hasText "structured" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_studies> a :Generic ;
    :hasText "studies" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_target> a :Generic ;
    :hasText "target" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_technologies> a :Generic ;
    :hasText "technologies" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/7255-attend-and-predict-understanding-gene-regulation-by-selective-attention-on-chromatin_visualization_methods> a :Method ;
    :hasText "visualization_methods" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_alignment> a :Task ;
    :hasText "alignment" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_effectiveness> a :Eval ;
    :hasText "effectiveness" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_family> a :Generic ;
    :hasText "family" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_machine_translation> a :Task ;
    :hasText "machine_translation" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_one> a :Generic ;
    :hasText "one" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_parallel_text> a :Material ;
    :hasText "parallel_text" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_problems> a :Generic ;
    :hasText "problems" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_recovery> a :Task ;
    :hasText "recovery" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_sentiment> a :Other ;
    :hasText "sentiment" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_tasks> a :Generic ;
    :hasText "tasks" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_text> a :Material ;
    :hasText "text" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_transfer> a :Other ;
    :hasText "transfer" .

<https://github.com/deepcurator/DCC/7259-style-transfer-from-non-parallel-text-by-cross-alignment_word_order> a :Other ;
    :hasText "word_order" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_cifar-10> a :Material ;
    :hasText "cifar-10" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_classifier> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_datasets> a :Material ;
    :hasText "datasets" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_detection_method> a :Method ;
    :hasText "detection_method" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_imagenet> a :Material ;
    :hasText "imagenet" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_interpretable> a :Generic ;
    :hasText "interpretable" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_maps> a :Eval ;
    :hasText "maps" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_other> a :Generic ;
    :hasText "other" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_our_method> a :Other ;
    :hasText "our_method" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_supervised_methods> a :Method ;
    :hasText "supervised_methods" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_systems> a :Generic ;
    :hasText "systems" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_time> a :Generic ;
    :hasText "time" .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_bias> a :Method ;
    :hasText "bias" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_matrix> a :Generic ;
    :hasText "matrix" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_metric> a :Generic ;
    :hasText "metric" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_observations> a :Generic ;
    :hasText "observations" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_theory> a :Generic ;
    :hasText "theory" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_understanding> a :Task ;
    :hasText "understanding" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_variance> a :Generic ;
    :hasText "variance" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_word_embedding> a :Method ;
    :hasText "word_embedding" .

<https://github.com/deepcurator/DCC/7368-on-the-dimensionality-of-word-embedding_word_embeddings> a :Method ;
    :hasText "word_embeddings" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_devices> a :Generic ;
    :hasText "devices" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_efficiency> a :Eval ;
    :hasText "efficiency" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_learning_algorithm> a :Method ;
    :hasText "learning_algorithm" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_linear_classification> a :Task ;
    :hasText "linear_classification" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_machine_learning> a :Task ;
    :hasText "machine_learning" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_network> a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_paradigm> a :Generic ;
    :hasText "paradigm" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_privacy> a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/privacy> ;
    :hasText "privacy" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_scalability> a :Eval ;
    :hasText "scalability" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7705-cola-decentralized-linear-learning_training_algorithm> a :Method ;
    :hasText "training_algorithm" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_adversarial> a :Generic ;
    :hasText "adversarial" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_classes> a :Generic ;
    :hasText "classes" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_classification> a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_classifier> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_deep_models> a :Method ;
    :hasText "deep_models" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_deep_neural_networks> a :Method ;
    :hasText "deep_neural_networks" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_experiments> a :Generic ;
    :hasText "experiments" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_gaussian_distributions> a :Method ;
    :hasText "gaussian_distributions" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_machine_learning_applications> a :Task ;
    :hasText "machine_learning_applications" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_mahalanobis_distance> a :Task ;
    :hasText "mahalanobis_distance" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_methods> a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_posterior> a :Generic ;
    :hasText "posterior" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_prior> a :Generic ;
    :hasText "prior" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_real-world> a :Generic ;
    :hasText "real-world" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_rule> a :Other ;
    :hasText "rule" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_architectures> a :Generic ;
    :hasText "architectures" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_complex> a :Method ;
    :hasText "complex" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_components> a :Generic ;
    :hasText "components" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_deep_learning> a :Generic ;
    :hasText "deep_learning" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_framework> a :Generic ;
    :hasText "framework" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_global_context> a :Other ;
    :hasText "global_context" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_images> a :Material ;
    :hasText "images" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model> a :Generic ;
    :hasText "model" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_model_design> a :Generic ;
    :hasText "model_design" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_modeling> a :Task ;
    :hasText "modeling" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_objects> a :Generic ;
    :hasText "objects" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction> a :Task ;
    :hasText "prediction" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_prediction_model> a :Method ;
    :hasText "prediction_model" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_principles> a :Generic ;
    :hasText "principles" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scene> a :Generic ;
    :hasText "scene" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_scenes> a :Generic ;
    :hasText "scenes" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_structured> a :Generic ;
    :hasText "structured" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_task> a :Generic ;
    :hasText "task" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_understanding> a :Task ;
    :hasText "understanding" .

<https://github.com/deepcurator/DCC/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_accuracy> a :Eval ;
    :hasText "accuracy" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_approach> a :Generic ;
    :hasText "approach" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_approaches> a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_architecture> a :Generic ;
    :hasText "architecture" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_baseline> a :Generic ;
    :hasText "baseline" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_concept> a :Other ;
    :hasText "concept" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_concepts> a :Other ;
    :hasText "concepts" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_convolutions> a :Method ;
    :hasText "convolutions" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_features> a :Generic ;
    :hasText "features" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_graph> a :Other ;
    :hasText "graph" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_image> a :Material ;
    :hasText "image" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_input> a :Generic ;
    :hasText "input" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_interactions> a :Generic ;
    :hasText "interactions" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_interpretability> a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/interpretability> ;
    :hasText "interpretability" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_learn> a :Task ;
    :hasText "learn" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_learner> a :Generic ;
    :hasText "learner" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_learns> a :Task ;
    :hasText "learns" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_module> a :Generic ;
    :hasText "module" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_natural_language_processing> a :Method ;
    :hasText "natural_language_processing" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_problem> a :Generic ;
    :hasText "problem" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_question_answering> a :Task ;
    :hasText "question_answering" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_representation> a :Generic ;
    :hasText "representation" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_representations> a :Generic ;
    :hasText "representations" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_results> a :Generic ;
    :hasText "results" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_semantic> a :Other ;
    :hasText "semantic" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_spatial_relationships> a :Other ;
    :hasText "spatial_relationships" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_strategy> a :Generic ;
    :hasText "strategy" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_techniques> a :Generic ;
    :hasText "techniques" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_this> a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_two> a :Generic ;
    :hasText "two" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_vision> a :Generic ;
    :hasText "vision" .

<https://github.com/deepcurator/DCC/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering_we> a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_activations> a :Other ;
    :hasText "activations" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_backpropagation> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_data> a :Generic ;
    :hasText "data" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_decoder> a :Method ;
    :hasText "decoder" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_encoder> a :Method ;
    :hasText "encoder" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_first> a :Generic ;
    :hasText "first" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_flexibility> a :Eval ;
    :hasText "flexibility" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_information> a :Generic ;
    :hasText "information" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_memory> a :Other ;
    :hasText "memory" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_method> a :Generic ;
    :hasText "method" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_models> a :Generic ;
    :hasText "models" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_neural_networks> a :Method ;
    :hasText "neural_networks" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_processing> a :Generic ;
    :hasText "processing" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_rnn> a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_rnns> a :Method ;
    :hasText "rnns" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_scheme> a :Generic ;
    :hasText "scheme" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_sequence-to-sequence> a :Method ;
    :hasText "sequence-to-sequence" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_state> a :Generic ;
    :hasText "state" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_states> a :Generic ;
    :hasText "states" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_technique> a :Generic ;
    :hasText "technique" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_that> a :Generic ;
    :hasText "that" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_they> a :Generic ;
    :hasText "they" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_training> a :Task ;
    :hasText "training" .

<https://github.com/deepcurator/DCC/8117-reversible-recurrent-neural-networks_we> a :Generic ;
    :hasText "we" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_complex a :Method ;
    :hasText "complex" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_convolutions a :Method ;
    :hasText "convolutions" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_domains a :Generic ;
    :hasText "domains" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_faces a :Other ;
    :hasText "faces" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_mappings a :Other ;
    :hasText "mappings" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_shape a :Other ;
    :hasText "shape" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_texture a :Other ;
    :hasText "texture" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Aaron_Gokaslan_Improving_Shape_Deformation_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_activations a :Other ;
    :hasText "activations" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_classification_performance a :Method ;
    :hasText "classification_performance" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_hyperparameter a :Generic ;
    :hasText "hyperparameter" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_overfitting a :Other ;
    :hasText "overfitting" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_overhead a :Other ;
    :hasText "overhead" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_prior_work a :Generic ;
    :hasText "prior_work" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_procedure a :Generic ;
    :hasText "procedure" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_segmentation_techniques a :Eval ;
    :hasText "segmentation_techniques" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_sizes a :Other ;
    :hasText "sizes" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_visual_classification a :Task ;
    :hasText "visual_classification" .

:Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_architecture a :Generic ;
    :hasText "architecture" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_data a :Generic ;
    :hasText "data" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_features a :Generic ;
    :hasText "features" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_image a :Material ;
    :hasText "image" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_images a :Material ;
    :hasText "images" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_input a :Generic ;
    :hasText "input" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_layers a :Other ;
    :hasText "layers" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_method a :Generic ;
    :hasText "method" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_metric a :Generic ;
    :hasText "metric" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_neighborhood a :Generic ;
    :hasText "neighborhood" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_network a :Generic ;
    :hasText "network" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_of_re-identification a :Task ;
    :hasText "of_re-identification" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_outputs a :Generic ;
    :hasText "outputs" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_overfitting a :Other ;
    :hasText "overfitting" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_person_re-identification a :Task ;
    :hasText "person_re-identification" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_problem a :Generic ;
    :hasText "problem" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_results a :Generic ;
    :hasText "results" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_state a :Generic ;
    :hasText "state" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_target a :Generic ;
    :hasText "target" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_that a :Generic ;
    :hasText "that" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_this a :Generic ;
    :hasText "this" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_training a :Task ;
    :hasText "training" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_two a :Generic ;
    :hasText "two" .

:Ahmed_An_Improved_Deep_2015_CVPR_paper_we a :Generic ;
    :hasText "we" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_action a :Generic ;
    :hasText "action" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_attention_mechanisms a :Method ;
    :hasText "attention_mechanisms" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_changing_backgrounds a :Other ;
    :hasText "changing_backgrounds" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_conditions a :Generic ;
    :hasText "conditions" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_continuous a :Generic ;
    :hasText "continuous" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_discrete a :Generic ;
    :hasText "discrete" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_expressions a :Generic ;
    :hasText "expressions" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_gans a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_generation a :Task ;
    :hasText "generation" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_lighting a :Other ;
    :hasText "lighting" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_magnitude a :Other ;
    :hasText "magnitude" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_scheme a :Generic ;
    :hasText "scheme" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Albert_Pumarola_Anatomically_Coherent_Facial_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_architectures a :Generic ;
    :hasText "architectures" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_connectivity a :Eval ;
    :hasText "connectivity" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_convolutions a :Method ;
    :hasText "convolutions" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_flow a :Other ;
    :hasText "flow" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_graph a :Other ;
    :hasText "graph" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_graphs a :Other ;
    :hasText "graphs" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_improvement a :Eval ;
    :hasText "improvement" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_layers a :Other ;
    :hasText "layers" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_node a :Other ;
    :hasText "node" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_nodes a :Other ;
    :hasText "nodes" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_properties a :Generic ;
    :hasText "properties" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_pruning a :Method ;
    :hasText "pruning" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_resnet a :Method ;
    :hasText "resnet" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_resnets a :Method ;
    :hasText "resnets" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_sizes a :Other ;
    :hasText "sizes" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_sparsity a :Other ;
    :hasText "sparsity" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_technique a :Generic ;
    :hasText "technique" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_theory a :Generic ;
    :hasText "theory" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_these_techniques a :Generic ;
    :hasText "these_techniques" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_architectures a :Generic ;
    :hasText "architectures" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_baselines a :Generic ;
    :hasText "baselines" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_color a :Other ;
    :hasText "color" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_data_modalities a :Generic ;
    :hasText "data_modalities" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_environments a :Generic ;
    :hasText "environments" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_mesh a :Other ;
    :hasText "mesh" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_per a :Eval ;
    :hasText "per" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_rgb_images a :Material ;
    :hasText "rgb_images" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_target a :Generic ;
    :hasText "target" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_collection a :Generic ;
    :hasText "collection" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_collections a :Generic ;
    :hasText "collections" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_mesh a :Other ;
    :hasText "mesh" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_object_category a :Other ;
    :hasText "object_category" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_per a :Eval ;
    :hasText "per" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_shape a :Other ;
    :hasText "shape" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_shapes a :Other ;
    :hasText "shapes" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_texture a :Other ;
    :hasText "texture" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_textures a :Other ;
    :hasText "textures" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_categories a :Generic ;
    :hasText "categories" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_classes a :Generic ;
    :hasText "classes" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_embeddings a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_prior_works a :Generic ;
    :hasText "prior_works" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_research a :Generic ;
    :hasText "research" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_sampling a :Method ;
    :hasText "sampling" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_solution a :Generic ;
    :hasText "solution" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper_visualgenome a :Material ;
    :hasText "visualgenome" .

:Antol_VQA_Visual_Question_ICCV_2015_paper_question_answering a :Task ;
    :hasText "question_answering" .

:Antol_VQA_Visual_Question_ICCV_2015_paper_task a :Generic ;
    :hasText "task" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_bias a :Method ;
    :hasText "bias" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_encoder-decoder a :Task ;
    :hasText "encoder-decoder" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_measure a :Generic ;
    :hasText "measure" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_patches a :Generic ;
    :hasText "patches" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_processing a :Generic ;
    :hasText "processing" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_rank a :Other ;
    :hasText "rank" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_research a :Generic ;
    :hasText "research" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_resolution a :Eval ;
    :hasText "resolution" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_rgb_images a :Material ;
    :hasText "rgb_images" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_transfer a :Other ;
    :hasText "transfer" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_architectures a :Generic ;
    :hasText "architectures" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_classification_tasks a :Method ;
    :hasText "classification_tasks" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_overhead a :Other ;
    :hasText "overhead" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_per a :Eval ;
    :hasText "per" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_pruning a :Method ;
    :hasText "pruning" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_quantization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/quantization> ;
    :hasText "quantization" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper_weights a :Generic ;
    :hasText "weights" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_diversity a :Other ;
    :hasText "diversity" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_expressions a :Generic ;
    :hasText "expressions" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_face a :Other ;
    :hasText "face" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_face_images a :Material ;
    :hasText "face_images" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_face_recognition a :Task ;
    :hasText "face_recognition" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_illuminations a :Other ;
    :hasText "illuminations" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_lighting a :Other ;
    :hasText "lighting" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_poses a :Other ;
    :hasText "poses" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_real_data a :Material ;
    :hasText "real_data" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_transfer a :Other ;
    :hasText "transfer" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_color a :Other ;
    :hasText "color" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_color_constancy a :Task ;
    :hasText "color_constancy" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_colors a :Other ;
    :hasText "colors" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_images a :Material ;
    :hasText "images" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_model a :Generic ;
    :hasText "model" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_modeling a :Task ;
    :hasText "modeling" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_objects a :Generic ;
    :hasText "objects" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_prediction a :Task ;
    :hasText "prediction" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_problem a :Generic ;
    :hasText "problem" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_scene a :Generic ;
    :hasText "scene" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_structured a :Generic ;
    :hasText "structured" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_task a :Generic ;
    :hasText "task" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_techniques a :Generic ;
    :hasText "techniques" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_that a :Generic ;
    :hasText "that" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_this a :Generic ;
    :hasText "this" .

:Barron_Convolutional_Color_Constancy_ICCV_2015_paper_we a :Generic ;
    :hasText "we" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_baseline_methods a :Generic ;
    :hasText "baseline_methods" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_field a :Generic ;
    :hasText "field" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_pose.pytorch a :Task ;
    :hasText "pose.pytorch" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_system a :Generic ;
    :hasText "system" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper_tracking a :Task ;
    :hasText "tracking" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_correspondences a :Generic ;
    :hasText "correspondences" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_generation a :Task ;
    :hasText "generation" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_gmm a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gaussian_mixture_model> ;
    :hasText "gmm" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_logo a :Other ;
    :hasText "logo" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_prior_works a :Generic ;
    :hasText "prior_works" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_research a :Generic ;
    :hasText "research" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_shape a :Other ;
    :hasText "shape" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_smoothness a :Other ;
    :hasText "smoothness" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_systems a :Generic ;
    :hasText "systems" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_target a :Generic ;
    :hasText "target" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_object_detectors a :Method ;
    :hasText "object_detectors" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_procedure a :Generic ;
    :hasText "procedure" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_baseline a :Generic ;
    :hasText "baseline" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concept a :Other ;
    :hasText "concept" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_concepts a :Other ;
    :hasText "concepts" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_embeddings a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_grounding a :Task ;
    :hasText "grounding" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_improvement a :Eval ;
    :hasText "improvement" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_layers a :Other ;
    :hasText "layers" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_phrases a :Generic ;
    :hasText "phrases" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_prior_works a :Generic ;
    :hasText "prior_works" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_region a :Generic ;
    :hasText "region" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_solution a :Generic ;
    :hasText "solution" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_subspaces a :Other ;
    :hasText "subspaces" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_text a :Material ;
    :hasText "text" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Buch_SST_Single-Stream_Temporal_CVPR_2017_paper_approach a :Generic ;
    :hasText "approach" .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper_novel_approach a :Generic ;
    :hasText "novel_approach" .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_cnn a :Method ;
    :hasText "cnn" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_data a :Generic ;
    :hasText "data" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_input a :Generic ;
    :hasText "input" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_magnitude a :Other ;
    :hasText "magnitude" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_method a :Generic ;
    :hasText "method" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_per a :Eval ;
    :hasText "per" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_resolution a :Eval ;
    :hasText "resolution" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_scene a :Generic ;
    :hasText "scene" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_state a :Generic ;
    :hasText "state" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_structure a :Generic ;
    :hasText "structure" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_text a :Material ;
    :hasText "text" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_text_recognition a :Task ;
    :hasText "text_recognition" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_training a :Task ;
    :hasText "training" .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper_two a :Generic ;
    :hasText "two" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_architecture a :Generic ;
    :hasText "architecture" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_databases a :Material ;
    :hasText "databases" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_experiments a :Generic ;
    :hasText "experiments" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_first a :Generic ;
    :hasText "first" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_learn a :Task ;
    :hasText "learn" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_model a :Generic ;
    :hasText "model" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_one a :Generic ;
    :hasText "one" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_results a :Generic ;
    :hasText "results" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_semantic_information a :Other ;
    :hasText "semantic_information" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_state a :Generic ;
    :hasText "state" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_task a :Generic ;
    :hasText "task" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_technique a :Generic ;
    :hasText "technique" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_transfer a :Other ;
    :hasText "transfer" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_two a :Generic ;
    :hasText "two" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_video a :Material ;
    :hasText "video" .

:Caelles_One-Shot_Video_Object_CVPR_2017_paper_video_segmentation a :Method ;
    :hasText "video_segmentation" .

:Camgoz_Neural_Sign_Language_CVPR_2018_paper_language a :Generic ;
    :hasText "language" .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_alignment a :Task ;
    :hasText "alignment" .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_approach a :Generic ;
    :hasText "approach" .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_problems a :Generic ;
    :hasText "problems" .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper_sequence-to-sequence a :Method ;
    :hasText "sequence-to-sequence" .

:Cao_Realtime_Multi-Person_2D_CVPR_2017_paper_approach a :Generic ;
    :hasText "approach" .

:CaptionText a owl:Class ;
    rdfs:subClassOf :Text .

:Castrejon_Annotating_Object_Instances_CVPR_2017_paper_approach a :Generic ;
    :hasText "approach" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_classification a :Task ;
    :hasText "classification" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_clustering_accuracy a :Eval ;
    :hasText "clustering_accuracy" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_clusters a :Other ;
    :hasText "clusters" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_distance a :Other ;
    :hasText "distance" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_feature a :Other ;
    :hasText "feature" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_features a :Generic ;
    :hasText "features" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_framework a :Generic ;
    :hasText "framework" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_image a :Material ;
    :hasText "image" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_images a :Material ;
    :hasText "images" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_learning_algorithm a :Method ;
    :hasText "learning_algorithm" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_machine_learning a :Task ;
    :hasText "machine_learning" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_mnist a :Material ;
    :hasText "mnist" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_network a :Generic ;
    :hasText "network" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_one a :Generic ;
    :hasText "one" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_results a :Generic ;
    :hasText "results" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_similarities a :Generic ;
    :hasText "similarities" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_state a :Generic ;
    :hasText "state" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_task a :Generic ;
    :hasText "task" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_vision a :Generic ;
    :hasText "vision" .

:Chang_Deep_Adaptive_Image_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_architectures a :Generic ;
    :hasText "architectures" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_codes a :Material ;
    :hasText "codes" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_context_information a :Other ;
    :hasText "context_information" .

<https://github.com/deepcurator/DCC/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_correspondence a :Generic ;
    :hasText "correspondence" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_estimation a :Generic ;
    :hasText "estimation" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_global_context a :Other ;
    :hasText "global_context" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_kitti a :Material ;
    :hasText "kitti" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_learns a :Task ;
    :hasText "learns" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_locations a :Generic ;
    :hasText "locations" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_means a :Generic ;
    :hasText "means" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_module a :Generic ;
    :hasText "module" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_modules a :Generic ;
    :hasText "modules" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_scales a :Other ;
    :hasText "scales" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_stereo_matching a :Task ;
    :hasText "stereo_matching" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_stereo_pair a :Material ;
    :hasText "stereo_pair" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_supervised_learning a :Method ;
    :hasText "supervised_learning" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_task a :Generic ;
    :hasText "task" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Chang_Pyramid_Stereo_Matching_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_application a :Generic ;
    :hasText "application" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_crowdsourcing a :Method ;
    :hasText "crowdsourcing" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_distance a :Other ;
    :hasText "distance" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_family a :Generic ;
    :hasText "family" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_research a :Generic ;
    :hasText "research" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_templates a :Other ;
    :hasText "templates" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_trees a :Other ;
    :hasText "trees" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_cameras a :Other ;
    :hasText "cameras" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_information a :Generic ;
    :hasText "information" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_one a :Generic ;
    :hasText "one" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_point_clouds a :Other ;
    :hasText "point_clouds" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_policies a :Other ;
    :hasText "policies" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_research a :Generic ;
    :hasText "research" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_scale a :Other ;
    :hasText "scale" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Chen_Photographic_Image_Synthesis_ICCV_2017_paper_approach a :Generic ;
    :hasText "approach" .

:Chen_Photographic_Image_Synthesis_ICCV_2017_paper_images a :Material ;
    :hasText "images" .

:Chen_Photographic_Image_Synthesis_ICCV_2017_paper_scenes a :Generic ;
    :hasText "scenes" .

:Chen_Photographic_Image_Synthesis_ICCV_2017_paper_semantic a :Other ;
    :hasText "semantic" .

:Chen_Photographic_Image_Synthesis_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Chen_Photographic_Image_Synthesis_ICCV_2017_paper_training a :Task ;
    :hasText "training" .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_natural_images a :Material ;
    :hasText "natural_images" .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_research a :Generic ;
    :hasText "research" .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper_text a :Material ;
    :hasText "text" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_accuracies a :Eval ;
    :hasText "accuracies" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

<https://github.com/deepcurator/DCC/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_rl a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search a :Task ;
    :hasText "search" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_search_space a :Other ;
    :hasText "search_space" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_sequential_model-based a :Method ;
    :hasText "sequential_model-based" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_times a :Generic ;
    :hasText "times" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_generator_network a :Method ;
    :hasText "generator_network" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Choi_StarGAN_Unified_Generative_CVPR_2018_paper_transferring_knowledge a :Task ;
    :hasText "transferring_knowledge" .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper_inception a :Method ;
    :hasText "inception" .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper_modules a :Generic ;
    :hasText "modules" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_approaches a :Generic ;
    :hasText "approaches" .

<https://github.com/deepcurator/DCC/Clark_VidLoc_A_Deep_CVPR_2017_paper_convolutional_neural_networks_(cnn)> a :Method ;
    :hasText "convolutional_neural_networks_(cnn)" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_estimates a :Generic ;
    :hasText "estimates" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_forests a :Other ;
    :hasText "forests" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_images a :Material ;
    :hasText "images" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_learning_techniques a :Method ;
    :hasText "learning_techniques" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_means a :Generic ;
    :hasText "means" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_model a :Generic ;
    :hasText "model" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_motion a :Generic ;
    :hasText "motion" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_our_method a :Other ;
    :hasText "our_method" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_per a :Eval ;
    :hasText "per" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_pose a :Other ;
    :hasText "pose" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_smoothness a :Other ;
    :hasText "smoothness" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_this a :Generic ;
    :hasText "this" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_video a :Material ;
    :hasText "video" .

:Clark_VidLoc_A_Deep_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Code a owl:Class ;
    rdfs:subClassOf :Modality .

:Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_knowledge a :Generic ;
    :hasText "knowledge" .

:Cui_Large_Scale_Fine-Grained_CVPR_2018_paper_scale a :Other ;
    :hasText "scale" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_bleu a :Eval ;
    :hasText "bleu" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_constructions a :Task ;
    :hasText "constructions" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_correlation a :Other ;
    :hasText "correlation" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_data_augmentation a :Method ;
    :hasText "data_augmentation" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_examples a :Generic ;
    :hasText "examples" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_face a :Other ;
    :hasText "face" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_human_judgments a :Other ;
    :hasText "human_judgments" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_metric a :Generic ;
    :hasText "metric" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_metrics a :Generic ;
    :hasText "metrics" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_other a :Generic ;
    :hasText "other" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_rule a :Other ;
    :hasText "rule" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_scheme a :Generic ;
    :hasText "scheme" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_spots a :Method ;
    :hasText "spots" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_syntactic_structure a :Other ;
    :hasText "syntactic_structure" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_system a :Generic ;
    :hasText "system" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_tests a :Generic ;
    :hasText "tests" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_these a :Generic ;
    :hasText "these" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_transformations a :Generic ;
    :hasText "transformations" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Cui_Learning_to_Evaluate_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Dai_Deformable_Convolutional_Networks_ICCV_2017_paper_cnns a :Method ;
    :hasText "cnns" .

<https://github.com/deepcurator/DCC/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper_neural_networks_(> a :Method ;
    :hasText "neural_networks_(" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_3d_reconstructions a :Task ;
    :hasText "3d_reconstructions" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_distance a :Other ;
    :hasText "distance" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_environments a :Generic ;
    :hasText "environments" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_functions a :Generic ;
    :hasText "functions" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_occlusions a :Other ;
    :hasText "occlusions" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_sensor a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/sensors> ;
    :hasText "sensor" .

:Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Demirel_Attributes2Classname_A_Discriminative_ICCV_2017_paper_novel_approach a :Generic ;
    :hasText "novel_approach" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_baseline a :Generic ;
    :hasText "baseline" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_classes a :Generic ;
    :hasText "classes" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_deep_convolutional_networks a :Method ;
    :hasText "deep_convolutional_networks" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_frameworks a :Generic ;
    :hasText "frameworks" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Dolhansky_Eye_In-Painting_With_CVPR_2018_paper_novel_approach a :Generic ;
    :hasText "novel_approach" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_adversarial_examples a :Task ;
    :hasText "adversarial_examples" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_evaluation_criterion a :Eval ;
    :hasText "evaluation_criterion" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_experimental_results a :Generic ;
    :hasText "experimental_results" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_family a :Generic ;
    :hasText "family" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_metrics a :Generic ;
    :hasText "metrics" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_natural_images a :Material ;
    :hasText "natural_images" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_one_model a :Generic ;
    :hasText "one_model" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_prediction_accuracy a :Eval ;
    :hasText "prediction_accuracy" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_studies a :Generic ;
    :hasText "studies" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Dong_Su_Is_Robustness_the_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_flow a :Other ;
    :hasText "flow" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_gradients a :Other ;
    :hasText "gradients" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_improvements a :Eval ;
    :hasText "improvements" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_location a :Generic ;
    :hasText "location" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_loss_function a :Method ;
    :hasText "loss_function" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_precision a :Eval ;
    :hasText "precision" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_registration a :Task ;
    :hasText "registration" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_tracking a :Task ;
    :hasText "tracking" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_encoder-decoder a :Task ;
    :hasText "encoder-decoder" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_generator_network a :Method ;
    :hasText "generator_network" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_geometric_prior a :Other ;
    :hasText "geometric_prior" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_locations a :Generic ;
    :hasText "locations" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_patches a :Generic ;
    :hasText "patches" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_predicting a :Task ;
    :hasText "predicting" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_scenes a :Generic ;
    :hasText "scenes" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_signals a :Generic ;
    :hasText "signals" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_target a :Generic ;
    :hasText "target" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Donghoon_Lee_Unsupervised_holistic_image_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_activations a :Other ;
    :hasText "activations" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_compression a :Task ;
    :hasText "compression" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_operations a :Generic ;
    :hasText "operations" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_precision a :Eval ;
    :hasText "precision" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_prediction_accuracy a :Eval ;
    :hasText "prediction_accuracy" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_quantization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/quantization> ;
    :hasText "quantization" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_resnet a :Method ;
    :hasText "resnet" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_schemes a :Generic ;
    :hasText "schemes" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper_weights a :Generic ;
    :hasText "weights" .

:DropoutBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_action a :Generic ;
    :hasText "action" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_action_recognition a :Task ;
    :hasText "action_recognition" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_action_videos a :Material ;
    :hasText "action_videos" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_annotation a :Task ;
    :hasText "annotation" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_complex a :Method ;
    :hasText "complex" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_features a :Generic ;
    :hasText "features" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_human_pose a :Other ;
    :hasText "human_pose" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_learn a :Task ;
    :hasText "learn" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_learns a :Task ;
    :hasText "learns" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_modeling a :Task ;
    :hasText "modeling" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_motion a :Generic ;
    :hasText "motion" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_network a :Generic ;
    :hasText "network" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_one a :Generic ;
    :hasText "one" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_parameters a :Generic ;
    :hasText "parameters" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_pooling a :Method ;
    :hasText "pooling" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_pose a :Other ;
    :hasText "pose" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_prediction a :Task ;
    :hasText "prediction" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_recurrent_network a :Method ;
    :hasText "recurrent_network" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_representation a :Generic ;
    :hasText "representation" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_results a :Generic ;
    :hasText "results" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_rnns a :Method ;
    :hasText "rnns" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_state a :Generic ;
    :hasText "state" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_structures a :Generic ;
    :hasText "structures" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_studies a :Generic ;
    :hasText "studies" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_supervision a :Other ;
    :hasText "supervision" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_these a :Generic ;
    :hasText "these" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_time a :Generic ;
    :hasText "time" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_two a :Generic ;
    :hasText "two" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_unified_framework a :Method ;
    :hasText "unified_framework" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_video a :Material ;
    :hasText "video" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_videos a :Material ;
    :hasText "videos" .

:Du_RPAN_An_End-To-End_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_average_improvement a :Eval ;
    :hasText "average_improvement" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_codes a :Material ;
    :hasText "codes" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_deep_reinforcement_learning a :Method ;
    :hasText "deep_reinforcement_learning" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_edges a :Other ;
    :hasText "edges" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_graph a :Other ;
    :hasText "graph" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_information a :Generic ;
    :hasText "information" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_interaction a :Generic ;
    :hasText "interaction" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_interactions a :Generic ;
    :hasText "interactions" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_maximizing a :Task ;
    :hasText "maximizing" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_mutual_information a :Other ;
    :hasText "mutual_information" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_nodes a :Other ;
    :hasText "nodes" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_noise a :Other ;
    :hasText "noise" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_one a :Generic ;
    :hasText "one" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_prior_knowledge a :Other ;
    :hasText "prior_knowledge" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_representation_learning_methods a :Task ;
    :hasText "representation_learning_methods" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_uncertainty a :Generic ;
    :hasText "uncertainty" .

:Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_convolution a :Method ;
    :hasText "convolution" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_convolutions a :Method ;
    :hasText "convolutions" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_language a :Generic ;
    :hasText "language" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_natural_language a :Material ;
    :hasText "natural_language" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_segmentations a :Other ;
    :hasText "segmentations" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_segmenting a :Task ;
    :hasText "segmenting" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_visual_information a :Generic ;
    :hasText "visual_information" .

:Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Ehsani_Who_Let_the_CVPR_2018_paper_actions a :Task ;
    :hasText "actions" .

:Ehsani_Who_Let_the_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Ehsani_Who_Let_the_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Ehsani_Who_Let_the_CVPR_2018_paper_domains a :Generic ;
    :hasText "domains" .

:Ehsani_Who_Let_the_CVPR_2018_paper_encodes a :Generic ;
    :hasText "encodes" .

:Ehsani_Who_Let_the_CVPR_2018_paper_estimation a :Generic ;
    :hasText "estimation" .

:Ehsani_Who_Let_the_CVPR_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Ehsani_Who_Let_the_CVPR_2018_paper_information a :Generic ;
    :hasText "information" .

:Ehsani_Who_Let_the_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Ehsani_Who_Let_the_CVPR_2018_paper_learned_representation a :Generic ;
    :hasText "learned_representation" .

:Ehsani_Who_Let_the_CVPR_2018_paper_metrics a :Generic ;
    :hasText "metrics" .

:Ehsani_Who_Let_the_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Ehsani_Who_Let_the_CVPR_2018_paper_modelling a :Task ;
    :hasText "modelling" .

:Ehsani_Who_Let_the_CVPR_2018_paper_other a :Generic ;
    :hasText "other" .

:Ehsani_Who_Let_the_CVPR_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Ehsani_Who_Let_the_CVPR_2018_paper_representation_learning a :Task ;
    :hasText "representation_learning" .

:Ehsani_Who_Let_the_CVPR_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Ehsani_Who_Let_the_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Ehsani_Who_Let_the_CVPR_2018_paper_scene_classification a :Task ;
    :hasText "scene_classification" .

:Ehsani_Who_Let_the_CVPR_2018_paper_surface a :Other ;
    :hasText "surface" .

:Ehsani_Who_Let_the_CVPR_2018_paper_task a :Generic ;
    :hasText "task" .

:Ehsani_Who_Let_the_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Ehsani_Who_Let_the_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Ehsani_Who_Let_the_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Ehsani_Who_Let_the_CVPR_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Ehsani_Who_Let_the_CVPR_2018_paper_visual_information a :Generic ;
    :hasText "visual_information" .

:Ehsani_Who_Let_the_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_dnns a :Method ;
    :hasText "dnns" .

:Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_studies a :Generic ;
    :hasText "studies" .

:Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Fan_A_Point_Set_CVPR_2017_paper_data a :Generic ;
    :hasText "data" .

:Fan_A_Point_Set_CVPR_2017_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Fan_End-to-End_Learning_of_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Fan_End-to-End_Learning_of_CVPR_2018_paper_learned_representations a :Generic ;
    :hasText "learned_representations" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_conditions a :Generic ;
    :hasText "conditions" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_face a :Other ;
    :hasText "face" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_few-shot_learning a :Task ;
    :hasText "few-shot_learning" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_generation a :Task ;
    :hasText "generation" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_grounding a :Task ;
    :hasText "grounding" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper_weights a :Generic ;
    :hasText "weights" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_environment a :Generic ;
    :hasText "environment" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_interaction a :Generic ;
    :hasText "interaction" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_navigation a :Task ;
    :hasText "navigation" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_robot a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/robots> ;
    :hasText "robot" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_semantic_labels a :Other ;
    :hasText "semantic_labels" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_surfaces a :Other ;
    :hasText "surfaces" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_3d_reconstruction a :Method ;
    :hasText "3d_reconstruction" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotation a :Task ;
    :hasText "annotation" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_measure a :Generic ;
    :hasText "measure" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_paradigms a :Generic ;
    :hasText "paradigms" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_transfer_learning a :Method ;
    :hasText "transfer_learning" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_unified_framework a :Method ;
    :hasText "unified_framework" .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_color a :Other ;
    :hasText "color" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_convolution a :Method ;
    :hasText "convolution" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_convolutions a :Method ;
    :hasText "convolutions" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_image_inpainting a :Task ;
    :hasText "image_inpainting" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper_validate a :Task ;
    :hasText "validate" .

:Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper_domain_adaptation a :Method ;
    :hasText "domain_adaptation" .

:Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper_technique a :Generic ;
    :hasText "technique" .

:Haeusser_Learning_by_Association_CVPR_2017_paper_data a :Generic ;
    :hasText "data" .

:Haeusser_Learning_by_Association_CVPR_2017_paper_machine_learning a :Task ;
    :hasText "machine_learning" .

:Haeusser_Learning_by_Association_CVPR_2017_paper_task a :Generic ;
    :hasText "task" .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:He_Deep_Residual_Learning_CVPR_2016_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:He_Deep_Residual_Learning_CVPR_2016_paper_classification_task a :Task ;
    :hasText "classification_task" .

:He_Deep_Residual_Learning_CVPR_2016_paper_deep_representations a :Method ;
    :hasText "deep_representations" .

:He_Deep_Residual_Learning_CVPR_2016_paper_ensemble a :Task ;
    :hasText "ensemble" .

:He_Deep_Residual_Learning_CVPR_2016_paper_framework a :Generic ;
    :hasText "framework" .

:He_Deep_Residual_Learning_CVPR_2016_paper_functions a :Generic ;
    :hasText "functions" .

:He_Deep_Residual_Learning_CVPR_2016_paper_imagenet a :Material ;
    :hasText "imagenet" .

:He_Deep_Residual_Learning_CVPR_2016_paper_improvement a :Eval ;
    :hasText "improvement" .

:He_Deep_Residual_Learning_CVPR_2016_paper_inputs a :Generic ;
    :hasText "inputs" .

:He_Deep_Residual_Learning_CVPR_2016_paper_layers a :Other ;
    :hasText "layers" .

:He_Deep_Residual_Learning_CVPR_2016_paper_networks a :Generic ;
    :hasText "networks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_neural_networks a :Method ;
    :hasText "neural_networks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_object_detection a :Task ;
    :hasText "object_detection" .

:He_Deep_Residual_Learning_CVPR_2016_paper_recognition_tasks a :Task ;
    :hasText "recognition_tasks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_representations a :Generic ;
    :hasText "representations" .

:He_Deep_Residual_Learning_CVPR_2016_paper_segmentation a :Task ;
    :hasText "segmentation" .

:He_Deep_Residual_Learning_CVPR_2016_paper_tasks a :Generic ;
    :hasText "tasks" .

:He_Deep_Residual_Learning_CVPR_2016_paper_that a :Generic ;
    :hasText "that" .

:He_Deep_Residual_Learning_CVPR_2016_paper_these a :Generic ;
    :hasText "these" .

:He_Deep_Residual_Learning_CVPR_2016_paper_those a :Generic ;
    :hasText "those" .

:He_Deep_Residual_Learning_CVPR_2016_paper_training a :Task ;
    :hasText "training" .

:He_Deep_Residual_Learning_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:He_Mask_R-CNN_ICCV_2017_paper_framework a :Generic ;
    :hasText "framework" .

:He_Mask_R-CNN_ICCV_2017_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_deep_networks a :Method ;
    :hasText "deep_networks" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_encoder-decoder a :Task ;
    :hasText "encoder-decoder" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_encodes a :Generic ;
    :hasText "encodes" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_human_pose a :Other ;
    :hasText "human_pose" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_multi-view_images a :Material ;
    :hasText "multi-view_images" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_poses a :Other ;
    :hasText "poses" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_semi-supervised_methods a :Method ;
    :hasText "semi-supervised_methods" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_supervised_methods a :Method ;
    :hasText "supervised_methods" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_viewpoint a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/viewpoint> ;
    :hasText "viewpoint" .

:Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_complex a :Method ;
    :hasText "complex" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_distributed a :Generic ;
    :hasText "distributed" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_flow a :Other ;
    :hasText "flow" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_local_neighborhood a :Generic ;
    :hasText "local_neighborhood" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_parsing a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/parsing_algorithm> ;
    :hasText "parsing" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_pascal_voc a :Material ;
    :hasText "pascal_voc" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_position a :Other ;
    :hasText "position" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_positions a :Other ;
    :hasText "positions" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_scenes a :Generic ;
    :hasText "scenes" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_ensemble a :Task ;
    :hasText "ensemble" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_ensembles a :Task ;
    :hasText "ensembles" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_family a :Generic ;
    :hasText "family" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_feature_space a :Other ;
    :hasText "feature_space" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_function a :Generic ;
    :hasText "function" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_functions a :Generic ;
    :hasText "functions" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_information_retrieval_tasks a :Task ;
    :hasText "information_retrieval_tasks" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_locations a :Generic ;
    :hasText "locations" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_metric a :Generic ;
    :hasText "metric" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_architecture a :Generic ;
    :hasText "architecture" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_boundary_detection a :Method ;
    :hasText "boundary_detection" .

<https://github.com/deepcurator/DCC/Hou_Deeply_Supervised_Salient_CVPR_2017_paper_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_development a :Material ;
    :hasText "development" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_edge a :Other ;
    :hasText "edge" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_fcn a :Method ;
    :hasText "fcn" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_fcns a :Method ;
    :hasText "fcns" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_feature a :Other ;
    :hasText "feature" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_framework a :Generic ;
    :hasText "framework" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_fully_convolutional_neural_networks a :Method ;
    :hasText "fully_convolutional_neural_networks" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_improvement a :Eval ;
    :hasText "improvement" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_maps a :Eval ;
    :hasText "maps" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_method a :Generic ;
    :hasText "method" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_models a :Generic ;
    :hasText "models" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_per a :Eval ;
    :hasText "per" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_results a :Generic ;
    :hasText "results" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_scale a :Other ;
    :hasText "scale" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_structure a :Generic ;
    :hasText "structure" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_structures a :Generic ;
    :hasText "structures" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_supervision a :Other ;
    :hasText "supervision" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_this a :Generic ;
    :hasText "this" .

:Hou_Deeply_Supervised_Salient_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Hu_Deep_360_Pilot_CVPR_2017_paper_dash a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/dash> ;
    :hasText "dash" .

:Hu_Deep_360_Pilot_CVPR_2017_paper_recurrent_neural_network a :Method ;
    :hasText "recurrent_neural_network" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_architecture a :Generic ;
    :hasText "architecture" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_cnns a :Method ;
    :hasText "cnns" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_color_constancy a :Task ;
    :hasText "color_constancy" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_colors a :Other ;
    :hasText "colors" .

<https://github.com/deepcurator/DCC/Hu_FC4_Fully_Convolutional_CVPR_2017_paper_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_estimates a :Generic ;
    :hasText "estimates" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_estimation a :Generic ;
    :hasText "estimation" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_formulation a :Generic ;
    :hasText "formulation" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_information a :Generic ;
    :hasText "information" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_learn a :Task ;
    :hasText "learn" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_network a :Generic ;
    :hasText "network" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_patches a :Generic ;
    :hasText "patches" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_pooling a :Method ;
    :hasText "pooling" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_quality a :Eval ;
    :hasText "quality" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_solution a :Generic ;
    :hasText "solution" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_state a :Generic ;
    :hasText "state" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_supervision a :Other ;
    :hasText "supervision" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_they a :Generic ;
    :hasText "they" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_this a :Generic ;
    :hasText "this" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_training a :Task ;
    :hasText "training" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Hu_FC4_Fully_Convolutional_CVPR_2017_paper_weights a :Generic ;
    :hasText "weights" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_errors a :Other ;
    :hasText "errors" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_face a :Other ;
    :hasText "face" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_faces a :Other ;
    :hasText "faces" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_models a :Generic ;
    :hasText "models" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_objects a :Generic ;
    :hasText "objects" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_prior a :Generic ;
    :hasText "prior" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_resolution a :Eval ;
    :hasText "resolution" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_results a :Generic ;
    :hasText "results" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_scale a :Other ;
    :hasText "scale" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Hu_Finding_Tiny_Faces_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Hu_Learning_Answer_Embeddings_CVPR_2018_paper_probabilistic_model a :Method ;
    :hasText "probabilistic_model" .

:Hu_Learning_Answer_Embeddings_CVPR_2018_paper_question_answering a :Task ;
    :hasText "question_answering" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_map a :Eval ;
    :hasText "map" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_these a :Generic ;
    :hasText "these" .

:Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_computation a :Other ;
    :hasText "computation" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_improvements a :Eval ;
    :hasText "improvements" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_models a :Generic ;
    :hasText "models" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_state a :Generic ;
    :hasText "state" .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper_them a :Generic ;
    :hasText "them" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_approach a :Generic ;
    :hasText "approach" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_class_imbalance a :Other ;
    :hasText "class_imbalance" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_classes a :Generic ;
    :hasText "classes" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_classification_methods a :Method ;
    :hasText "classification_methods" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_classification_tasks a :Method ;
    :hasText "classification_tasks" .

<https://github.com/deepcurator/DCC/Huang_Learning_Deep_Representation_CVPR_2016_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_data a :Generic ;
    :hasText "data" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_domain a :Generic ;
    :hasText "domain" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_experiments a :Generic ;
    :hasText "experiments" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_framework a :Generic ;
    :hasText "framework" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_improvements a :Eval ;
    :hasText "improvements" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_instances a :Generic ;
    :hasText "instances" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_k-nearest_neighbor a :Method ;
    :hasText "k-nearest_neighbor" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_knn a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/k-nearest_neighbors> ;
    :hasText "knn" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_methods a :Generic ;
    :hasText "methods" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_neighborhood a :Generic ;
    :hasText "neighborhood" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_network a :Generic ;
    :hasText "network" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_representation a :Generic ;
    :hasText "representation" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_representation_learning a :Task ;
    :hasText "representation_learning" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_sampling a :Method ;
    :hasText "sampling" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_schemes a :Generic ;
    :hasText "schemes" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_strategies a :Generic ;
    :hasText "strategies" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_that a :Generic ;
    :hasText "that" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_these a :Generic ;
    :hasText "these" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_this a :Generic ;
    :hasText "this" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_training a :Task ;
    :hasText "training" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_validate a :Task ;
    :hasText "validate" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_vision a :Generic ;
    :hasText "vision" .

:Huang_Learning_Deep_Representation_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_activity a :Other ;
    :hasText "activity" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_dynamic_scenes a :Other ;
    :hasText "dynamic_scenes" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_gaze a :Other ;
    :hasText "gaze" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_hybrid_model a :Method ;
    :hasText "hybrid_model" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_recurrent_neural_network a :Method ;
    :hasText "recurrent_neural_network" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Huang_Predicting_Gaze_in_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_convolutional_neural_networks_(cnn)> a :Method ;
    :hasText "convolutional_neural_networks_(cnn)" .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_face a :Other ;
    :hasText "face" .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper_super-resolution a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/superresolution> ;
    :hasText "super-resolution" .

<https://github.com/deepcurator/DCC/Hui_Fast_and_Accurate_CVPR_2018_paper_deep_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "deep_convolutional_neural_networks_(cnns)" .

<https://github.com/deepcurator/DCC/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_color a :Other ;
    :hasText "color" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_conditional_generative_adversarial_networks a :Method ;
    :hasText "conditional_generative_adversarial_networks" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_evaluation_results a :Generic ;
    :hasText "evaluation_results" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_former a :Generic ;
    :hasText "former" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_generation a :Task ;
    :hasText "generation" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_latter a :Generic ;
    :hasText "latter" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_novel_approach a :Generic ;
    :hasText "novel_approach" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_semantics a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/semantics> ;
    :hasText "semantics" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_text a :Material ;
    :hasText "text" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_text_input a :Material ;
    :hasText "text_input" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:ImageComponent a owl:Class .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_anatomy a :Other ;
    :hasText "anatomy" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_imaging a :Generic ;
    :hasText "imaging" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_learning_algorithm a :Method ;
    :hasText "learning_algorithm" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_patches a :Generic ;
    :hasText "patches" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_region a :Generic ;
    :hasText "region" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_signal a :Generic ;
    :hasText "signal" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_supervised_learning a :Method ;
    :hasText "supervised_learning" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_tool a :Generic ;
    :hasText "tool" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_tools a :Generic ;
    :hasText "tools" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_visual_information a :Generic ;
    :hasText "visual_information" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_ways a :Generic ;
    :hasText "ways" .

:Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Jas_Image_Specificity_2015_CVPR_paper_application a :Generic ;
    :hasText "application" .

:Jas_Image_Specificity_2015_CVPR_paper_descriptions a :Generic ;
    :hasText "descriptions" .

:Jas_Image_Specificity_2015_CVPR_paper_features a :Generic ;
    :hasText "features" .

:Jas_Image_Specificity_2015_CVPR_paper_image a :Material ;
    :hasText "image" .

:Jas_Image_Specificity_2015_CVPR_paper_image_content a :Other ;
    :hasText "image_content" .

:Jas_Image_Specificity_2015_CVPR_paper_images a :Material ;
    :hasText "images" .

:Jas_Image_Specificity_2015_CVPR_paper_improvements a :Eval ;
    :hasText "improvements" .

:Jas_Image_Specificity_2015_CVPR_paper_measure a :Generic ;
    :hasText "measure" .

:Jas_Image_Specificity_2015_CVPR_paper_mechanisms a :Generic ;
    :hasText "mechanisms" .

:Jas_Image_Specificity_2015_CVPR_paper_modeling a :Task ;
    :hasText "modeling" .

:Jas_Image_Specificity_2015_CVPR_paper_models a :Generic ;
    :hasText "models" .

:Jas_Image_Specificity_2015_CVPR_paper_ones a :Generic ;
    :hasText "ones" .

:Jas_Image_Specificity_2015_CVPR_paper_other a :Generic ;
    :hasText "other" .

:Jas_Image_Specificity_2015_CVPR_paper_properties a :Generic ;
    :hasText "properties" .

:Jas_Image_Specificity_2015_CVPR_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Jas_Image_Specificity_2015_CVPR_paper_specificity a :Eval ;
    :hasText "specificity" .

:Jas_Image_Specificity_2015_CVPR_paper_text a :Material ;
    :hasText "text" .

:Jas_Image_Specificity_2015_CVPR_paper_that a :Generic ;
    :hasText "that" .

:Jas_Image_Specificity_2015_CVPR_paper_this a :Generic ;
    :hasText "this" .

:Jas_Image_Specificity_2015_CVPR_paper_two a :Generic ;
    :hasText "two" .

:Jas_Image_Specificity_2015_CVPR_paper_understanding a :Task ;
    :hasText "understanding" .

:Jas_Image_Specificity_2015_CVPR_paper_we a :Generic ;
    :hasText "we" .

:Jas_Image_Specificity_2015_CVPR_paper_words a :Generic ;
    :hasText "words" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_adversarial_examples a :Task ;
    :hasText "adversarial_examples" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_approximations a :Generic ;
    :hasText "approximations" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_compression a :Task ;
    :hasText "compression" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_decoder a :Method ;
    :hasText "decoder" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_encoder a :Method ;
    :hasText "encoder" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_gaussian_blurring a :Other ;
    :hasText "gaussian_blurring" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_neural_networks a :Method ;
    :hasText "neural_networks" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_noise a :Other ;
    :hasText "noise" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_visual_quality a :Eval ;
    :hasText "visual_quality" .

:Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_mnist a :Material ;
    :hasText "mnist" .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper_pascal_voc a :Material ;
    :hasText "pascal_voc" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_convolution a :Method ;
    :hasText "convolution" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_hierarchical a :Generic ;
    :hasText "hierarchical" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_image_information a :Other ;
    :hasText "image_information" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_image_super-resolution a :Task ;
    :hasText "image_super-resolution" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_kernels a :Method ;
    :hasText "kernels" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_outputs a :Generic ;
    :hasText "outputs" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_scales a :Other ;
    :hasText "scales" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_sizes a :Other ;
    :hasText "sizes" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_studies a :Generic ;
    :hasText "studies" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_training_process a :Generic ;
    :hasText "training_process" .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_alternative a :Generic ;
    :hasText "alternative" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_collection a :Generic ;
    :hasText "collection" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_concept a :Other ;
    :hasText "concept" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_conditional_random_fields a :Method ;
    :hasText "conditional_random_fields" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_crf a :Method ;
    :hasText "crf" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_domains a :Generic ;
    :hasText "domains" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_evaluations a :Generic ;
    :hasText "evaluations" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_field a :Generic ;
    :hasText "field" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_fields a :Generic ;
    :hasText "fields" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_generalization a :Generic ;
    :hasText "generalization" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_pascal_voc a :Material ;
    :hasText "pascal_voc" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_priors a :Other ;
    :hasText "priors" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_relations a :Generic ;
    :hasText "relations" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_semantic_labels a :Other ;
    :hasText "semantic_labels" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_semantic_relations a :Other ;
    :hasText "semantic_relations" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_activations a :Other ;
    :hasText "activations" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_compression a :Task ;
    :hasText "compression" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_deep_networks a :Method ;
    :hasText "deep_networks" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_distributed a :Generic ;
    :hasText "distributed" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_magnitude a :Other ;
    :hasText "magnitude" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_operations a :Generic ;
    :hasText "operations" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_statistical_analysis a :Method ;
    :hasText "statistical_analysis" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_human_pose a :Other ;
    :hasText "human_pose" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_mesh a :Other ;
    :hasText "mesh" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_occlusions a :Other ;
    :hasText "occlusions" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_orientations a :Other ;
    :hasText "orientations" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_recovery a :Task ;
    :hasText "recovery" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_shape a :Other ;
    :hasText "shape" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_time a :Generic ;
    :hasText "time" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_approach a :Generic ;
    :hasText "approach" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_cnn a :Method ;
    :hasText "cnn" .

<https://github.com/deepcurator/DCC/Kang_Convolutional_Neural_Networks_2014_CVPR_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_domain a :Generic ;
    :hasText "domain" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_estimation a :Generic ;
    :hasText "estimation" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_experiments a :Generic ;
    :hasText "experiments" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_feature a :Other ;
    :hasText "feature" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_features a :Generic ;
    :hasText "features" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_generalization a :Generic ;
    :hasText "generalization" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_image a :Material ;
    :hasText "image" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_images a :Material ;
    :hasText "images" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_input a :Generic ;
    :hasText "input" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_layers a :Other ;
    :hasText "layers" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_methods a :Generic ;
    :hasText "methods" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_model a :Generic ;
    :hasText "model" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_network a :Generic ;
    :hasText "network" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_node a :Other ;
    :hasText "node" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_one a :Generic ;
    :hasText "one" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_patches a :Generic ;
    :hasText "patches" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_pooling a :Method ;
    :hasText "pooling" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_quality a :Eval ;
    :hasText "quality" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_state a :Generic ;
    :hasText "state" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_structure a :Generic ;
    :hasText "structure" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_that a :Generic ;
    :hasText "that" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_this a :Generic ;
    :hasText "this" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_two a :Generic ;
    :hasText "two" .

:Kang_Convolutional_Neural_Networks_2014_CVPR_paper_we a :Generic ;
    :hasText "we" .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_image a :Material ;
    :hasText "image" .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_input a :Generic ;
    :hasText "input" .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_pose a :Other ;
    :hasText "pose" .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_results a :Generic ;
    :hasText "results" .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_scenes a :Generic ;
    :hasText "scenes" .

:Kendall_PoseNet_A_Convolutional_ICCV_2015_paper_system a :Generic ;
    :hasText "system" .

:Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper_image_super-resolution a :Task ;
    :hasText "image_super-resolution" .

:Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper_method a :Generic ;
    :hasText "method" .

:Klokov_Escape_From_Cells_ICCV_2017_paper_architecture a :Generic ;
    :hasText "architecture" .

:Klokov_Escape_From_Cells_ICCV_2017_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Kostrikov_Surface_Networks_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_alignment a :Task ;
    :hasText "alignment" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_alignments a :Other ;
    :hasText "alignments" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_interpretable a :Generic ;
    :hasText "interpretable" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_language a :Generic ;
    :hasText "language" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_text a :Material ;
    :hasText "text" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper_words a :Generic ;
    :hasText "words" .

:Kuen_Stochastic_Downsampling_for_CVPR_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Kuen_Stochastic_Downsampling_for_CVPR_2018_paper_convolutional_networks a :Method ;
    :hasText "convolutional_networks" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_classes a :Generic ;
    :hasText "classes" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_domain_adaptation a :Method ;
    :hasText "domain_adaptation" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_source_domain a :Material ;
    :hasText "source_domain" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_target a :Generic ;
    :hasText "target" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_target_domain a :Material ;
    :hasText "target_domain" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_transferring_knowledge a :Task ;
    :hasText "transferring_knowledge" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_applications a :Generic ;
    :hasText "applications" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_computational_complexity a :Eval ;
    :hasText "computational_complexity" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_convolutions a :Method ;
    :hasText "convolutions" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_evaluations a :Generic ;
    :hasText "evaluations" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_feature a :Other ;
    :hasText "feature" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_images a :Material ;
    :hasText "images" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_input a :Generic ;
    :hasText "input" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_laplacian a :Generic ;
    :hasText "laplacian" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_loss_function a :Method ;
    :hasText "loss_function" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_maps a :Eval ;
    :hasText "maps" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_method a :Generic ;
    :hasText "method" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_model a :Generic ;
    :hasText "model" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_network a :Generic ;
    :hasText "network" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_neural_networks a :Method ;
    :hasText "neural_networks" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_one a :Generic ;
    :hasText "one" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_pre-processing_step a :Method ;
    :hasText "pre-processing_step" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_predictions a :Task ;
    :hasText "predictions" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_quality a :Eval ;
    :hasText "quality" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_resolution a :Eval ;
    :hasText "resolution" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_resource a :Generic ;
    :hasText "resource" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_scale a :Other ;
    :hasText "scale" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_state a :Generic ;
    :hasText "state" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_super-resolution a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/superresolution> ;
    :hasText "super-resolution" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_superresolution a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/superresolution> ;
    :hasText "superresolution" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_supervision a :Other ;
    :hasText "supervision" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_this a :Generic ;
    :hasText "this" .

:Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_bias a :Method ;
    :hasText "bias" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_convlstm a :Method ;
    :hasText "convlstm" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_correlation a :Other ;
    :hasText "correlation" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_database a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/database_systems> ;
    :hasText "database" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_experimental_results a :Generic ;
    :hasText "experimental_results" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_hierarchical a :Generic ;
    :hasText "hierarchical" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_long_short-term_memory a :Method ;
    :hasText "long_short-term_memory" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_motion a :Generic ;
    :hasText "motion" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_objectness a :Other ;
    :hasText "objectness" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_structured a :Generic ;
    :hasText "structured" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_tracking a :Task ;
    :hasText "tracking" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_pooling a :Method ;
    :hasText "pooling" .

:Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_that a :Generic ;
    :hasText "that" .

:Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_this a :Generic ;
    :hasText "this" .

:Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_topology a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/topology> ;
    :hasText "topology" .

:Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:Lavin_Fast_Algorithms_for_CVPR_2016_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_applications a :Generic ;
    :hasText "applications" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_classification a :Task ;
    :hasText "classification" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_cnn a :Method ;
    :hasText "cnn" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_data a :Generic ;
    :hasText "data" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_deep_convolutional_neural_networks a :Method ;
    :hasText "deep_convolutional_neural_networks" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_experiments a :Generic ;
    :hasText "experiments" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_face a :Other ;
    :hasText "face" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_feature a :Other ;
    :hasText "feature" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_generative_model a :Method ;
    :hasText "generative_model" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_learns a :Task ;
    :hasText "learns" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_model a :Generic ;
    :hasText "model" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_modeling a :Task ;
    :hasText "modeling" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_properties a :Generic ;
    :hasText "properties" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_results a :Generic ;
    :hasText "results" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_texture a :Other ;
    :hasText "texture" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_training a :Task ;
    :hasText "training" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_unsupervised_learning a :Method ;
    :hasText "unsupervised_learning" .

:Lazarow_Introspective_Neural_Networks_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper_image_super-resolution a :Task ;
    :hasText "image_super-resolution" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_models a :Generic ;
    :hasText "models" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_noise a :Other ;
    :hasText "noise" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_3d_reconstructions a :Task ;
    :hasText "3d_reconstructions" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_component a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/component> ;
    :hasText "component" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_distances a :Other ;
    :hasText "distances" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edge a :Other ;
    :hasText "edge" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_edges a :Other ;
    :hasText "edges" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_loss_function a :Method ;
    :hasText "loss_function" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_minimize a :Task ;
    :hasText "minimize" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_patches a :Generic ;
    :hasText "patches" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_point_clouds a :Other ;
    :hasText "point_clouds" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_surface a :Other ;
    :hasText "surface" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_technique a :Generic ;
    :hasText "technique" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Li_Towards_Faster_Training_CVPR_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Li_Towards_Faster_Training_CVPR_2018_paper_covariance a :Other ;
    :hasText "covariance" .

:Li_Towards_Faster_Training_CVPR_2018_paper_improvement a :Eval ;
    :hasText "improvement" .

:Li_Towards_Faster_Training_CVPR_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_contextual_information a :Other ;
    :hasText "contextual_information" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_convolution a :Method ;
    :hasText "convolution" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_decoder a :Method ;
    :hasText "decoder" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_encoder-decoder a :Task ;
    :hasText "encoder-decoder" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_fields a :Generic ;
    :hasText "fields" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_former a :Generic ;
    :hasText "former" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_implementation a :Material ;
    :hasText "implementation" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_latter a :Generic ;
    :hasText "latter" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_modules a :Generic ;
    :hasText "modules" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_object_boundaries a :Other ;
    :hasText "object_boundaries" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_operations a :Generic ;
    :hasText "operations" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_pascal_voc a :Material ;
    :hasText "pascal_voc" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_post-processing a :Task ;
    :hasText "post-processing" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_research a :Generic ;
    :hasText "research" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_segmentation_results a :Eval ;
    :hasText "segmentation_results" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_spatial_information a :Other ;
    :hasText "spatial_information" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_tensorflow a :Other ;
    :hasText "tensorflow" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_tree a :Other ;
    :hasText "tree" .

:Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_activities a :Generic ;
    :hasText "activities" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_activity a :Other ;
    :hasText "activity" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_architecture a :Generic ;
    :hasText "architecture" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_cifar-100 a :Material ;
    :hasText "cifar-100" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_cnn a :Method ;
    :hasText "cnn" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_computer_vision_tasks a :Task ;
    :hasText "computer_vision_tasks" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_context_information a :Other ;
    :hasText "context_information" .

<https://github.com/deepcurator/DCC/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_datasets a :Material ;
    :hasText "datasets" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_input a :Generic ;
    :hasText "input" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_mnist a :Material ;
    :hasText "mnist" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_model a :Generic ;
    :hasText "model" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_models a :Generic ;
    :hasText "models" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_network a :Generic ;
    :hasText "network" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_object_recognition a :Task ;
    :hasText "object_recognition" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_other a :Generic ;
    :hasText "other" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_parameters a :Generic ;
    :hasText "parameters" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_properties a :Generic ;
    :hasText "properties" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_results a :Generic ;
    :hasText "results" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_state a :Generic ;
    :hasText "state" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_structure a :Generic ;
    :hasText "structure" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_svhn a :Material ;
    :hasText "svhn" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_system a :Generic ;
    :hasText "system" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_that a :Generic ;
    :hasText "that" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_these a :Generic ;
    :hasText "these" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_this a :Generic ;
    :hasText "this" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_time a :Generic ;
    :hasText "time" .

:Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper_we a :Generic ;
    :hasText "we" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_alternative a :Generic ;
    :hasText "alternative" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_encoder a :Method ;
    :hasText "encoder" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_formulation a :Generic ;
    :hasText "formulation" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_learns a :Task ;
    :hasText "learns" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_loss_functions a :Other ;
    :hasText "loss_functions" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_point_clouds a :Other ;
    :hasText "point_clouds" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_post-processing a :Task ;
    :hasText "post-processing" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_predicting a :Task ;
    :hasText "predicting" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_shape a :Other ;
    :hasText "shape" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_shapes a :Other ;
    :hasText "shapes" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_solutions a :Generic ;
    :hasText "solutions" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_surface a :Other ;
    :hasText "surface" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_task a :Generic ;
    :hasText "task" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_they a :Generic ;
    :hasText "they" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_topology a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/topology> ;
    :hasText "topology" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Liao_Deep_Marching_Cubes_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Lin_Inverse_Compositional_Spatial_CVPR_2017_paper_this a :Generic ;
    :hasText "this" .

:Lin_Inverse_Compositional_Spatial_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_bound a :Generic ;
    :hasText "bound" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_bounds a :Other ;
    :hasText "bounds" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_cross-domain a :Task ;
    :hasText "cross-domain" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_domains a :Generic ;
    :hasText "domains" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_gans a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_generalization a :Generic ;
    :hasText "generalization" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_hyperparameters a :Generic ;
    :hasText "hyperparameters" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_per a :Eval ;
    :hasText "per" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_predicting a :Task ;
    :hasText "predicting" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_stopping_criterion a :Other ;
    :hasText "stopping_criterion" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_supervised_learning a :Method ;
    :hasText "supervised_learning" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_utility a :Eval ;
    :hasText "utility" .

:Lior_Wolf_Estimating_the_Success_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Liu_Decoupled_Networks_CVPR_2018_paper_component a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/component> ;
    :hasText "component" .

:Liu_Decoupled_Networks_CVPR_2018_paper_convolution a :Method ;
    :hasText "convolution" .

<https://github.com/deepcurator/DCC/Liu_Decoupled_Networks_CVPR_2018_paper_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

<https://github.com/deepcurator/DCC/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper_deep_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "deep_convolutional_neural_networks_(cnns)" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_3d_model a :Method ;
    :hasText "3d_model" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_texture a :Other ;
    :hasText "texture" .

:Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

<https://github.com/deepcurator/DCC/Liu_Predicting_Salient_Face_CVPR_2017_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_3d_model a :Method ;
    :hasText "3d_model" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_image a :Material ;
    :hasText "image" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_images a :Material ;
    :hasText "images" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_input a :Generic ;
    :hasText "input" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_representation a :Generic ;
    :hasText "representation" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_representations a :Generic ;
    :hasText "representations" .

:Liu_Raster-To-Vector_Revisiting_Floorplan_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_action a :Generic ;
    :hasText "action" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_action_recognition a :Task ;
    :hasText "action_recognition" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_actions a :Task ;
    :hasText "actions" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_cues a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/cues> ;
    :hasText "cues" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_deep_convolutional_neural_networks a :Method ;
    :hasText "deep_convolutional_neural_networks" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_human_action a :Other ;
    :hasText "human_action" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_human_pose a :Other ;
    :hasText "human_pose" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_human_poses a :Other ;
    :hasText "human_poses" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_modeling a :Task ;
    :hasText "modeling" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_motions a :Generic ;
    :hasText "motions" .

<https://github.com/deepcurator/DCC/Liu_Recognizing_Human_Actions_CVPR_2018_paper_ntu_rgb+d> a :Material ;
    :hasText "ntu_rgb+d" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_pose a :Other ;
    :hasText "pose" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_poses a :Other ;
    :hasText "poses" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_properties a :Generic ;
    :hasText "properties" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_rank a :Other ;
    :hasText "rank" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_sampling a :Method ;
    :hasText "sampling" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_shape a :Other ;
    :hasText "shape" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_these a :Generic ;
    :hasText "these" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_they a :Generic ;
    :hasText "they" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Liu_Recognizing_Human_Actions_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Liu_Richer_Convolutional_Features_CVPR_2017_paper_edge a :Other ;
    :hasText "edge" .

:Liu_Richer_Convolutional_Features_CVPR_2017_paper_features a :Generic ;
    :hasText "features" .

:Liu_Richer_Convolutional_Features_CVPR_2017_paper_this a :Generic ;
    :hasText "this" .

:Liu_Richer_Convolutional_Features_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper_face_recognition a :Task ;
    :hasText "face_recognition" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_database a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/database_systems> ;
    :hasText "database" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_nearest_neighbor a :Method ;
    :hasText "nearest_neighbor" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_outputs a :Generic ;
    :hasText "outputs" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_processing a :Generic ;
    :hasText "processing" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_search a :Task ;
    :hasText "search" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_semantic_information a :Other ;
    :hasText "semantic_information" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_system a :Generic ;
    :hasText "system" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_text a :Material ;
    :hasText "text" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper_words a :Generic ;
    :hasText "words" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_appearance_information a :Other ;
    :hasText "appearance_information" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_application a :Generic ;
    :hasText "application" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_architecture a :Generic ;
    :hasText "architecture" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_classification a :Task ;
    :hasText "classification" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_convolutional_networks a :Method ;
    :hasText "convolutional_networks" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_features a :Generic ;
    :hasText "features" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_flow a :Other ;
    :hasText "flow" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_fully_convolutional_networks a :Method ;
    :hasText "fully_convolutional_networks" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_image a :Material ;
    :hasText "image" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_improvement a :Eval ;
    :hasText "improvement" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_input a :Generic ;
    :hasText "input" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_learned_representations a :Generic ;
    :hasText "learned_representations" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_models a :Generic ;
    :hasText "models" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_network a :Generic ;
    :hasText "network" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_networks a :Generic ;
    :hasText "networks" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_one a :Generic ;
    :hasText "one" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_pascal_voc a :Material ;
    :hasText "pascal_voc" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_prediction a :Task ;
    :hasText "prediction" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_prior a :Generic ;
    :hasText "prior" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_segmentations a :Other ;
    :hasText "segmentations" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_semantic_information a :Other ;
    :hasText "semantic_information" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_sift a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/sift> ;
    :hasText "sift" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_state a :Generic ;
    :hasText "state" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_task a :Generic ;
    :hasText "task" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_tasks a :Generic ;
    :hasText "tasks" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_that a :Generic ;
    :hasText "that" .

:Long_Fully_Convolutional_Networks_2015_CVPR_paper_transfer a :Other ;
    :hasText "transfer" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_color a :Other ;
    :hasText "color" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_colors a :Other ;
    :hasText "colors" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_input a :Generic ;
    :hasText "input" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_scene a :Generic ;
    :hasText "scene" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_transfer a :Other ;
    :hasText "transfer" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_transfers a :Other ;
    :hasText "transfers" .

:Luan_Deep_Photo_Style_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_improvement a :Eval ;
    :hasText "improvement" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_learned_representation a :Generic ;
    :hasText "learned_representation" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Mariya_Vasileva_Learning_Type-Aware_Embeddings_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_architectures a :Generic ;
    :hasText "architectures" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_attention_mechanisms a :Method ;
    :hasText "attention_mechanisms" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_criterion a :Generic ;
    :hasText "criterion" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_magnitudes a :Other ;
    :hasText "magnitudes" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_mechanisms a :Generic ;
    :hasText "mechanisms" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_operations a :Generic ;
    :hasText "operations" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_our_mechanism a :Other ;
    :hasText "our_mechanism" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_processing a :Generic ;
    :hasText "processing" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_question_answering a :Task ;
    :hasText "question_answering" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_relevance a :Eval ;
    :hasText "relevance" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_signal a :Generic ;
    :hasText "signal" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_clustering_method a :Method ;
    :hasText "clustering_method" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_large_datasets a :Material ;
    :hasText "large_datasets" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_unsupervised_learning_methods a :Method ;
    :hasText "unsupervised_learning_methods" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper_weights a :Generic ;
    :hasText "weights" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_cameras a :Other ;
    :hasText "cameras" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_collections a :Generic ;
    :hasText "collections" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_correspondences a :Generic ;
    :hasText "correspondences" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_spaces a :Generic ;
    :hasText "spaces" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_activations a :Other ;
    :hasText "activations" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_approach a :Generic ;
    :hasText "approach" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_approaches a :Generic ;
    :hasText "approaches" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_architectures a :Generic ;
    :hasText "architectures" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_baseline_methods a :Generic ;
    :hasText "baseline_methods" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_categories a :Generic ;
    :hasText "categories" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_convolutional_networks a :Method ;
    :hasText "convolutional_networks" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_field a :Generic ;
    :hasText "field" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_learn a :Task ;
    :hasText "learn" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_method a :Generic ;
    :hasText "method" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_network a :Generic ;
    :hasText "network" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_networks a :Generic ;
    :hasText "networks" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_representations a :Generic ;
    :hasText "representations" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_task a :Generic ;
    :hasText "task" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_tasks a :Generic ;
    :hasText "tasks" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_that a :Generic ;
    :hasText "that" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_this a :Generic ;
    :hasText "this" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_training_examples a :Material ;
    :hasText "training_examples" .

:Misra_Cross-Stitch_Networks_for_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_action_classification a :Task ;
    :hasText "action_classification" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_actions a :Task ;
    :hasText "actions" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_activities a :Generic ;
    :hasText "activities" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_local_methods a :Generic ;
    :hasText "local_methods" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_long-term a :Generic ;
    :hasText "long-term" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_per a :Eval ;
    :hasText "per" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_processing a :Generic ;
    :hasText "processing" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_sampling a :Method ;
    :hasText "sampling" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_descriptions a :Generic ;
    :hasText "descriptions" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_diversity a :Other ;
    :hasText "diversity" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_encoder a :Method ;
    :hasText "encoder" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_formulation a :Generic ;
    :hasText "formulation" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_generation a :Task ;
    :hasText "generation" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_modeling a :Task ;
    :hasText "modeling" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_popularity a :Other ;
    :hasText "popularity" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_summarization a :Task ;
    :hasText "summarization" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_systems a :Generic ;
    :hasText "systems" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_aerial_images a :Material ;
    :hasText "aerial_images" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_architectures a :Generic ;
    :hasText "architectures" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_automatic a :Task ;
    :hasText "automatic" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_cross-entropy a :Method ;
    :hasText "cross-entropy" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_predictions a :Task ;
    :hasText "predictions" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_method a :Generic ;
    :hasText "method" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Movshovitz-Attias_No_Fuss_Distance_ICCV_2017_paper_distance a :Other ;
    :hasText "distance" .

:Movshovitz-Attias_No_Fuss_Distance_ICCV_2017_paper_metric a :Generic ;
    :hasText "metric" .

:Movshovitz-Attias_No_Fuss_Distance_ICCV_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_instances a :Generic ;
    :hasText "instances" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_our_pose a :Other ;
    :hasText "our_pose" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_poses a :Other ;
    :hasText "poses" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_system a :Generic ;
    :hasText "system" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Nguyen_Improved_Fusion_of_CVPR_2018_paper_question_answering a :Task ;
    :hasText "question_answering" .

:Nguyen_Improved_Fusion_of_CVPR_2018_paper_solution a :Generic ;
    :hasText "solution" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_agents a :Generic ;
    :hasText "agents" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_complex a :Method ;
    :hasText "complex" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_principles a :Generic ;
    :hasText "principles" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_semantics a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/semantics> ;
    :hasText "semantics" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_stack a :Other ;
    :hasText "stack" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_stacks a :Other ;
    :hasText "stacks" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Osokin_GANs_for_Biological_ICCV_2017_paper_application a :Generic ;
    :hasText "application" .

:Osokin_GANs_for_Biological_ICCV_2017_paper_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:Osokin_GANs_for_Biological_ICCV_2017_paper_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:Osokin_GANs_for_Biological_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Osokin_GANs_for_Biological_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_errors a :Other ;
    :hasText "errors" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_learning_algorithms a :Method ;
    :hasText "learning_algorithms" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_nearest_neighbor a :Method ;
    :hasText "nearest_neighbor" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_signal a :Generic ;
    :hasText "signal" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Pan_Learning_Dual_Convolutional_CVPR_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_entropy a :Other ;
    :hasText "entropy" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_examples a :Generic ;
    :hasText "examples" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_implementation a :Material ;
    :hasText "implementation" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_outputs a :Generic ;
    :hasText "outputs" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_predictions a :Task ;
    :hasText "predictions" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper_weights a :Generic ;
    :hasText "weights" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_data_augmentation a :Method ;
    :hasText "data_augmentation" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_deep_neural_network_models a :Method ;
    :hasText "deep_neural_network_models" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_human_pose a :Other ;
    :hasText "human_pose" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_latter a :Generic ;
    :hasText "latter" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_learns a :Task ;
    :hasText "learns" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_models a :Generic ;
    :hasText "models" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_operations a :Generic ;
    :hasText "operations" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_overfitting a :Other ;
    :hasText "overfitting" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_penalty a :Eval ;
    :hasText "penalty" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_reward a :Eval ;
    :hasText "reward" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_target a :Generic ;
    :hasText "target" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_technique a :Generic ;
    :hasText "technique" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Peng_Jointly_Optimize_Data_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_activity a :Other ;
    :hasText "activity" .

:Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_concept a :Other ;
    :hasText "concept" .

:Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_events a :Other ;
    :hasText "events" .

:Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_counterpart a :Generic ;
    :hasText "counterpart" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_learned_representations a :Generic ;
    :hasText "learned_representations" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_magnitude a :Other ;
    :hasText "magnitude" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_one a :Generic ;
    :hasText "one" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_pose a :Other ;
    :hasText "pose" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_pose_information a :Other ;
    :hasText "pose_information" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_poses a :Other ;
    :hasText "poses" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_shape a :Other ;
    :hasText "shape" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_time a :Generic ;
    :hasText "time" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_training_process a :Generic ;
    :hasText "training_process" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_unlabeled_data a :Other ;
    :hasText "unlabeled_data" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_viewpoint a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/viewpoint> ;
    :hasText "viewpoint" .

:Poier_Learning_Pose_Specific_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:PublicationAuthor a owl:Class .

:Qi_Frustum_PointNets_for_CVPR_2018_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Qi_Frustum_PointNets_for_CVPR_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Qi_Frustum_PointNets_for_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Qi_Frustum_PointNets_for_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_constraints a :Generic ;
    :hasText "constraints" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_estimation a :Generic ;
    :hasText "estimation" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_map a :Eval ;
    :hasText "map" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_module a :Generic ;
    :hasText "module" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_solution a :Generic ;
    :hasText "solution" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_surface a :Other ;
    :hasText "surface" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Qi_GeoNet_Geometric_Neural_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_classes a :Generic ;
    :hasText "classes" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_classification_tasks a :Method ;
    :hasText "classification_tasks" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_concepts a :Other ;
    :hasText "concepts" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_deep_networks a :Method ;
    :hasText "deep_networks" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_domain_knowledge a :Other ;
    :hasText "domain_knowledge" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_improvements a :Eval ;
    :hasText "improvements" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_mappings a :Other ;
    :hasText "mappings" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_names a :Other ;
    :hasText "names" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_natural_language a :Material ;
    :hasText "natural_language" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_predictions a :Task ;
    :hasText "predictions" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_shapes a :Other ;
    :hasText "shapes" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_textures a :Other ;
    :hasText "textures" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_transfer a :Other ;
    :hasText "transfer" .

:Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_actions a :Task ;
    :hasText "actions" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_approach a :Generic ;
    :hasText "approach" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_gaze a :Other ;
    :hasText "gaze" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_knowledge a :Generic ;
    :hasText "knowledge" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_models a :Generic ;
    :hasText "models" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_objects a :Generic ;
    :hasText "objects" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_predicting a :Task ;
    :hasText "predicting" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_semantic a :Other ;
    :hasText "semantic" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_signal a :Generic ;
    :hasText "signal" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_system a :Generic ;
    :hasText "system" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_targets a :Generic ;
    :hasText "targets" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_times a :Generic ;
    :hasText "times" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_understanding a :Task ;
    :hasText "understanding" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_video a :Material ;
    :hasText "video" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_videos a :Material ;
    :hasText "videos" .

:Recasens_Following_Gaze_in_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_applications a :Generic ;
    :hasText "applications" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_architecture a :Generic ;
    :hasText "architecture" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_graphical_models a :Method ;
    :hasText "graphical_models" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_instances a :Generic ;
    :hasText "instances" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_model a :Generic ;
    :hasText "model" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_network a :Generic ;
    :hasText "network" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_prediction a :Task ;
    :hasText "prediction" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_problems a :Generic ;
    :hasText "problems" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_question_answering a :Task ;
    :hasText "question_answering" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_recurrent_neural_network a :Method ;
    :hasText "recurrent_neural_network" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_region a :Generic ;
    :hasText "region" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_regions a :Generic ;
    :hasText "regions" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_rnn a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_scene a :Generic ;
    :hasText "scene" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_segmentations a :Other ;
    :hasText "segmentations" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_structured a :Generic ;
    :hasText "structured" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_this a :Generic ;
    :hasText "this" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_vision a :Generic ;
    :hasText "vision" .

:Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper_policy a :Other ;
    :hasText "policy" .

:Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Repository a owl:Class .

:Runia_Real-World_Repetition_Estimation_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Runia_Real-World_Repetition_Estimation_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_audio a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_events a :Other ;
    :hasText "events" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_modeling a :Task ;
    :hasText "modeling" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_object_models a :Method ;
    :hasText "object_models" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_per a :Eval ;
    :hasText "per" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_scenes a :Generic ;
    :hasText "scenes" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_source_separation a :Task ;
    :hasText "source_separation" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_baseline a :Generic ;
    :hasText "baseline" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_concept a :Other ;
    :hasText "concept" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_deep_models a :Method ;
    :hasText "deep_models" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_deep_networks a :Method ;
    :hasText "deep_networks" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_learns a :Task ;
    :hasText "learns" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_mnist a :Material ;
    :hasText "mnist" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_observations a :Generic ;
    :hasText "observations" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_one-class_classification a :Method ;
    :hasText "one-class_classification" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_one-class_classifiers a :Method ;
    :hasText "one-class_classifiers" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_other a :Generic ;
    :hasText "other" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_outlier_detection a :Task ;
    :hasText "outlier_detection" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_outliers a :Other ;
    :hasText "outliers" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_target a :Generic ;
    :hasText "target" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_task a :Generic ;
    :hasText "task" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_computation a :Other ;
    :hasText "computation" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_constraints a :Generic ;
    :hasText "constraints" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_device a :Generic ;
    :hasText "device" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_devices a :Generic ;
    :hasText "devices" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_edge a :Other ;
    :hasText "edge" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_measure a :Generic ;
    :hasText "measure" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_metrics a :Generic ;
    :hasText "metrics" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_pascal_voc a :Material ;
    :hasText "pascal_voc" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_per a :Eval ;
    :hasText "per" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_resolution a :Eval ;
    :hasText "resolution" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_resource a :Generic ;
    :hasText "resource" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper_times a :Generic ;
    :hasText "times" .

:Sage_Logo_Synthesis_and_CVPR_2018_paper_logo a :Other ;
    :hasText "logo" .

:Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_image a :Material ;
    :hasText "image" .

:Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_image_super-resolution a :Task ;
    :hasText "image_super-resolution" .

:Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_resolution a :Eval ;
    :hasText "resolution" .

:Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper_task a :Generic ;
    :hasText "task" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_complex a :Method ;
    :hasText "complex" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_dialog a :Other ;
    :hasText "dialog" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_granularity a :Eval ;
    :hasText "granularity" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_grounding a :Task ;
    :hasText "grounding" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_interpretable a :Generic ;
    :hasText "interpretable" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_mnist a :Material ;
    :hasText "mnist" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_modules a :Generic ;
    :hasText "modules" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_phrases a :Generic ;
    :hasText "phrases" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_pronouns a :Other ;
    :hasText "pronouns" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_question_answering a :Task ;
    :hasText "question_answering" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_resolution a :Eval ;
    :hasText "resolution" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper_words a :Generic ;
    :hasText "words" .

:Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper_technique a :Generic ;
    :hasText "technique" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_adaptation a :Task ;
    :hasText "adaptation" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_automatic a :Task ;
    :hasText "automatic" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_commercial_systems a :Method ;
    :hasText "commercial_systems" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_conditions a :Generic ;
    :hasText "conditions" .

<https://github.com/deepcurator/DCC/Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_experimental_results a :Generic ;
    :hasText "experimental_results" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_manual_annotations a :Task ;
    :hasText "manual_annotations" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_region a :Generic ;
    :hasText "region" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_system a :Generic ;
    :hasText "system" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_color a :Other ;
    :hasText "color" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_dynamic_scenes a :Other ;
    :hasText "dynamic_scenes" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_flow a :Other ;
    :hasText "flow" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_flows a :Other ;
    :hasText "flows" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_imaging a :Generic ;
    :hasText "imaging" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_motions a :Generic ;
    :hasText "motions" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_occlusion a :Other ;
    :hasText "occlusion" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_flow a :Other ;
    :hasText "flow" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_generation a :Task ;
    :hasText "generation" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_learns a :Task ;
    :hasText "learns" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_modules a :Generic ;
    :hasText "modules" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_object_models a :Method ;
    :hasText "object_models" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_predictions a :Task ;
    :hasText "predictions" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_priors a :Other ;
    :hasText "priors" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_scenes a :Generic ;
    :hasText "scenes" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_target a :Generic ;
    :hasText "target" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_viewpoints a :Other ;
    :hasText "viewpoints" .

:Shao-Hua_Sun_Multi-view_to_Novel_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_classes a :Generic ;
    :hasText "classes" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_models a :Generic ;
    :hasText "models" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_object_category a :Other ;
    :hasText "object_category" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_object_shape a :Other ;
    :hasText "object_shape" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_outputs a :Generic ;
    :hasText "outputs" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_predicting a :Task ;
    :hasText "predicting" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_resolution a :Eval ;
    :hasText "resolution" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_shape a :Other ;
    :hasText "shape" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_shape_representation a :Method ;
    :hasText "shape_representation" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_shape_representations a :Method ;
    :hasText "shape_representations" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_surface a :Other ;
    :hasText "surface" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_surfaces a :Other ;
    :hasText "surfaces" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_viewpoints a :Other ;
    :hasText "viewpoints" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_way a :Generic ;
    :hasText "way" .

:Shin_Pixels_Voxels_and_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_signal a :Generic ;
    :hasText "signal" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_structured a :Generic ;
    :hasText "structured" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_texture a :Other ;
    :hasText "texture" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_actions a :Task ;
    :hasText "actions" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_data a :Generic ;
    :hasText "data" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_human_action_recognition a :Task ;
    :hasText "human_action_recognition" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_human_actions a :Other ;
    :hasText "human_actions" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_interactions a :Generic ;
    :hasText "interactions" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_knowledge a :Generic ;
    :hasText "knowledge" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_learns a :Task ;
    :hasText "learns" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_modalities a :Generic ;
    :hasText "modalities" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_models a :Generic ;
    :hasText "models" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_one a :Generic ;
    :hasText "one" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_research a :Generic ;
    :hasText "research" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_scale a :Other ;
    :hasText "scale" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_these a :Generic ;
    :hasText "these" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_they a :Generic ;
    :hasText "they" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_transfer a :Other ;
    :hasText "transfer" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_transferring_knowledge a :Task ;
    :hasText "transferring_knowledge" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_understanding a :Task ;
    :hasText "understanding" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Sigurdsson_Actor_and_Observer_CVPR_2018_paper_web a :Material ;
    :hasText "web" .

:Snape_Face_Flow_ICCV_2015_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Snape_Face_Flow_ICCV_2015_paper_compositional a :Other ;
    :hasText "compositional" .

:Snape_Face_Flow_ICCV_2015_paper_computation a :Other ;
    :hasText "computation" .

:Snape_Face_Flow_ICCV_2015_paper_correlation a :Other ;
    :hasText "correlation" .

:Snape_Face_Flow_ICCV_2015_paper_correspondences a :Generic ;
    :hasText "correspondences" .

:Snape_Face_Flow_ICCV_2015_paper_data a :Generic ;
    :hasText "data" .

:Snape_Face_Flow_ICCV_2015_paper_experiments a :Generic ;
    :hasText "experiments" .

:Snape_Face_Flow_ICCV_2015_paper_face a :Other ;
    :hasText "face" .

:Snape_Face_Flow_ICCV_2015_paper_flow a :Other ;
    :hasText "flow" .

:Snape_Face_Flow_ICCV_2015_paper_formulation a :Generic ;
    :hasText "formulation" .

:Snape_Face_Flow_ICCV_2015_paper_human_expressions a :Other ;
    :hasText "human_expressions" .

:Snape_Face_Flow_ICCV_2015_paper_images a :Material ;
    :hasText "images" .

:Snape_Face_Flow_ICCV_2015_paper_input a :Generic ;
    :hasText "input" .

:Snape_Face_Flow_ICCV_2015_paper_magnitude a :Other ;
    :hasText "magnitude" .

:Snape_Face_Flow_ICCV_2015_paper_matrix a :Generic ;
    :hasText "matrix" .

:Snape_Face_Flow_ICCV_2015_paper_method a :Generic ;
    :hasText "method" .

:Snape_Face_Flow_ICCV_2015_paper_model-based a :Generic ;
    :hasText "model-based" .

:Snape_Face_Flow_ICCV_2015_paper_motion a :Generic ;
    :hasText "motion" .

:Snape_Face_Flow_ICCV_2015_paper_occlusions a :Other ;
    :hasText "occlusions" .

:Snape_Face_Flow_ICCV_2015_paper_prior a :Generic ;
    :hasText "prior" .

:Snape_Face_Flow_ICCV_2015_paper_problem a :Generic ;
    :hasText "problem" .

:Snape_Face_Flow_ICCV_2015_paper_rank a :Other ;
    :hasText "rank" .

:Snape_Face_Flow_ICCV_2015_paper_registration a :Task ;
    :hasText "registration" .

:Snape_Face_Flow_ICCV_2015_paper_results a :Generic ;
    :hasText "results" .

:Snape_Face_Flow_ICCV_2015_paper_state a :Generic ;
    :hasText "state" .

:Snape_Face_Flow_ICCV_2015_paper_strategy a :Generic ;
    :hasText "strategy" .

:Snape_Face_Flow_ICCV_2015_paper_techniques a :Generic ;
    :hasText "techniques" .

:Snape_Face_Flow_ICCV_2015_paper_that a :Generic ;
    :hasText "that" .

:Snape_Face_Flow_ICCV_2015_paper_this a :Generic ;
    :hasText "this" .

:Snape_Face_Flow_ICCV_2015_paper_we a :Generic ;
    :hasText "we" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_appearance_variations a :Other ;
    :hasText "appearance_variations" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_class_imbalance a :Other ;
    :hasText "class_imbalance" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_classification a :Task ;
    :hasText "classification" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_target a :Generic ;
    :hasText "target" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_these a :Generic ;
    :hasText "these" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_they a :Generic ;
    :hasText "they" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_tracker a :Generic ;
    :hasText "tracker" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_trackers a :Generic ;
    :hasText "trackers" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_tracking a :Task ;
    :hasText "tracking" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Song_VITAL_VIsual_Tracking_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Su_Reasoning_About_Fine-Grained_ICCV_2017_paper_framework a :Generic ;
    :hasText "framework" .

:Su_Reasoning_About_Fine-Grained_ICCV_2017_paper_instances a :Generic ;
    :hasText "instances" .

:Su_Reasoning_About_Fine-Grained_ICCV_2017_paper_phrases a :Generic ;
    :hasText "phrases" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_3d_models a :Method ;
    :hasText "3d_models" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_alignment a :Task ;
    :hasText "alignment" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_annotation a :Task ;
    :hasText "annotation" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_pose a :Other ;
    :hasText "pose" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_scale a :Other ;
    :hasText "scale" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_shape a :Other ;
    :hasText "shape" .

:Sun_Pix3D_Dataset_and_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

<https://github.com/deepcurator/DCC/Tai_MemNet_A_Persistent_ICCV_2017_paper_deep_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "deep_convolutional_neural_networks_(cnns)" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_experiments a :Generic ;
    :hasText "experiments" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_information a :Generic ;
    :hasText "information" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_instances a :Generic ;
    :hasText "instances" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_latter a :Generic ;
    :hasText "latter" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_location a :Generic ;
    :hasText "location" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_map a :Eval ;
    :hasText "map" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_network a :Generic ;
    :hasText "network" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_nodes a :Other ;
    :hasText "nodes" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_object_detectors a :Method ;
    :hasText "object_detectors" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_object_recognition a :Task ;
    :hasText "object_recognition" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_pascal_voc a :Material ;
    :hasText "pascal_voc" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_procedure a :Generic ;
    :hasText "procedure" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_results a :Generic ;
    :hasText "results" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_state a :Generic ;
    :hasText "state" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_supervised_detectors a :Method ;
    :hasText "supervised_detectors" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_supervision a :Other ;
    :hasText "supervision" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Tang_Multiple_Instance_Detection_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_image_deblurring a :Task ;
    :hasText "image_deblurring" .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper_scheme a :Generic ;
    :hasText "scheme" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_locations a :Generic ;
    :hasText "locations" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_object_pose_estimation a :Task ;
    :hasText "object_pose_estimation" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_occlusion a :Other ;
    :hasText "occlusion" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_other a :Generic ;
    :hasText "other" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_pose a :Other ;
    :hasText "pose" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_post-processing a :Task ;
    :hasText "post-processing" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_these a :Generic ;
    :hasText "these" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_they a :Generic ;
    :hasText "they" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Tekin_Real-Time_Seamless_Single_CVPR_2018_paper_vertices a :Other ;
    :hasText "vertices" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_approach a :Generic ;
    :hasText "approach" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_correspondences a :Generic ;
    :hasText "correspondences" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_face a :Other ;
    :hasText "face" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_image a :Material ;
    :hasText "image" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_instances a :Generic ;
    :hasText "instances" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_learn a :Task ;
    :hasText "learn" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_method a :Generic ;
    :hasText "method" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_object_categories a :Generic ;
    :hasText "object_categories" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_structure a :Generic ;
    :hasText "structure" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_these a :Generic ;
    :hasText "these" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_viewpoint a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/viewpoint> ;
    :hasText "viewpoint" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_vision a :Generic ;
    :hasText "vision" .

:Thewlis_Unsupervised_Learning_of_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_correspondence a :Generic ;
    :hasText "correspondence" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_correspondences a :Generic ;
    :hasText "correspondences" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_distance a :Other ;
    :hasText "distance" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_predicting a :Task ;
    :hasText "predicting" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_shape a :Other ;
    :hasText "shape" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_shapes a :Other ;
    :hasText "shapes" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_surface a :Other ;
    :hasText "surface" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Thibault_Groueix_Shape_correspondences_from_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_branches a :Other ;
    :hasText "branches" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_cifar-10 a :Material ;
    :hasText "cifar-10" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_encoder-decoder a :Task ;
    :hasText "encoder-decoder" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_extraction a :Task ;
    :hasText "extraction" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_generalization_performances a :Eval ;
    :hasText "generalization_performances" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_learned_representations a :Generic ;
    :hasText "learned_representations" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_signal a :Generic ;
    :hasText "signal" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_studies a :Generic ;
    :hasText "studies" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_svhn a :Material ;
    :hasText "svhn" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_unlabeled_data a :Other ;
    :hasText "unlabeled_data" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_validate a :Task ;
    :hasText "validate" .

:Thomas_Robert_HybridNet_Classification_and_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_conditions a :Generic ;
    :hasText "conditions" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_factors a :Generic ;
    :hasText "factors" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_interpretable a :Generic ;
    :hasText "interpretable" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_latent_variables a :Other ;
    :hasText "latent_variables" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_layers a :Other ;
    :hasText "layers" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_properties a :Generic ;
    :hasText "properties" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_scenes a :Generic ;
    :hasText "scenes" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_shapes a :Other ;
    :hasText "shapes" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_spheres a :Generic ;
    :hasText "spheres" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_system a :Generic ;
    :hasText "system" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_variables a :Other ;
    :hasText "variables" .

:Tian_Ye_Interpretable_Intuitive_Physics_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_appearance_variations a :Other ;
    :hasText "appearance_variations" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_detection_methods a :Method ;
    :hasText "detection_methods" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_location a :Generic ;
    :hasText "location" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_long-term a :Generic ;
    :hasText "long-term" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_lstm a :Method ;
    :hasText "lstm" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_neural_networks a :Method ;
    :hasText "neural_networks" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_outputs a :Generic ;
    :hasText "outputs" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_popularity a :Other ;
    :hasText "popularity" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_search a :Task ;
    :hasText "search" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_signals a :Generic ;
    :hasText "signals" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_target a :Generic ;
    :hasText "target" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_tracker a :Generic ;
    :hasText "tracker" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_tracking a :Task ;
    :hasText "tracking" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_tracking_methods a :Method ;
    :hasText "tracking_methods" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_ways a :Generic ;
    :hasText "ways" .

:Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Tome_Lifting_From_the_CVPR_2017_paper_errors a :Other ;
    :hasText "errors" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_adaptation a :Task ;
    :hasText "adaptation" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_approach a :Generic ;
    :hasText "approach" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_confidence_measures a :Eval ;
    :hasText "confidence_measures" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_corpus a :Material ;
    :hasText "corpus" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_data a :Generic ;
    :hasText "data" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_environments a :Generic ;
    :hasText "environments" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_former a :Generic ;
    :hasText "former" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_function a :Generic ;
    :hasText "function" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_gather a :Method ;
    :hasText "gather" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_image a :Material ;
    :hasText "image" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_images a :Material ;
    :hasText "images" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_information a :Generic ;
    :hasText "information" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_kitti a :Material ;
    :hasText "kitti" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_latter a :Generic ;
    :hasText "latter" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_maps a :Eval ;
    :hasText "maps" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_model a :Generic ;
    :hasText "model" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_network a :Generic ;
    :hasText "network" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_networks a :Generic ;
    :hasText "networks" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_other a :Generic ;
    :hasText "other" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_predictions a :Task ;
    :hasText "predictions" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_proposal a :Generic ;
    :hasText "proposal" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_smoothness a :Other ;
    :hasText "smoothness" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_state a :Generic ;
    :hasText "state" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_stereo_algorithms a :Method ;
    :hasText "stereo_algorithms" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_domain_adaptation a :Method ;
    :hasText "domain_adaptation" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_domains a :Generic ;
    :hasText "domains" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_outputs a :Generic ;
    :hasText "outputs" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_semantic_segmentations a :Task ;
    :hasText "semantic_segmentations" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_similarities a :Generic ;
    :hasText "similarities" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_structured a :Generic ;
    :hasText "structured" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_target_domain a :Material ;
    :hasText "target_domain" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_target_domains a :Material ;
    :hasText "target_domains" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_visual_quality a :Eval ;
    :hasText "visual_quality" .

:Tsai_Learning_to_Adapt_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_algorithm a :Generic ;
    :hasText "algorithm" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_computer_vision_tasks a :Task ;
    :hasText "computer_vision_tasks" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_experimental_results a :Generic ;
    :hasText "experimental_results" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_improvements a :Eval ;
    :hasText "improvements" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_loss_function a :Method ;
    :hasText "loss_function" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_object_boundaries a :Other ;
    :hasText "object_boundaries" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_these a :Generic ;
    :hasText "these" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_vision_applications a :Task ;
    :hasText "vision_applications" .

:Tu_Learning_Superpixels_With_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_applications a :Generic ;
    :hasText "applications" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_collection a :Generic ;
    :hasText "collection" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_complex a :Method ;
    :hasText "complex" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_data a :Generic ;
    :hasText "data" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_examples a :Generic ;
    :hasText "examples" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_framework a :Generic ;
    :hasText "framework" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_instances a :Generic ;
    :hasText "instances" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_interpretable a :Generic ;
    :hasText "interpretable" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_objects a :Generic ;
    :hasText "objects" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_our_method a :Other ;
    :hasText "our_method" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_parsing a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/parsing_algorithm> ;
    :hasText "parsing" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_predicting a :Task ;
    :hasText "predicting" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_prediction a :Task ;
    :hasText "prediction" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_representation a :Generic ;
    :hasText "representation" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_shape a :Other ;
    :hasText "shape" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_shape_representations a :Method ;
    :hasText "shape_representations" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_shapes a :Other ;
    :hasText "shapes" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_similarity_measure a :Eval ;
    :hasText "similarity_measure" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_structure a :Generic ;
    :hasText "structure" .

:Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_motion a :Generic ;
    :hasText "motion" .

:Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_target a :Generic ;
    :hasText "target" .

:Villegas_Neural_Kinematic_Networks_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_approach a :Generic ;
    :hasText "approach" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_architecture a :Generic ;
    :hasText "architecture" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_datasets a :Material ;
    :hasText "datasets" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_generative_model a :Method ;
    :hasText "generative_model" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_image a :Material ;
    :hasText "image" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_image_descriptions a :Task ;
    :hasText "image_descriptions" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_improvements a :Eval ;
    :hasText "improvements" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_language a :Generic ;
    :hasText "language" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_learns a :Task ;
    :hasText "learns" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_machine_translation a :Task ;
    :hasText "machine_translation" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_model a :Generic ;
    :hasText "model" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_natural_language_processing a :Method ;
    :hasText "natural_language_processing" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_problem a :Generic ;
    :hasText "problem" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_state a :Generic ;
    :hasText "state" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_target a :Generic ;
    :hasText "target" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_that a :Generic ;
    :hasText "that" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_this a :Generic ;
    :hasText "this" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_training a :Task ;
    :hasText "training" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_vision a :Generic ;
    :hasText "vision" .

:Vinyals_Show_and_Tell_2015_CVPR_paper_we a :Generic ;
    :hasText "we" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_evaluation_method a :Generic ;
    :hasText "evaluation_method" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_gradient a :Other ;
    :hasText "gradient" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_large_datasets a :Material ;
    :hasText "large_datasets" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_machine_learning a :Task ;
    :hasText "machine_learning" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_policy a :Other ;
    :hasText "policy" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_systems a :Generic ;
    :hasText "systems" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_data_augmentation a :Method ;
    :hasText "data_augmentation" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_domain_adaptation a :Method ;
    :hasText "domain_adaptation" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_feature_space a :Other ;
    :hasText "feature_space" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_gans a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_means a :Generic ;
    :hasText "means" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_objective_function a :Other ;
    :hasText "objective_function" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_target a :Generic ;
    :hasText "target" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_technique a :Generic ;
    :hasText "technique" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_entropy a :Other ;
    :hasText "entropy" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_instances a :Generic ;
    :hasText "instances" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_learning_algorithm a :Method ;
    :hasText "learning_algorithm" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_locations a :Generic ;
    :hasText "locations" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_measure a :Generic ;
    :hasText "measure" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_metric a :Generic ;
    :hasText "metric" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_models a :Generic ;
    :hasText "models" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_object_detectors a :Method ;
    :hasText "object_detectors" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_task a :Generic ;
    :hasText "task" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_time a :Generic ;
    :hasText "time" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_transfers a :Other ;
    :hasText "transfers" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Wan_Min-Entropy_Latent_Model_CVPR_2018_paper_variance a :Generic ;
    :hasText "variance" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_adversarial_examples a :Task ;
    :hasText "adversarial_examples" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_classification a :Task ;
    :hasText "classification" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_classification_performance a :Method ;
    :hasText "classification_performance" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_classification_tasks a :Method ;
    :hasText "classification_tasks" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_cross-entropy a :Method ;
    :hasText "cross-entropy" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_gaussian_mixture a :Method ;
    :hasText "gaussian_mixture" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_mnist a :Material ;
    :hasText "mnist" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_modeling a :Task ;
    :hasText "modeling" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_proposal a :Generic ;
    :hasText "proposal" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Wan_Rethinking_Feature_Distribution_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_color a :Other ;
    :hasText "color" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_conditional_gans a :Method ;
    :hasText "conditional_gans" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_conditional_generative_adversarial_networks a :Method ;
    :hasText "conditional_generative_adversarial_networks" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_map a :Eval ;
    :hasText "map" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_scenes a :Generic ;
    :hasText "scenes" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_texture a :Other ;
    :hasText "texture" .

:Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper_trees a :Other ;
    :hasText "trees" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_classification a :Task ;
    :hasText "classification" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_discriminative_patches a :Method ;
    :hasText "discriminative_patches" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_frameworks a :Generic ;
    :hasText "frameworks" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_information a :Generic ;
    :hasText "information" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_method a :Generic ;
    :hasText "method" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_representation_learning a :Task ;
    :hasText "representation_learning" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_statistics a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/correlation_analysis> ;
    :hasText "statistics" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_structured a :Generic ;
    :hasText "structured" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_studies a :Generic ;
    :hasText "studies" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Wang_Learning_a_Discriminative_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Wang_Revisiting_Video_Saliency_CVPR_2018_paper_research a :Generic ;
    :hasText "research" .

:Wang_Revisiting_Video_Saliency_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Wang_Revisiting_Video_Saliency_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Wang_Revisiting_Video_Saliency_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Wang_Revisiting_Video_Saliency_CVPR_2018_paper_ways a :Generic ;
    :hasText "ways" .

:Wang_Revisiting_Video_Saliency_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Wei_Dense_Human_Body_CVPR_2016_paper_3d_models a :Method ;
    :hasText "3d_models" .

:Wei_Dense_Human_Body_CVPR_2016_paper_approach a :Generic ;
    :hasText "approach" .

:Wei_Dense_Human_Body_CVPR_2016_paper_classification a :Task ;
    :hasText "classification" .

:Wei_Dense_Human_Body_CVPR_2016_paper_complex a :Method ;
    :hasText "complex" .

:Wei_Dense_Human_Body_CVPR_2016_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Wei_Dense_Human_Body_CVPR_2016_paper_correspondence a :Generic ;
    :hasText "correspondence" .

:Wei_Dense_Human_Body_CVPR_2016_paper_correspondences a :Generic ;
    :hasText "correspondences" .

:Wei_Dense_Human_Body_CVPR_2016_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Wei_Dense_Human_Body_CVPR_2016_paper_feature a :Other ;
    :hasText "feature" .

:Wei_Dense_Human_Body_CVPR_2016_paper_feature_space a :Other ;
    :hasText "feature_space" .

:Wei_Dense_Human_Body_CVPR_2016_paper_framework a :Generic ;
    :hasText "framework" .

:Wei_Dense_Human_Body_CVPR_2016_paper_information a :Generic ;
    :hasText "information" .

:Wei_Dense_Human_Body_CVPR_2016_paper_map a :Eval ;
    :hasText "map" .

:Wei_Dense_Human_Body_CVPR_2016_paper_maps a :Eval ;
    :hasText "maps" .

:Wei_Dense_Human_Body_CVPR_2016_paper_method a :Generic ;
    :hasText "method" .

:Wei_Dense_Human_Body_CVPR_2016_paper_methods a :Generic ;
    :hasText "methods" .

:Wei_Dense_Human_Body_CVPR_2016_paper_network a :Generic ;
    :hasText "network" .

:Wei_Dense_Human_Body_CVPR_2016_paper_our_method a :Other ;
    :hasText "our_method" .

:Wei_Dense_Human_Body_CVPR_2016_paper_poses a :Other ;
    :hasText "poses" .

:Wei_Dense_Human_Body_CVPR_2016_paper_problem a :Generic ;
    :hasText "problem" .

:Wei_Dense_Human_Body_CVPR_2016_paper_region a :Generic ;
    :hasText "region" .

:Wei_Dense_Human_Body_CVPR_2016_paper_shape a :Other ;
    :hasText "shape" .

:Wei_Dense_Human_Body_CVPR_2016_paper_shapes a :Other ;
    :hasText "shapes" .

:Wei_Dense_Human_Body_CVPR_2016_paper_smoothness a :Other ;
    :hasText "smoothness" .

:Wei_Dense_Human_Body_CVPR_2016_paper_state a :Generic ;
    :hasText "state" .

:Wei_Dense_Human_Body_CVPR_2016_paper_surfaces a :Other ;
    :hasText "surfaces" .

:Wei_Dense_Human_Body_CVPR_2016_paper_synthetic_data a :Material ;
    :hasText "synthetic_data" .

:Wei_Dense_Human_Body_CVPR_2016_paper_target a :Generic ;
    :hasText "target" .

:Wei_Dense_Human_Body_CVPR_2016_paper_that a :Generic ;
    :hasText "that" .

:Wei_Dense_Human_Body_CVPR_2016_paper_those a :Generic ;
    :hasText "those" .

:Wei_Dense_Human_Body_CVPR_2016_paper_time a :Generic ;
    :hasText "time" .

:Wei_Dense_Human_Body_CVPR_2016_paper_training a :Task ;
    :hasText "training" .

:Wei_Dense_Human_Body_CVPR_2016_paper_two a :Generic ;
    :hasText "two" .

:Wei_Dense_Human_Body_CVPR_2016_paper_unsupervised_methods a :Method ;
    :hasText "unsupervised_methods" .

:Wei_Dense_Human_Body_CVPR_2016_paper_validate a :Task ;
    :hasText "validate" .

:Wei_Dense_Human_Body_CVPR_2016_paper_viewpoints a :Other ;
    :hasText "viewpoints" .

:Wei_Dense_Human_Body_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_detection_accuracy a :Eval ;
    :hasText "detection_accuracy" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_flow a :Other ;
    :hasText "flow" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_latter a :Generic ;
    :hasText "latter" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_solution a :Generic ;
    :hasText "solution" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_stacks a :Other ;
    :hasText "stacks" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_computation a :Other ;
    :hasText "computation" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_convolution a :Method ;
    :hasText "convolution" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_flexibility a :Eval ;
    :hasText "flexibility" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

<https://github.com/deepcurator/DCC/Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_neural_networks_(> a :Method ;
    :hasText "neural_networks_(" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_operations a :Generic ;
    :hasText "operations" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_operators a :Generic ;
    :hasText "operators" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_point_clouds a :Other ;
    :hasText "point_clouds" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_semantic_segmentation a :Task ;
    :hasText "semantic_segmentation" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_spatial_information a :Other ;
    :hasText "spatial_information" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_studies a :Generic ;
    :hasText "studies" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_validate a :Task ;
    :hasText "validate" .

:Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_deblurring a :Task ;
    :hasText "deblurring" .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_feature a :Other ;
    :hasText "feature" .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_inputs a :Generic ;
    :hasText "inputs" .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_learning-based_approach a :Method ;
    :hasText "learning-based_approach" .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_state a :Generic ;
    :hasText "state" .

:Wieschollek_Learning_Blind_Motion_ICCV_2017_paper_time a :Generic ;
    :hasText "time" .

:Wu_Exploit_the_Unknown_CVPR_2018_paper_one a :Generic ;
    :hasText "one" .

:Wu_Exploit_the_Unknown_CVPR_2018_paper_person_re-identification a :Task ;
    :hasText "person_re-identification" .

:Wu_Exploit_the_Unknown_CVPR_2018_paper_video a :Material ;
    :hasText "video" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_agents a :Generic ;
    :hasText "agents" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_board a :Other ;
    :hasText "board" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_constraints a :Generic ;
    :hasText "constraints" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_environment a :Generic ;
    :hasText "environment" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_modalities a :Generic ;
    :hasText "modalities" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_observations a :Generic ;
    :hasText "observations" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_semantic_labels a :Other ;
    :hasText "semantic_labels" .

:Xia_Gibson_Env_Real-World_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_algorithms a :Generic ;
    :hasText "algorithms" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_contextual_information a :Other ;
    :hasText "contextual_information" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_evaluation_metrics a :Eval ;
    :hasText "evaluation_metrics" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_field a :Generic ;
    :hasText "field" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_layers a :Other ;
    :hasText "layers" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_real-world_datasets a :Material ;
    :hasText "real-world_datasets" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_shapes a :Other ;
    :hasText "shapes" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computation_efficiency a :Eval ;
    :hasText "computation_efficiency" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_computer_vision_tasks a :Task ;
    :hasText "computer_vision_tasks" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_face a :Other ;
    :hasText "face" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_gradient a :Other ;
    :hasText "gradient" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_interference a :Other ;
    :hasText "interference" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_multi-task_learning_methods a :Task ;
    :hasText "multi-task_learning_methods" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_optimum a :Other ;
    :hasText "optimum" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_quality a :Eval ;
    :hasText "quality" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_relevance a :Eval ;
    :hasText "relevance" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_retrieval a :Task ;
    :hasText "retrieval" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_scales a :Other ;
    :hasText "scales" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_human_pose a :Other ;
    :hasText "human_pose" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_quantization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/quantization> ;
    :hasText "quantization" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_color a :Other ;
    :hasText "color" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_conditions a :Generic ;
    :hasText "conditions" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_detection_method a :Method ;
    :hasText "detection_method" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_fcn a :Method ;
    :hasText "fcn" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_focal_loss_function a :Generic ;
    :hasText "focal_loss_function" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_gaussian_mixture_model a :Method ;
    :hasText "gaussian_mixture_model" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_gmm a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gaussian_mixture_model> ;
    :hasText "gmm" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_metrics a :Generic ;
    :hasText "metrics" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_precision a :Eval ;
    :hasText "precision" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_recall a :Eval ;
    :hasText "recall" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_reflection a :Other ;
    :hasText "reflection" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_stereo_images a :Material ;
    :hasText "stereo_images" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_surface a :Other ;
    :hasText "surface" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_color a :Other ;
    :hasText "color" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_convolution a :Method ;
    :hasText "convolution" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_extensions a :Generic ;
    :hasText "extensions" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_fields a :Generic ;
    :hasText "fields" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_matrices a :Generic ;
    :hasText "matrices" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_modules a :Generic ;
    :hasText "modules" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_neural_network_models a :Method ;
    :hasText "neural_network_models" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_real_domain a :Material ;
    :hasText "real_domain" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_techniques a :Generic ;
    :hasText "techniques" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper_pooling a :Method ;
    :hasText "pooling" .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper_texture a :Other ;
    :hasText "texture" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_baselines a :Generic ;
    :hasText "baselines" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_counterparts a :Generic ;
    :hasText "counterparts" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_model_parameters a :Other ;
    :hasText "model_parameters" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_multi-task_learning a :Task ;
    :hasText "multi-task_learning" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_parsing a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/parsing_algorithm> ;
    :hasText "parsing" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_post-processing a :Task ;
    :hasText "post-processing" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_candidates a :Generic ;
    :hasText "candidates" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_computation a :Other ;
    :hasText "computation" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_cues a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/cues> ;
    :hasText "cues" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_graph a :Other ;
    :hasText "graph" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_human_pose a :Other ;
    :hasText "human_pose" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_pose a :Other ;
    :hasText "pose" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_pose_estimation a :Task ;
    :hasText "pose_estimation" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper_way a :Generic ;
    :hasText "way" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_applications a :Generic ;
    :hasText "applications" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_baseline_methods a :Generic ;
    :hasText "baseline_methods" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_concept a :Other ;
    :hasText "concept" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_correspondence a :Generic ;
    :hasText "correspondence" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_experimental_results a :Generic ;
    :hasText "experimental_results" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_prediction a :Task ;
    :hasText "prediction" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_research a :Generic ;
    :hasText "research" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_segments a :Generic ;
    :hasText "segments" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_semantic_coherence a :Other ;
    :hasText "semantic_coherence" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_technology a :Generic ;
    :hasText "technology" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_they a :Generic ;
    :hasText "they" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_time a :Generic ;
    :hasText "time" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_video_surveillance a :Task ;
    :hasText "video_surveillance" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Yang_Feng_Video_Re-localization_via_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yang_Learning_Face_Age_CVPR_2018_paper_face a :Other ;
    :hasText "face" .

:Yang_Learning_Face_Age_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Yang_Object_Contour_Detection_CVPR_2016_paper_deep_learning_algorithm a :Generic ;
    :hasText "deep_learning_algorithm" .

:Yang_Robust_Classification_With_CVPR_2018_paper_accuracies a :Eval ;
    :hasText "accuracies" .

:Yang_Robust_Classification_With_CVPR_2018_paper_adversarial_examples a :Task ;
    :hasText "adversarial_examples" .

:Yang_Robust_Classification_With_CVPR_2018_paper_categories a :Generic ;
    :hasText "categories" .

:Yang_Robust_Classification_With_CVPR_2018_paper_classes a :Generic ;
    :hasText "classes" .

:Yang_Robust_Classification_With_CVPR_2018_paper_classification a :Task ;
    :hasText "classification" .

:Yang_Robust_Classification_With_CVPR_2018_paper_cnn a :Method ;
    :hasText "cnn" .

:Yang_Robust_Classification_With_CVPR_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Yang_Robust_Classification_With_CVPR_2018_paper_criteria a :Generic ;
    :hasText "criteria" .

:Yang_Robust_Classification_With_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Yang_Robust_Classification_With_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Yang_Robust_Classification_With_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Yang_Robust_Classification_With_CVPR_2018_paper_generative_model a :Method ;
    :hasText "generative_model" .

:Yang_Robust_Classification_With_CVPR_2018_paper_image_classification a :Task ;
    :hasText "image_classification" .

:Yang_Robust_Classification_With_CVPR_2018_paper_model a :Generic ;
    :hasText "model" .

:Yang_Robust_Classification_With_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

<https://github.com/deepcurator/DCC/Yang_Robust_Classification_With_CVPR_2018_paper_neural_networks_(> a :Method ;
    :hasText "neural_networks_(" .

:Yang_Robust_Classification_With_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Yang_Robust_Classification_With_CVPR_2018_paper_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:Yang_Robust_Classification_With_CVPR_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Yang_Robust_Classification_With_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Yang_Robust_Classification_With_CVPR_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Yang_Robust_Classification_With_CVPR_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Yang_Robust_Classification_With_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Yang_Robust_Classification_With_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Yang_Robust_Classification_With_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_alignment a :Task ;
    :hasText "alignment" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_face a :Other ;
    :hasText "face" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_loss_function a :Method ;
    :hasText "loss_function" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_position a :Other ;
    :hasText "position" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_prior a :Generic ;
    :hasText "prior" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_records a :Material ;
    :hasText "records" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_shape a :Other ;
    :hasText "shape" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Yao_Feng_Joint_3D_Face_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_complex a :Method ;
    :hasText "complex" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_convolutions a :Method ;
    :hasText "convolutions" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_generalization a :Generic ;
    :hasText "generalization" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_homography a :Other ;
    :hasText "homography" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_inputs a :Generic ;
    :hasText "inputs" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_metric a :Generic ;
    :hasText "metric" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_post-processing a :Task ;
    :hasText "post-processing" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_ranks a :Other ;
    :hasText "ranks" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_times a :Generic ;
    :hasText "times" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_variance a :Generic ;
    :hasText "variance" .

:Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_alignment a :Task ;
    :hasText "alignment" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_audio a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_correlations a :Other ;
    :hasText "correlations" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_distance a :Other ;
    :hasText "distance" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_event a :Other ;
    :hasText "event" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modalities a :Generic ;
    :hasText "modalities" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modality a :Other ;
    :hasText "modality" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_modeling a :Task ;
    :hasText "modeling" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_semantics a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/semantics> ;
    :hasText "semantics" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_unconstrained_videos a :Material ;
    :hasText "unconstrained_videos" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_blur a :Other ;
    :hasText "blur" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_convergence a :Generic ;
    :hasText "convergence" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_generalization a :Generic ;
    :hasText "generalization" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_map a :Eval ;
    :hasText "map" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_mechanisms a :Generic ;
    :hasText "mechanisms" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_parsing a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/parsing_algorithm> ;
    :hasText "parsing" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_parsing_accuracy a :Eval ;
    :hasText "parsing_accuracy" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_patches a :Generic ;
    :hasText "patches" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_resolution a :Eval ;
    :hasText "resolution" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_them a :Generic ;
    :hasText "them" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_validate a :Task ;
    :hasText "validate" .

:Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_cnns a :Method ;
    :hasText "cnns" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_domains a :Generic ;
    :hasText "domains" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_family a :Generic ;
    :hasText "family" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_function a :Generic ;
    :hasText "function" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_hierarchical a :Generic ;
    :hasText "hierarchical" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_neural_networks a :Method ;
    :hasText "neural_networks" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_operations a :Generic ;
    :hasText "operations" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_point_clouds a :Other ;
    :hasText "point_clouds" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_regular a :Generic ;
    :hasText "regular" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_vision a :Generic ;
    :hasText "vision" .

:Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_alternatives a :Generic ;
    :hasText "alternatives" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_constraints a :Generic ;
    :hasText "constraints" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_convolutional_neural_network a :Method ;
    :hasText "convolutional_neural_network" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_formulation a :Generic ;
    :hasText "formulation" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_frameworks a :Generic ;
    :hasText "frameworks" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_images a :Material ;
    :hasText "images" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_information a :Generic ;
    :hasText "information" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_outputs a :Generic ;
    :hasText "outputs" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_patches a :Generic ;
    :hasText "patches" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_rgb a :Material ;
    :hasText "rgb" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_scenes a :Generic ;
    :hasText "scenes" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_surfaces a :Other ;
    :hasText "surfaces" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_task a :Generic ;
    :hasText "task" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yifei_Shi_PlaneMatch_Patch_Coplanarity_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_clustering_method a :Method ;
    :hasText "clustering_method" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_computation a :Other ;
    :hasText "computation" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_efficiency a :Eval ;
    :hasText "efficiency" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_generation a :Task ;
    :hasText "generation" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_graph a :Other ;
    :hasText "graph" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_interactions a :Generic ;
    :hasText "interactions" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_representations a :Generic ;
    :hasText "representations" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_spatial_information a :Other ;
    :hasText "spatial_information" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_structures a :Generic ;
    :hasText "structures" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_subgraphs a :Other ;
    :hasText "subgraphs" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_these a :Generic ;
    :hasText "these" .

:Yikang_LI_Factorizable_Net_An_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_ambiguities a :Other ;
    :hasText "ambiguities" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_components a :Generic ;
    :hasText "components" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_dynamic_scene a :Other ;
    :hasText "dynamic_scene" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_estimation a :Generic ;
    :hasText "estimation" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_flow a :Other ;
    :hasText "flow" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_framework a :Generic ;
    :hasText "framework" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_kitti a :Material ;
    :hasText "kitti" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_modules a :Generic ;
    :hasText "modules" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_occlusions a :Other ;
    :hasText "occlusions" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_ones a :Generic ;
    :hasText "ones" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_outliers a :Other ;
    :hasText "outliers" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_predictions a :Task ;
    :hasText "predictions" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_scene_geometry a :Other ;
    :hasText "scene_geometry" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_scheme a :Generic ;
    :hasText "scheme" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_texture a :Other ;
    :hasText "texture" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_unsupervised_learning a :Method ;
    :hasText "unsupervised_learning" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_unsupervised_methods a :Method ;
    :hasText "unsupervised_methods" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_videos a :Material ;
    :hasText "videos" .

:Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_face a :Other ;
    :hasText "face" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_input a :Generic ;
    :hasText "input" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_neural_networks a :Method ;
    :hasText "neural_networks" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_our_method a :Other ;
    :hasText "our_method" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_post-processing a :Task ;
    :hasText "post-processing" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_results a :Generic ;
    :hasText "results" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_scene a :Generic ;
    :hasText "scene" .

:Yu_Generative_Image_Inpainting_CVPR_2018_paper_texture a :Other ;
    :hasText "texture" .

:Yu_Video_Paragraph_Captioning_CVPR_2016_paper_approach a :Generic ;
    :hasText "approach" .

:Yu_Video_Paragraph_Captioning_CVPR_2016_paper_hierarchical a :Generic ;
    :hasText "hierarchical" .

:Yu_Video_Paragraph_Captioning_CVPR_2016_paper_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:Yu_Video_Paragraph_Captioning_CVPR_2016_paper_rnns a :Method ;
    :hasText "rnns" .

:Yu_Video_Paragraph_Captioning_CVPR_2016_paper_that a :Generic ;
    :hasText "that" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_alternative a :Generic ;
    :hasText "alternative" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_bn a :Method ;
    :hasText "bn" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_computation a :Other ;
    :hasText "computation" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_computer_vision_tasks a :Task ;
    :hasText "computer_vision_tasks" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_counterpart a :Generic ;
    :hasText "counterpart" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_counterparts a :Generic ;
    :hasText "counterparts" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_deep_learning a :Generic ;
    :hasText "deep_learning" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_development a :Material ;
    :hasText "development" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_estimation a :Generic ;
    :hasText "estimation" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_models a :Generic ;
    :hasText "models" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_pre-training a :Method ;
    :hasText "pre-training" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_problems a :Generic ;
    :hasText "problems" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_resnet-50 a :Other ;
    :hasText "resnet-50" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_segmentation a :Task ;
    :hasText "segmentation" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_sizes a :Other ;
    :hasText "sizes" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_statistics a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/correlation_analysis> ;
    :hasText "statistics" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_technique a :Generic ;
    :hasText "technique" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_variance a :Generic ;
    :hasText "variance" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Yuxin_Wu_Group_Normalization_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_agents a :Generic ;
    :hasText "agents" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_annotations a :Material ;
    :hasText "annotations" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_discriminative_features a :Other ;
    :hasText "discriminative_features" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_network a :Generic ;
    :hasText "network" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_other a :Generic ;
    :hasText "other" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_paradigm a :Generic ;
    :hasText "paradigm" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_predictions a :Task ;
    :hasText "predictions" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_those a :Generic ;
    :hasText "those" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Ze_Yang_Learning_to_Navigate_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Zhai_S3Pool_Pooling_With_CVPR_2017_paper_layers a :Other ;
    :hasText "layers" .

:Zhai_S3Pool_Pooling_With_CVPR_2017_paper_pooling a :Method ;
    :hasText "pooling" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_architecture a :Generic ;
    :hasText "architecture" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_classification a :Task ;
    :hasText "classification" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_counterpart a :Generic ;
    :hasText "counterpart" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_error_rate a :Eval ;
    :hasText "error_rate" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_first a :Generic ;
    :hasText "first" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_maps a :Eval ;
    :hasText "maps" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_object_regions a :Other ;
    :hasText "object_regions" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_objects a :Generic ;
    :hasText "objects" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_one a :Generic ;
    :hasText "one" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_regions a :Generic ;
    :hasText "regions" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_supervision a :Other ;
    :hasText "supervision" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_target a :Generic ;
    :hasText "target" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_two a :Generic ;
    :hasText "two" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_way a :Generic ;
    :hasText "way" .

:Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Zhang_Densely_Connected_Pyramid_CVPR_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Zhang_Grounding_Referring_Expressions_CVPR_2018_paper_grounding a :Task ;
    :hasText "grounding" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_cnn a :Method ;
    :hasText "cnn" .

<https://github.com/deepcurator/DCC/Zhang_Residual_Dense_Network_CVPR_2018_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_feature a :Other ;
    :hasText "feature" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_features a :Generic ;
    :hasText "features" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_hierarchical a :Generic ;
    :hasText "hierarchical" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_image a :Material ;
    :hasText "image" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_image_super-resolution a :Task ;
    :hasText "image_super-resolution" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_images a :Material ;
    :hasText "images" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_layers a :Other ;
    :hasText "layers" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_learn a :Task ;
    :hasText "learn" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_local_features a :Other ;
    :hasText "local_features" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_memory a :Other ;
    :hasText "memory" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_models a :Generic ;
    :hasText "models" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_network a :Generic ;
    :hasText "network" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_resolution a :Eval ;
    :hasText "resolution" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_state a :Generic ;
    :hasText "state" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_that a :Generic ;
    :hasText "that" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_this a :Generic ;
    :hasText "this" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_training a :Task ;
    :hasText "training" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_way a :Generic ;
    :hasText "way" .

:Zhang_Residual_Dense_Network_CVPR_2018_paper_we a :Generic ;
    :hasText "we" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_feature_extraction a :Other ;
    :hasText "feature_extraction" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_first a :Generic ;
    :hasText "first" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_image a :Material ;
    :hasText "image" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_knowledge a :Generic ;
    :hasText "knowledge" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_knowledge_bases a :Material ;
    :hasText "knowledge_bases" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_language a :Generic ;
    :hasText "language" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_methods a :Generic ;
    :hasText "methods" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_model a :Generic ;
    :hasText "model" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_modeling a :Task ;
    :hasText "modeling" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_natural_language a :Material ;
    :hasText "natural_language" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_network a :Generic ;
    :hasText "network" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_networks a :Generic ;
    :hasText "networks" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_object_detection a :Task ;
    :hasText "object_detection" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_object_relation a :Task ;
    :hasText "object_relation" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_objects a :Generic ;
    :hasText "objects" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_other a :Generic ;
    :hasText "other" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_priors a :Other ;
    :hasText "priors" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_relations a :Generic ;
    :hasText "relations" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_representation_learning a :Task ;
    :hasText "representation_learning" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_scale a :Other ;
    :hasText "scale" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_scene a :Generic ;
    :hasText "scene" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_state a :Generic ;
    :hasText "state" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_that a :Generic ;
    :hasText "that" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_training a :Task ;
    :hasText "training" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_transfer a :Other ;
    :hasText "transfer" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_two a :Generic ;
    :hasText "two" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_understanding a :Task ;
    :hasText "understanding" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_utility a :Eval ;
    :hasText "utility" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_vision a :Generic ;
    :hasText "vision" .

:Zhang_Visual_Translation_Embedding_CVPR_2017_paper_we a :Generic ;
    :hasText "we" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_classes a :Generic ;
    :hasText "classes" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_datasets a :Material ;
    :hasText "datasets" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_deep_models a :Method ;
    :hasText "deep_models" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_domain_adaptation a :Method ;
    :hasText "domain_adaptation" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_domains a :Generic ;
    :hasText "domains" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_matching a :Task ;
    :hasText "matching" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_methods a :Generic ;
    :hasText "methods" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_mismatch a :Other ;
    :hasText "mismatch" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_problem a :Generic ;
    :hasText "problem" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_results a :Generic ;
    :hasText "results" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_source_domain a :Material ;
    :hasText "source_domain" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_spaces a :Generic ;
    :hasText "spaces" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_target a :Generic ;
    :hasText "target" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_target_domain a :Material ;
    :hasText "target_domain" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_target_domains a :Material ;
    :hasText "target_domains" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_tasks a :Generic ;
    :hasText "tasks" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_transfer a :Other ;
    :hasText "transfer" .

:Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper_two a :Generic ;
    :hasText "two" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_adversarial a :Generic ;
    :hasText "adversarial" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_baseline a :Generic ;
    :hasText "baseline" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cameras a :Other ;
    :hasText "cameras" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_cnn_embeddings a :Other ;
    :hasText "cnn_embeddings" .

<https://github.com/deepcurator/DCC/Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_convolutional_neural_network_(cnn)> a :Method ;
    :hasText "convolutional_neural_network_(cnn)" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_data a :Generic ;
    :hasText "data" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_datasets a :Material ;
    :hasText "datasets" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_generation a :Task ;
    :hasText "generation" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_images a :Material ;
    :hasText "images" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_improvement a :Eval ;
    :hasText "improvement" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_method a :Generic ;
    :hasText "method" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_model a :Generic ;
    :hasText "model" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_network a :Generic ;
    :hasText "network" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_other a :Generic ;
    :hasText "other" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_outliers a :Other ;
    :hasText "outliers" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_person_re-identification a :Task ;
    :hasText "person_re-identification" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_problem a :Generic ;
    :hasText "problem" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_representation_learning a :Task ;
    :hasText "representation_learning" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_scale a :Other ;
    :hasText "scale" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_smoothing a :Method ;
    :hasText "smoothing" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_task a :Generic ;
    :hasText "task" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_that a :Generic ;
    :hasText "that" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_this a :Generic ;
    :hasText "this" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_training a :Task ;
    :hasText "training" .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper_we a :Generic ;
    :hasText "we" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_accuracy a :Eval ;
    :hasText "accuracy" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_benchmarks a :Material ;
    :hasText "benchmarks" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_domain a :Generic ;
    :hasText "domain" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_features a :Generic ;
    :hasText "features" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_first a :Generic ;
    :hasText "first" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_learned_features a :Generic ;
    :hasText "learned_features" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_long-term a :Generic ;
    :hasText "long-term" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_model a :Generic ;
    :hasText "model" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_module a :Generic ;
    :hasText "module" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_networks a :Generic ;
    :hasText "networks" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_region a :Generic ;
    :hasText "region" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_robustness a :Eval ;
    :hasText "robustness" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_sampling a :Method ;
    :hasText "sampling" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_search a :Task ;
    :hasText "search" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_short-term a :Generic ;
    :hasText "short-term" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_state a :Generic ;
    :hasText "state" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_strategy a :Generic ;
    :hasText "strategy" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_tracker a :Generic ;
    :hasText "tracker" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_trackers a :Generic ;
    :hasText "trackers" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_tracking a :Task ;
    :hasText "tracking" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_transfer a :Other ;
    :hasText "transfer" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_approaches a :Generic ;
    :hasText "approaches" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_categories a :Generic ;
    :hasText "categories" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_classification a :Task ;
    :hasText "classification" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_component a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/component> ;
    :hasText "component" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_criterion a :Generic ;
    :hasText "criterion" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_data a :Generic ;
    :hasText "data" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_deep_networks a :Method ;
    :hasText "deep_networks" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_embeddings a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_end-to-end a :Generic ;
    :hasText "end-to-end" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_examples a :Generic ;
    :hasText "examples" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_feature a :Other ;
    :hasText "feature" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_formulation a :Generic ;
    :hasText "formulation" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_image a :Material ;
    :hasText "image" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_imagenet a :Material ;
    :hasText "imagenet" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_input a :Generic ;
    :hasText "input" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_large_datasets a :Material ;
    :hasText "large_datasets" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_learn a :Task ;
    :hasText "learn" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_mechanism a :Generic ;
    :hasText "mechanism" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_memory a :Other ;
    :hasText "memory" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_method a :Generic ;
    :hasText "method" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_nca a :Method ;
    :hasText "nca" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_neighborhood a :Generic ;
    :hasText "neighborhood" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_nonparametric a :Other ;
    :hasText "nonparametric" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_one a :Generic ;
    :hasText "one" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_representation a :Generic ;
    :hasText "representation" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_semantic a :Other ;
    :hasText "semantic" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_structure a :Generic ;
    :hasText "structure" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_this a :Generic ;
    :hasText "this" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_training a :Task ;
    :hasText "training" .

:Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:Zhou_Learning_Deep_Features_CVPR_2016_paper_pooling a :Method ;
    :hasText "pooling" .

:Zhou_Learning_Deep_Features_CVPR_2016_paper_this a :Generic ;
    :hasText "this" .

:Zhou_Learning_Deep_Features_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:Zhuang_Fast_Training_of_CVPR_2016_paper_learn a :Task ;
    :hasText "learn" .

:Zhuang_Fast_Training_of_CVPR_2016_paper_this a :Generic ;
    :hasText "this" .

:Zhuang_Fast_Training_of_CVPR_2016_paper_we a :Generic ;
    :hasText "we" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_approach a :Generic ;
    :hasText "approach" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_convolution a :Method ;
    :hasText "convolution" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_effectiveness a :Eval ;
    :hasText "effectiveness" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_experiments a :Generic ;
    :hasText "experiments" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_location a :Generic ;
    :hasText "location" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_neural_network a :Method ;
    :hasText "neural_network" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_parameters a :Generic ;
    :hasText "parameters" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_rotation a :Other ;
    :hasText "rotation" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_scale a :Other ;
    :hasText "scale" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_scheme a :Generic ;
    :hasText "scheme" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_sphere a :Generic ;
    :hasText "sphere" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_spherical_convolutional_neural_network a :Method ;
    :hasText "spherical_convolutional_neural_network" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_that a :Generic ;
    :hasText "that" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_u-net a :Method ;
    :hasText "u-net" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_validate a :Task ;
    :hasText "validate" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_video a :Material ;
    :hasText "video" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_videos a :Material ;
    :hasText "videos" .

:Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper_we a :Generic ;
    :hasText "we" .

:amos17a_application a :Generic ;
    :hasText "application" .

:amos17a_approaches a :Generic ;
    :hasText "approaches" .

:amos17a_architecture a :Generic ;
    :hasText "architecture" .

:amos17a_architectures a :Generic ;
    :hasText "architectures" .

:amos17a_backpropagation a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

:amos17a_complex a :Method ;
    :hasText "complex" .

:amos17a_constraints a :Generic ;
    :hasText "constraints" .

:amos17a_deep_networks a :Method ;
    :hasText "deep_networks" .

:amos17a_end-to-end a :Generic ;
    :hasText "end-to-end" .

:amos17a_gradients a :Other ;
    :hasText "gradients" .

:amos17a_information a :Generic ;
    :hasText "information" .

:amos17a_input a :Generic ;
    :hasText "input" .

:amos17a_layers a :Other ;
    :hasText "layers" .

:amos17a_learn a :Task ;
    :hasText "learn" .

:amos17a_method a :Generic ;
    :hasText "method" .

:amos17a_network a :Generic ;
    :hasText "network" .

:amos17a_one a :Generic ;
    :hasText "one" .

:amos17a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:amos17a_optimization_problems a :Task ;
    :hasText "optimization_problems" .

:amos17a_other a :Generic ;
    :hasText "other" .

:amos17a_parameters a :Generic ;
    :hasText "parameters" .

:amos17a_priori a :Other ;
    :hasText "priori" .

:amos17a_problems a :Generic ;
    :hasText "problems" .

:amos17a_programs a :Generic ;
    :hasText "programs" .

:amos17a_rules a :Other ;
    :hasText "rules" .

:amos17a_states a :Generic ;
    :hasText "states" .

:amos17a_techniques a :Generic ;
    :hasText "techniques" .

:amos17a_that a :Generic ;
    :hasText "that" .

:amos17a_these a :Generic ;
    :hasText "these" .

:amos17a_this a :Generic ;
    :hasText "this" .

:amos17a_we a :Generic ;
    :hasText "we" .

:amos17b_architecture a :Generic ;
    :hasText "architecture" .

:amos17b_constraints a :Generic ;
    :hasText "constraints" .

:amos17b_convex a :Other ;
    :hasText "convex" .

:amos17b_data a :Generic ;
    :hasText "data" .

:amos17b_function a :Generic ;
    :hasText "function" .

:amos17b_image_completion a :Task ;
    :hasText "image_completion" .

:amos17b_improvement a :Eval ;
    :hasText "improvement" .

:amos17b_input a :Generic ;
    :hasText "input" .

:amos17b_inputs a :Generic ;
    :hasText "inputs" .

:amos17b_methods a :Generic ;
    :hasText "methods" .

:amos17b_models a :Generic ;
    :hasText "models" .

:amos17b_network a :Generic ;
    :hasText "network" .

:amos17b_networks a :Generic ;
    :hasText "networks" .

:amos17b_neural_network a :Method ;
    :hasText "neural_network" .

:amos17b_neural_network_architectures a :Generic ;
    :hasText "neural_network_architectures" .

:amos17b_neural_networks a :Method ;
    :hasText "neural_networks" .

:amos17b_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:amos17b_optimization_algorithms a :Method ;
    :hasText "optimization_algorithms" .

:amos17b_others a :Generic ;
    :hasText "others" .

:amos17b_parameters a :Generic ;
    :hasText "parameters" .

:amos17b_prediction a :Task ;
    :hasText "prediction" .

:amos17b_problems a :Generic ;
    :hasText "problems" .

:amos17b_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:amos17b_state a :Generic ;
    :hasText "state" .

:amos17b_structured a :Generic ;
    :hasText "structured" .

:amos17b_that a :Generic ;
    :hasText "that" .

:amos17b_these a :Generic ;
    :hasText "these" .

:amos17b_this a :Generic ;
    :hasText "this" .

:amos17b_we a :Generic ;
    :hasText "we" .

:arjovsky17a_algorithm a :Generic ;
    :hasText "algorithm" .

:arjovsky17a_alternative a :Generic ;
    :hasText "alternative" .

:arjovsky17a_distances a :Other ;
    :hasText "distances" .

:arjovsky17a_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:arjovsky17a_hyperparameter a :Generic ;
    :hasText "hyperparameter" .

:arjovsky17a_model a :Generic ;
    :hasText "model" .

:arjovsky17a_optimization_problem a :Task ;
    :hasText "optimization_problem" .

:arjovsky17a_problems a :Generic ;
    :hasText "problems" .

:arjovsky17a_that a :Generic ;
    :hasText "that" .

:arjovsky17a_theoretical_work a :Generic ;
    :hasText "theoretical_work" .

:arjovsky17a_this a :Generic ;
    :hasText "this" .

:arjovsky17a_training a :Task ;
    :hasText "training" .

:arjovsky17a_we a :Generic ;
    :hasText "we" .

:athalye18b_adversarial a :Generic ;
    :hasText "adversarial" .

:athalye18b_adversarial_examples a :Task ;
    :hasText "adversarial_examples" .

:athalye18b_algorithm a :Generic ;
    :hasText "algorithm" .

:athalye18b_complex a :Method ;
    :hasText "complex" .

:athalye18b_examples a :Generic ;
    :hasText "examples" .

:athalye18b_first a :Generic ;
    :hasText "first" .

:athalye18b_images a :Material ;
    :hasText "images" .

:athalye18b_methods a :Generic ;
    :hasText "methods" .

:athalye18b_neural_network_classifiers a :Method ;
    :hasText "neural_network_classifiers" .

:athalye18b_neural_networks a :Method ;
    :hasText "neural_networks" .

:athalye18b_noise a :Other ;
    :hasText "noise" .

:athalye18b_objects a :Generic ;
    :hasText "objects" .

:athalye18b_other a :Generic ;
    :hasText "other" .

:athalye18b_relevance a :Eval ;
    :hasText "relevance" .

:athalye18b_results a :Generic ;
    :hasText "results" .

:athalye18b_systems a :Generic ;
    :hasText "systems" .

:athalye18b_that a :Generic ;
    :hasText "that" .

:athalye18b_transformations a :Generic ;
    :hasText "transformations" .

:athalye18b_two a :Generic ;
    :hasText "two" .

:athalye18b_viewpoint a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/viewpoint> ;
    :hasText "viewpoint" .

:athalye18b_we a :Generic ;
    :hasText "we" .

:balog18a_accuracy a :Eval ;
    :hasText "accuracy" .

:balog18a_algorithm a :Generic ;
    :hasText "algorithm" .

:balog18a_algorithms a :Generic ;
    :hasText "algorithms" .

:balog18a_data a :Generic ;
    :hasText "data" .

:balog18a_database a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/database_systems> ;
    :hasText "database" .

:balog18a_framework a :Generic ;
    :hasText "framework" .

:balog18a_hilbert_space a :Other ;
    :hasText "hilbert_space" .

:balog18a_mechanisms a :Generic ;
    :hasText "mechanisms" .

:balog18a_metric a :Generic ;
    :hasText "metric" .

:balog18a_outputs a :Generic ;
    :hasText "outputs" .

:balog18a_privacy a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/privacy> ;
    :hasText "privacy" .

:balog18a_results a :Generic ;
    :hasText "results" .

:balog18a_statistics a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/correlation_analysis> ;
    :hasText "statistics" .

:balog18a_synthetic_data a :Material ;
    :hasText "synthetic_data" .

:balog18a_that a :Generic ;
    :hasText "that" .

:balog18a_two a :Generic ;
    :hasText "two" .

:bojanowski18a_adversarial a :Generic ;
    :hasText "adversarial" .

:bojanowski18a_applications a :Generic ;
    :hasText "applications" .

:bojanowski18a_deep_convolutional_neural_networks a :Method ;
    :hasText "deep_convolutional_neural_networks" .

:bojanowski18a_experiments a :Generic ;
    :hasText "experiments" .

:bojanowski18a_factors a :Generic ;
    :hasText "factors" .

:bojanowski18a_framework a :Generic ;
    :hasText "framework" .

:bojanowski18a_functions a :Generic ;
    :hasText "functions" .

:bojanowski18a_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:bojanowski18a_gans a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

:bojanowski18a_models a :Generic ;
    :hasText "models" .

:bojanowski18a_natural_images a :Material ;
    :hasText "natural_images" .

:bojanowski18a_networks a :Generic ;
    :hasText "networks" .

:bojanowski18a_noise a :Other ;
    :hasText "noise" .

:bojanowski18a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:bojanowski18a_optimization_problem a :Task ;
    :hasText "optimization_problem" .

:bojanowski18a_optimization_scheme a :Method ;
    :hasText "optimization_scheme" .

:bojanowski18a_properties a :Generic ;
    :hasText "properties" .

:bojanowski18a_results a :Generic ;
    :hasText "results" .

:bojanowski18a_task a :Generic ;
    :hasText "task" .

:bojanowski18a_that a :Generic ;
    :hasText "that" .

:bojanowski18a_these a :Generic ;
    :hasText "these" .

:bojanowski18a_this a :Generic ;
    :hasText "this" .

:bojanowski18a_two a :Generic ;
    :hasText "two" .

:bojanowski18a_we a :Generic ;
    :hasText "we" .

:bojchevski18a_approach a :Generic ;
    :hasText "approach" .

:bojchevski18a_discrete a :Generic ;
    :hasText "discrete" .

:bojchevski18a_first a :Generic ;
    :hasText "first" .

:bojchevski18a_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:bojchevski18a_generalization a :Generic ;
    :hasText "generalization" .

:bojchevski18a_generation a :Task ;
    :hasText "generation" .

:bojchevski18a_generative_model a :Method ;
    :hasText "generative_model" .

:bojchevski18a_graph a :Other ;
    :hasText "graph" .

:bojchevski18a_graphs a :Other ;
    :hasText "graphs" .

:bojchevski18a_input a :Generic ;
    :hasText "input" .

:bojchevski18a_model a :Generic ;
    :hasText "model" .

:bojchevski18a_network a :Generic ;
    :hasText "network" .

:bojchevski18a_neural_network a :Method ;
    :hasText "neural_network" .

:bojchevski18a_pose a :Other ;
    :hasText "pose" .

:bojchevski18a_prediction a :Task ;
    :hasText "prediction" .

:bojchevski18a_problem a :Generic ;
    :hasText "problem" .

:bojchevski18a_properties a :Generic ;
    :hasText "properties" .

:bojchevski18a_real-world_networks a :Generic ;
    :hasText "real-world_networks" .

:bojchevski18a_research a :Generic ;
    :hasText "research" .

:bojchevski18a_task a :Generic ;
    :hasText "task" .

:bojchevski18a_that a :Generic ;
    :hasText "that" .

:bojchevski18a_them a :Generic ;
    :hasText "them" .

:bojchevski18a_these a :Generic ;
    :hasText "these" .

:bojchevski18a_this a :Generic ;
    :hasText "this" .

:bojchevski18a_time a :Generic ;
    :hasText "time" .

:bora17a_accuracy a :Eval ;
    :hasText "accuracy" .

:bora17a_autoencoder a :Method ;
    :hasText "autoencoder" .

:bora17a_domain a :Generic ;
    :hasText "domain" .

:bora17a_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:bora17a_generative_model a :Method ;
    :hasText "generative_model" .

:bora17a_generative_models a :Method ;
    :hasText "generative_models" .

:bora17a_method a :Generic ;
    :hasText "method" .

:bora17a_prior_knowledge a :Other ;
    :hasText "prior_knowledge" .

:bora17a_recovery a :Task ;
    :hasText "recovery" .

:bora17a_results a :Generic ;
    :hasText "results" .

:bora17a_sparsity a :Other ;
    :hasText "sparsity" .

:bora17a_structure a :Generic ;
    :hasText "structure" .

:bora17a_system a :Generic ;
    :hasText "system" .

:bora17a_that a :Generic ;
    :hasText "that" .

:bora17a_this a :Generic ;
    :hasText "this" .

:bora17a_we a :Generic ;
    :hasText "we" .

:brukhim18a_approach a :Generic ;
    :hasText "approach" .

:brukhim18a_approaches a :Generic ;
    :hasText "approaches" .

:brukhim18a_architecture a :Generic ;
    :hasText "architecture" .

:brukhim18a_baselines a :Generic ;
    :hasText "baselines" .

:brukhim18a_benchmarks a :Material ;
    :hasText "benchmarks" .

:brukhim18a_classification a :Task ;
    :hasText "classification" .

:brukhim18a_constraints a :Generic ;
    :hasText "constraints" .

:brukhim18a_deep_learning a :Generic ;
    :hasText "deep_learning" .

:brukhim18a_framework a :Generic ;
    :hasText "framework" .

:brukhim18a_machine_learning a :Task ;
    :hasText "machine_learning" .

:brukhim18a_model a :Generic ;
    :hasText "model" .

:brukhim18a_modeling a :Task ;
    :hasText "modeling" .

:brukhim18a_models a :Generic ;
    :hasText "models" .

:brukhim18a_outputs a :Generic ;
    :hasText "outputs" .

:brukhim18a_prediction a :Task ;
    :hasText "prediction" .

:brukhim18a_prediction_models a :Method ;
    :hasText "prediction_models" .

:brukhim18a_problems a :Generic ;
    :hasText "problems" .

:brukhim18a_results a :Generic ;
    :hasText "results" .

:brukhim18a_state a :Generic ;
    :hasText "state" .

:brukhim18a_structured a :Generic ;
    :hasText "structured" .

:brukhim18a_that a :Generic ;
    :hasText "that" .

:brukhim18a_them a :Generic ;
    :hasText "them" .

:brukhim18a_this a :Generic ;
    :hasText "this" .

:brukhim18a_we a :Generic ;
    :hasText "we" .

:cai18a_accuracy a :Eval ;
    :hasText "accuracy" .

:cai18a_architecture a :Generic ;
    :hasText "architecture" .

:cai18a_architectures a :Generic ;
    :hasText "architectures" .

:cai18a_cifar-10 a :Material ;
    :hasText "cifar-10" .

:cai18a_complex a :Method ;
    :hasText "complex" .

:cai18a_datasets a :Material ;
    :hasText "datasets" .

:cai18a_effectiveness a :Eval ;
    :hasText "effectiveness" .

:cai18a_efficiency a :Eval ;
    :hasText "efficiency" .

:cai18a_function a :Generic ;
    :hasText "function" .

:cai18a_generalization a :Generic ;
    :hasText "generalization" .

:cai18a_image_classification a :Task ;
    :hasText "image_classification" .

:cai18a_imagenet a :Material ;
    :hasText "imagenet" .

:cai18a_inception a :Method ;
    :hasText "inception" .

:cai18a_limited_computational_resources a :Material ;
    :hasText "limited_computational_resources" .

:cai18a_models a :Generic ;
    :hasText "models" .

:cai18a_network a :Generic ;
    :hasText "network" .

:cai18a_networks a :Generic ;
    :hasText "networks" .

:cai18a_neural_architecture_search a :Task ;
    :hasText "neural_architecture_search" .

:cai18a_operations a :Generic ;
    :hasText "operations" .

:cai18a_parameters a :Generic ;
    :hasText "parameters" .

:cai18a_pruning a :Method ;
    :hasText "pruning" .

:cai18a_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:cai18a_results a :Generic ;
    :hasText "results" .

:cai18a_structures a :Generic ;
    :hasText "structures" .

:cai18a_that a :Generic ;
    :hasText "that" .

:cai18a_topology a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/topology> ;
    :hasText "topology" .

:cai18a_we a :Generic ;
    :hasText "we" .

:cai18a_weights a :Generic ;
    :hasText "weights" .

:cao18a_adversarial a :Generic ;
    :hasText "adversarial" .

:cao18a_bound a :Generic ;
    :hasText "bound" .

:cao18a_data a :Generic ;
    :hasText "data" .

:cao18a_effectiveness a :Eval ;
    :hasText "effectiveness" .

:cao18a_experiments a :Generic ;
    :hasText "experiments" .

:cao18a_gans a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

:cao18a_generalization a :Generic ;
    :hasText "generalization" .

:cao18a_generalization_performance a :Eval ;
    :hasText "generalization_performance" .

:cao18a_images a :Material ;
    :hasText "images" .

:cao18a_input a :Generic ;
    :hasText "input" .

:cao18a_method a :Generic ;
    :hasText "method" .

:cao18a_networks a :Generic ;
    :hasText "networks" .

:cao18a_noises a :Other ;
    :hasText "noises" .

:cao18a_prior a :Generic ;
    :hasText "prior" .

:cao18a_real-world_datasets a :Material ;
    :hasText "real-world_datasets" .

:cao18a_real_data a :Material ;
    :hasText "real_data" .

:cao18a_sampling a :Method ;
    :hasText "sampling" .

:cao18a_semantic_information a :Other ;
    :hasText "semantic_information" .

:cao18a_structure a :Generic ;
    :hasText "structure" .

:cao18a_that a :Generic ;
    :hasText "that" .

:cao18a_this a :Generic ;
    :hasText "this" .

:cao18a_we a :Generic ;
    :hasText "we" .

:chapfuwa18a_adversarial_learning a :Method ;
    :hasText "adversarial_learning" .

:chapfuwa18a_alternative a :Generic ;
    :hasText "alternative" .

:chapfuwa18a_applications a :Generic ;
    :hasText "applications" .

:chapfuwa18a_approach a :Generic ;
    :hasText "approach" .

:chapfuwa18a_clinical a :Generic ;
    :hasText "clinical" .

:chapfuwa18a_cost_function a :Generic ;
    :hasText "cost_function" .

:chapfuwa18a_data a :Generic ;
    :hasText "data" .

:chapfuwa18a_estimation a :Generic ;
    :hasText "estimation" .

:chapfuwa18a_event a :Other ;
    :hasText "event" .

:chapfuwa18a_events a :Other ;
    :hasText "events" .

:chapfuwa18a_examples a :Generic ;
    :hasText "examples" .

:chapfuwa18a_formulation a :Generic ;
    :hasText "formulation" .

:chapfuwa18a_information a :Generic ;
    :hasText "information" .

:chapfuwa18a_machine_learning a :Task ;
    :hasText "machine_learning" .

:chapfuwa18a_model a :Generic ;
    :hasText "model" .

:chapfuwa18a_modeling a :Task ;
    :hasText "modeling" .

:chapfuwa18a_models a :Generic ;
    :hasText "models" .

:chapfuwa18a_network a :Generic ;
    :hasText "network" .

:chapfuwa18a_nonparametric a :Other ;
    :hasText "nonparametric" .

:chapfuwa18a_one a :Generic ;
    :hasText "one" .

:chapfuwa18a_real_datasets a :Generic ;
    :hasText "real_datasets" .

:chapfuwa18a_statistical_models a :Method ;
    :hasText "statistical_models" .

:chapfuwa18a_that a :Generic ;
    :hasText "that" .

:chapfuwa18a_time a :Generic ;
    :hasText "time" .

:chapfuwa18a_validate a :Task ;
    :hasText "validate" .

:chapfuwa18a_we a :Generic ;
    :hasText "we" .

:chen18g_applications a :Generic ;
    :hasText "applications" .

:chen18g_approach a :Generic ;
    :hasText "approach" .

:chen18g_codes a :Material ;
    :hasText "codes" .

:chen18g_continuous a :Generic ;
    :hasText "continuous" .

:chen18g_convolutional_networks a :Method ;
    :hasText "convolutional_networks" .

:chen18g_discrete a :Generic ;
    :hasText "discrete" .

:chen18g_embedding_methods a :Method ;
    :hasText "embedding_methods" .

:chen18g_end-to-end a :Generic ;
    :hasText "end-to-end" .

:chen18g_experiments a :Generic ;
    :hasText "experiments" .

:chen18g_graph a :Other ;
    :hasText "graph" .

:chen18g_learn a :Task ;
    :hasText "learn" .

:chen18g_natural_language_processing a :Method ;
    :hasText "natural_language_processing" .

:chen18g_one a :Generic ;
    :hasText "one" .

:chen18g_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:chen18g_overfitting a :Other ;
    :hasText "overfitting" .

:chen18g_parameters a :Generic ;
    :hasText "parameters" .

:chen18g_scheme a :Generic ;
    :hasText "scheme" .

:chen18g_stochastic_gradient_descent a :Method ;
    :hasText "stochastic_gradient_descent" .

:chen18g_that a :Generic ;
    :hasText "that" .

:chen18g_this a :Generic ;
    :hasText "this" .

:chen18g_way a :Generic ;
    :hasText "way" .

:chen18g_we a :Generic ;
    :hasText "we" .

:chen18l_adversarial a :Generic ;
    :hasText "adversarial" .

:chen18l_approaches a :Generic ;
    :hasText "approaches" .

:chen18l_assumptions a :Generic ;
    :hasText "assumptions" .

:chen18l_computational_overhead a :Eval ;
    :hasText "computational_overhead" .

:chen18l_convergence_guarantees a :Other ;
    :hasText "convergence_guarantees" .

:chen18l_distributed a :Generic ;
    :hasText "distributed" .

:chen18l_experiments a :Generic ;
    :hasText "experiments" .

:chen18l_framework a :Generic ;
    :hasText "framework" .

:chen18l_gradient a :Other ;
    :hasText "gradient" .

:chen18l_gradients a :Other ;
    :hasText "gradients" .

:chen18l_magnitude a :Other ;
    :hasText "magnitude" .

:chen18l_model a :Generic ;
    :hasText "model" .

:chen18l_model_training a :Task ;
    :hasText "model_training" .

:chen18l_models a :Generic ;
    :hasText "models" .

:chen18l_node a :Other ;
    :hasText "node" .

:chen18l_nodes a :Other ;
    :hasText "nodes" .

:chen18l_one a :Generic ;
    :hasText "one" .

:chen18l_real_datasets a :Generic ;
    :hasText "real_datasets" .

:chen18l_robustness a :Eval ;
    :hasText "robustness" .

:chen18l_rule a :Other ;
    :hasText "rule" .

:chen18l_rules a :Other ;
    :hasText "rules" .

:chen18l_scale a :Other ;
    :hasText "scale" .

:chen18l_system a :Generic ;
    :hasText "system" .

:chen18l_that a :Generic ;
    :hasText "that" .

:chen18l_theory a :Generic ;
    :hasText "theory" .

:chen18l_this a :Generic ;
    :hasText "this" .

:chen18l_times a :Generic ;
    :hasText "times" .

:chen18l_training a :Task ;
    :hasText "training" .

:chen18l_we a :Generic ;
    :hasText "we" .

:cortes17a_accuracies a :Eval ;
    :hasText "accuracies" .

:cortes17a_algorithm a :Generic ;
    :hasText "algorithm" .

:cortes17a_algorithms a :Generic ;
    :hasText "algorithms" .

:cortes17a_approaches a :Generic ;
    :hasText "approaches" .

:cortes17a_artificial_neural_networks a :Method ;
    :hasText "artificial_neural_networks" .

:cortes17a_binary_classification_tasks a :Method ;
    :hasText "binary_classification_tasks" .

:cortes17a_cifar-10 a :Material ;
    :hasText "cifar-10" .

:cortes17a_data a :Generic ;
    :hasText "data" .

:cortes17a_experiments a :Generic ;
    :hasText "experiments" .

:cortes17a_generalization_guarantees a :Other ;
    :hasText "generalization_guarantees" .

:cortes17a_learn a :Task ;
    :hasText "learn" .

:cortes17a_network a :Generic ;
    :hasText "network" .

:cortes17a_neural_networks a :Method ;
    :hasText "neural_networks" .

:cortes17a_one a :Generic ;
    :hasText "one" .

:cortes17a_results a :Generic ;
    :hasText "results" .

:cortes17a_scale a :Other ;
    :hasText "scale" .

:cortes17a_structure a :Generic ;
    :hasText "structure" .

:cortes17a_structures a :Generic ;
    :hasText "structures" .

:cortes17a_that a :Generic ;
    :hasText "that" .

:cortes17a_theoretical_analysis a :Generic ;
    :hasText "theoretical_analysis" .

:cortes17a_those a :Generic ;
    :hasText "those" .

:cortes17a_we a :Generic ;
    :hasText "we" .

:cortes17a_weights a :Generic ;
    :hasText "weights" .

:cremer18a_approximating a :Task ;
    :hasText "approximating" .

:cremer18a_approximation a :Generic ;
    :hasText "approximation" .

:cremer18a_autoencoders a :Method ;
    :hasText "autoencoders" .

:cremer18a_divergence a :Generic ;
    :hasText "divergence" .

:cremer18a_factors a :Generic ;
    :hasText "factors" .

:cremer18a_large_datasets a :Material ;
    :hasText "large_datasets" .

:cremer18a_models a :Generic ;
    :hasText "models" .

:cremer18a_parameters a :Generic ;
    :hasText "parameters" .

:cremer18a_posterior a :Generic ;
    :hasText "posterior" .

:cremer18a_quality a :Eval ;
    :hasText "quality" .

:cremer18a_recognition_network a :Method ;
    :hasText "recognition_network" .

:cremer18a_recognition_networks a :Method ;
    :hasText "recognition_networks" .

:cremer18a_scale a :Other ;
    :hasText "scale" .

:cremer18a_that a :Generic ;
    :hasText "that" .

:cremer18a_these a :Generic ;
    :hasText "these" .

:cremer18a_this a :Generic ;
    :hasText "this" .

:cremer18a_two a :Generic ;
    :hasText "two" .

:cremer18a_we a :Generic ;
    :hasText "we" .

:dai17a_approach a :Generic ;
    :hasText "approach" .

:dai17a_codes a :Material ;
    :hasText "codes" .

:dai17a_constraints a :Generic ;
    :hasText "constraints" .

:dai17a_databases a :Material ;
    :hasText "databases" .

:dai17a_datasets a :Material ;
    :hasText "datasets" .

:dai17a_discrete a :Generic ;
    :hasText "discrete" .

:dai17a_experiments a :Generic ;
    :hasText "experiments" .

:dai17a_function a :Generic ;
    :hasText "function" .

:dai17a_functions a :Generic ;
    :hasText "functions" .

:dai17a_generative_model a :Method ;
    :hasText "generative_model" .

:dai17a_gradient a :Other ;
    :hasText "gradient" .

:dai17a_inputs a :Generic ;
    :hasText "inputs" .

:dai17a_learn a :Task ;
    :hasText "learn" .

:dai17a_learning_algorithm a :Method ;
    :hasText "learning_algorithm" .

:dai17a_method a :Generic ;
    :hasText "method" .

:dai17a_methods a :Generic ;
    :hasText "methods" .

:dai17a_minimum_description_length_principle a :Method ;
    :hasText "minimum_description_length_principle" .

:dai17a_objective_functions a :Other ;
    :hasText "objective_functions" .

:dai17a_outputs a :Generic ;
    :hasText "outputs" .

:dai17a_paradigm a :Generic ;
    :hasText "paradigm" .

:dai17a_parameters a :Generic ;
    :hasText "parameters" .

:dai17a_results a :Generic ;
    :hasText "results" .

:dai17a_retrieval a :Task ;
    :hasText "retrieval" .

:dai17a_scale a :Other ;
    :hasText "scale" .

:dai17a_search a :Task ;
    :hasText "search" .

:dai17a_state a :Generic ;
    :hasText "state" .

:dai17a_techniques a :Generic ;
    :hasText "techniques" .

:dai17a_that a :Generic ;
    :hasText "that" .

:dai17a_this a :Generic ;
    :hasText "this" .

:dai17a_we a :Generic ;
    :hasText "we" .

:dai18b_adversarial a :Generic ;
    :hasText "adversarial" .

:dai18b_applications a :Generic ;
    :hasText "applications" .

:dai18b_classification_tasks a :Method ;
    :hasText "classification_tasks" .

:dai18b_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:dai18b_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:dai18b_data a :Generic ;
    :hasText "data" .

:dai18b_deep_learning_models a :Method ;
    :hasText "deep_learning_models" .

:dai18b_family a :Generic ;
    :hasText "family" .

:dai18b_first a :Generic ;
    :hasText "first" .

:dai18b_genetic_algorithms a :Generic ;
    :hasText "genetic_algorithms" .

:dai18b_gradient_descent a :Method ;
    :hasText "gradient_descent" .

:dai18b_gradients a :Other ;
    :hasText "gradients" .

:dai18b_graph a :Other ;
    :hasText "graph" .

:dai18b_graph_structures a :Other ;
    :hasText "graph_structures" .

:dai18b_image a :Material ;
    :hasText "image" .

:dai18b_learns a :Task ;
    :hasText "learns" .

:dai18b_method a :Generic ;
    :hasText "method" .

:dai18b_methods a :Generic ;
    :hasText "methods" .

:dai18b_models a :Generic ;
    :hasText "models" .

:dai18b_neural_network_models a :Method ;
    :hasText "neural_network_models" .

:dai18b_node a :Other ;
    :hasText "node" .

:dai18b_policy a :Other ;
    :hasText "policy" .

:dai18b_prediction a :Task ;
    :hasText "prediction" .

:dai18b_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:dai18b_research a :Generic ;
    :hasText "research" .

:dai18b_results a :Generic ;
    :hasText "results" .

:dai18b_robustness a :Eval ;
    :hasText "robustness" .

:dai18b_structure a :Generic ;
    :hasText "structure" .

:dai18b_target a :Generic ;
    :hasText "target" .

:dai18b_text a :Material ;
    :hasText "text" .

:dai18b_that a :Generic ;
    :hasText "that" .

:dai18b_these a :Generic ;
    :hasText "these" .

:dai18b_this a :Generic ;
    :hasText "this" .

:dai18b_we a :Generic ;
    :hasText "we" .

:dauphin17a_approach a :Generic ;
    :hasText "approach" .

:dauphin17a_baseline a :Generic ;
    :hasText "baseline" .

:dauphin17a_convolutions a :Method ;
    :hasText "convolutions" .

:dauphin17a_decisions a :Generic ;
    :hasText "decisions" .

:dauphin17a_features a :Generic ;
    :hasText "features" .

:dauphin17a_first a :Generic ;
    :hasText "first" .

:dauphin17a_knowledge a :Generic ;
    :hasText "knowledge" .

:dauphin17a_language a :Generic ;
    :hasText "language" .

:dauphin17a_language_modeling a :Task ;
    :hasText "language_modeling" .

:dauphin17a_magnitude a :Other ;
    :hasText "magnitude" .

:dauphin17a_mechanism a :Generic ;
    :hasText "mechanism" .

:dauphin17a_model a :Generic ;
    :hasText "model" .

:dauphin17a_models a :Generic ;
    :hasText "models" .

:dauphin17a_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:dauphin17a_results a :Generic ;
    :hasText "results" .

:dauphin17a_scale a :Other ;
    :hasText "scale" .

:dauphin17a_state a :Generic ;
    :hasText "state" .

:dauphin17a_task a :Generic ;
    :hasText "task" .

:dauphin17a_tasks a :Generic ;
    :hasText "tasks" .

:dauphin17a_that a :Generic ;
    :hasText "that" .

:dauphin17a_these a :Generic ;
    :hasText "these" .

:dauphin17a_they a :Generic ;
    :hasText "they" .

:dauphin17a_this a :Generic ;
    :hasText "this" .

:dauphin17a_time a :Generic ;
    :hasText "time" .

:dauphin17a_we a :Generic ;
    :hasText "we" .

:dauphin17a_words a :Generic ;
    :hasText "words" .

:denton18a_approach a :Generic ;
    :hasText "approach" .

:denton18a_approaches a :Generic ;
    :hasText "approaches" .

:denton18a_datasets a :Material ;
    :hasText "datasets" .

:denton18a_end-to-end a :Generic ;
    :hasText "end-to-end" .

:denton18a_generation a :Task ;
    :hasText "generation" .

:denton18a_generations a :Task ;
    :hasText "generations" .

:denton18a_latent_variables a :Other ;
    :hasText "latent_variables" .

:denton18a_learned_prior a :Generic ;
    :hasText "learned_prior" .

:denton18a_model a :Generic ;
    :hasText "model" .

:denton18a_prior a :Generic ;
    :hasText "prior" .

:denton18a_states a :Generic ;
    :hasText "states" .

:denton18a_that a :Generic ;
    :hasText "that" .

:denton18a_them a :Generic ;
    :hasText "them" .

:denton18a_this a :Generic ;
    :hasText "this" .

:denton18a_those a :Generic ;
    :hasText "those" .

:denton18a_time a :Generic ;
    :hasText "time" .

:denton18a_video a :Material ;
    :hasText "video" .

:denton18a_we a :Generic ;
    :hasText "we" .

:donahue17a_approaches a :Generic ;
    :hasText "approaches" .

:donahue17a_audio a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

:donahue17a_chart a :Other ;
    :hasText "chart" .

:donahue17a_charts a :Other ;
    :hasText "charts" .

:donahue17a_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:donahue17a_features a :Generic ;
    :hasText "features" .

:donahue17a_generative_model a :Method ;
    :hasText "generative_model" .

:donahue17a_lstm a :Method ;
    :hasText "lstm" .

:donahue17a_music a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/multiple_signal_classification> ;
    :hasText "music" .

:donahue17a_platform a :Generic ;
    :hasText "platform" .

:donahue17a_rhythm a :Material ;
    :hasText "rhythm" .

:donahue17a_task a :Generic ;
    :hasText "task" .

:donahue17a_that a :Generic ;
    :hasText "that" .

:donahue17a_two a :Generic ;
    :hasText "two" .

:donahue17a_video a :Material ;
    :hasText "video" .

:donahue17a_we a :Generic ;
    :hasText "we" .

:finn17a_algorithm a :Generic ;
    :hasText "algorithm" .

:finn17a_approach a :Generic ;
    :hasText "approach" .

:finn17a_benchmarks a :Material ;
    :hasText "benchmarks" .

:finn17a_classification a :Task ;
    :hasText "classification" .

:finn17a_data a :Generic ;
    :hasText "data" .

:finn17a_generalization_performance a :Eval ;
    :hasText "generalization_performance" .

:finn17a_gradient a :Other ;
    :hasText "gradient" .

:finn17a_gradient_descent a :Method ;
    :hasText "gradient_descent" .

:finn17a_image_classification a :Task ;
    :hasText "image_classification" .

:finn17a_model a :Generic ;
    :hasText "model" .

:finn17a_neural_network a :Method ;
    :hasText "neural_network" .

:finn17a_our_method a :Other ;
    :hasText "our_method" .

:finn17a_parameters a :Generic ;
    :hasText "parameters" .

:finn17a_policies a :Other ;
    :hasText "policies" .

:finn17a_policy a :Other ;
    :hasText "policy" .

:finn17a_problems a :Generic ;
    :hasText "problems" .

:finn17a_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:finn17a_results a :Generic ;
    :hasText "results" .

:finn17a_state a :Generic ;
    :hasText "state" .

:finn17a_task a :Generic ;
    :hasText "task" .

:finn17a_tasks a :Generic ;
    :hasText "tasks" .

:finn17a_that a :Generic ;
    :hasText "that" .

:finn17a_this a :Generic ;
    :hasText "this" .

:finn17a_training a :Task ;
    :hasText "training" .

:finn17a_two a :Generic ;
    :hasText "two" .

:foerster17a_architecture a :Generic ;
    :hasText "architecture" .

:foerster17a_computational_efficiency a :Eval ;
    :hasText "computational_efficiency" .

:foerster17a_domains a :Generic ;
    :hasText "domains" .

:foerster17a_input a :Generic ;
    :hasText "input" .

:foerster17a_interpretability a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/interpretability> ;
    :hasText "interpretability" .

:foerster17a_linear_methods a :Method ;
    :hasText "linear_methods" .

:foerster17a_model a :Generic ;
    :hasText "model" .

:foerster17a_modeling a :Task ;
    :hasText "modeling" .

:foerster17a_network a :Generic ;
    :hasText "network" .

:foerster17a_networks a :Generic ;
    :hasText "networks" .

:foerster17a_neural_network_models a :Method ;
    :hasText "neural_network_models" .

:foerster17a_nonlinearities a :Other ;
    :hasText "nonlinearities" .

:foerster17a_other a :Generic ;
    :hasText "other" .

:foerster17a_predictions a :Task ;
    :hasText "predictions" .

:foerster17a_problem a :Generic ;
    :hasText "problem" .

:foerster17a_rnn a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

:foerster17a_solution a :Generic ;
    :hasText "solution" .

:foerster17a_subspaces a :Other ;
    :hasText "subspaces" .

:foerster17a_task a :Generic ;
    :hasText "task" .

:foerster17a_tasks a :Generic ;
    :hasText "tasks" .

:foerster17a_text a :Material ;
    :hasText "text" .

:foerster17a_this a :Generic ;
    :hasText "this" .

:foerster17a_transformations a :Generic ;
    :hasText "transformations" .

:foerster17a_we a :Generic ;
    :hasText "we" .

:foerster17a_weights a :Generic ;
    :hasText "weights" .

:foerster17a_words a :Generic ;
    :hasText "words" .

:franceschi18a_approach a :Generic ;
    :hasText "approach" .

:franceschi18a_approaches a :Generic ;
    :hasText "approaches" .

:franceschi18a_conditions a :Generic ;
    :hasText "conditions" .

:franceschi18a_deep_learning a :Generic ;
    :hasText "deep_learning" .

:franceschi18a_dynamics a :Generic ;
    :hasText "dynamics" .

:franceschi18a_experiments a :Generic ;
    :hasText "experiments" .

:franceschi18a_few-shot_learning a :Task ;
    :hasText "few-shot_learning" .

:franceschi18a_framework a :Generic ;
    :hasText "framework" .

:franceschi18a_gradient a :Other ;
    :hasText "gradient" .

:franceschi18a_hyperparameter a :Generic ;
    :hasText "hyperparameter" .

:franceschi18a_hyperparameters a :Generic ;
    :hasText "hyperparameters" .

:franceschi18a_layers a :Other ;
    :hasText "layers" .

:franceschi18a_learn a :Task ;
    :hasText "learn" .

:franceschi18a_learner a :Generic ;
    :hasText "learner" .

:franceschi18a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:franceschi18a_parameters a :Generic ;
    :hasText "parameters" .

:franceschi18a_problem a :Generic ;
    :hasText "problem" .

:franceschi18a_representation a :Generic ;
    :hasText "representation" .

:franceschi18a_results a :Generic ;
    :hasText "results" .

:franceschi18a_solutions a :Generic ;
    :hasText "solutions" .

:franceschi18a_supervised_learning a :Method ;
    :hasText "supervised_learning" .

:franceschi18a_that a :Generic ;
    :hasText "that" .

:franceschi18a_those a :Generic ;
    :hasText "those" .

:franceschi18a_training a :Task ;
    :hasText "training" .

:franceschi18a_variables a :Other ;
    :hasText "variables" .

:franceschi18a_we a :Generic ;
    :hasText "we" .

:gehring17a_accuracy a :Eval ;
    :hasText "accuracy" .

:gehring17a_approach a :Generic ;
    :hasText "approach" .

:gehring17a_architecture a :Generic ;
    :hasText "architecture" .

:gehring17a_computations a :Generic ;
    :hasText "computations" .

:gehring17a_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:gehring17a_decoder a :Method ;
    :hasText "decoder" .

:gehring17a_french a :Material ;
    :hasText "french" .

:gehring17a_gradient a :Other ;
    :hasText "gradient" .

:gehring17a_hardware a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/computer_hardware> ;
    :hasText "hardware" .

:gehring17a_input a :Generic ;
    :hasText "input" .

:gehring17a_lstm a :Method ;
    :hasText "lstm" .

:gehring17a_magnitude a :Other ;
    :hasText "magnitude" .

:gehring17a_maps a :Eval ;
    :hasText "maps" .

:gehring17a_models a :Generic ;
    :hasText "models" .

:gehring17a_module a :Generic ;
    :hasText "module" .

:gehring17a_non-linearities a :Other ;
    :hasText "non-linearities" .

:gehring17a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:gehring17a_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:gehring17a_training a :Task ;
    :hasText "training" .

:gehring17a_we a :Generic ;
    :hasText "we" .

:greydanus18a_agents a :Generic ;
    :hasText "agents" .

:greydanus18a_decisions a :Generic ;
    :hasText "decisions" .

:greydanus18a_deep_reinforcement_learning a :Method ;
    :hasText "deep_reinforcement_learning" .

:greydanus18a_deep_rl a :Method ;
    :hasText "deep_rl" .

:greydanus18a_environments a :Generic ;
    :hasText "environments" .

:greydanus18a_information a :Generic ;
    :hasText "information" .

:greydanus18a_learns a :Task ;
    :hasText "learns" .

:greydanus18a_maps a :Eval ;
    :hasText "maps" .

:greydanus18a_maximizing a :Task ;
    :hasText "maximizing" .

:greydanus18a_method a :Generic ;
    :hasText "method" .

:greydanus18a_our_method a :Other ;
    :hasText "our_method" .

:greydanus18a_policy a :Other ;
    :hasText "policy" .

:greydanus18a_results a :Generic ;
    :hasText "results" .

:greydanus18a_rewards a :Eval ;
    :hasText "rewards" .

:greydanus18a_rl a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

:greydanus18a_strategies a :Generic ;
    :hasText "strategies" .

:greydanus18a_that a :Generic ;
    :hasText "that" .

:greydanus18a_these a :Generic ;
    :hasText "these" .

:greydanus18a_they a :Generic ;
    :hasText "they" .

:greydanus18a_this a :Generic ;
    :hasText "this" .

:greydanus18a_we a :Generic ;
    :hasText "we" .

:guo17a_applications a :Generic ;
    :hasText "applications" .

:guo17a_architectures a :Generic ;
    :hasText "architectures" .

:guo17a_classification a :Task ;
    :hasText "classification" .

:guo17a_datasets a :Material ;
    :hasText "datasets" .

:guo17a_document_classification a :Task ;
    :hasText "document_classification" .

:guo17a_estimates a :Generic ;
    :hasText "estimates" .

:guo17a_experiments a :Generic ;
    :hasText "experiments" .

:guo17a_factors a :Generic ;
    :hasText "factors" .

:guo17a_image a :Material ;
    :hasText "image" .

:guo17a_methods a :Generic ;
    :hasText "methods" .

:guo17a_models a :Generic ;
    :hasText "models" .

:guo17a_neural_network a :Method ;
    :hasText "neural_network" .

:guo17a_neural_networks a :Method ;
    :hasText "neural_networks" .

:guo17a_post-processing a :Task ;
    :hasText "post-processing" .

:guo17a_predicting a :Task ;
    :hasText "predicting" .

:guo17a_predictions a :Task ;
    :hasText "predictions" .

:guo17a_problem a :Generic ;
    :hasText "problem" .

:guo17a_state a :Generic ;
    :hasText "state" .

:guo17a_that a :Generic ;
    :hasText "that" .

:guo17a_those a :Generic ;
    :hasText "those" .

:guo17a_we a :Generic ;
    :hasText "we" .

:hadjeres17a_approaches a :Generic ;
    :hasText "approaches" .

:hadjeres17a_automatic a :Task ;
    :hasText "automatic" .

:hadjeres17a_constraints a :Generic ;
    :hasText "constraints" .

:hadjeres17a_data a :Generic ;
    :hasText "data" .

:hadjeres17a_editor a :Method ;
    :hasText "editor" .

:hadjeres17a_generation a :Task ;
    :hasText "generation" .

:hadjeres17a_graphical_model a :Method ;
    :hasText "graphical_model" .

:hadjeres17a_interaction a :Generic ;
    :hasText "interaction" .

:hadjeres17a_model a :Generic ;
    :hasText "model" .

:hadjeres17a_modeling a :Task ;
    :hasText "modeling" .

:hadjeres17a_music a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/multiple_signal_classification> ;
    :hasText "music" .

:hadjeres17a_representation a :Generic ;
    :hasText "representation" .

:hadjeres17a_rhythms a :Material ;
    :hasText "rhythms" .

:hadjeres17a_sampling a :Method ;
    :hasText "sampling" .

:hadjeres17a_that a :Generic ;
    :hasText "that" .

:heinonen18a_complex a :Method ;
    :hasText "complex" .

:heinonen18a_continuous a :Generic ;
    :hasText "continuous" .

:heinonen18a_dynamics a :Generic ;
    :hasText "dynamics" .

:heinonen18a_fields a :Generic ;
    :hasText "fields" .

:heinonen18a_formalism a :Generic ;
    :hasText "formalism" .

:heinonen18a_functions a :Generic ;
    :hasText "functions" .

:heinonen18a_interactions a :Generic ;
    :hasText "interactions" .

:heinonen18a_learn a :Task ;
    :hasText "learn" .

:heinonen18a_model a :Generic ;
    :hasText "model" .

:heinonen18a_nonparametric a :Other ;
    :hasText "nonparametric" .

:heinonen18a_observations a :Generic ;
    :hasText "observations" .

:heinonen18a_paradigm a :Generic ;
    :hasText "paradigm" .

:heinonen18a_prior_knowledge a :Other ;
    :hasText "prior_knowledge" .

:heinonen18a_sparse_data a :Material ;
    :hasText "sparse_data" .

:heinonen18a_state a :Generic ;
    :hasText "state" .

:heinonen18a_system a :Generic ;
    :hasText "system" .

:heinonen18a_systems a :Generic ;
    :hasText "systems" .

:heinonen18a_that a :Generic ;
    :hasText "that" .

:heinonen18a_these a :Generic ;
    :hasText "these" .

:heinonen18a_this a :Generic ;
    :hasText "this" .

:heinonen18a_time a :Generic ;
    :hasText "time" .

:heinonen18a_we a :Generic ;
    :hasText "we" .

:hu17e_accuracy a :Eval ;
    :hasText "accuracy" .

:hu17e_algorithm a :Generic ;
    :hasText "algorithm" .

:hu17e_annotations a :Material ;
    :hasText "annotations" .

:hu17e_approximation a :Generic ;
    :hasText "approximation" .

:hu17e_auto-encoders a :Method ;
    :hasText "auto-encoders" .

:hu17e_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:hu17e_constraints a :Generic ;
    :hasText "constraints" .

:hu17e_data a :Generic ;
    :hasText "data" .

:hu17e_discrete a :Generic ;
    :hasText "discrete" .

:hu17e_domain a :Generic ;
    :hasText "domain" .

:hu17e_experiments a :Generic ;
    :hasText "experiments" .

:hu17e_generation a :Task ;
    :hasText "generation" .

:hu17e_generative_model a :Method ;
    :hasText "generative_model" .

:hu17e_interpretable a :Generic ;
    :hasText "interpretable" .

:hu17e_learns a :Task ;
    :hasText "learns" .

:hu17e_model a :Generic ;
    :hasText "model" .

:hu17e_modeling a :Task ;
    :hasText "modeling" .

:hu17e_representations a :Generic ;
    :hasText "representations" .

:hu17e_semantic a :Other ;
    :hasText "semantic" .

:hu17e_semantics a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/semantics> ;
    :hasText "semantics" .

:hu17e_sentiment a :Other ;
    :hasText "sentiment" .

:hu17e_structures a :Generic ;
    :hasText "structures" .

:hu17e_text a :Material ;
    :hasText "text" .

:hu17e_training a :Task ;
    :hasText "training" .

:hu17e_validate a :Task ;
    :hasText "validate" .

:huang18d_approaches a :Generic ;
    :hasText "approaches" .

:huang18d_autoencoders a :Method ;
    :hasText "autoencoders" .

:huang18d_autoregressive_models a :Generic ;
    :hasText "autoregressive_models" .

:huang18d_continuous a :Generic ;
    :hasText "continuous" .

:huang18d_density_estimation a :Task ;
    :hasText "density_estimation" .

:huang18d_flows a :Other ;
    :hasText "flows" .

:huang18d_mnist a :Material ;
    :hasText "mnist" .

:huang18d_neural_networks a :Method ;
    :hasText "neural_networks" .

:huang18d_probability_distributions a :Other ;
    :hasText "probability_distributions" .

:huang18d_results a :Generic ;
    :hasText "results" .

:huang18d_state a :Generic ;
    :hasText "state" .

:huang18d_target a :Generic ;
    :hasText "target" .

:huang18d_tasks a :Generic ;
    :hasText "tasks" .

:huang18d_that a :Generic ;
    :hasText "that" .

:huang18d_them a :Generic ;
    :hasText "them" .

:huang18d_these a :Generic ;
    :hasText "these" .

:huang18d_time a :Generic ;
    :hasText "time" .

:huang18d_transformations a :Generic ;
    :hasText "transformations" .

:huang18d_wavenet a :Method ;
    :hasText "wavenet" .

:huo18a_accuracy a :Eval ;
    :hasText "accuracy" .

:huo18a_algorithm a :Generic ;
    :hasText "algorithm" .

:huo18a_algorithms a :Generic ;
    :hasText "algorithms" .

:huo18a_backpropagation a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

:huo18a_backpropagation_algorithm a :Method ;
    :hasText "backpropagation_algorithm" .

:huo18a_convergence a :Generic ;
    :hasText "convergence" .

:huo18a_convex_problem a :Task ;
    :hasText "convex_problem" .

:huo18a_datasets a :Material ;
    :hasText "datasets" .

:huo18a_deep_convolutional_neural_networks a :Method ;
    :hasText "deep_convolutional_neural_networks" .

:huo18a_deep_learning a :Generic ;
    :hasText "deep_learning" .

:huo18a_experimental_results a :Generic ;
    :hasText "experimental_results" .

:huo18a_experiments a :Generic ;
    :hasText "experiments" .

:huo18a_gradients a :Other ;
    :hasText "gradients" .

:huo18a_input a :Generic ;
    :hasText "input" .

:huo18a_layers a :Other ;
    :hasText "layers" .

:huo18a_method a :Generic ;
    :hasText "method" .

:huo18a_methods a :Generic ;
    :hasText "methods" .

:huo18a_modules a :Generic ;
    :hasText "modules" .

:huo18a_network a :Generic ;
    :hasText "network" .

:huo18a_networks a :Generic ;
    :hasText "networks" .

:huo18a_neural_networks a :Method ;
    :hasText "neural_networks" .

:huo18a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:huo18a_our_method a :Other ;
    :hasText "our_method" .

:huo18a_resources a :Generic ;
    :hasText "resources" .

:huo18a_that a :Generic ;
    :hasText "that" .

:huo18a_theoretical_analysis a :Generic ;
    :hasText "theoretical_analysis" .

:huo18a_this a :Generic ;
    :hasText "this" .

:huo18a_training a :Task ;
    :hasText "training" .

:huo18a_two a :Generic ;
    :hasText "two" .

:huo18a_way a :Generic ;
    :hasText "way" .

:huo18a_we a :Generic ;
    :hasText "we" .

:ilse18a_application a :Generic ;
    :hasText "application" .

:ilse18a_approach a :Generic ;
    :hasText "approach" .

:ilse18a_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:ilse18a_datasets a :Material ;
    :hasText "datasets" .

:ilse18a_instances a :Generic ;
    :hasText "instances" .

:ilse18a_interpretability a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/interpretability> ;
    :hasText "interpretability" .

:ilse18a_methods a :Generic ;
    :hasText "methods" .

:ilse18a_mnist a :Material ;
    :hasText "mnist" .

:ilse18a_neural_network a :Method ;
    :hasText "neural_network" .

:ilse18a_neural_networks a :Method ;
    :hasText "neural_networks" .

:ilse18a_other a :Generic ;
    :hasText "other" .

:ilse18a_problem a :Generic ;
    :hasText "problem" .

:ilse18a_state a :Generic ;
    :hasText "state" .

:ilse18a_supervised_learning a :Method ;
    :hasText "supervised_learning" .

:ilse18a_that a :Generic ;
    :hasText "that" .

:ilse18a_this a :Generic ;
    :hasText "this" .

:ilse18a_two a :Generic ;
    :hasText "two" .

:ilse18a_we a :Generic ;
    :hasText "we" .

:ilyas18a_adversarial_examples a :Task ;
    :hasText "adversarial_examples" .

:ilyas18a_classifier a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifier" .

:ilyas18a_classifiers a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/classifiers> ;
    :hasText "classifiers" .

:ilyas18a_imagenet a :Material ;
    :hasText "imagenet" .

:ilyas18a_information a :Generic ;
    :hasText "information" .

:ilyas18a_inputs a :Generic ;
    :hasText "inputs" .

:ilyas18a_methods a :Generic ;
    :hasText "methods" .

:ilyas18a_model a :Generic ;
    :hasText "model" .

:ilyas18a_models a :Generic ;
    :hasText "models" .

:ilyas18a_network a :Generic ;
    :hasText "network" .

:ilyas18a_neural_network a :Method ;
    :hasText "neural_network" .

:ilyas18a_other a :Generic ;
    :hasText "other" .

:ilyas18a_our_methods a :Other ;
    :hasText "our_methods" .

:ilyas18a_systems a :Generic ;
    :hasText "systems" .

:ilyas18a_that a :Generic ;
    :hasText "that" .

:ilyas18a_these a :Generic ;
    :hasText "these" .

:ilyas18a_vision a :Generic ;
    :hasText "vision" .

:jaderberg17a_computation a :Other ;
    :hasText "computation" .

:jaderberg17a_data a :Generic ;
    :hasText "data" .

:jaderberg17a_framework a :Generic ;
    :hasText "framework" .

:jaderberg17a_gradient a :Other ;
    :hasText "gradient" .

:jaderberg17a_gradients a :Other ;
    :hasText "gradients" .

:jaderberg17a_graph a :Other ;
    :hasText "graph" .

:jaderberg17a_hierarchical a :Generic ;
    :hasText "hierarchical" .

:jaderberg17a_information a :Generic ;
    :hasText "information" .

:jaderberg17a_inputs a :Generic ;
    :hasText "inputs" .

:jaderberg17a_interfaces a :Other ;
    :hasText "interfaces" .

:jaderberg17a_layers a :Other ;
    :hasText "layers" .

:jaderberg17a_learn a :Task ;
    :hasText "learn" .

:jaderberg17a_model a :Generic ;
    :hasText "model" .

:jaderberg17a_models a :Generic ;
    :hasText "models" .

:jaderberg17a_modules a :Generic ;
    :hasText "modules" .

:jaderberg17a_network a :Generic ;
    :hasText "network" .

:jaderberg17a_networks a :Generic ;
    :hasText "networks" .

:jaderberg17a_neural_networks a :Method ;
    :hasText "neural_networks" .

:jaderberg17a_one a :Generic ;
    :hasText "one" .

:jaderberg17a_predicting a :Task ;
    :hasText "predicting" .

:jaderberg17a_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:jaderberg17a_results a :Generic ;
    :hasText "results" .

:jaderberg17a_rnn a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

:jaderberg17a_rnns a :Method ;
    :hasText "rnns" .

:jaderberg17a_signal a :Generic ;
    :hasText "signal" .

:jaderberg17a_subgraphs a :Other ;
    :hasText "subgraphs" .

:jaderberg17a_system a :Generic ;
    :hasText "system" .

:jaderberg17a_that a :Generic ;
    :hasText "that" .

:jaderberg17a_them a :Generic ;
    :hasText "them" .

:jaderberg17a_they a :Generic ;
    :hasText "they" .

:jaderberg17a_this a :Generic ;
    :hasText "this" .

:jaderberg17a_time a :Generic ;
    :hasText "time" .

:jaderberg17a_we a :Generic ;
    :hasText "we" .

:jeong18a_accuracy a :Eval ;
    :hasText "accuracy" .

:jeong18a_algorithms a :Generic ;
    :hasText "algorithms" .

:jeong18a_cifar-100 a :Material ;
    :hasText "cifar-100" .

:jeong18a_data a :Generic ;
    :hasText "data" .

:jeong18a_datasets a :Material ;
    :hasText "datasets" .

:jeong18a_efficiency a :Eval ;
    :hasText "efficiency" .

:jeong18a_end-to-end a :Generic ;
    :hasText "end-to-end" .

:jeong18a_flow a :Other ;
    :hasText "flow" .

:jeong18a_imagenet a :Material ;
    :hasText "imagenet" .

:jeong18a_learning_methods a :Method ;
    :hasText "learning_methods" .

:jeong18a_metric a :Generic ;
    :hasText "metric" .

:jeong18a_metrics a :Generic ;
    :hasText "metrics" .

:jeong18a_neural_networks a :Method ;
    :hasText "neural_networks" .

:jeong18a_per a :Eval ;
    :hasText "per" .

:jeong18a_polynomial_time a :Other ;
    :hasText "polynomial_time" .

:jeong18a_problem a :Generic ;
    :hasText "problem" .

:jeong18a_representation a :Generic ;
    :hasText "representation" .

:jeong18a_representation_learning a :Task ;
    :hasText "representation_learning" .

:jeong18a_representations a :Generic ;
    :hasText "representations" .

:jeong18a_results a :Generic ;
    :hasText "results" .

:jeong18a_search a :Task ;
    :hasText "search" .

:jeong18a_source_code a :Generic ;
    :hasText "source_code" .

:jeong18a_state a :Generic ;
    :hasText "state" .

:jeong18a_that a :Generic ;
    :hasText "that" .

:jeong18a_this a :Generic ;
    :hasText "this" .

:jeong18a_we a :Generic ;
    :hasText "we" .

:johnson18a_adversarial a :Generic ;
    :hasText "adversarial" .

:johnson18a_data a :Generic ;
    :hasText "data" .

:johnson18a_divergence a :Generic ;
    :hasText "divergence" .

:johnson18a_effectiveness a :Eval ;
    :hasText "effectiveness" .

:johnson18a_experiments a :Generic ;
    :hasText "experiments" .

:johnson18a_first a :Generic ;
    :hasText "first" .

:johnson18a_formulation a :Generic ;
    :hasText "formulation" .

:johnson18a_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:johnson18a_generation a :Task ;
    :hasText "generation" .

:johnson18a_gradient a :Other ;
    :hasText "gradient" .

:johnson18a_image a :Material ;
    :hasText "image" .

:johnson18a_kl a :Generic ;
    :hasText "kl" .

:johnson18a_method a :Generic ;
    :hasText "method" .

:johnson18a_methods a :Generic ;
    :hasText "methods" .

:johnson18a_real_data a :Material ;
    :hasText "real_data" .

:johnson18a_that a :Generic ;
    :hasText "that" .

:johnson18a_theory a :Generic ;
    :hasText "theory" .

:johnson18a_this a :Generic ;
    :hasText "this" .

:johnson18a_viewpoint a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/viewpoint> ;
    :hasText "viewpoint" .

:johnson18a_we a :Generic ;
    :hasText "we" .

:kalchbrenner17a_action a :Generic ;
    :hasText "action" .

:kalchbrenner17a_approaches a :Generic ;
    :hasText "approaches" .

:kalchbrenner17a_architecture a :Generic ;
    :hasText "architecture" .

:kalchbrenner17a_color a :Other ;
    :hasText "color" .

:kalchbrenner17a_discrete a :Generic ;
    :hasText "discrete" .

:kalchbrenner17a_estimates a :Generic ;
    :hasText "estimates" .

:kalchbrenner17a_model a :Generic ;
    :hasText "model" .

:kalchbrenner17a_motion a :Generic ;
    :hasText "motion" .

:kalchbrenner17a_moving_mnist a :Material ;
    :hasText "moving_mnist" .

:kalchbrenner17a_network a :Generic ;
    :hasText "network" .

:kalchbrenner17a_objects a :Generic ;
    :hasText "objects" .

:kalchbrenner17a_state a :Generic ;
    :hasText "state" .

:kalchbrenner17a_structure a :Generic ;
    :hasText "structure" .

:kalchbrenner17a_tensors a :Generic ;
    :hasText "tensors" .

:kalchbrenner17a_that a :Generic ;
    :hasText "that" .

:kalchbrenner17a_time a :Generic ;
    :hasText "time" .

:kalchbrenner17a_video a :Material ;
    :hasText "video" .

:kalchbrenner17a_videos a :Material ;
    :hasText "videos" .

:kalchbrenner18a_audio a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

:kalchbrenner18a_data a :Generic ;
    :hasText "data" .

:kalchbrenner18a_efficiency a :Eval ;
    :hasText "efficiency" .

:kalchbrenner18a_first a :Generic ;
    :hasText "first" .

:kalchbrenner18a_generation a :Task ;
    :hasText "generation" .

:kalchbrenner18a_method a :Generic ;
    :hasText "method" .

:kalchbrenner18a_model a :Generic ;
    :hasText "model" .

:kalchbrenner18a_models a :Generic ;
    :hasText "models" .

:kalchbrenner18a_network a :Generic ;
    :hasText "network" .

:kalchbrenner18a_networks a :Generic ;
    :hasText "networks" .

:kalchbrenner18a_one a :Generic ;
    :hasText "one" .

:kalchbrenner18a_parameters a :Generic ;
    :hasText "parameters" .

:kalchbrenner18a_per a :Eval ;
    :hasText "per" .

:kalchbrenner18a_problem a :Generic ;
    :hasText "problem" .

:kalchbrenner18a_pruning a :Method ;
    :hasText "pruning" .

:kalchbrenner18a_quality a :Eval ;
    :hasText "quality" .

:kalchbrenner18a_recurrent_neural_network a :Method ;
    :hasText "recurrent_neural_network" .

:kalchbrenner18a_results a :Generic ;
    :hasText "results" .

:kalchbrenner18a_sampling a :Method ;
    :hasText "sampling" .

:kalchbrenner18a_scheme a :Generic ;
    :hasText "scheme" .

:kalchbrenner18a_sparsity a :Other ;
    :hasText "sparsity" .

:kalchbrenner18a_state a :Generic ;
    :hasText "state" .

:kalchbrenner18a_technique a :Generic ;
    :hasText "technique" .

:kalchbrenner18a_techniques a :Generic ;
    :hasText "techniques" .

:kalchbrenner18a_text a :Material ;
    :hasText "text" .

:kalchbrenner18a_textual_domains a :Material ;
    :hasText "textual_domains" .

:kalchbrenner18a_that a :Generic ;
    :hasText "that" .

:kalchbrenner18a_this a :Generic ;
    :hasText "this" .

:kalchbrenner18a_time a :Generic ;
    :hasText "time" .

:kalchbrenner18a_wavenet a :Method ;
    :hasText "wavenet" .

:kalchbrenner18a_we a :Generic ;
    :hasText "we" .

:kalchbrenner18a_weights a :Generic ;
    :hasText "weights" .

:kim17a_cross-domain a :Task ;
    :hasText "cross-domain" .

:kim17a_data a :Generic ;
    :hasText "data" .

:kim17a_domain a :Generic ;
    :hasText "domain" .

:kim17a_domains a :Generic ;
    :hasText "domains" .

:kim17a_face a :Other ;
    :hasText "face" .

:kim17a_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:kim17a_learns a :Task ;
    :hasText "learns" .

:kim17a_method a :Generic ;
    :hasText "method" .

:kim17a_network a :Generic ;
    :hasText "network" .

:kim17a_one a :Generic ;
    :hasText "one" .

:kim17a_orientation a :Other ;
    :hasText "orientation" .

:kim17a_relations a :Generic ;
    :hasText "relations" .

:kim17a_supervision a :Other ;
    :hasText "supervision" .

:kim17a_task a :Generic ;
    :hasText "task" .

:kim17a_that a :Generic ;
    :hasText "that" .

:kim17a_them a :Generic ;
    :hasText "them" .

:kim17a_transfers a :Other ;
    :hasText "transfers" .

:kim17a_we a :Generic ;
    :hasText "we" .

:kim17b_accuracies a :Eval ;
    :hasText "accuracies" .

:kim17b_cifar-100 a :Material ;
    :hasText "cifar-100" .

:kim17b_computations a :Generic ;
    :hasText "computations" .

:kim17b_datasets a :Material ;
    :hasText "datasets" .

:kim17b_deep_networks a :Method ;
    :hasText "deep_networks" .

:kim17b_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:kim17b_feature a :Other ;
    :hasText "feature" .

:kim17b_features a :Generic ;
    :hasText "features" .

:kim17b_image_classification a :Task ;
    :hasText "image_classification" .

:kim17b_learns a :Task ;
    :hasText "learns" .

:kim17b_matrices a :Generic ;
    :hasText "matrices" .

:kim17b_model a :Generic ;
    :hasText "model" .

:kim17b_models a :Generic ;
    :hasText "models" .

:kim17b_network a :Generic ;
    :hasText "network" .

:kim17b_networks a :Generic ;
    :hasText "networks" .

:kim17b_our_method a :Other ;
    :hasText "our_method" .

:kim17b_parameters a :Generic ;
    :hasText "parameters" .

:kim17b_resnet a :Method ;
    :hasText "resnet" .

:kim17b_structured a :Generic ;
    :hasText "structured" .

:kim17b_that a :Generic ;
    :hasText "that" .

:kim17b_time a :Generic ;
    :hasText "time" .

:kim17b_two a :Generic ;
    :hasText "two" .

:kim17b_validate a :Task ;
    :hasText "validate" .

:kim17b_we a :Generic ;
    :hasText "we" .

:kim17b_weights a :Generic ;
    :hasText "weights" .

:kim18b_data a :Generic ;
    :hasText "data" .

:kim18b_factors a :Generic ;
    :hasText "factors" .

:kim18b_method a :Generic ;
    :hasText "method" .

:kim18b_metric a :Generic ;
    :hasText "metric" .

:kim18b_problem a :Generic ;
    :hasText "problem" .

:kim18b_problems a :Generic ;
    :hasText "problems" .

:kim18b_quality a :Eval ;
    :hasText "quality" .

:kim18b_representations a :Generic ;
    :hasText "representations" .

:kim18b_that a :Generic ;
    :hasText "that" .

:kim18b_them a :Generic ;
    :hasText "them" .

:kim18b_unsupervised_learning a :Method ;
    :hasText "unsupervised_learning" .

:kim18b_we a :Generic ;
    :hasText "we" .

:kipf18a_complex a :Method ;
    :hasText "complex" .

:kipf18a_components a :Generic ;
    :hasText "components" .

:kipf18a_data a :Generic ;
    :hasText "data" .

:kipf18a_dynamics a :Generic ;
    :hasText "dynamics" .

:kipf18a_encoder a :Method ;
    :hasText "encoder" .

:kipf18a_experiments a :Generic ;
    :hasText "experiments" .

:kipf18a_graph a :Other ;
    :hasText "graph" .

:kipf18a_interaction a :Generic ;
    :hasText "interaction" .

:kipf18a_interactions a :Generic ;
    :hasText "interactions" .

:kipf18a_interpretable a :Generic ;
    :hasText "interpretable" .

:kipf18a_learns a :Task ;
    :hasText "learns" .

:kipf18a_model a :Generic ;
    :hasText "model" .

:kipf18a_motion a :Generic ;
    :hasText "motion" .

:kipf18a_neural_networks a :Method ;
    :hasText "neural_networks" .

:kipf18a_structure a :Generic ;
    :hasText "structure" .

:kipf18a_system a :Generic ;
    :hasText "system" .

:kipf18a_systems a :Generic ;
    :hasText "systems" .

:kipf18a_that a :Generic ;
    :hasText "that" .

:kipf18a_this a :Generic ;
    :hasText "this" .

:kipf18a_tracking a :Task ;
    :hasText "tracking" .

:kipf18a_unsupervised_manner a :Method ;
    :hasText "unsupervised_manner" .

:kipf18a_we a :Generic ;
    :hasText "we" .

:kusner17a_audio a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

:kusner17a_autoencoder a :Method ;
    :hasText "autoencoder" .

:kusner17a_continuous a :Generic ;
    :hasText "continuous" .

:kusner17a_data a :Generic ;
    :hasText "data" .

:kusner17a_discrete a :Generic ;
    :hasText "discrete" .

:kusner17a_discrete_data a :Material ;
    :hasText "discrete_data" .

:kusner17a_effectiveness a :Eval ;
    :hasText "effectiveness" .

:kusner17a_encodes a :Generic ;
    :hasText "encodes" .

:kusner17a_expressions a :Generic ;
    :hasText "expressions" .

:kusner17a_generation a :Task ;
    :hasText "generation" .

:kusner17a_generative_models a :Method ;
    :hasText "generative_models" .

:kusner17a_grammar a :Method ;
    :hasText "grammar" .

:kusner17a_learns a :Task ;
    :hasText "learns" .

:kusner17a_methods a :Generic ;
    :hasText "methods" .

:kusner17a_model a :Generic ;
    :hasText "model" .

:kusner17a_modeling a :Task ;
    :hasText "modeling" .

:kusner17a_models a :Generic ;
    :hasText "models" .

:kusner17a_natural_images a :Material ;
    :hasText "natural_images" .

:kusner17a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:kusner17a_outputs a :Generic ;
    :hasText "outputs" .

:kusner17a_parse_tree a :Other ;
    :hasText "parse_tree" .

:kusner17a_parse_trees a :Other ;
    :hasText "parse_trees" .

:kusner17a_poses a :Other ;
    :hasText "poses" .

:kusner17a_representations a :Generic ;
    :hasText "representations" .

:kusner17a_state a :Generic ;
    :hasText "state" .

:kusner17a_structures a :Generic ;
    :hasText "structures" .

:kusner17a_that a :Generic ;
    :hasText "that" .

:kusner17a_these a :Generic ;
    :hasText "these" .

:kusner17a_we a :Generic ;
    :hasText "we" .

:law17a_approach a :Generic ;
    :hasText "approach" .

:law17a_approaches a :Generic ;
    :hasText "approaches" .

:law17a_clusters a :Other ;
    :hasText "clusters" .

:law17a_data a :Generic ;
    :hasText "data" .

:law17a_data_representation a :Method ;
    :hasText "data_representation" .

:law17a_datasets a :Material ;
    :hasText "datasets" .

:law17a_examples a :Generic ;
    :hasText "examples" .

:law17a_factors a :Generic ;
    :hasText "factors" .

:law17a_gradient a :Other ;
    :hasText "gradient" .

:law17a_gradients a :Other ;
    :hasText "gradients" .

:law17a_grouping a :Generic ;
    :hasText "grouping" .

:law17a_learn a :Task ;
    :hasText "learn" .

:law17a_loss_function a :Method ;
    :hasText "loss_function" .

:law17a_method a :Generic ;
    :hasText "method" .

:law17a_methods a :Generic ;
    :hasText "methods" .

:law17a_metric a :Generic ;
    :hasText "metric" .

:law17a_problem a :Generic ;
    :hasText "problem" .

:law17a_quality a :Eval ;
    :hasText "quality" .

:law17a_real-world_datasets a :Material ;
    :hasText "real-world_datasets" .

:law17a_representation a :Generic ;
    :hasText "representation" .

:law17a_spectral_clustering a :Method ;
    :hasText "spectral_clustering" .

:law17a_state a :Generic ;
    :hasText "state" .

:law17a_task a :Generic ;
    :hasText "task" .

:law17a_that a :Generic ;
    :hasText "that" .

:law17a_these a :Generic ;
    :hasText "these" .

:law17a_they a :Generic ;
    :hasText "they" .

:law17a_this a :Generic ;
    :hasText "this" .

:law17a_training a :Task ;
    :hasText "training" .

:law17a_two a :Generic ;
    :hasText "two" .

:law17a_we a :Generic ;
    :hasText "we" .

:le18a_approach a :Generic ;
    :hasText "approach" .

:le18a_benchmarks a :Material ;
    :hasText "benchmarks" .

:le18a_decision-making a :Task ;
    :hasText "decision-making" .

:le18a_feedback a :Other ;
    :hasText "feedback" .

:le18a_framework a :Generic ;
    :hasText "framework" .

:le18a_hierarchical a :Generic ;
    :hasText "hierarchical" .

:le18a_hierarchical_structure a :Other ;
    :hasText "hierarchical_structure" .

:le18a_imitation_learning a :Method ;
    :hasText "imitation_learning" .

:le18a_interaction a :Generic ;
    :hasText "interaction" .

:le18a_learn a :Task ;
    :hasText "learn" .

:le18a_policies a :Other ;
    :hasText "policies" .

:le18a_pose a :Other ;
    :hasText "pose" .

:le18a_problem a :Generic ;
    :hasText "problem" .

:le18a_problems a :Generic ;
    :hasText "problems" .

:le18a_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

<https://github.com/deepcurator/DCC/le18a_reinforcement_learning_(rl)> a :Task ;
    :hasText "reinforcement_learning_(rl)" .

:le18a_rewards a :Eval ;
    :hasText "rewards" .

:le18a_rl a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

:le18a_that a :Generic ;
    :hasText "that" .

:le18a_time a :Generic ;
    :hasText "time" .

:le18a_we a :Generic ;
    :hasText "we" .

:lee17b_applications a :Generic ;
    :hasText "applications" .

:lee17b_architecture a :Generic ;
    :hasText "architecture" .

:lee17b_boosting a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/boosting> ;
    :hasText "boosting" .

:lee17b_classification_task a :Task ;
    :hasText "classification_task" .

:lee17b_components a :Generic ;
    :hasText "components" .

:lee17b_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:lee17b_ensemble a :Task ;
    :hasText "ensemble" .

:lee17b_ensemble_methods a :Method ;
    :hasText "ensemble_methods" .

:lee17b_ensembles a :Task ;
    :hasText "ensembles" .

:lee17b_error_rates a :Eval ;
    :hasText "error_rates" .

:lee17b_experiments a :Generic ;
    :hasText "experiments" .

:lee17b_feature a :Other ;
    :hasText "feature" .

:lee17b_ie a :Task ;
    :hasText "ie" .

:lee17b_image_classification a :Task ;
    :hasText "image_classification" .

:lee17b_machine_learning a :Task ;
    :hasText "machine_learning" .

:lee17b_methods a :Generic ;
    :hasText "methods" .

:lee17b_models a :Generic ;
    :hasText "models" .

:lee17b_networks a :Generic ;
    :hasText "networks" .

:lee17b_scheme a :Generic ;
    :hasText "scheme" .

:lee17b_segmentation a :Task ;
    :hasText "segmentation" .

:lee17b_svhn a :Material ;
    :hasText "svhn" .

:lee17b_techniques a :Generic ;
    :hasText "techniques" .

:lee17b_they a :Generic ;
    :hasText "they" .

:lee17b_this a :Generic ;
    :hasText "this" .

:lee17b_training_method a :Method ;
    :hasText "training_method" .

:lee17b_we a :Generic ;
    :hasText "we" .

:lee18c_agents a :Generic ;
    :hasText "agents" .

:lee18c_architecture a :Generic ;
    :hasText "architecture" .

:lee18c_convolutional_networks a :Method ;
    :hasText "convolutional_networks" .

:lee18c_convolutions a :Method ;
    :hasText "convolutions" .

:lee18c_differentiability a :Generic ;
    :hasText "differentiability" .

:lee18c_effectiveness a :Eval ;
    :hasText "effectiveness" .

:lee18c_end-to-end a :Generic ;
    :hasText "end-to-end" .

:lee18c_environment a :Generic ;
    :hasText "environment" .

:lee18c_first a :Generic ;
    :hasText "first" .

:lee18c_generalization a :Generic ;
    :hasText "generalization" .

:lee18c_hyperparameter a :Generic ;
    :hasText "hyperparameter" .

:lee18c_metrics a :Generic ;
    :hasText "metrics" .

:lee18c_modules a :Generic ;
    :hasText "modules" .

:lee18c_navigation a :Task ;
    :hasText "navigation" .

:lee18c_network a :Generic ;
    :hasText "network" .

:lee18c_networks a :Generic ;
    :hasText "networks" .

:lee18c_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:lee18c_optimization_problems a :Task ;
    :hasText "optimization_problems" .

:lee18c_other a :Generic ;
    :hasText "other" .

:lee18c_planning a :Other ;
    :hasText "planning" .

:lee18c_pooling a :Method ;
    :hasText "pooling" .

:lee18c_rgb_images a :Material ;
    :hasText "rgb_images" .

:lee18c_sizes a :Other ;
    :hasText "sizes" .

:lee18c_that a :Generic ;
    :hasText "that" .

:lee18c_they a :Generic ;
    :hasText "they" .

:lee18c_this a :Generic ;
    :hasText "this" .

:lee18c_training a :Task ;
    :hasText "training" .

:lee18c_we a :Generic ;
    :hasText "we" .

:lehtinen18a_data a :Generic ;
    :hasText "data" .

:lehtinen18a_examples a :Generic ;
    :hasText "examples" .

:lehtinen18a_image a :Material ;
    :hasText "image" .

:lehtinen18a_images a :Material ;
    :hasText "images" .

:lehtinen18a_learn a :Task ;
    :hasText "learn" .

:lehtinen18a_learns a :Task ;
    :hasText "learns" .

:lehtinen18a_machine_learning a :Task ;
    :hasText "machine_learning" .

:lehtinen18a_map a :Eval ;
    :hasText "map" .

:lehtinen18a_model a :Generic ;
    :hasText "model" .

:lehtinen18a_models a :Generic ;
    :hasText "models" .

:lehtinen18a_mri a :Material ;
    :hasText "mri" .

:lehtinen18a_noise a :Other ;
    :hasText "noise" .

:lehtinen18a_observations a :Generic ;
    :hasText "observations" .

:lehtinen18a_priors a :Other ;
    :hasText "priors" .

:lehtinen18a_signal a :Generic ;
    :hasText "signal" .

:lehtinen18a_signals a :Generic ;
    :hasText "signals" .

:lehtinen18a_that a :Generic ;
    :hasText "that" .

:lehtinen18a_training a :Task ;
    :hasText "training" .

:lehtinen18a_we a :Generic ;
    :hasText "we" .

:li17a_accuracy a :Eval ;
    :hasText "accuracy" .

:li17a_adversarial a :Generic ;
    :hasText "adversarial" .

:li17a_alternative a :Generic ;
    :hasText "alternative" .

:li17a_applications a :Generic ;
    :hasText "applications" .

:li17a_approximating a :Task ;
    :hasText "approximating" .

:li17a_approximations a :Generic ;
    :hasText "approximations" .

:li17a_data a :Generic ;
    :hasText "data" .

:li17a_deep_learning_models a :Method ;
    :hasText "deep_learning_models" .

:li17a_divergence a :Generic ;
    :hasText "divergence" .

:li17a_divergences a :Generic ;
    :hasText "divergences" .

:li17a_estimates a :Generic ;
    :hasText "estimates" .

:li17a_images a :Material ;
    :hasText "images" .

:li17a_kl a :Generic ;
    :hasText "kl" .

:li17a_model a :Generic ;
    :hasText "model" .

:li17a_models a :Generic ;
    :hasText "models" .

:li17a_networks a :Generic ;
    :hasText "networks" .

:li17a_technique a :Generic ;
    :hasText "technique" .

:li17a_techniques a :Generic ;
    :hasText "techniques" .

:li17a_that a :Generic ;
    :hasText "that" .

:li17a_these a :Generic ;
    :hasText "these" .

:li17a_uncertainty a :Generic ;
    :hasText "uncertainty" .

:li17a_variational_inference a :Method ;
    :hasText "variational_inference" .

:li17a_vision a :Generic ;
    :hasText "vision" .

:li18a_baseline a :Generic ;
    :hasText "baseline" .

:li18a_bias a :Method ;
    :hasText "bias" .

:li18a_convolutional_networks a :Method ;
    :hasText "convolutional_networks" .

:li18a_data a :Generic ;
    :hasText "data" .

:li18a_features a :Generic ;
    :hasText "features" .

:li18a_finetuning a :Generic ;
    :hasText "finetuning" .

:li18a_mechanism a :Generic ;
    :hasText "mechanism" .

:li18a_model a :Generic ;
    :hasText "model" .

:li18a_penalty a :Eval ;
    :hasText "penalty" .

:li18a_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:li18a_schemes a :Generic ;
    :hasText "schemes" .

:li18a_solution a :Generic ;
    :hasText "solution" .

:li18a_target a :Generic ;
    :hasText "target" .

:li18a_task a :Generic ;
    :hasText "task" .

:li18a_tasks a :Generic ;
    :hasText "tasks" .

:li18a_that a :Generic ;
    :hasText "that" .

:li18a_this a :Generic ;
    :hasText "this" .

:li18a_training a :Task ;
    :hasText "training" .

:li18a_transfer_learning a :Method ;
    :hasText "transfer_learning" .

:li18a_we a :Generic ;
    :hasText "we" .

:li18c_approximation a :Generic ;
    :hasText "approximation" .

:li18c_baseline a :Generic ;
    :hasText "baseline" .

:li18c_compression a :Task ;
    :hasText "compression" .

:li18c_computations a :Generic ;
    :hasText "computations" .

:li18c_flow a :Other ;
    :hasText "flow" .

:li18c_generalization a :Generic ;
    :hasText "generalization" .

:li18c_implementation a :Material ;
    :hasText "implementation" .

:li18c_information a :Generic ;
    :hasText "information" .

:li18c_inputs a :Generic ;
    :hasText "inputs" .

:li18c_interpretable a :Generic ;
    :hasText "interpretable" .

:li18c_lstm a :Method ;
    :hasText "lstm" .

:li18c_memory a :Other ;
    :hasText "memory" .

:li18c_model a :Generic ;
    :hasText "model" .

:li18c_modeling a :Task ;
    :hasText "modeling" .

:li18c_models a :Generic ;
    :hasText "models" .

:li18c_one a :Generic ;
    :hasText "one" .

:li18c_outputs a :Generic ;
    :hasText "outputs" .

:li18c_precision a :Eval ;
    :hasText "precision" .

:li18c_rank a :Other ;
    :hasText "rank" .

:li18c_results a :Generic ;
    :hasText "results" .

:li18c_short-term a :Generic ;
    :hasText "short-term" .

:li18c_state a :Generic ;
    :hasText "state" .

:li18c_structures a :Generic ;
    :hasText "structures" .

:li18c_studies a :Generic ;
    :hasText "studies" .

:li18c_that a :Generic ;
    :hasText "that" .

:li18c_this a :Generic ;
    :hasText "this" .

:li18c_training a :Task ;
    :hasText "training" .

:li18c_way a :Generic ;
    :hasText "way" .

:li18c_ways a :Generic ;
    :hasText "ways" .

:li18c_we a :Generic ;
    :hasText "we" .

:liu17a_approaches a :Generic ;
    :hasText "approaches" .

:liu17a_data a :Generic ;
    :hasText "data" .

:liu17a_embedding_methods a :Method ;
    :hasText "embedding_methods" .

:liu17a_embeddings a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

:liu17a_family a :Generic ;
    :hasText "family" .

:liu17a_interaction a :Generic ;
    :hasText "interaction" .

:liu17a_language a :Generic ;
    :hasText "language" .

:liu17a_learn a :Task ;
    :hasText "learn" .

:liu17a_method a :Generic ;
    :hasText "method" .

:liu17a_model a :Generic ;
    :hasText "model" .

:liu17a_observations a :Generic ;
    :hasText "observations" .

:liu17a_other a :Generic ;
    :hasText "other" .

:liu17a_representation a :Generic ;
    :hasText "representation" .

:liu17a_sparse_data a :Material ;
    :hasText "sparse_data" .

:liu17a_technique a :Generic ;
    :hasText "technique" .

:liu17a_term_matrix a :Other ;
    :hasText "term_matrix" .

:liu17a_that a :Generic ;
    :hasText "that" .

:liu17a_this a :Generic ;
    :hasText "this" .

:liu17a_tool a :Generic ;
    :hasText "tool" .

:liu17a_we a :Generic ;
    :hasText "we" .

:ma18d_approach a :Generic ;
    :hasText "approach" .

:ma18d_data a :Generic ;
    :hasText "data" .

:ma18d_datasets a :Material ;
    :hasText "datasets" .

:ma18d_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:ma18d_dnns a :Method ;
    :hasText "dnns" .

:ma18d_generalization a :Generic ;
    :hasText "generalization" .

:ma18d_learn a :Task ;
    :hasText "learn" .

:ma18d_loss_function a :Method ;
    :hasText "loss_function" .

:ma18d_representation a :Generic ;
    :hasText "representation" .

:ma18d_strategy a :Generic ;
    :hasText "strategy" .

:ma18d_subspace a :Other ;
    :hasText "subspace" .

:ma18d_subspaces a :Other ;
    :hasText "subspaces" .

:ma18d_that a :Generic ;
    :hasText "that" .

:ma18d_this a :Generic ;
    :hasText "this" .

:ma18d_training a :Task ;
    :hasText "training" .

:ma18d_understanding a :Task ;
    :hasText "understanding" .

:ma18d_we a :Generic ;
    :hasText "we" .

:mescheder17a_adversarial a :Generic ;
    :hasText "adversarial" .

:mescheder17a_approach a :Generic ;
    :hasText "approach" .

:mescheder17a_approaches a :Generic ;
    :hasText "approaches" .

:mescheder17a_autoencoders a :Method ;
    :hasText "autoencoders" .

:mescheder17a_complex a :Method ;
    :hasText "complex" .

:mescheder17a_data a :Generic ;
    :hasText "data" .

:mescheder17a_gans a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gans" .

:mescheder17a_generative_adversarial_networks a :Method ;
    :hasText "generative_adversarial_networks" .

:mescheder17a_generative_model a :Method ;
    :hasText "generative_model" .

:mescheder17a_latent_variable_models a :Method ;
    :hasText "latent_variable_models" .

:mescheder17a_latent_variables a :Other ;
    :hasText "latent_variables" .

:mescheder17a_learn a :Task ;
    :hasText "learn" .

:mescheder17a_maximum-likelihood a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/maximum_likelihood> ;
    :hasText "maximum-likelihood" .

:mescheder17a_model a :Generic ;
    :hasText "model" .

:mescheder17a_models a :Generic ;
    :hasText "models" .

:mescheder17a_network a :Generic ;
    :hasText "network" .

:mescheder17a_nonparametric a :Other ;
    :hasText "nonparametric" .

:mescheder17a_our_method a :Other ;
    :hasText "our_method" .

:mescheder17a_parameters a :Generic ;
    :hasText "parameters" .

:mescheder17a_posterior a :Generic ;
    :hasText "posterior" .

:mescheder17a_probability_distributions a :Other ;
    :hasText "probability_distributions" .

:mescheder17a_quality a :Eval ;
    :hasText "quality" .

:mescheder17a_technique a :Generic ;
    :hasText "technique" .

:mescheder17a_that a :Generic ;
    :hasText "that" .

:mescheder17a_this a :Generic ;
    :hasText "this" .

:mescheder17a_training a :Task ;
    :hasText "training" .

:mescheder17a_two a :Generic ;
    :hasText "two" .

:mirhoseini17a_approach a :Generic ;
    :hasText "approach" .

:mirhoseini17a_classification a :Task ;
    :hasText "classification" .

:mirhoseini17a_device a :Generic ;
    :hasText "device" .

:mirhoseini17a_devices a :Generic ;
    :hasText "devices" .

:mirhoseini17a_distributed a :Generic ;
    :hasText "distributed" .

:mirhoseini17a_environment a :Generic ;
    :hasText "environment" .

:mirhoseini17a_graph a :Other ;
    :hasText "graph" .

:mirhoseini17a_graphs a :Other ;
    :hasText "graphs" .

:mirhoseini17a_hardware a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/computer_hardware> ;
    :hasText "hardware" .

:mirhoseini17a_heuristics a :Method ;
    :hasText "heuristics" .

:mirhoseini17a_imagenet a :Material ;
    :hasText "imagenet" .

:mirhoseini17a_inception a :Method ;
    :hasText "inception" .

:mirhoseini17a_language_modeling a :Task ;
    :hasText "language_modeling" .

:mirhoseini17a_learns a :Task ;
    :hasText "learns" .

:mirhoseini17a_lstm a :Method ;
    :hasText "lstm" .

:mirhoseini17a_method a :Generic ;
    :hasText "method" .

:mirhoseini17a_methods a :Generic ;
    :hasText "methods" .

:mirhoseini17a_model a :Generic ;
    :hasText "model" .

:mirhoseini17a_models a :Generic ;
    :hasText "models" .

:mirhoseini17a_neural_machine_translation a :Task ;
    :hasText "neural_machine_translation" .

:mirhoseini17a_neural_networks a :Method ;
    :hasText "neural_networks" .

:mirhoseini17a_operations a :Generic ;
    :hasText "operations" .

:mirhoseini17a_our_method a :Other ;
    :hasText "our_method" .

:mirhoseini17a_parameters a :Generic ;
    :hasText "parameters" .

:mirhoseini17a_reward a :Eval ;
    :hasText "reward" .

:mirhoseini17a_rnn a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/recurrent_neural_networks> ;
    :hasText "rnn" .

:mirhoseini17a_sequence-to-sequence a :Method ;
    :hasText "sequence-to-sequence" .

:mirhoseini17a_signal a :Generic ;
    :hasText "signal" .

:mirhoseini17a_tensorflow a :Other ;
    :hasText "tensorflow" .

:mirhoseini17a_that a :Generic ;
    :hasText "that" .

:mirhoseini17a_these a :Generic ;
    :hasText "these" .

:mirhoseini17a_this a :Generic ;
    :hasText "this" .

:mirhoseini17a_time a :Generic ;
    :hasText "time" .

:mirhoseini17a_training a :Task ;
    :hasText "training" .

:mirhoseini17a_we a :Generic ;
    :hasText "we" .

:niculae18a_accuracy a :Eval ;
    :hasText "accuracy" .

:niculae18a_ambiguities a :Other ;
    :hasText "ambiguities" .

:niculae18a_backpropagation a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

:niculae18a_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:niculae18a_dependency_parsing a :Task ;
    :hasText "dependency_parsing" .

:niculae18a_gradient a :Other ;
    :hasText "gradient" .

:niculae18a_hidden_layers a :Other ;
    :hasText "hidden_layers" .

:niculae18a_interpretability a :Eval ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/interpretability> ;
    :hasText "interpretability" .

:niculae18a_loss_function a :Method ;
    :hasText "loss_function" .

:niculae18a_map a :Eval ;
    :hasText "map" .

:niculae18a_method a :Generic ;
    :hasText "method" .

:niculae18a_natural_language a :Material ;
    :hasText "natural_language" .

:niculae18a_ones a :Generic ;
    :hasText "ones" .

:niculae18a_prediction a :Task ;
    :hasText "prediction" .

:niculae18a_problems a :Generic ;
    :hasText "problems" .

:niculae18a_structure a :Generic ;
    :hasText "structure" .

:niculae18a_structured a :Generic ;
    :hasText "structured" .

:niculae18a_structures a :Generic ;
    :hasText "structures" .

:niculae18a_systems a :Generic ;
    :hasText "systems" .

:niculae18a_we a :Generic ;
    :hasText "we" .

:nie18a_backpropagation a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

:nie18a_cnns a :Method ;
    :hasText "cnns" .

<https://github.com/deepcurator/DCC/nie18a_convolutional_neural_networks_(cnns)> a :Method ;
    :hasText "convolutional_neural_networks_(cnns)" .

:nie18a_decisions a :Generic ;
    :hasText "decisions" .

:nie18a_experiments a :Generic ;
    :hasText "experiments" .

:nie18a_image a :Material ;
    :hasText "image" .

:nie18a_interpretable a :Generic ;
    :hasText "interpretable" .

:nie18a_map a :Eval ;
    :hasText "map" .

:nie18a_network a :Generic ;
    :hasText "network" .

:nie18a_recovery a :Task ;
    :hasText "recovery" .

:nie18a_relu a :Other ;
    :hasText "relu" .

:nie18a_that a :Generic ;
    :hasText "that" .

:nie18a_theoretical_analysis a :Generic ;
    :hasText "theoretical_analysis" .

:nie18a_theory a :Generic ;
    :hasText "theory" .

:nie18a_this a :Generic ;
    :hasText "this" .

:nie18a_two a :Generic ;
    :hasText "two" .

:nie18a_we a :Generic ;
    :hasText "we" .

:oliva17a_alternatives a :Generic ;
    :hasText "alternatives" .

:oliva17a_applications a :Generic ;
    :hasText "applications" .

:oliva17a_architecture a :Generic ;
    :hasText "architecture" .

:oliva17a_architectures a :Generic ;
    :hasText "architectures" .

:oliva17a_data a :Generic ;
    :hasText "data" .

:oliva17a_gru a :Method ;
    :hasText "gru" .

:oliva17a_grus a :Method ;
    :hasText "grus" .

:oliva17a_hyperparameters a :Generic ;
    :hasText "hyperparameters" .

:oliva17a_learn a :Task ;
    :hasText "learn" .

:oliva17a_lstm a :Method ;
    :hasText "lstm" .

:oliva17a_lstms a :Method ;
    :hasText "lstms" .

:oliva17a_one a :Generic ;
    :hasText "one" .

:oliva17a_parameters a :Generic ;
    :hasText "parameters" .

:oliva17a_real-world_tasks a :Generic ;
    :hasText "real-world_tasks" .

:oliva17a_recurrent_neural_network_architectures a :Method ;
    :hasText "recurrent_neural_network_architectures" .

:oliva17a_statistics a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/correlation_analysis> ;
    :hasText "statistics" .

:oliva17a_tasks a :Generic ;
    :hasText "tasks" .

:oliva17a_that a :Generic ;
    :hasText "that" .

:ostrovski17a_algorithm a :Generic ;
    :hasText "algorithm" .

:ostrovski17a_approach a :Generic ;
    :hasText "approach" .

:ostrovski17a_architectures a :Generic ;
    :hasText "architectures" .

:ostrovski17a_assumptions a :Generic ;
    :hasText "assumptions" .

:ostrovski17a_first a :Generic ;
    :hasText "first" .

:ostrovski17a_images a :Material ;
    :hasText "images" .

:ostrovski17a_model a :Generic ;
    :hasText "model" .

:ostrovski17a_quality a :Eval ;
    :hasText "quality" .

:ostrovski17a_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:ostrovski17a_state a :Generic ;
    :hasText "state" .

:ostrovski17a_that a :Generic ;
    :hasText "that" .

:ostrovski17a_two a :Generic ;
    :hasText "two" .

:ostrovski17a_we a :Generic ;
    :hasText "we" .

:pham18a-Figure6-1 a :Figure .

:pham18a_approach a :Generic ;
    :hasText "approach" .

:pham18a_approaches a :Generic ;
    :hasText "approaches" .

:pham18a_architecture a :Generic ;
    :hasText "architecture" .

:pham18a_architectures a :Generic ;
    :hasText "architectures" .

:pham18a_automatic a :Task ;
    :hasText "automatic" .

:pham18a_cifar-10 a :Material ;
    :hasText "cifar-10" .

:pham18a_enas a :Task ;
    :hasText "enas" .

:pham18a_entropy a :Other ;
    :hasText "entropy" .

:pham18a_gradient a :Other ;
    :hasText "gradient" .

:pham18a_graph a :Other ;
    :hasText "graph" .

:pham18a_methods a :Generic ;
    :hasText "methods" .

:pham18a_minimize a :Task ;
    :hasText "minimize" .

:pham18a_model a :Generic ;
    :hasText "model" .

:pham18a_model_design a :Generic ;
    :hasText "model_design" .

:pham18a_models a :Generic ;
    :hasText "models" .

:pham18a_neural_architecture_search a :Task ;
    :hasText "neural_architecture_search" .

:pham18a_neural_network a :Method ;
    :hasText "neural_network" .

:pham18a_parameters a :Generic ;
    :hasText "parameters" .

:pham18a_penn_treebank a :Material ;
    :hasText "penn_treebank" .

:pham18a_policy a :Other ;
    :hasText "policy" .

:pham18a_processing a :Generic ;
    :hasText "processing" .

:pham18a_reward a :Eval ;
    :hasText "reward" .

:pham18a_search a :Task ;
    :hasText "search" .

:pham18a_state a :Generic ;
    :hasText "state" .

:pham18a_that a :Generic ;
    :hasText "that" .

:pham18a_training a :Task ;
    :hasText "training" .

:pinto17a_adversarial a :Generic ;
    :hasText "adversarial" .

:pinto17a_approaches a :Generic ;
    :hasText "approaches" .

:pinto17a_baseline a :Generic ;
    :hasText "baseline" .

:pinto17a_computation a :Other ;
    :hasText "computation" .

:pinto17a_conditions a :Generic ;
    :hasText "conditions" .

:pinto17a_data a :Generic ;
    :hasText "data" .

:pinto17a_environments a :Generic ;
    :hasText "environments" .

:pinto17a_errors a :Other ;
    :hasText "errors" .

:pinto17a_experiments a :Generic ;
    :hasText "experiments" .

:pinto17a_field a :Generic ;
    :hasText "field" .

:pinto17a_generalization a :Generic ;
    :hasText "generalization" .

:pinto17a_learns a :Task ;
    :hasText "learns" .

:pinto17a_methods a :Generic ;
    :hasText "methods" .

:pinto17a_modeling a :Task ;
    :hasText "modeling" .

:pinto17a_neural_networks a :Method ;
    :hasText "neural_networks" .

:pinto17a_objective_function a :Other ;
    :hasText "objective_function" .

:pinto17a_our_method a :Other ;
    :hasText "our_method" .

:pinto17a_policy a :Other ;
    :hasText "policy" .

:pinto17a_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

<https://github.com/deepcurator/DCC/pinto17a_reinforcement_learning_(rl)> a :Task ;
    :hasText "reinforcement_learning_(rl)" .

:pinto17a_rl a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/reinforcement_learning> ;
    :hasText "rl" .

:pinto17a_system a :Generic ;
    :hasText "system" .

:pinto17a_that a :Generic ;
    :hasText "that" .

:pinto17a_training a :Task ;
    :hasText "training" .

:pinto17a_transfer a :Other ;
    :hasText "transfer" .

:pinto17a_we a :Generic ;
    :hasText "we" .

:pritzel17a_agents a :Generic ;
    :hasText "agents" .

:pritzel17a_data a :Generic ;
    :hasText "data" .

:pritzel17a_deep_reinforcement_learning a :Method ;
    :hasText "deep_reinforcement_learning" .

:pritzel17a_environments a :Generic ;
    :hasText "environments" .

:pritzel17a_estimates a :Generic ;
    :hasText "estimates" .

:pritzel17a_function a :Generic ;
    :hasText "function" .

:pritzel17a_learns a :Task ;
    :hasText "learns" .

:pritzel17a_magnitudes a :Other ;
    :hasText "magnitudes" .

:pritzel17a_methods a :Generic ;
    :hasText "methods" .

:pritzel17a_other a :Generic ;
    :hasText "other" .

:pritzel17a_reinforcement_learning_methods a :Task ;
    :hasText "reinforcement_learning_methods" .

:pritzel17a_representation a :Generic ;
    :hasText "representation" .

:pritzel17a_representations a :Generic ;
    :hasText "representations" .

:pritzel17a_state a :Generic ;
    :hasText "state" .

:pritzel17a_that a :Generic ;
    :hasText "that" .

:pritzel17a_them a :Generic ;
    :hasText "them" .

:raffel17a_alignment a :Task ;
    :hasText "alignment" .

:raffel17a_alignments a :Other ;
    :hasText "alignments" .

:raffel17a_approach a :Generic ;
    :hasText "approach" .

:raffel17a_attention_mechanism a :Method ;
    :hasText "attention_mechanism" .

:raffel17a_attention_mechanisms a :Method ;
    :hasText "attention_mechanisms" .

:raffel17a_end-to-end a :Generic ;
    :hasText "end-to-end" .

:raffel17a_input a :Generic ;
    :hasText "input" .

:raffel17a_machine_translation a :Task ;
    :hasText "machine_translation" .

:raffel17a_method a :Generic ;
    :hasText "method" .

:raffel17a_models a :Generic ;
    :hasText "models" .

:raffel17a_neural_network_models a :Method ;
    :hasText "neural_network_models" .

:raffel17a_problems a :Generic ;
    :hasText "problems" .

:raffel17a_quadratic_time a :Other ;
    :hasText "quadratic_time" .

:raffel17a_results a :Generic ;
    :hasText "results" .

:raffel17a_speech_recognition a :Task ;
    :hasText "speech_recognition" .

:raffel17a_summarization a :Task ;
    :hasText "summarization" .

:raffel17a_that a :Generic ;
    :hasText "that" .

:raffel17a_time a :Generic ;
    :hasText "time" .

:raffel17a_validate a :Task ;
    :hasText "validate" .

:raffel17a_we a :Generic ;
    :hasText "we" .

:ren18a_algorithms a :Generic ;
    :hasText "algorithms" .

:ren18a_class_imbalance a :Other ;
    :hasText "class_imbalance" .

:ren18a_complex a :Method ;
    :hasText "complex" .

:ren18a_data a :Generic ;
    :hasText "data" .

:ren18a_functions a :Generic ;
    :hasText "functions" .

:ren18a_gradient a :Other ;
    :hasText "gradient" .

:ren18a_gradient_descent a :Method ;
    :hasText "gradient_descent" .

:ren18a_hyperparameter a :Generic ;
    :hasText "hyperparameter" .

:ren18a_hyperparameters a :Generic ;
    :hasText "hyperparameters" .

:ren18a_input a :Generic ;
    :hasText "input" .

:ren18a_learning_algorithm a :Method ;
    :hasText "learning_algorithm" .

:ren18a_learns a :Task ;
    :hasText "learns" .

:ren18a_method a :Generic ;
    :hasText "method" .

:ren18a_methods a :Generic ;
    :hasText "methods" .

:ren18a_minimize a :Task ;
    :hasText "minimize" .

:ren18a_modeling a :Task ;
    :hasText "modeling" .

:ren18a_network a :Generic ;
    :hasText "network" .

:ren18a_neural_networks a :Method ;
    :hasText "neural_networks" .

:ren18a_noises a :Other ;
    :hasText "noises" .

:ren18a_our_method a :Other ;
    :hasText "our_method" .

:ren18a_problems a :Generic ;
    :hasText "problems" .

:ren18a_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:ren18a_regularizers a :Method ;
    :hasText "regularizers" .

:ren18a_solutions a :Generic ;
    :hasText "solutions" .

:ren18a_supervised_learning a :Method ;
    :hasText "supervised_learning" .

:ren18a_tasks a :Generic ;
    :hasText "tasks" .

:ren18a_that a :Generic ;
    :hasText "that" .

:ren18a_these a :Generic ;
    :hasText "these" .

:ren18a_they a :Generic ;
    :hasText "they" .

:ren18a_this a :Generic ;
    :hasText "this" .

:ren18a_tools a :Generic ;
    :hasText "tools" .

:ren18a_training a :Task ;
    :hasText "training" .

:ren18a_training_examples a :Material ;
    :hasText "training_examples" .

:ren18a_we a :Generic ;
    :hasText "we" .

:ren18a_weights a :Generic ;
    :hasText "weights" .

:ruff18a_adaptation a :Task ;
    :hasText "adaptation" .

:ruff18a_adversarial_examples a :Task ;
    :hasText "adversarial_examples" .

:ruff18a_approaches a :Generic ;
    :hasText "approaches" .

:ruff18a_cifar-10 a :Material ;
    :hasText "cifar-10" .

:ruff18a_compression a :Task ;
    :hasText "compression" .

:ruff18a_data a :Generic ;
    :hasText "data" .

:ruff18a_datasets a :Material ;
    :hasText "datasets" .

:ruff18a_deep_learning a :Generic ;
    :hasText "deep_learning" .

:ruff18a_detection_method a :Method ;
    :hasText "detection_method" .

:ruff18a_effectiveness a :Eval ;
    :hasText "effectiveness" .

:ruff18a_generative_models a :Method ;
    :hasText "generative_models" .

:ruff18a_image a :Material ;
    :hasText "image" .

:ruff18a_machine_learning a :Task ;
    :hasText "machine_learning" .

:ruff18a_mnist a :Material ;
    :hasText "mnist" .

:ruff18a_networks a :Generic ;
    :hasText "networks" .

:ruff18a_neural_network a :Method ;
    :hasText "neural_network" .

:ruff18a_other a :Generic ;
    :hasText "other" .

:ruff18a_our_method a :Other ;
    :hasText "our_method" .

:ruff18a_problems a :Generic ;
    :hasText "problems" .

:ruff18a_procedure a :Generic ;
    :hasText "procedure" .

:ruff18a_properties a :Generic ;
    :hasText "properties" .

:ruff18a_task a :Generic ;
    :hasText "task" .

:ruff18a_that a :Generic ;
    :hasText "that" .

:ruff18a_they a :Generic ;
    :hasText "they" .

:ruff18a_this a :Generic ;
    :hasText "this" .

:ruff18a_training a :Task ;
    :hasText "training" .

:ruff18a_we a :Generic ;
    :hasText "we" .

:seward18a_cifar-10 a :Material ;
    :hasText "cifar-10" .

:seward18a_derivation a :Other ;
    :hasText "derivation" .

:seward18a_distance a :Other ;
    :hasText "distance" .

:seward18a_divergence a :Generic ;
    :hasText "divergence" .

:seward18a_examples a :Generic ;
    :hasText "examples" .

:seward18a_first a :Generic ;
    :hasText "first" .

:seward18a_framework a :Generic ;
    :hasText "framework" .

:seward18a_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:seward18a_generation a :Task ;
    :hasText "generation" .

:seward18a_image a :Material ;
    :hasText "image" .

:seward18a_information a :Generic ;
    :hasText "information" .

:seward18a_language a :Generic ;
    :hasText "language" .

:seward18a_method a :Generic ;
    :hasText "method" .

:seward18a_one a :Generic ;
    :hasText "one" .

:seward18a_our_method a :Other ;
    :hasText "our_method" .

:seward18a_parameters a :Generic ;
    :hasText "parameters" .

:seward18a_state a :Generic ;
    :hasText "state" .

:seward18a_task a :Generic ;
    :hasText "task" .

:seward18a_that a :Generic ;
    :hasText "that" .

:seward18a_these a :Generic ;
    :hasText "these" .

:seward18a_they a :Generic ;
    :hasText "they" .

:seward18a_this a :Generic ;
    :hasText "this" .

:seward18a_those a :Generic ;
    :hasText "those" .

:seward18a_we a :Generic ;
    :hasText "we" .

:sharchilev18a_approach a :Generic ;
    :hasText "approach" .

:sharchilev18a_approaches a :Generic ;
    :hasText "approaches" .

:sharchilev18a_approximations a :Generic ;
    :hasText "approximations" .

:sharchilev18a_baselines a :Generic ;
    :hasText "baselines" .

:sharchilev18a_computational_complexity a :Eval ;
    :hasText "computational_complexity" .

:sharchilev18a_computational_efficiency a :Eval ;
    :hasText "computational_efficiency" .

:sharchilev18a_decision_trees a :Method ;
    :hasText "decision_trees" .

:sharchilev18a_ensemble a :Task ;
    :hasText "ensemble" .

:sharchilev18a_ensembles a :Task ;
    :hasText "ensembles" .

:sharchilev18a_framework a :Generic ;
    :hasText "framework" .

:sharchilev18a_gradient a :Other ;
    :hasText "gradient" .

:sharchilev18a_model a :Generic ;
    :hasText "model" .

:sharchilev18a_models a :Generic ;
    :hasText "models" .

:sharchilev18a_one a :Generic ;
    :hasText "one" .

:sharchilev18a_our_method a :Other ;
    :hasText "our_method" .

:sharchilev18a_parametric_models a :Method ;
    :hasText "parametric_models" .

:sharchilev18a_predictions a :Task ;
    :hasText "predictions" .

:sharchilev18a_problem a :Generic ;
    :hasText "problem" .

:sharchilev18a_quality a :Eval ;
    :hasText "quality" .

:sharchilev18a_scheme a :Generic ;
    :hasText "scheme" .

:sharchilev18a_that a :Generic ;
    :hasText "that" .

:sharchilev18a_this a :Generic ;
    :hasText "this" .

:sharchilev18a_training a :Task ;
    :hasText "training" .

:sharchilev18a_tree a :Other ;
    :hasText "tree" .

:sharchilev18a_tree_structures a :Other ;
    :hasText "tree_structures" .

:sharchilev18a_way a :Generic ;
    :hasText "way" .

:sharchilev18a_ways a :Generic ;
    :hasText "ways" .

:sharchilev18a_we a :Generic ;
    :hasText "we" .

:shi18a_applications a :Generic ;
    :hasText "applications" .

:shi18a_approach a :Generic ;
    :hasText "approach" .

:shi18a_bias a :Method ;
    :hasText "bias" .

:shi18a_effectiveness a :Eval ;
    :hasText "effectiveness" .

:shi18a_error_bound a :Other ;
    :hasText "error_bound" .

:shi18a_estimates a :Generic ;
    :hasText "estimates" .

:shi18a_extension a :Generic ;
    :hasText "extension" .

:shi18a_function a :Generic ;
    :hasText "function" .

:shi18a_gradient a :Other ;
    :hasText "gradient" .

:shi18a_method a :Generic ;
    :hasText "method" .

:shi18a_operators a :Generic ;
    :hasText "operators" .

:shi18a_our_method a :Other ;
    :hasText "our_method" .

:shi18a_pca a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/principle_component_analysis> ;
    :hasText "pca" .

:shi18a_results a :Generic ;
    :hasText "results" .

:shi18a_spectral_decomposition a :Method ;
    :hasText "spectral_decomposition" .

:shi18a_that a :Generic ;
    :hasText "that" .

:shi18a_this a :Generic ;
    :hasText "this" .

:shi18a_variance a :Generic ;
    :hasText "variance" .

:shi18a_variational_inference a :Method ;
    :hasText "variational_inference" .

:shi18a_we a :Generic ;
    :hasText "we" .

:shu17a_autoencoder a :Method ;
    :hasText "autoencoder" .

:shu17a_benchmarks a :Material ;
    :hasText "benchmarks" .

:shu17a_deep_generative_models a :Method ;
    :hasText "deep_generative_models" .

:shu17a_density_estimation a :Task ;
    :hasText "density_estimation" .

:shu17a_framework a :Generic ;
    :hasText "framework" .

:shu17a_generative_model a :Method ;
    :hasText "generative_model" .

:shu17a_input a :Generic ;
    :hasText "input" .

:shu17a_mechanism a :Generic ;
    :hasText "mechanism" .

:shu17a_mnist a :Material ;
    :hasText "mnist" .

:shu17a_models a :Generic ;
    :hasText "models" .

:shu17a_overfitting a :Other ;
    :hasText "overfitting" .

:shu17a_prediction a :Task ;
    :hasText "prediction" .

:shu17a_procedure a :Generic ;
    :hasText "procedure" .

:shu17a_results a :Generic ;
    :hasText "results" .

:shu17a_stochastic_variables a :Other ;
    :hasText "stochastic_variables" .

:shu17a_supervised_regime a :Task ;
    :hasText "supervised_regime" .

:shu17a_svhn a :Material ;
    :hasText "svhn" .

:shu17a_target a :Generic ;
    :hasText "target" .

:shu17a_task a :Generic ;
    :hasText "task" .

:shu17a_that a :Generic ;
    :hasText "that" .

:shu17a_training a :Task ;
    :hasText "training" .

:shu17a_training_method a :Method ;
    :hasText "training_method" .

:shu17a_unlabeled_data a :Other ;
    :hasText "unlabeled_data" .

:shu17a_we a :Generic ;
    :hasText "we" .

:shyam17a_approximating a :Task ;
    :hasText "approximating" .

:shyam17a_classification a :Task ;
    :hasText "classification" .

:shyam17a_comparators a :Method ;
    :hasText "comparators" .

:shyam17a_dynamic_representation a :Method ;
    :hasText "dynamic_representation" .

:shyam17a_error_rate a :Eval ;
    :hasText "error_rate" .

:shyam17a_first a :Generic ;
    :hasText "first" .

:shyam17a_information a :Generic ;
    :hasText "information" .

:shyam17a_model a :Generic ;
    :hasText "model" .

:shyam17a_models a :Generic ;
    :hasText "models" .

:shyam17a_objects a :Generic ;
    :hasText "objects" .

:shyam17a_observations a :Generic ;
    :hasText "observations" .

:shyam17a_one a :Generic ;
    :hasText "one" .

:shyam17a_representations a :Generic ;
    :hasText "representations" .

:shyam17a_state a :Generic ;
    :hasText "state" .

:shyam17a_task a :Generic ;
    :hasText "task" .

:shyam17a_that a :Generic ;
    :hasText "that" .

:shyam17a_them a :Generic ;
    :hasText "them" .

:shyam17a_this a :Generic ;
    :hasText "this" .

:shyam17a_way a :Generic ;
    :hasText "way" .

:shyam17a_we a :Generic ;
    :hasText "we" .

:silver17a_architecture a :Generic ;
    :hasText "architecture" .

:silver17a_deep_neural_network_architectures a :Method ;
    :hasText "deep_neural_network_architectures" .

:silver17a_fully_model a :Method ;
    :hasText "fully_model" .

:silver17a_function a :Generic ;
    :hasText "function" .

:silver17a_learn a :Task ;
    :hasText "learn" .

:silver17a_models a :Generic ;
    :hasText "models" .

:silver17a_planning a :Other ;
    :hasText "planning" .

:silver17a_predictions a :Task ;
    :hasText "predictions" .

:silver17a_reward a :Eval ;
    :hasText "reward" .

:silver17a_rewards a :Eval ;
    :hasText "rewards" .

:silver17a_that a :Generic ;
    :hasText "that" .

:silver17a_these a :Generic ;
    :hasText "these" .

:silver17a_this a :Generic ;
    :hasText "this" .

:silver17a_we a :Generic ;
    :hasText "we" .

:song18a_convergence a :Generic ;
    :hasText "convergence" .

:song18a_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:song18a_gradient a :Other ;
    :hasText "gradient" .

:song18a_implementations a :Generic ;
    :hasText "implementations" .

:song18a_method a :Generic ;
    :hasText "method" .

:song18a_model a :Generic ;
    :hasText "model" .

:song18a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:song18a_policy a :Other ;
    :hasText "policy" .

:song18a_properties a :Generic ;
    :hasText "properties" .

:song18a_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:song18a_sizes a :Other ;
    :hasText "sizes" .

:song18a_solution a :Generic ;
    :hasText "solution" .

:song18a_techniques a :Generic ;
    :hasText "techniques" .

:song18a_that a :Generic ;
    :hasText "that" .

:song18a_they a :Generic ;
    :hasText "they" .

:song18a_this a :Generic ;
    :hasText "this" .

:song18a_training a :Task ;
    :hasText "training" .

:song18a_we a :Generic ;
    :hasText "we" .

:suganuma18a_adversarial a :Generic ;
    :hasText "adversarial" .

:suganuma18a_algorithm a :Generic ;
    :hasText "algorithm" .

:suganuma18a_approach a :Generic ;
    :hasText "approach" .

:suganuma18a_architectures a :Generic ;
    :hasText "architectures" .

:suganuma18a_autoencoders a :Method ;
    :hasText "autoencoders" .

:suganuma18a_caes a :Method ;
    :hasText "caes" .

:suganuma18a_components a :Generic ;
    :hasText "components" .

:suganuma18a_deep_neural_networks a :Method ;
    :hasText "deep_neural_networks" .

:suganuma18a_experimental_results a :Generic ;
    :hasText "experimental_results" .

:suganuma18a_former a :Generic ;
    :hasText "former" .

:suganuma18a_image_restoration a :Task ;
    :hasText "image_restoration" .

:suganuma18a_images a :Material ;
    :hasText "images" .

:suganuma18a_layers a :Other ;
    :hasText "layers" .

:suganuma18a_loss_functions a :Other ;
    :hasText "loss_functions" .

:suganuma18a_methods a :Generic ;
    :hasText "methods" .

:suganuma18a_network a :Generic ;
    :hasText "network" .

:suganuma18a_noise a :Other ;
    :hasText "noise" .

:suganuma18a_search a :Task ;
    :hasText "search" .

:suganuma18a_signal a :Generic ;
    :hasText "signal" .

:suganuma18a_skip_connections a :Other ;
    :hasText "skip_connections" .

:suganuma18a_state a :Generic ;
    :hasText "state" .

:suganuma18a_studies a :Generic ;
    :hasText "studies" .

:suganuma18a_svhn a :Material ;
    :hasText "svhn" .

:suganuma18a_tasks a :Generic ;
    :hasText "tasks" .

:suganuma18a_that a :Generic ;
    :hasText "that" .

:suganuma18a_they a :Generic ;
    :hasText "they" .

:suganuma18a_this a :Generic ;
    :hasText "this" .

:suganuma18a_training a :Task ;
    :hasText "training" .

:suganuma18a_training_methods a :Method ;
    :hasText "training_methods" .

:suganuma18a_we a :Generic ;
    :hasText "we" .

:sun18a_end-to-end a :Generic ;
    :hasText "end-to-end" .

:sun18a_learn a :Task ;
    :hasText "learn" .

:sun18a_logic a :Other ;
    :hasText "logic" .

:sun18a_model a :Generic ;
    :hasText "model" .

:sun18a_module a :Generic ;
    :hasText "module" .

:sun18a_network a :Generic ;
    :hasText "network" .

:sun18a_program a :Generic ;
    :hasText "program" .

:sun18a_programs a :Generic ;
    :hasText "programs" .

:sun18a_representations a :Generic ;
    :hasText "representations" .

:sun18a_task a :Generic ;
    :hasText "task" .

:sun18a_that a :Generic ;
    :hasText "that" .

:sun18a_this a :Generic ;
    :hasText "this" .

:sun18a_training a :Task ;
    :hasText "training" .

:sun18a_videos a :Material ;
    :hasText "videos" .

:sun18a_we a :Generic ;
    :hasText "we" .

:sun18e_architecture a :Generic ;
    :hasText "architecture" .

:sun18e_automatic a :Task ;
    :hasText "automatic" .

:sun18e_compositional a :Other ;
    :hasText "compositional" .

:sun18e_end-to-end a :Generic ;
    :hasText "end-to-end" .

:sun18e_family a :Generic ;
    :hasText "family" .

:sun18e_generalization a :Generic ;
    :hasText "generalization" .

:sun18e_kernels a :Method ;
    :hasText "kernels" .

:sun18e_network a :Generic ;
    :hasText "network" .

:sun18e_neural_network a :Method ;
    :hasText "neural_network" .

:sun18e_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:sun18e_properties a :Generic ;
    :hasText "properties" .

:sun18e_rules a :Other ;
    :hasText "rules" .

:sun18e_structure a :Generic ;
    :hasText "structure" .

:sun18e_structures a :Generic ;
    :hasText "structures" .

:sun18e_tasks a :Generic ;
    :hasText "tasks" .

:sun18e_texture a :Other ;
    :hasText "texture" .

:sun18e_that a :Generic ;
    :hasText "that" .

:sun18e_this a :Generic ;
    :hasText "this" .

:sun18e_those a :Generic ;
    :hasText "those" .

:sun18e_time_series a :Generic ;
    :hasText "time_series" .

:sun18e_we a :Generic ;
    :hasText "we" .

:sundararajan17a_applications a :Generic ;
    :hasText "applications" .

:sundararajan17a_approach a :Generic ;
    :hasText "approach" .

:sundararajan17a_baseline a :Generic ;
    :hasText "baseline" .

:sundararajan17a_deep_networks a :Method ;
    :hasText "deep_networks" .

:sundararajan17a_errors a :Other ;
    :hasText "errors" .

:sundararajan17a_extraction a :Task ;
    :hasText "extraction" .

:sundararajan17a_features a :Generic ;
    :hasText "features" .

:sundararajan17a_function a :Generic ;
    :hasText "function" .

:sundararajan17a_gradient a :Other ;
    :hasText "gradient" .

:sundararajan17a_gradients a :Other ;
    :hasText "gradients" .

:sundararajan17a_image a :Material ;
    :hasText "image" .

:sundararajan17a_images a :Material ;
    :hasText "images" .

:sundararajan17a_imaging a :Generic ;
    :hasText "imaging" .

:sundararajan17a_implementation a :Material ;
    :hasText "implementation" .

:sundararajan17a_input a :Generic ;
    :hasText "input" .

:sundararajan17a_machine_learning a :Task ;
    :hasText "machine_learning" .

:sundararajan17a_method a :Generic ;
    :hasText "method" .

:sundararajan17a_methods a :Generic ;
    :hasText "methods" .

:sundararajan17a_model a :Generic ;
    :hasText "model" .

:sundararajan17a_models a :Generic ;
    :hasText "models" .

:sundararajan17a_network a :Generic ;
    :hasText "network" .

:sundararajan17a_networks a :Generic ;
    :hasText "networks" .

:sundararajan17a_object_recognition a :Task ;
    :hasText "object_recognition" .

:sundararajan17a_one a :Generic ;
    :hasText "one" .

:sundararajan17a_original_network a :Generic ;
    :hasText "original_network" .

:sundararajan17a_other a :Generic ;
    :hasText "other" .

:sundararajan17a_prediction a :Task ;
    :hasText "prediction" .

:sundararajan17a_problem a :Generic ;
    :hasText "problem" .

:sundararajan17a_programs a :Generic ;
    :hasText "programs" .

:sundararajan17a_results a :Generic ;
    :hasText "results" .

:sundararajan17a_rule a :Other ;
    :hasText "rule" .

:sundararajan17a_rules a :Other ;
    :hasText "rules" .

:sundararajan17a_system a :Generic ;
    :hasText "system" .

:sundararajan17a_technique a :Generic ;
    :hasText "technique" .

:sundararajan17a_text_models a :Method ;
    :hasText "text_models" .

:sundararajan17a_text_processing a :Task ;
    :hasText "text_processing" .

:sundararajan17a_that a :Generic ;
    :hasText "that" .

:sundararajan17a_these a :Generic ;
    :hasText "these" .

:sundararajan17a_they a :Generic ;
    :hasText "they" .

:sundararajan17a_this a :Generic ;
    :hasText "this" .

:sundararajan17a_those a :Generic ;
    :hasText "those" .

:sundararajan17a_two a :Generic ;
    :hasText "two" .

:sundararajan17a_understanding a :Task ;
    :hasText "understanding" .

:sundararajan17a_way a :Generic ;
    :hasText "way" .

:sundararajan17a_we a :Generic ;
    :hasText "we" .

:tansey18a_benchmarks a :Material ;
    :hasText "benchmarks" .

:tansey18a_deep_neural_network_prior a :Method ;
    :hasText "deep_neural_network_prior" .

:tansey18a_experiments a :Generic ;
    :hasText "experiments" .

:tansey18a_features a :Generic ;
    :hasText "features" .

:tansey18a_learns a :Task ;
    :hasText "learns" .

:tansey18a_method a :Generic ;
    :hasText "method" .

:tansey18a_methods a :Generic ;
    :hasText "methods" .

:tansey18a_model a :Generic ;
    :hasText "model" .

:tansey18a_per a :Eval ;
    :hasText "per" .

:tansey18a_predictive_models a :Generic ;
    :hasText "predictive_models" .

:tansey18a_results a :Generic ;
    :hasText "results" .

:tansey18a_scale a :Other ;
    :hasText "scale" .

:tansey18a_significance a :Other ;
    :hasText "significance" .

:tansey18a_state a :Generic ;
    :hasText "state" .

:tansey18a_studies a :Generic ;
    :hasText "studies" .

:tansey18a_that a :Generic ;
    :hasText "that" .

:tansey18a_two a :Generic ;
    :hasText "two" .

:tansey18a_variables a :Other ;
    :hasText "variables" .

:trinh18a_approach a :Generic ;
    :hasText "approach" .

:trinh18a_approaches a :Generic ;
    :hasText "approaches" .

:trinh18a_backpropagation a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

:trinh18a_baselines a :Generic ;
    :hasText "baselines" .

:trinh18a_document_classification a :Task ;
    :hasText "document_classification" .

:trinh18a_efficiency a :Eval ;
    :hasText "efficiency" .

:trinh18a_events a :Other ;
    :hasText "events" .

:trinh18a_image_classification a :Task ;
    :hasText "image_classification" .

:trinh18a_long-term a :Generic ;
    :hasText "long-term" .

:trinh18a_method a :Generic ;
    :hasText "method" .

:trinh18a_models a :Generic ;
    :hasText "models" .

:trinh18a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:trinh18a_other a :Generic ;
    :hasText "other" .

:trinh18a_our_method a :Other ;
    :hasText "our_method" .

:trinh18a_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:trinh18a_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:trinh18a_resource a :Generic ;
    :hasText "resource" .

:trinh18a_results a :Generic ;
    :hasText "results" .

:trinh18a_rnns a :Method ;
    :hasText "rnns" .

:trinh18a_scale a :Other ;
    :hasText "scale" .

:trinh18a_that a :Generic ;
    :hasText "that" .

:trinh18a_this a :Generic ;
    :hasText "this" .

:trinh18a_time a :Generic ;
    :hasText "time" .

:trinh18a_training a :Task ;
    :hasText "training" .

:tucker18a_action a :Generic ;
    :hasText "action" .

:tucker18a_baseline a :Generic ;
    :hasText "baseline" .

:tucker18a_baselines a :Generic ;
    :hasText "baselines" .

:tucker18a_bias a :Method ;
    :hasText "bias" .

:tucker18a_decisions a :Generic ;
    :hasText "decisions" .

:tucker18a_development a :Material ;
    :hasText "development" .

:tucker18a_domains a :Generic ;
    :hasText "domains" .

:tucker18a_efficiency a :Eval ;
    :hasText "efficiency" .

:tucker18a_estimates a :Generic ;
    :hasText "estimates" .

:tucker18a_function a :Generic ;
    :hasText "function" .

:tucker18a_gradient a :Other ;
    :hasText "gradient" .

:tucker18a_implementation a :Material ;
    :hasText "implementation" .

:tucker18a_improvement a :Eval ;
    :hasText "improvement" .

:tucker18a_methods a :Generic ;
    :hasText "methods" .

:tucker18a_model-free a :Generic ;
    :hasText "model-free" .

:tucker18a_policy a :Other ;
    :hasText "policy" .

:tucker18a_prior a :Generic ;
    :hasText "prior" .

:tucker18a_reinforcement_learning_algorithms a :Task ;
    :hasText "reinforcement_learning_algorithms" .

:tucker18a_source_code a :Generic ;
    :hasText "source_code" .

:tucker18a_state a :Generic ;
    :hasText "state" .

:tucker18a_that a :Generic ;
    :hasText "that" .

:tucker18a_these a :Generic ;
    :hasText "these" .

:tucker18a_this a :Generic ;
    :hasText "this" .

:tucker18a_variance a :Generic ;
    :hasText "variance" .

:tucker18a_we a :Generic ;
    :hasText "we" .

:villegas17a_actions a :Task ;
    :hasText "actions" .

:villegas17a_analogy a :Other ;
    :hasText "analogy" .

:villegas17a_approach a :Generic ;
    :hasText "approach" .

:villegas17a_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:villegas17a_datasets a :Material ;
    :hasText "datasets" .

:villegas17a_encoder-decoder a :Task ;
    :hasText "encoder-decoder" .

:villegas17a_errors a :Other ;
    :hasText "errors" .

:villegas17a_experiments a :Generic ;
    :hasText "experiments" .

:villegas17a_first a :Generic ;
    :hasText "first" .

:villegas17a_hierarchical a :Generic ;
    :hasText "hierarchical" .

:villegas17a_input a :Generic ;
    :hasText "input" .

:villegas17a_long-term a :Generic ;
    :hasText "long-term" .

:villegas17a_lstm a :Method ;
    :hasText "lstm" .

:villegas17a_model a :Generic ;
    :hasText "model" .

:villegas17a_penn_action a :Material ;
    :hasText "penn_action" .

:villegas17a_prediction a :Task ;
    :hasText "prediction" .

:villegas17a_predictions a :Task ;
    :hasText "predictions" .

:villegas17a_results a :Generic ;
    :hasText "results" .

:villegas17a_state a :Generic ;
    :hasText "state" .

:villegas17a_structure a :Generic ;
    :hasText "structure" .

:villegas17a_task a :Generic ;
    :hasText "task" .

:villegas17a_that a :Generic ;
    :hasText "that" .

:villegas17a_video a :Material ;
    :hasText "video" .

:villegas17a_we a :Generic ;
    :hasText "we" .

:wang18b_alternative a :Generic ;
    :hasText "alternative" .

:wang18b_datasets a :Material ;
    :hasText "datasets" .

:wang18b_deep_predictive_models a :Method ;
    :hasText "deep_predictive_models" .

:wang18b_dynamics a :Generic ;
    :hasText "dynamics" .

:wang18b_flows a :Other ;
    :hasText "flows" .

:wang18b_gradient a :Other ;
    :hasText "gradient" .

:wang18b_inputs a :Generic ;
    :hasText "inputs" .

:wang18b_long-term a :Generic ;
    :hasText "long-term" .

:wang18b_lstm a :Method ;
    :hasText "lstm" .

:wang18b_lstms a :Method ;
    :hasText "lstms" .

:wang18b_model a :Generic ;
    :hasText "model" .

:wang18b_modeling a :Task ;
    :hasText "modeling" .

:wang18b_motions a :Generic ;
    :hasText "motions" .

:wang18b_network a :Generic ;
    :hasText "network" .

:wang18b_outputs a :Generic ;
    :hasText "outputs" .

:wang18b_prediction a :Task ;
    :hasText "prediction" .

:wang18b_recurrent_network a :Method ;
    :hasText "recurrent_network" .

:wang18b_results a :Generic ;
    :hasText "results" .

:wang18b_short-term a :Generic ;
    :hasText "short-term" .

:wang18b_state a :Generic ;
    :hasText "state" .

:wang18b_structure a :Generic ;
    :hasText "structure" .

:wang18b_time a :Generic ;
    :hasText "time" .

:wang18b_video a :Material ;
    :hasText "video" .

:wang18b_we a :Generic ;
    :hasText "we" .

:wang18h_audio a :Material ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/audio> ;
    :hasText "audio" .

:wang18h_corpus a :Material ;
    :hasText "corpus" .

:wang18h_data a :Generic ;
    :hasText "data" .

:wang18h_embeddings a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/embeddings> ;
    :hasText "embeddings" .

:wang18h_interpretable a :Generic ;
    :hasText "interpretable" .

:wang18h_learn a :Task ;
    :hasText "learn" .

:wang18h_model a :Generic ;
    :hasText "model" .

:wang18h_noise a :Other ;
    :hasText "noise" .

:wang18h_results a :Generic ;
    :hasText "results" .

:wang18h_state a :Generic ;
    :hasText "state" .

:wang18h_system a :Generic ;
    :hasText "system" .

:wang18h_text a :Material ;
    :hasText "text" .

:wang18h_that a :Generic ;
    :hasText "that" .

:wang18h_they a :Generic ;
    :hasText "they" .

:wang18h_this a :Generic ;
    :hasText "this" .

:wang18h_transfer a :Other ;
    :hasText "transfer" .

:wang18h_ways a :Generic ;
    :hasText "ways" .

:wang18h_we a :Generic ;
    :hasText "we" .

:wei18a_bound a :Generic ;
    :hasText "bound" .

:wei18a_domains a :Generic ;
    :hasText "domains" .

:wei18a_educational_psychology a :Eval ;
    :hasText "educational_psychology" .

:wei18a_framework a :Generic ;
    :hasText "framework" .

:wei18a_function a :Generic ;
    :hasText "function" .

:wei18a_generalization a :Generic ;
    :hasText "generalization" .

:wei18a_knowledge a :Generic ;
    :hasText "knowledge" .

:wei18a_learn a :Task ;
    :hasText "learn" .

:wei18a_one a :Generic ;
    :hasText "one" .

:wei18a_performance_improvement a :Eval ;
    :hasText "performance_improvement" .

:wei18a_reflection a :Other ;
    :hasText "reflection" .

:wei18a_state a :Generic ;
    :hasText "state" .

:wei18a_target_domain a :Material ;
    :hasText "target_domain" .

:wei18a_that a :Generic ;
    :hasText "that" .

:wei18a_this a :Generic ;
    :hasText "this" .

:wei18a_transfer a :Other ;
    :hasText "transfer" .

:wei18a_transfer_learning a :Method ;
    :hasText "transfer_learning" .

:wei18a_transfer_learning_algorithms a :Method ;
    :hasText "transfer_learning_algorithms" .

:wei18a_two a :Generic ;
    :hasText "two" .

:wei18a_we a :Generic ;
    :hasText "we" .

:wichers18a_adversarial a :Generic ;
    :hasText "adversarial" .

:wichers18a_annotation a :Task ;
    :hasText "annotation" .

:wichers18a_decoder a :Method ;
    :hasText "decoder" .

:wichers18a_encoder a :Method ;
    :hasText "encoder" .

:wichers18a_encodes a :Generic ;
    :hasText "encodes" .

:wichers18a_feature_space a :Other ;
    :hasText "feature_space" .

:wichers18a_first a :Generic ;
    :hasText "first" .

:wichers18a_generation a :Task ;
    :hasText "generation" .

:wichers18a_hierarchical a :Generic ;
    :hasText "hierarchical" .

:wichers18a_image a :Material ;
    :hasText "image" .

:wichers18a_input a :Generic ;
    :hasText "input" .

:wichers18a_long-term a :Generic ;
    :hasText "long-term" .

:wichers18a_method a :Generic ;
    :hasText "method" .

:wichers18a_network a :Generic ;
    :hasText "network" .

:wichers18a_prediction a :Task ;
    :hasText "prediction" .

:wichers18a_research a :Generic ;
    :hasText "research" .

:wichers18a_results a :Generic ;
    :hasText "results" .

:wichers18a_short-term a :Generic ;
    :hasText "short-term" .

:wichers18a_state a :Generic ;
    :hasText "state" .

:wichers18a_structures a :Generic ;
    :hasText "structures" .

:wichers18a_supervision a :Other ;
    :hasText "supervision" .

:wichers18a_that a :Generic ;
    :hasText "that" .

:wichers18a_this a :Generic ;
    :hasText "this" .

:wichers18a_training_method a :Method ;
    :hasText "training_method" .

:wichers18a_training_time a :Eval ;
    :hasText "training_time" .

:wichers18a_video a :Material ;
    :hasText "video" .

:wichers18a_videos a :Material ;
    :hasText "videos" .

:wichers18a_we a :Generic ;
    :hasText "we" .

:wu18g_algorithms a :Generic ;
    :hasText "algorithms" .

:wu18g_baseline a :Generic ;
    :hasText "baseline" .

:wu18g_bounds a :Other ;
    :hasText "bounds" .

:wu18g_data a :Generic ;
    :hasText "data" .

:wu18g_divergence a :Generic ;
    :hasText "divergence" .

:wu18g_end-to-end a :Generic ;
    :hasText "end-to-end" .

:wu18g_feedbacks a :Other ;
    :hasText "feedbacks" .

:wu18g_generalization a :Generic ;
    :hasText "generalization" .

:wu18g_importance_sampling a :Method ;
    :hasText "importance_sampling" .

:wu18g_improvement a :Eval ;
    :hasText "improvement" .

:wu18g_method a :Generic ;
    :hasText "method" .

:wu18g_minimization a :Task ;
    :hasText "minimization" .

:wu18g_neural_network a :Method ;
    :hasText "neural_network" .

:wu18g_policies a :Other ;
    :hasText "policies" .

:wu18g_policy a :Other ;
    :hasText "policy" .

:wu18g_prior_work a :Generic ;
    :hasText "prior_work" .

:wu18g_problems a :Generic ;
    :hasText "problems" .

:wu18g_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:wu18g_results a :Generic ;
    :hasText "results" .

:wu18g_task a :Generic ;
    :hasText "task" .

:wu18g_that a :Generic ;
    :hasText "that" .

:wu18g_this a :Generic ;
    :hasText "this" .

:wu18g_training a :Task ;
    :hasText "training" .

:wu18g_training_algorithms a :Method ;
    :hasText "training_algorithms" .

:wu18g_variance a :Generic ;
    :hasText "variance" .

:wu18g_we a :Generic ;
    :hasText "we" .

:wu18h_accuracy a :Eval ;
    :hasText "accuracy" .

:wu18h_cnn a :Method ;
    :hasText "cnn" .

:wu18h_cnn_models a :Method ;
    :hasText "cnn_models" .

:wu18h_cnns a :Method ;
    :hasText "cnns" .

:wu18h_compression a :Task ;
    :hasText "compression" .

:wu18h_compression_ratio a :Eval ;
    :hasText "compression_ratio" .

:wu18h_computation a :Other ;
    :hasText "computation" .

:wu18h_convolutions a :Method ;
    :hasText "convolutions" .

:wu18h_devices a :Generic ;
    :hasText "devices" .

:wu18h_estimation a :Generic ;
    :hasText "estimation" .

:wu18h_hardware a :Generic ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/computer_hardware> ;
    :hasText "hardware" .

:wu18h_implementations a :Generic ;
    :hasText "implementations" .

:wu18h_k-means a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/k-means_algorithm> ;
    :hasText "k-means" .

:wu18h_k-means_clustering a :Method ;
    :hasText "k-means_clustering" .

:wu18h_metrics a :Generic ;
    :hasText "metrics" .

:wu18h_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:wu18h_resnet a :Method ;
    :hasText "resnet" .

:wu18h_results a :Generic ;
    :hasText "results" .

:wu18h_scheme a :Generic ;
    :hasText "scheme" .

:wu18h_this a :Generic ;
    :hasText "this" .

:wu18h_tool a :Generic ;
    :hasText "tool" .

:wu18h_training a :Task ;
    :hasText "training" .

:wu18h_we a :Generic ;
    :hasText "we" .

:wu18h_weights a :Generic ;
    :hasText "weights" .

:xie18c_alignment a :Task ;
    :hasText "alignment" .

:xie18c_classification_accuracy a :Eval ;
    :hasText "classification_accuracy" .

:xie18c_datasets a :Material ;
    :hasText "datasets" .

:xie18c_domain_adaptation a :Method ;
    :hasText "domain_adaptation" .

:xie18c_domains a :Generic ;
    :hasText "domains" .

:xie18c_features a :Generic ;
    :hasText "features" .

:xie18c_information a :Generic ;
    :hasText "information" .

:xie18c_knowledge a :Generic ;
    :hasText "knowledge" .

:xie18c_learn a :Task ;
    :hasText "learn" .

:xie18c_methods a :Generic ;
    :hasText "methods" .

:xie18c_model a :Generic ;
    :hasText "model" .

:xie18c_network a :Generic ;
    :hasText "network" .

:xie18c_prior a :Generic ;
    :hasText "prior" .

:xie18c_problem a :Generic ;
    :hasText "problem" .

:xie18c_results a :Generic ;
    :hasText "results" .

:xie18c_semantic a :Other ;
    :hasText "semantic" .

:xie18c_semantic_information a :Other ;
    :hasText "semantic_information" .

:xie18c_semantic_representations a :Method ;
    :hasText "semantic_representations" .

:xie18c_source_domain a :Material ;
    :hasText "source_domain" .

:xie18c_state a :Generic ;
    :hasText "state" .

:xie18c_statistics a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/correlation_analysis> ;
    :hasText "statistics" .

:xie18c_target a :Generic ;
    :hasText "target" .

:xie18c_target_domain a :Material ;
    :hasText "target_domain" .

:xie18c_that a :Generic ;
    :hasText "that" .

:xie18c_they a :Generic ;
    :hasText "they" .

:xie18c_this a :Generic ;
    :hasText "this" .

:xie18c_transfer a :Other ;
    :hasText "transfer" .

:xie18c_we a :Generic ;
    :hasText "we" .

:yang17d_architecture a :Generic ;
    :hasText "architecture" .

:yang17d_autoencoders a :Method ;
    :hasText "autoencoders" .

:yang17d_baselines a :Generic ;
    :hasText "baselines" .

:yang17d_cnn a :Method ;
    :hasText "cnn" .

:yang17d_datasets a :Material ;
    :hasText "datasets" .

:yang17d_decoder a :Method ;
    :hasText "decoder" .

:yang17d_decoders a :Method ;
    :hasText "decoders" .

:yang17d_encoder a :Method ;
    :hasText "encoder" .

:yang17d_experiments a :Generic ;
    :hasText "experiments" .

:yang17d_first a :Generic ;
    :hasText "first" .

:yang17d_information a :Generic ;
    :hasText "information" .

:yang17d_language_modeling a :Task ;
    :hasText "language_modeling" .

:yang17d_language_models a :Task ;
    :hasText "language_models" .

:yang17d_lstm a :Method ;
    :hasText "lstm" .

:yang17d_modeling a :Task ;
    :hasText "modeling" .

:yang17d_tasks a :Generic ;
    :hasText "tasks" .

:yang17d_text a :Material ;
    :hasText "text" .

:yang17d_that a :Generic ;
    :hasText "that" .

:yang17d_this a :Generic ;
    :hasText "this" .

:yang17d_two a :Generic ;
    :hasText "two" .

:yang17d_we a :Generic ;
    :hasText "we" .

:yang17d_words a :Generic ;
    :hasText "words" .

:yang18d_agents a :Generic ;
    :hasText "agents" .

:yang18d_algorithms a :Generic ;
    :hasText "algorithms" .

:yang18d_approaches a :Generic ;
    :hasText "approaches" .

:yang18d_convergence a :Generic ;
    :hasText "convergence" .

:yang18d_dynamics a :Generic ;
    :hasText "dynamics" .

:yang18d_effectiveness a :Eval ;
    :hasText "effectiveness" .

:yang18d_field a :Generic ;
    :hasText "field" .

:yang18d_first a :Generic ;
    :hasText "first" .

:yang18d_interactions a :Generic ;
    :hasText "interactions" .

:yang18d_model a :Generic ;
    :hasText "model" .

:yang18d_model-free a :Generic ;
    :hasText "model-free" .

:yang18d_policies a :Other ;
    :hasText "policies" .

:yang18d_policy a :Other ;
    :hasText "policy" .

:yang18d_q-learning a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/q-learning> ;
    :hasText "q-learning" .

:yang18d_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:yang18d_reinforcement_learning_methods a :Task ;
    :hasText "reinforcement_learning_methods" .

:yang18d_solution a :Generic ;
    :hasText "solution" .

:yang18d_this a :Generic ;
    :hasText "this" .

:yang18d_those a :Generic ;
    :hasText "those" .

:yang18d_two a :Generic ;
    :hasText "two" .

:yang18d_we a :Generic ;
    :hasText "we" .

:yoon17a_correlations a :Other ;
    :hasText "correlations" .

:yoon17a_datasets a :Material ;
    :hasText "datasets" .

:yoon17a_deep_neural_network a :Method ;
    :hasText "deep_neural_network" .

:yoon17a_efficiency a :Eval ;
    :hasText "efficiency" .

:yoon17a_features a :Generic ;
    :hasText "features" .

:yoon17a_memory a :Other ;
    :hasText "memory" .

:yoon17a_method a :Generic ;
    :hasText "method" .

:yoon17a_network a :Generic ;
    :hasText "network" .

:yoon17a_networks a :Generic ;
    :hasText "networks" .

:yoon17a_our_method a :Other ;
    :hasText "our_method" .

:yoon17a_overfitting a :Other ;
    :hasText "overfitting" .

:yoon17a_parameters a :Generic ;
    :hasText "parameters" .

:yoon17a_prediction_accuracy a :Eval ;
    :hasText "prediction_accuracy" .

:yoon17a_regularization a :Other ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/regularization> ;
    :hasText "regularization" .

:yoon17a_regularizations a :Other ;
    :hasText "regularizations" .

:yoon17a_results a :Generic ;
    :hasText "results" .

:yoon17a_scalability a :Eval ;
    :hasText "scalability" .

:yoon17a_sparsity a :Other ;
    :hasText "sparsity" .

:yoon17a_that a :Generic ;
    :hasText "that" .

:yoon17a_them a :Generic ;
    :hasText "them" .

:yoon17a_this a :Generic ;
    :hasText "this" .

:yoon17a_time a :Generic ;
    :hasText "time" .

:yoon17a_training a :Task ;
    :hasText "training" .

:yoon17a_validate a :Task ;
    :hasText "validate" .

:yoon17a_we a :Generic ;
    :hasText "we" .

:yoon17a_weights a :Generic ;
    :hasText "weights" .

:you18a_assumptions a :Generic ;
    :hasText "assumptions" .

:you18a_baselines a :Generic ;
    :hasText "baselines" .

:you18a_biology a :Material ;
    :hasText "biology" .

:you18a_complex a :Method ;
    :hasText "complex" .

:you18a_datasets a :Material ;
    :hasText "datasets" .

:you18a_deep_models a :Method ;
    :hasText "deep_models" .

:you18a_distances a :Other ;
    :hasText "distances" .

:you18a_edge a :Other ;
    :hasText "edge" .

:you18a_edges a :Other ;
    :hasText "edges" .

:you18a_evaluation_metrics a :Eval ;
    :hasText "evaluation_metrics" .

:you18a_experiments a :Generic ;
    :hasText "experiments" .

:you18a_generation a :Task ;
    :hasText "generation" .

:you18a_graph a :Other ;
    :hasText "graph" .

:you18a_graph_structure a :Other ;
    :hasText "graph_structure" .

:you18a_graphs a :Other ;
    :hasText "graphs" .

:you18a_learns a :Task ;
    :hasText "learns" .

:you18a_measure a :Generic ;
    :hasText "measure" .

:you18a_model a :Generic ;
    :hasText "model" .

:you18a_modeling a :Task ;
    :hasText "modeling" .

:you18a_networks a :Generic ;
    :hasText "networks" .

:you18a_node a :Other ;
    :hasText "node" .

:you18a_sampling a :Method ;
    :hasText "sampling" .

:you18a_structure a :Generic ;
    :hasText "structure" .

:you18a_target a :Generic ;
    :hasText "target" .

:you18a_that a :Generic ;
    :hasText "that" .

:you18a_these a :Generic ;
    :hasText "these" .

:you18a_training a :Task ;
    :hasText "training" .

:you18a_we a :Generic ;
    :hasText "we" .

:zenke17a_applications a :Generic ;
    :hasText "applications" .

:zenke17a_approach a :Generic ;
    :hasText "approach" .

:zenke17a_artificial_neural_networks a :Method ;
    :hasText "artificial_neural_networks" .

:zenke17a_classification_tasks a :Method ;
    :hasText "classification_tasks" .

:zenke17a_complex a :Method ;
    :hasText "complex" .

:zenke17a_computational_efficiency a :Eval ;
    :hasText "computational_efficiency" .

:zenke17a_data a :Generic ;
    :hasText "data" .

:zenke17a_deep_learning a :Generic ;
    :hasText "deep_learning" .

:zenke17a_domains a :Generic ;
    :hasText "domains" .

:zenke17a_information a :Generic ;
    :hasText "information" .

:zenke17a_neural_networks a :Method ;
    :hasText "neural_networks" .

:zenke17a_ones a :Generic ;
    :hasText "ones" .

:zenke17a_relevant_information a :Generic ;
    :hasText "relevant_information" .

:zenke17a_task a :Generic ;
    :hasText "task" .

:zenke17a_tasks a :Generic ;
    :hasText "tasks" .

:zenke17a_that a :Generic ;
    :hasText "that" .

:zenke17a_this a :Generic ;
    :hasText "this" .

:zenke17a_time a :Generic ;
    :hasText "time" .

:zenke17a_we a :Generic ;
    :hasText "we" .

:zhang17b_adversarial a :Generic ;
    :hasText "adversarial" .

:zhang17b_convergence a :Generic ;
    :hasText "convergence" .

:zhang17b_discrete_data a :Material ;
    :hasText "discrete_data" .

:zhang17b_experiments a :Generic ;
    :hasText "experiments" .

:zhang17b_feature a :Other ;
    :hasText "feature" .

:zhang17b_framework a :Generic ;
    :hasText "framework" .

:zhang17b_gan a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/gan> ;
    :hasText "gan" .

:zhang17b_matching a :Task ;
    :hasText "matching" .

:zhang17b_memory a :Other ;
    :hasText "memory" .

:zhang17b_metric a :Generic ;
    :hasText "metric" .

:zhang17b_model a :Generic ;
    :hasText "model" .

:zhang17b_network a :Generic ;
    :hasText "network" .

:zhang17b_problem a :Generic ;
    :hasText "problem" .

:zhang17b_synthetic_data a :Material ;
    :hasText "synthetic_data" .

:zhang17b_text a :Material ;
    :hasText "text" .

:zhang17b_that a :Generic ;
    :hasText "that" .

:zhang17b_training a :Task ;
    :hasText "training" .

:zhang17b_we a :Generic ;
    :hasText "we" .

:zhang17f_auto-encoders a :Method ;
    :hasText "auto-encoders" .

:zhang17f_backpropagation a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/backpropagation_algorithm> ;
    :hasText "backpropagation" .

:zhang17f_baseline_methods a :Generic ;
    :hasText "baseline_methods" .

:zhang17f_cnn a :Method ;
    :hasText "cnn" .

:zhang17f_cnns a :Method ;
    :hasText "cnns" .

:zhang17f_convex a :Other ;
    :hasText "convex" .

:zhang17f_convex_optimization_problem a :Task ;
    :hasText "convex_optimization_problem" .

:zhang17f_convolutional_neural_networks a :Method ;
    :hasText "convolutional_neural_networks" .

:zhang17f_generalization a :Generic ;
    :hasText "generalization" .

:zhang17f_hilbert_space a :Other ;
    :hasText "hilbert_space" .

:zhang17f_matrix a :Generic ;
    :hasText "matrix" .

:zhang17f_networks a :Generic ;
    :hasText "networks" .

:zhang17f_neural_networks a :Method ;
    :hasText "neural_networks" .

:zhang17f_other a :Generic ;
    :hasText "other" .

:zhang17f_parameters a :Generic ;
    :hasText "parameters" .

:zhang17f_rank a :Other ;
    :hasText "rank" .

:zhang17f_svms a :Method ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/support_vector_machine> ;
    :hasText "svms" .

:zhang17f_that a :Generic ;
    :hasText "that" .

:zhang17f_two a :Generic ;
    :hasText "two" .

:zhang17f_we a :Generic ;
    :hasText "we" .

:zhang18l_active_learning a :Task ;
    :hasText "active_learning" .

:zhang18l_algorithm a :Generic ;
    :hasText "algorithm" .

:zhang18l_benchmarks a :Material ;
    :hasText "benchmarks" .

:zhang18l_covariance a :Other ;
    :hasText "covariance" .

:zhang18l_deep_learning a :Generic ;
    :hasText "deep_learning" .

:zhang18l_estimates a :Generic ;
    :hasText "estimates" .

:zhang18l_estimation a :Generic ;
    :hasText "estimation" .

:zhang18l_flexibility a :Eval ;
    :hasText "flexibility" .

:zhang18l_gradient a :Other ;
    :hasText "gradient" .

:zhang18l_lower_bound a :Material ;
    :hasText "lower_bound" .

:zhang18l_matrix a :Generic ;
    :hasText "matrix" .

:zhang18l_methods a :Generic ;
    :hasText "methods" .

:zhang18l_noise a :Other ;
    :hasText "noise" .

:zhang18l_posterior a :Generic ;
    :hasText "posterior" .

:zhang18l_posteriors a :Generic ;
    :hasText "posteriors" .

:zhang18l_predictions a :Task ;
    :hasText "predictions" .

:zhang18l_procedures a :Generic ;
    :hasText "procedures" .

:zhang18l_reinforcement_learning a :Task ;
    :hasText "reinforcement_learning" .

:zhang18l_scale a :Other ;
    :hasText "scale" .

:zhang18l_that a :Generic ;
    :hasText "that" .

:zhang18l_uncertainty a :Generic ;
    :hasText "uncertainty" .

:zhang18l_variances a :Generic ;
    :hasText "variances" .

:zheng17a_algorithm a :Generic ;
    :hasText "algorithm" .

:zheng17a_algorithms a :Generic ;
    :hasText "algorithms" .

:zheng17a_data a :Generic ;
    :hasText "data" .

:zheng17a_deep_learning_models a :Method ;
    :hasText "deep_learning_models" .

:zheng17a_family a :Generic ;
    :hasText "family" .

:zheng17a_networks a :Generic ;
    :hasText "networks" .

:zheng17a_one a :Generic ;
    :hasText "one" .

:zheng17a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:zheng17a_other a :Generic ;
    :hasText "other" .

:zheng17a_properties a :Generic ;
    :hasText "properties" .

:zheng17a_results a :Generic ;
    :hasText "results" .

:zheng17a_state a :Generic ;
    :hasText "state" .

:zheng17a_tasks a :Generic ;
    :hasText "tasks" .

:zheng17a_that a :Generic ;
    :hasText "that" .

:zheng17a_training a :Task ;
    :hasText "training" .

:zheng17a_we a :Generic ;
    :hasText "we" .

:zilly17a_architecture a :Generic ;
    :hasText "architecture" .

:zilly17a_complex a :Method ;
    :hasText "complex" .

:zilly17a_corpus a :Material ;
    :hasText "corpus" .

:zilly17a_datasets a :Material ;
    :hasText "datasets" .

:zilly17a_entropy a :Other ;
    :hasText "entropy" .

:zilly17a_experiments a :Generic ;
    :hasText "experiments" .

:zilly17a_functions a :Generic ;
    :hasText "functions" .

:zilly17a_language_modeling a :Task ;
    :hasText "language_modeling" .

:zilly17a_long_short-term_memory a :Method ;
    :hasText "long_short-term_memory" .

:zilly17a_lstm a :Method ;
    :hasText "lstm" .

:zilly17a_modeling a :Task ;
    :hasText "modeling" .

:zilly17a_models a :Generic ;
    :hasText "models" .

:zilly17a_networks a :Generic ;
    :hasText "networks" .

:zilly17a_one a :Generic ;
    :hasText "one" .

:zilly17a_optimization a :Task ;
    :hasCSOEquivalent <https://cso.kmi.open.ac.uk/topics/optimization> ;
    :hasText "optimization" .

:zilly17a_parameters a :Generic ;
    :hasText "parameters" .

:zilly17a_penn_treebank a :Material ;
    :hasText "penn_treebank" .

:zilly17a_per a :Eval ;
    :hasText "per" .

:zilly17a_prediction a :Task ;
    :hasText "prediction" .

:zilly17a_processing a :Generic ;
    :hasText "processing" .

:zilly17a_recurrent_networks a :Method ;
    :hasText "recurrent_networks" .

:zilly17a_recurrent_neural_networks a :Method ;
    :hasText "recurrent_neural_networks" .

:zilly17a_results a :Generic ;
    :hasText "results" .

:zilly17a_tasks a :Generic ;
    :hasText "tasks" .

:zilly17a_that a :Generic ;
    :hasText "that" .

:zilly17a_theoretical_analysis a :Generic ;
    :hasText "theoretical_analysis" .

:zilly17a_this a :Generic ;
    :hasText "this" .

:zilly17a_understanding a :Task ;
    :hasText "understanding" .

:zilly17a_we a :Generic ;
    :hasText "we" .

:zilly17a_wikipedia a :Material ;
    :hasText "wikipedia" .

:BasicLSTMCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:BasicRNNCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:CodeEntity a owl:Class .

:DeviceWrapper a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:DropoutWrapper a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:Function a owl:Class .

:GRUCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:LSTMCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:LSTMStateTuple a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:MultiRNNCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:RNNCell a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:ResidualWrapper a owl:Class ;
    rdfs:subClassOf :rnn_cell .

:all_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:atrous_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:atrous_conv2d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:autograph a owl:Class ;
    rdfs:subClassOf :Module .

:avg_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool1d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool3d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:avg_pool_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:batch_norm_with_global_normalization a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:batch_normalization a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:bias_add a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:bidirectional_dynamic_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:bitwise a owl:Class ;
    rdfs:subClassOf :Module .

:collapse_repeated a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:compat a owl:Class ;
    rdfs:subClassOf :Module .

:compute_accidental_hits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:config a owl:Class ;
    rdfs:subClassOf :Module .

:contrib a owl:Class ;
    rdfs:subClassOf :Module .

:conv1d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv1d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d_backprop_filter a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d_backprop_input a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv2d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv3d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv3d_backprop_filter_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv3d_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:conv_transpose a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:convolution a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:crelu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_beam_search_decoder a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_beam_search_decoder_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_greedy_decoder a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_loss_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:ctc_unique_labels a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:data a owl:Class ;
    rdfs:subClassOf :Module .

:debugging a owl:Class ;
    rdfs:subClassOf :Module .

:depth_to_space a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_backprop_filter a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_backprop_input a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_native a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_native_backprop_filter a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:depthwise_conv2d_native_backprop_input a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:dilation2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:distribute a owl:Class ;
    rdfs:subClassOf :Module .

:distributions a owl:Class ;
    rdfs:subClassOf :Module .

:dropout a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:dtypes a owl:Class ;
    rdfs:subClassOf :Module .

:dynamic_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:elu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:embedding_lookup a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:embedding_lookup_sparse a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:erosion2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:errors a owl:Class ;
    rdfs:subClassOf :Module .

:estimator a owl:Class ;
    rdfs:subClassOf :Module .

:experimental a owl:Class ;
    rdfs:subClassOf :Module .

:feature_column a owl:Class ;
    rdfs:subClassOf :Module .

:fixed_unigram_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:fractional_avg_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:fractional_max_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:fused_batch_norm a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:gfile a owl:Class ;
    rdfs:subClassOf :Module .

:graph_util a owl:Class ;
    rdfs:subClassOf :Module .

:image a owl:Class ;
    rdfs:subClassOf :Module .

:in_top_k a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:initializers a owl:Class ;
    rdfs:subClassOf :Module .

:io a owl:Class ;
    rdfs:subClassOf :Module .

:keras a owl:Class ;
    rdfs:subClassOf :Module .

:l2_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:l2_normalize a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:layers a owl:Class ;
    rdfs:subClassOf :Module .

:leaky_relu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:learned_unigram_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:linalg a owl:Class ;
    rdfs:subClassOf :Module .

:lite a owl:Class ;
    rdfs:subClassOf :Module .

:local_response_normalization a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:log_poisson_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:log_softmax a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:log_uniform_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:logging a owl:Class ;
    rdfs:subClassOf :Module .

:lookup a owl:Class ;
    rdfs:subClassOf :Module .

:losses a owl:Class ;
    rdfs:subClassOf :Module .

:lrn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:manip a owl:Class ;
    rdfs:subClassOf :Module .

:math a owl:Class ;
    rdfs:subClassOf :Module .

:max_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool1d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool3d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_pool_with_argmax a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:max_poolv2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:metrics a owl:Class ;
    rdfs:subClassOf :Module .

:moments a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:nce_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:nest a owl:Class ;
    rdfs:subClassOf :Module .

:normalize_moments a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:profiler a owl:Class ;
    rdfs:subClassOf :Module .

:python_io a owl:Class ;
    rdfs:subClassOf :Module .

:quantization a owl:Class ;
    rdfs:subClassOf :Module .

:quantized_avg_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:quantized_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:quantized_max_pool a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:quantized_relu_x a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:queue a owl:Class ;
    rdfs:subClassOf :Module .

:ragged a owl:Class ;
    rdfs:subClassOf :Module .

:random a owl:Class ;
    rdfs:subClassOf :Module .

:raw_ops a owl:Class ;
    rdfs:subClassOf :Module .

:raw_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:relu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:relu6 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:relu_layer a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:resource_loader a owl:Class ;
    rdfs:subClassOf :Module .

:safe_embedding_lookup_sparse a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sampled_softmax_loss a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:saved_model a owl:Class ;
    rdfs:subClassOf :Module .

:selu a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:separable_conv2d a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sets a owl:Class ;
    rdfs:subClassOf :Module .

:sigmoid a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sigmoid_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:signal a owl:Class ;
    rdfs:subClassOf :Module .

:softmax a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softmax_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softmax_cross_entropy_with_logits_v2 a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softplus a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:softsign a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:space_to_batch a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:space_to_depth a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sparse a owl:Class ;
    rdfs:subClassOf :Module .

:sparse_softmax_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:spectral a owl:Class ;
    rdfs:subClassOf :Module .

:static_bidirectional_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:static_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:static_state_saving_rnn a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:sufficient_statistics a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:tanh a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:test a owl:Class ;
    rdfs:subClassOf :Module .

:top_k a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:tpu a owl:Class ;
    rdfs:subClassOf :Module .

:uniform_candidate_sampler a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:user_ops a owl:Class ;
    rdfs:subClassOf :Module .

:version a owl:Class ;
    rdfs:subClassOf :Module .

:weighted_cross_entropy_with_logits a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:weighted_moments a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:with_space_to_batch a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:xla a owl:Class ;
    rdfs:subClassOf :Module .

:xw_plus_b a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:zero_fraction a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

<https://github.com/deepcurator/DCC/6898-hierarchical-attentive-recurrent-tracking-Figure3-1> a :Figure .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure2-1 a :Figure .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure3-1 a :Figure .

:app a owl:Class ;
    rdfs:subClassOf :Module .

:audio a owl:Class ;
    rdfs:subClassOf :Module .

:decode_wav a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:encode_wav a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:mirhoseini17a-Figure1-1 a :Figure .

:nn a owl:Class ;
    rdfs:subClassOf :Module .

:run a owl:Class ;
    rdfs:subClassOf :TensorFlowDefined .

:strings a owl:Class ;
    rdfs:subClassOf :Module .

:suganuma18a-Figure5-1 a :Figure .

:summary a owl:Class ;
    rdfs:subClassOf :Module .

:sysconfig a owl:Class ;
    rdfs:subClassOf :Module .

:FlattenBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure3-1 a :Figure .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure4-1 a :Figure .

:Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper-Figure1-1 a :Figure .

:hasText a owl:DatatypeProperty ;
    rdfs:domain :Text .

:tf a owl:Class .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure2-1> a :Figure .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure5-1 a :Figure .

:Huang_Arbitrary_Style_Transfer_ICCV_2017_paper-Figure2-1 a :Figure .

:Lu_Neural_Baby_Talk_CVPR_2018_paper-Figure4-1 a :Figure .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure5-1 a :Figure .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure3-1 a :Figure .

:Vo_Revisiting_IM2GPS_in_ICCV_2017_paper-Figure3-1 a :Figure .

:cai18a-Figure2-1 a :Figure .

:cai18a-Figure3-1 a :Figure .

:mirhoseini17a-Figure5-1 a :Figure .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure2-1 a :Figure .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure4-1 a :Figure .

:Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper-Figure1-1 a :Figure .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure3-1 a :Figure .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure3-1 a :Figure .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure4-1 a :Figure .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure1-1 a :Figure .

:Kong_RON_Reverse_Connection_CVPR_2017_paper-Figure4-1 a :Figure .

:Luo_Efficient_Deep_Learning_CVPR_2016_paper-Figure2-1 a :Figure .

:Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper-Figure2-1 a :Figure .

:Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper-Figure2-1 a :Figure .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure8-1 a :Figure .

:Text a owl:Class ;
    rdfs:subClassOf :Modality .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure4-1 a :Figure .

:kalchbrenner17a-Figure2-1 a :Figure .

:Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1 a :Figure .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure3-1 a :Figure .

:Andreas_Neural_Module_Networks_CVPR_2016_paper-Figure1-1 a :Figure .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure3-1 a :Figure .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure4-1 a :Figure .

:Chen_Show_Adapt_and_ICCV_2017_paper-Figure2-1 a :Figure .

:Fan_A_Point_Set_CVPR_2017_paper-Figure2-1 a :Figure .

:Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper-Figure4-1 a :Figure .

:Hui_Fast_and_Accurate_CVPR_2018_paper-Figure3-1 a :Figure .

:Liu_Learning_Efficient_Convolutional_ICCV_2017_paper-Figure2-1 a :Figure .

:Modality a owl:Class .

:Mousavian_3D_Bounding_Box_CVPR_2017_paper-Figure5-1 a :Figure .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure4-1 a :Figure .

:Sung_Learning_to_Compare_CVPR_2018_paper-Figure3-1 a :Figure .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure4-1 a :Figure .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure9-1 a :Figure .

:UnpoolingBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure1-1 a :Figure .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure3-1 a :Figure .

:Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper-Figure2-1 a :Figure .

:Xue_Deep_Texture_Manifold_CVPR_2018_paper-Figure6-1 a :Figure .

:amos17b-Figure1-1 a :Figure .

:brukhim18a-Figure1-1 a :Figure .

:donahue17a-Figure7-1 a :Figure .

<https://github.com/deepcurator/DCC/1802.07191-Figure3-1> a :Figure .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/7192-value-prediction-network-Figure1-1> a :Figure .

:Busta_Deep_TextSpotter_An_ICCV_2017_paper-Figure2-1 a :Figure .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure2-1 a :Figure .

:DeconvBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure4-1 a :Figure .

:Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper-Figure1-1 a :Figure .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure3-1 a :Figure .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure2-1 a :Figure .

:Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper-Figure2-1 a :Figure .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure6-1 a :Figure .

:Nie_Human_Pose_Estimation_CVPR_2018_paper-Figure2-1 a :Figure .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure5-1 a :Figure .

:amos17b-Figure2-1 a :Figure .

<https://github.com/deepcurator/DCC/6860-dual-discriminator-generative-adversarial-nets-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/7081-dropoutnet-addressing-cold-start-in-recommender-systems-Figure1-1> a :Figure .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure3-1 a :Figure .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure4-1 a :Figure .

:Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper-Figure2-1 a :Figure .

:Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper-Figure4-1 a :Figure .

:Lee_CleanNet_Transfer_Learning_CVPR_2018_paper-Figure4-1 a :Figure .

:Liu_Future_Frame_Prediction_CVPR_2018_paper-Figure3-1 a :Figure .

:Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper-Figure4-1 a :Figure .

:Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper-Figure1-1 a :Figure .

:Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper-Figure5-1 a :Figure .

:Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper-Figure3-1 a :Figure .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure6-1 a :Figure .

:TextEntity a owl:Class .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure4-1 a :Figure .

:Yu_MAttNet_Modular_Attention_CVPR_2018_paper-Figure2-1 a :Figure .

:pham18a-Figure4-1 a :Figure .

<https://github.com/deepcurator/DCC/1807.03039v2-Figure2-1> a :Figure .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure3-1> a :Figure .

:Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper-Figure4-1 a :Figure .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure1-1 a :Figure .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure4-1 a :Figure .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure3-1 a :Figure .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure5-1 a :Figure .

:Huang_Densely_Connected_Convolutional_CVPR_2017_paper-Figure2-1 a :Figure .

:Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper-Figure2-1 a :Figure .

:Kostrikov_Surface_Networks_CVPR_2018_paper-Figure2-1 a :Figure .

:Lee_A_Memory_Network_CVPR_2018_paper-Figure4-1 a :Figure .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure3-1 a :Figure .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure3-1 a :Figure .

:Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper-Figure5-1 a :Figure .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure3-1 a :Figure .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure4-1 a :Figure .

:donahue17a-Figure5-1 a :Figure .

:rnn_cell a owl:Class ;
    rdfs:subClassOf :nn .

:shyam17a-Figure1-1 a :Figure .

:suganuma18a-Figure1-1 a :Figure .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure9-1> a :Figure .

<https://github.com/deepcurator/DCC/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/7181-attention-is-all-you-need-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/7237-modulating-early-visual-processing-by-language-Figure1-1> a :Figure .

:Chen_Multi-View_3D_Object_CVPR_2017_paper-Figure3-1 a :Figure .

:Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper-Figure3-1 a :Figure .

:Ehsani_Who_Let_the_CVPR_2018_paper-Figure2-1 a :Figure .

:Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper-Figure1-1 a :Figure .

:Hu_Learning_to_Reason_ICCV_2017_paper-Figure1-1 a :Figure .

:Moitreya_Chatterjee_Diverse_and_Coherent_ECCV_2018_paper-Figure2-1 a :Figure .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure10-1 a :Figure .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure3-1 a :Figure .

:Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper-Figure2-1 a :Figure .

:Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper-Figure2-1 a :Figure .

:Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper-Figure3-1 a :Figure .

:Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper-Figure1-1 a :Figure .

<https://github.com/deepcurator/DCC/1802.07191-Figure15-1> a :Figure .

:Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper-Figure1-1 a :Figure .

:Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper-Figure1-1 a :Figure .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure4-1 a :Figure .

:Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper-Figure4-1 a :Figure .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Table2-1 a :Figure .

:Gupta_Social_GAN_Socially_CVPR_2018_paper-Figure2-1 a :Figure .

:Haris_Deep_Back-Projection_Networks_CVPR_2018_paper-Figure5-1 a :Figure .

:Liu_Structure_Inference_Net_CVPR_2018_paper-Figure3-1 a :Figure .

:Qi_Low-Shot_Learning_With_CVPR_2018_paper-Figure1-1 a :Figure .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure4-1 a :Figure .

:Szegedy_Rethinking_the_Inception_CVPR_2016_paper-Figure7-1 a :Figure .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure4-1 a :Figure .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure5-1 a :Figure .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure5-1 a :Figure .

:Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper-Figure3-1 a :Figure .

:Zhang_Density-Aware_Single_Image_CVPR_2018_paper-Figure2-1 a :Figure .

:pham18a-Figure8-1 a :Figure .

:Chen_Deep_Photo_Enhancer_CVPR_2018_paper-Figure2-1 a :Figure .

:Tai_MemNet_A_Persistent_ICCV_2017_paper-Figure2-1 a :Figure .

:Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper-Figure3-1 a :Figure .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure1-1 a :Figure .

:Yang_DenseASPP_for_Semantic_CVPR_2018_paper-Figure3-1 a :Figure .

:Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper-Figure3-1 a :Figure .

:Zhang_Deep_Mutual_Learning_CVPR_2018_paper-Figure1-1 a :Figure .

:pham18a-Figure3-1 a :Figure .

<https://github.com/deepcurator/DCC/7033-dual-path-networks-Figure1-1> a :Figure .

<https://github.com/deepcurator/DCC/7272-real-time-image-saliency-for-black-box-classifiers-Figure4-1> a :Figure .

:Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper-Figure3-1 a :Figure .

:Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper-Figure2-1 a :Figure .

:Osokin_GANs_for_Biological_ICCV_2017_paper-Figure2-1 a :Figure .

:Tao_Scale-Recurrent_Network_for_CVPR_2018_paper-Figure2-1 a :Figure .

:Xu_Scene_Graph_Generation_CVPR_2017_paper-Figure3-1 a :Figure .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure3-1 a :Figure .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure1-1 a :Figure .

:Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper-Figure5-1 a :Figure .

:Gan_StyleNet_Generating_Attractive_CVPR_2017_paper-Figure2-1 a :Figure .

:Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper-Figure3-1 a :Figure .

:Mascharka_Transparency_by_Design_CVPR_2018_paper-Figure3-1 a :Figure .

<https://github.com/deepcurator/DCC/6642-universal-style-transfer-via-feature-transforms-Figure1-1> a :Figure .

:Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper-Figure1-1 a :Figure .

:Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper-Figure8-1 a :Figure .

:Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper-Figure2-1 a :Figure .

:Hara_Can_Spatiotemporal_3D_CVPR_2018_paper-Figure3-1 a :Figure .

:Marwah_Attentive_Semantic_Video_ICCV_2017_paper-Figure2-1 a :Figure .

:Sergio_Silva_License_Plate_Detection_ECCV_2018_paper-Figure4-1 a :Figure .

:Toderici_Full_Resolution_Image_CVPR_2017_paper-Figure1-1 a :Figure .

:Tu_Learning_Superpixels_With_CVPR_2018_paper-Figure2-1 a :Figure .

:Wang_Learning_a_Discriminative_CVPR_2018_paper-Figure2-1 a :Figure .

:Wang_Recovering_Realistic_Texture_CVPR_2018_paper-Figure3-1 a :Figure .

<https://github.com/deepcurator/DCC/6650-toward-multimodal-image-to-image-translation-Figure2-1> a :Figure .

:Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper-Figure2-1 a :Figure .

:Chen_Query-Guided_Regression_Network_ICCV_2017_paper-Figure2-1 a :Figure .

:Huang_Real-Time_Neural_Style_CVPR_2017_paper-Figure2-1 a :Figure .

:pham18a-Figure5-1 a :Figure .

:pham18a-Figure7-1 a :Figure .

:Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper-Figure3-1 a :Figure .

:FigureComponent a owl:Class .

:Huang_CondenseNet_An_Efficient_CVPR_2018_paper-Figure1-1 a :Figure .

:Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper-Figure2-1 a :Figure .

:Venugopalan_Sequence_to_Sequence_ICCV_2015_paper-Figure2-1 a :Figure .

:Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper-Figure4-1 a :Figure .

:Zhou_EAST_An_Efficient_CVPR_2017_paper-Figure2-1 a :Figure .

<https://github.com/deepcurator/DCC/1802.07191-Figure19-1> a :Figure .

:Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper-Figure5-1 a :Figure .

:Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper-Figure4-1 a :Figure .

:Mentzer_Conditional_Probability_Models_CVPR_2018_paper-Figure2-1 a :Figure .

:Qi_PointNet_Deep_Learning_CVPR_2017_paper-Figure2-1 a :Figure .

:Tai_Image_Super-Resolution_via_CVPR_2017_paper-Figure2-1 a :Figure .

:Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper-Figure2-1 a :Figure .

:Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper-Figure1-1 a :Figure .

:Xie_Genetic_CNN_ICCV_2017_paper-Figure2-1 a :Figure .

:Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper-Figure1-1 a :Figure .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure1-1 a :Figure .

:Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper-Figure4-1 a :Figure .

:Xie_Aggregated_Residual_Transformations_CVPR_2017_paper-Figure3-1 a :Figure .

<https://github.com/deepcurator/DCC/1802.06006v2-Figure7-1> a :Figure .

:DenseBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure4-1 a :Figure .

:Guandao_Yang_A_Unified_Framework_ECCV_2018_paper-Figure4-1 a :Figure .

:wang18h-Figure1-1 a :Figure .

:Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper-Figure3-1 a :Figure .

:Han_Deep_Pyramidal_Residual_CVPR_2017_paper-Figure6-1 a :Figure .

:Qian_Attentive_Generative_Adversarial_CVPR_2018_paper-Figure2-1 a :Figure .

:Coskun_Long_Short-Term_Memory_ICCV_2017_paper-Figure3-1 a :Figure .

:Dong_Style_Aggregated_Network_CVPR_2018_paper-Figure3-1 a :Figure .

:Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper-Figure7-1 a :Figure .

:Chollet_Xception_Deep_Learning_CVPR_2017_paper-Figure5-1 a :Figure .

:NormBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Oh_Fast_Video_Object_CVPR_2018_paper-Figure2-1 a :Figure .

:ConcatBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:OutputBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper-Figure6-1 a :Figure .

:Tran_A_Closer_Look_CVPR_2018_paper-Figure1-1 a :Figure .

:Chung_Lip_Reading_Sentences_CVPR_2017_paper-Figure1-1 a :Figure .

:InputBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:He_Deep_Residual_Learning_CVPR_2016_paper-Figure3-1 a :Figure .

:PoolingBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:ActivationBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:LossBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper-Figure2-1 a :Figure .

:Module a owl:Class ;
    rdfs:subClassOf :tf .

:Yu_Dilated_Residual_Networks_CVPR_2017_paper-Figure5-1 a :Figure .

:train a owl:Class ;
    rdfs:subClassOf :Module .

:LSTMBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:TensorFlowDefined a owl:Class ;
    rdfs:subClassOf :CodeEntity .

:Figure a owl:Class ;
    rdfs:subClassOf :Modality .

:ConvBlock a owl:Class ;
    rdfs:subClassOf :FigureComponent .

:Material a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Publication a owl:Class .

:Method a owl:Class ;
    rdfs:subClassOf :TextEntity .

:Task a owl:Class ;
    rdfs:subClassOf :TextEntity .

[] a owl:AllDisjointClasses ;
    owl:members ( :BasicLSTMCell :BasicRNNCell :DeviceWrapper :DropoutWrapper :GRUCell :LSTMCell :LSTMStateTuple :MultiRNNCell :RNNCell :ResidualWrapper ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :Modality :TextEntity :tf ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :all_candidate_sampler :atrous_conv2d :atrous_conv2d_transpose :avg_pool :avg_pool1d :avg_pool2d :avg_pool3d :avg_pool_v2 :batch_norm_with_global_normalization :batch_normalization :bias_add :bidirectional_dynamic_rnn :collapse_repeated :compute_accidental_hits :conv1d :conv1d_transpose :conv2d :conv2d_backprop_filter :conv2d_backprop_input :conv2d_transpose :conv3d :conv3d_backprop_filter_v2 :conv3d_transpose :conv_transpose :convolution :crelu :ctc_beam_search_decoder :ctc_beam_search_decoder_v2 :ctc_greedy_decoder :ctc_loss :ctc_loss_v2 :ctc_unique_labels :decode_wav :depth_to_space :depthwise_conv2d :depthwise_conv2d_backprop_filter :depthwise_conv2d_backprop_input :depthwise_conv2d_native :depthwise_conv2d_native_backprop_filter :depthwise_conv2d_native_backprop_input :dilation2d :dropout :dynamic_rnn :elu :embedding_lookup :embedding_lookup_sparse :encode_wav :erosion2d :fixed_unigram_candidate_sampler :fractional_avg_pool :fractional_max_pool :fused_batch_norm :in_top_k :l2_loss :l2_normalize :leaky_relu :learned_unigram_candidate_sampler :local_response_normalization :log_poisson_loss :log_softmax :log_uniform_candidate_sampler :lrn :max_pool :max_pool1d :max_pool2d :max_pool3d :max_pool_with_argmax :max_poolv2 :moments :nce_loss :normalize_moments :pool :quantized_avg_pool :quantized_conv2d :quantized_max_pool :quantized_relu_x :raw_rnn :relu :relu6 :relu_layer :run :safe_embedding_lookup_sparse :sampled_softmax_loss :selu :separable_conv2d :sigmoid :sigmoid_cross_entropy_with_logits :softmax :softmax_cross_entropy_with_logits :softmax_cross_entropy_with_logits_v2 :softplus :softsign :space_to_batch :space_to_depth :sparse_softmax_cross_entropy_with_logits :static_bidirectional_rnn :static_rnn :static_state_saving_rnn :sufficient_statistics :tanh :top_k :uniform_candidate_sampler :weighted_cross_entropy_with_logits :weighted_moments :with_space_to_batch :xw_plus_b :zero_fraction ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :app :audio :autograph :bitwise :compat :config :contrib :data :debugging :distribute :distributions :dtypes :errors :estimator :experimental :feature_column :gfile :graph_util :image :initializers :io :keras :layers :linalg :lite :logging :lookup :losses :manip :math :metrics :nest :nn :profiler :python_io :quantization :queue :ragged :random :raw_ops :resource_loader :saved_model :sets :signal :sparse :spectral :strings :summary :sysconfig :test :tpu :train :user_ops :version :xla ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :BasicLSTMCell :BasicRNNCell :DeviceWrapper :DropoutWrapper :GRUCell :LSTMCell :LSTMStateTuple :MultiRNNCell :RNNCell :ResidualWrapper ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :Modality :TextEntity :tf ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :all_candidate_sampler :atrous_conv2d :atrous_conv2d_transpose :avg_pool :avg_pool1d :avg_pool2d :avg_pool3d :avg_pool_v2 :batch_norm_with_global_normalization :batch_normalization :bias_add :bidirectional_dynamic_rnn :collapse_repeated :compute_accidental_hits :conv1d :conv1d_transpose :conv2d :conv2d_backprop_filter :conv2d_backprop_input :conv2d_transpose :conv3d :conv3d_backprop_filter_v2 :conv3d_transpose :conv_transpose :convolution :crelu :ctc_beam_search_decoder :ctc_beam_search_decoder_v2 :ctc_greedy_decoder :ctc_loss :ctc_loss_v2 :ctc_unique_labels :decode_wav :depth_to_space :depthwise_conv2d :depthwise_conv2d_backprop_filter :depthwise_conv2d_backprop_input :depthwise_conv2d_native :depthwise_conv2d_native_backprop_filter :depthwise_conv2d_native_backprop_input :dilation2d :dropout :dynamic_rnn :elu :embedding_lookup :embedding_lookup_sparse :encode_wav :erosion2d :fixed_unigram_candidate_sampler :fractional_avg_pool :fractional_max_pool :fused_batch_norm :in_top_k :l2_loss :l2_normalize :leaky_relu :learned_unigram_candidate_sampler :local_response_normalization :log_poisson_loss :log_softmax :log_uniform_candidate_sampler :lrn :max_pool :max_pool1d :max_pool2d :max_pool3d :max_pool_with_argmax :max_poolv2 :moments :nce_loss :normalize_moments :pool :quantized_avg_pool :quantized_conv2d :quantized_max_pool :quantized_relu_x :raw_rnn :relu :relu6 :relu_layer :run :safe_embedding_lookup_sparse :sampled_softmax_loss :selu :separable_conv2d :sigmoid :sigmoid_cross_entropy_with_logits :softmax :softmax_cross_entropy_with_logits :softmax_cross_entropy_with_logits_v2 :softplus :softsign :space_to_batch :space_to_depth :sparse_softmax_cross_entropy_with_logits :static_bidirectional_rnn :static_rnn :static_state_saving_rnn :sufficient_statistics :tanh :top_k :uniform_candidate_sampler :weighted_cross_entropy_with_logits :weighted_moments :with_space_to_batch :xw_plus_b :zero_fraction ) .

[] a owl:AllDisjointClasses ;
    owl:members ( :app :audio :autograph :bitwise :compat :config :contrib :data :debugging :distribute :distributions :dtypes :errors :estimator :experimental :feature_column :gfile :graph_util :image :initializers :io :keras :layers :linalg :lite :logging :lookup :losses :manip :math :metrics :nest :nn :profiler :python_io :quantization :queue :ragged :random :raw_ops :resource_loader :saved_model :sets :signal :sparse :spectral :strings :summary :sysconfig :test :tpu :train :user_ops :version :xla ) .

