{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge graph generation from scientific text\n",
    "\n",
    "In this jupyter notebook we show the demo for the full end to end framework for knowledge graph generation. We first take the input text which is present in the `./Data/TestData/` folder. This folder contains abstracts of all the papers from the website [paperswithcode.com](https://paperswithcode.com/). We use this as our dataset to create reproducible graphs of different modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The required packages are listed in [the requirements file](./requirements.txt) located under `text2graph` folder. The following command could be used to install these packages in the target Python environment:\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "Detailed Instructions:\n",
    "\n",
    "Below are more detailed sample commands to create a new environment named 't2g' using conda command line, to install the requirements, and to run the notebook using this new environment as its kernel:\n",
    "\n",
    "`conda create --name t2g python=3.6\n",
    "activate t2g\n",
    "cd 'path-to-text2graph-folder'\n",
    "pip install -r requirements.txt\n",
    "python -m ipykernel install --user --name=t2g\n",
    "jupyter notebook`\n",
    "\n",
    "Select the kernel 't2g' in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the libraries\n",
    "\n",
    "Import all the requisite `spacy` libraries. Also instantiate the data folders. The different directories are:\n",
    "\n",
    "* NER Models at `./Models/`\n",
    "* Paper abstracts at `./Data/TestData/`\n",
    "* NER Model outputs at `./Output/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import plac\n",
    "\n",
    "model_dir = './Models/'\n",
    "test_dir = './Data/TestData/'\n",
    "output_dir = './Output/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NER Models\n",
    "\n",
    "Now load the NER models that we saved from the NER pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the abstracts\n",
    "\n",
    "Read the abstracts that are located at the `/Data/TestData/` folder. The abstracts are read and then *tokenized* using the `nltk` function `sent_tokenize`. This creates a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "abstracts = []\n",
    "sentences = []\n",
    "ent_tagged_text = []\n",
    "\n",
    "def tag_entity_text(sentence,text, replacementText):\n",
    "    newString1 = \"\"\n",
    "    for (t,r) in zip(text,replacementText):\n",
    "        newString1 = sentence.replace(t,r)\n",
    "        sentence = newString1\n",
    "    return sentence\n",
    "\n",
    "with open('./Data/TestData/LeNet.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    abstxt = ''.join(str(line) for line in lines)\n",
    "    abstxt = abstxt.lower()\n",
    "    abstracts.append(abstxt)\n",
    "    \n",
    "\n",
    "# tokenize the abstracts into a sentence\n",
    "for abstract in abstracts:\n",
    "    sents = nltk.sent_tokenize(abstract)\n",
    "    for sent in sents:\n",
    "        sentences.append(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NER tagged sentences from our Spacy model\n",
    "\n",
    "Each of these sentences is then sent to `spacy`'s `nlp` function, which tags the entities. A tuple is returned from which we can extract the following:\n",
    "\n",
    "* `.ents` returns all the entities tagged by `spacy`\n",
    "* `.text` gives the entity texts that have been tagged\n",
    "\n",
    "\n",
    "### Convert Spacy tagged sentences to semeval\n",
    "The `spacy` tagged sentences are converted to `semeval` style markup text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<e1>convolutional <e2>neural networks</e2></e1> are are a special kind of multi-layer <e2>neural networks</e2>.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "replacementText = []\n",
    "semeval_tagged = []\n",
    "for sentence in sentences:\n",
    "#     print(sentence + \" : : \")\n",
    "    ner_tagged = nlp(sentence)\n",
    "    tagged_entities = ner_tagged.ents\n",
    "    tuple_length = len(tagged_entities)\n",
    "    semevalify = \"\"\n",
    "    if(tuple_length == 2):\n",
    "        text = []\n",
    "        replacementText = []\n",
    "        for (i,ent) in enumerate(tagged_entities):\n",
    "#             print(str(i) + ent.text)\n",
    "            text.append(ent.text)\n",
    "            replacementText.append('<e' + str(i+1) + '>' + ent.text + '</e' + str(i+1) + '>')\n",
    "            semevalify = tag_entity_text(sentence,text,replacementText)\n",
    "    semeval_tagged.append(semevalify)\n",
    "            \n",
    "semeval_tagged = list(filter(None, semeval_tagged))\n",
    "# print(type(semeval_tagged))\n",
    "for sentence in semeval_tagged:\n",
    "    print(sentence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the saved Relationship classification module\n",
    "\n",
    "The `semeval` tagged list of sentences `semeval_tagged` is then sent to the next module, which will classify the relationship. For that we first import the requisite libraries. Then the saved relationship extraction model is loaded from `/DCC/src/text2graph/model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfradkin\\AppData\\Local\\Continuum\\anaconda3\\envs\\dcc\\lib\\site-packages\\scikit_learn-0.22.1-py3.7-win-amd64.egg\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.preprocessing.label module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\dfradkin\\AppData\\Local\\Continuum\\anaconda3\\envs\\dcc\\lib\\site-packages\\scikit_learn-0.22.1-py3.7-win-amd64.egg\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.21.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from models import KerasTextClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = KerasTextClassifier()\n",
    "clf.load('Models/re_model_latest_auto_ann_run_2019_25_09')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the available relationship classes\n",
    "\n",
    "The following lines of code lists the relationship classes that our model can handle and extract. Since the classes are `label-encoded` we create a dictionary to map them to their actual names. This would be helpful later in knowledge graph triple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Compare\n",
      "1: Conjunction\n",
      "2: Evaluate-for\n",
      "3: Feature-of\n",
      "4: Part-of\n",
      "5: Used-for\n",
      "6: isA\n",
      "7: sameAs\n"
     ]
    }
   ],
   "source": [
    "# Create a label dictionary\n",
    "label_dict = {}\n",
    "for i,c in enumerate(list(clf.encoder.classes_)):\n",
    "    print(str(i) + \": \" + c)\n",
    "    label_dict[i] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the relationship for the classes\n",
    "\n",
    "For the sentences from the abstract, we now classify and predict the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(semeval_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<e1>convolutional <e2>neural networks</e2></e1> are are a special kind of multi-layer <e2>neural networks</e2>.:\n",
      "sameAs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Show predictions side by side\n",
    "for i,sentence in enumerate(semeval_tagged):\n",
    "    print(sentence + \":\\n\" +  label_dict.get(y_pred[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_graph(sentence,predicted_class):\n",
    "    e1 = re.search('<e1>(.+?)</e1>',sentence)\n",
    "    e2 = re.search('<e2>(.+?)</e2>',sentence)\n",
    "    return (e1.group(1),predicted_class,e2.group(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('convolutional <e2>neural networks</e2>', 'sameAs', 'neural networks')\n"
     ]
    }
   ],
   "source": [
    "for i,sentence in enumerate(semeval_tagged):\n",
    "    print(generate_graph(sentence,label_dict.get(y_pred[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
