[{
  "renderDpi": 150,
  "name": "1",
  "page": 5,
  "figType": "Table",
  "regionBoundary": {
    "x1": 77.75999999999999,
    "y1": 66.72,
    "x2": 517.4399999999999,
    "y2": 160.32
  },
  "caption": "Table 1. Comparison of requirements for speaker adaptation and speaker encoding. Cloning time interval assumes 1-10 cloning audios. Inference time is for an average sentence. All assume implementation on a TitanX GPU.",
  "imageText": ["Inference", "time", "?", "0.4?", "0.6", "secs", "Parameters", "per", "speaker", "128", "?", "25", "million", "512", "512", "Data", "Text", "and", "audio", "Audio", "Cloning", "time", "?", "8", "hours", "?", "0.5?", "5", "mins", "?", "1.5?", "3.5", "secs", "?", "1.5?", "3.5", "secs", "Speaker", "adaptation", "Speaker", "encoding", "Approaches", "Embedding-only", "Whole-model", "Without", "?ne-tuning", "With", "?ne-tuning", "Pre-training", "Multi-speaker", "generative", "model"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Table1-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 171.58377075195312,
    "x2": 543.013427734375,
    "y2": 187.94500732421875
  }
}, {
  "renderDpi": 150,
  "name": "6",
  "page": 5,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 311.52,
    "y1": 260.64,
    "x2": 540.0,
    "y2": 464.64
  },
  "caption": "Figure 6. Visualization of estimated speaker embeddings by speaker encoder. The first two principal components of the average speaker embeddings for the speaker encoder with 5 sample count. Only British and North American regional accents are shown as they constitute the majority of the labeled speakers in the VCTK dataset. Please see Appendix E for more detailed analysis.",
  "imageText": ["Average", "North", "American", "Average", "British", "maleAverage", "British", "female", "Average", "British", "Average", "North", "American", "female", "0.3", "Average", "North", "American", "male", "0.2", "0.1", "0.0", "-0.1", "-0.2", "-0.6", "-0.4", "-0.2", "0.0", "0.2", "0.4", "0.6", "Average", "female", "Average", "male", "Average", "British", "maleAverage", "British", "female", "Average", "North", "American", "female", "0.3", "Average", "North", "American", "male", "0.2", "0.1", "0.0", "-0.1", "-0.2", "-0.6", "-0.4", "-0.2", "0.0", "0.2", "0.4", "0.6"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure6-1.png",
  "captionBoundary": {
    "x1": 306.93798828125,
    "y1": 496.5517578125,
    "x2": 543.0119018554688,
    "y2": 556.7490234375
  }
}, {
  "renderDpi": 150,
  "name": "7",
  "page": 10,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 178.07999999999998,
    "y1": 141.12,
    "x2": 418.08,
    "y2": 643.1999999999999
  },
  "caption": "Figure 7. Speaker encoder architecture with intermediate state dimensions. (batch: batch size, Nsamples: number of cloning audio samples |Ask |, T : number of mel spectrograms timeframes, Fmel: number of mel frequency channels, Fmapped: number of frequency channels after prenet, dembedding: speaker embedding dimension). Multiplication operation at the last layer represents inner product along the dimension of cloning samples.",
  "imageText": ["[batch,", "Nsamples,", "T,", "Fmapped]", "[batch,", "Nsamples,", "T,", "Fmapped]", "in", "g", "ce", "ss", "p", "ro", "tr", "al", "Sp", "ec", "in", "g", "ce", "ss", "l", "p", "ro", "p", "o", "ra", "Te", "m", "te", "n", "ti", "o", "n", "s", "at", "am", "p", "le", "g", "s", "n", "in", "C", "lo", "Speaker", "embedding", "Mel", "spectrograms", "ELU", "FC", "ELU", "FC", "×", "Nprenet", "×", "Nconv", "Global", "mean", "pooling", "Temporal", "masking", "?0.5", "×", "[batch,", "Nsamples,", "T,", "Fmel]", "×", "Gated", "linear", "unit", "Convolution", "[batch,", "dembedding]", "FC", "[batch,", "Nsamples]", "Normalize", "[batch,", "Nsamples]", "[batch,", "Nsamples]", "FC", "Softsign", "[batch,", "Nsamples,", "dattn]", "[batch,", "Nsamples,", "dattn]", "es", "va", "lu", "ri", "es", "q", "ue", "ke", "ys", "ELU", "FC", "ELU", "[batch,", "Nsamples,", "dembedding]", "[batch,", "Nsamples,", "Fmapped]", "[batch,", "Nsamples,", "T,", "Fmapped]", "FC", "Multi-head", "attention"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure7-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 663.7677001953125,
    "x2": 541.752685546875,
    "y2": 702.0460205078125
  }
}, {
  "renderDpi": 150,
  "name": "13",
  "page": 14,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 84.96,
    "y1": 214.07999999999998,
    "x2": 506.4,
    "y2": 532.3199999999999
  },
  "caption": "Figure 13. Inferred attention coefficients for the speaker encoder model with Nsamples = 5 vs. lengths of the cloning audio samples. The dashed line corresponds to the case of averaging all cloning audio samples.",
  "imageText": ["nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "s", "ie", "nt", "0.0", "0.2", "0.5", "Co", "ef", "fic", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "s", "ie", "nt", "0.0", "0.2", "0.5", "Co", "ef", "fic", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)", "nt", "s", "fic", "ie", "Co", "ef", "0.5", "0.0", "0.2", "0", "2", "4", "6", "8", "Length", "(sec)"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure13-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 554.2437133789062,
    "x2": 541.4439697265625,
    "y2": 570.60498046875
  }
}, {
  "renderDpi": 150,
  "name": "2",
  "page": 6,
  "figType": "Table",
  "regionBoundary": {
    "x1": 76.8,
    "y1": 66.72,
    "x2": 518.4,
    "y2": 173.28
  },
  "caption": "Table 2. Mean Opinion Score (MOS) evaluations for naturalness with 95% confidence intervals (when training is done with LibriSpeech dataset and cloning is done with the 108 speakers of the VCTK dataset).",
  "imageText": ["Speaker", "encoding:", "without", "?ne-tuning", "2.76±0.10", "2.76±0.09", "2.78±0.10", "2.75±0.10", "2.79±0.10", "Speaker", "encoding:", "with", "?ne-tuning", "2.93±0.10", "3.02±0.11", "2.97±0.1", "2.93±0.10", "2.99±0.12", "Speaker", "adaptation:", "embedding-only", "2.27±0.10", "2.38±0.10", "2.43±0.10", "2.46±0.09", "2.67±0.10", "Speaker", "adaptation:", "whole-model", "2.32±0.10", "2.87±0.09", "2.98±0.11", "2.67±0.11", "3.16±0.09", "Ground-truth", "(at", "16", "KHz)", "4.66±0.06", "Multi-speaker", "generative", "model", "2.61±0.10", "Approach", "Sample", "count", "1", "2", "3", "5", "10"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Table2-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 183.93777465820312,
    "x2": 541.438720703125,
    "y2": 200.29901123046875
  }
}, {
  "renderDpi": 150,
  "name": "3",
  "page": 6,
  "figType": "Table",
  "regionBoundary": {
    "x1": 76.8,
    "y1": 213.6,
    "x2": 518.4,
    "y2": 320.15999999999997
  },
  "caption": "Table 3. Similarity score evaluations with 95% confidence intervals (when training is done with LibriSpeech dataset and cloning is done with the 108 speakers of the VCTK dataset).",
  "imageText": ["Speaker", "encoding:", "without", "?ne-tuning", "2.48±0.10", "2.73±0.10", "2.70±0.11", "2.81±0.10", "2.85±0.10", "Speaker", "encoding:", "with", "?ne-tuning", "2.59±0.12", "2.67±0.12", "2.73±0.13", "2.77±0.12", "2.77±0.11", "Speaker", "adaptation:", "embedding-only", "2.66±0.09", "2.64±0.09", "2.71±0.09", "2.78±0.10", "2.95±0.09", "Speaker", "adaptation:", "whole-model", "2.59±0.09", "2.95±0.09", "3.01±0.10", "3.07±0.08", "3.16±0.08", "Ground-truth:", "same", "speaker", "3.91±0.03", "Ground-truth:", "different", "speakers", "1.52±0.09", "Approach", "Sample", "count", "1", "2", "3", "5", "10"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Table3-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 331.07275390625,
    "x2": 541.4375,
    "y2": 347.4330139160156
  }
}, {
  "renderDpi": 150,
  "name": "11",
  "page": 13,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 72.96,
    "y1": 99.84,
    "x2": 519.84,
    "y2": 269.76
  },
  "caption": "Figure 11. Speaker verification EER using (a) 1 enrollment audio (b) 5 enrollment audios vs. number of cloning audio samples. Multispeaker generative model is trained on a subset of VCTK dataset including 84 speakers, and voice cloning is performed on other 16 speakers. Speaker verification model is trained using the LibriSpeech dataset.",
  "imageText": ["(a)", "(b)", "30", "35", "LibriSpeech", "(unseen", "speakers)", "VCTK", "speaker", "adaption:", "embedding-only", "speaker", "adaption:", "whole-model", "25", "20", "15", "10", "5", "1", "5", "10", "20", "50", "100", "0", "LibriSpeech", "(unseen", "speakers)", "VCTK", "speaker", "adaption:", "embedding-only", "speaker", "adaption:", "whole-model", "%", ")", "e", "(in", "r", "r", "at", "l", "e", "rro", "Eq", "ua", "35", "30", "25", "20", "15", "10", "5", "0", "1", "5", "10", "20", "50", "100", "Number", "of", "samples"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure11-1.png",
  "captionBoundary": {
    "x1": 54.93799591064453,
    "y1": 288.4797668457031,
    "x2": 542.9290771484375,
    "y2": 315.79998779296875
  }
}, {
  "renderDpi": 150,
  "name": "12",
  "page": 13,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 198.72,
    "y1": 413.76,
    "x2": 395.03999999999996,
    "y2": 544.3199999999999
  },
  "caption": "Figure 12. Mean absolute error in embedding estimation vs. the number of cloning audios for a validation set of 25 speakers, shown with the attention mechanism and without attention mechanism (by simply averaging).",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure12-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 559.4827270507812,
    "x2": 541.4451293945312,
    "y2": 575.843994140625
  }
}, {
  "renderDpi": 150,
  "name": "1",
  "page": 2,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 103.67999999999999,
    "y1": 67.67999999999999,
    "x2": 491.03999999999996,
    "y2": 326.4
  },
  "caption": "Figure 1. Speaker adaptation and speaker encoding approaches for training, cloning and audio generation.",
  "imageText": ["Fixed", "Speaker", "adaptation", "Speaker", "encodingTrainable", "n", "at", "io", "en", "er", "io", "g", "A", "ud", "Speaker", "embedding", "Multi-speaker", "generative", "modelText", "Audio", "or", "ni", "ng", "C", "lo", "Cloning", "audios", "Cloning", "audios", "Cloning", "texts", "Cloning", "texts", "Speaker", "embedding", "Multi-speaker", "generative", "model", "Speaker", "embedding", "Multi-speaker", "generative", "model", "Speaker", "identity", "Text", "Audio", "Speaker", "embedding", "Multi-speaker", "generative", "model", "ni", "ng", "Tr", "ai", "Cloning", "audios", "Speaker", "encoder", "model", "Speaker", "embedding", "Cloning", "audios", "Speaker", "encoder", "model", "Text", "Audio", "Speaker", "embedding", "Multi-speaker", "generative", "model", "Cloning", "audios", "Speaker", "embedding", "Speaker", "encoder", "model", "Speaker", "identity", "Text", "Audio", "Speaker", "embedding", "Multi-speaker", "generative", "model", "Speaker", "embedding", "Multi-speaker", "generative", "modelText", "Audio"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure1-1.png",
  "captionBoundary": {
    "x1": 108.36199951171875,
    "y1": 343.10675048828125,
    "x2": 488.5222473144531,
    "y2": 348.5090026855469
  }
}, {
  "renderDpi": 150,
  "name": "2",
  "page": 2,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 96.0,
    "y1": 483.35999999999996,
    "x2": 249.12,
    "y2": 601.4399999999999
  },
  "caption": "Figure 2. Speaker encoder architecture. See Appendix A for details.",
  "imageText": ["PreNet", "Convolution", "Block", "Speaker", "embedding", "Mel", "spectrograms", "×", "Nconv", "Global", "mean", "pooling", "Self", "Attention"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure2-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 622.5897216796875,
    "x2": 290.9275817871094,
    "y2": 638.9509887695312
  }
}, {
  "renderDpi": 150,
  "name": "6",
  "page": 12,
  "figType": "Table",
  "regionBoundary": {
    "x1": 170.88,
    "y1": 339.84,
    "x2": 421.44,
    "y2": 455.03999999999996
  },
  "caption": "Table 6. Hyperparameters of speaker verification model for LibriSpeech dataset.",
  "imageText": ["Max", "gradient", "norm", "100", "Gradient", "clipping", "max.", "value", "5", "Learning", "Rate", "10?3", "Convolution", "layers,", "channels,", "?lter,", "strides", "1,", "64,", "20×", "5,", "8×", "2", "Recurrent", "layer", "size", "128", "Fully", "connected", "size", "128", "Dropout", "probability", "0.9", "Bands", "of", "Mel-spectrogram", "80", "Hop", "length", "400", "Parameter", "Audio", "resampling", "freq.", "16", "KHz"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Table6-1.png",
  "captionBoundary": {
    "x1": 154.1929931640625,
    "y1": 468.56475830078125,
    "x2": 442.6908874511719,
    "y2": 473.9670104980469
  }
}, {
  "renderDpi": 150,
  "name": "10",
  "page": 12,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 194.88,
    "y1": 505.91999999999996,
    "x2": 389.28,
    "y2": 654.24
  },
  "caption": "Figure 10. Speaker verification EER (using 1 enrollment audio) vs. number of cloning audio samples. Multi-speaker generative model and speaker verification model are trained using LibriSpeech dataset. Voice cloing is performed using VCTK dataset.",
  "imageText": ["LibriSpeech", "(unseen", "speakers)", "VCTK", "speaker", "adaption:", "embedding-only", "speaker", "adaption:", "whole-model", "speaker", "encoding:", "without", "fine-tuning", "speaker", "encoding:", "with", "fine-tuning", "%", ")", "e", "(in", "r", "r", "at", "l", "e", "rro", "Eq", "ua", "25", "20", "15", "10", "5", "0", "1", "2", "3", "5", "10", "20", "50", "100", "Number", "of", "samples"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure10-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 672.7077026367188,
    "x2": 541.4425048828125,
    "y2": 689.0689697265625
  }
}, {
  "renderDpi": 150,
  "name": "9",
  "page": 12,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 216.0,
    "y1": 67.67999999999999,
    "x2": 378.24,
    "y2": 306.24
  },
  "caption": "Figure 9. Architecture of speaker verification model.",
  "imageText": ["same", "or", "different", "speaker", "PLDA", "score", "SV", "module", "test", "audio", "Mel", "spectrogram", "pooling", "recurrent", "layer", "Conv.", "block", "enroll", "audios", "x", "N", "ReLU", "batch-nom", "!", "FC", "Mel", "spectrograms", "2D", "convolution"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure9-1.png",
  "captionBoundary": {
    "x1": 203.75399780273438,
    "y1": 322.5367431640625,
    "x2": 393.1282958984375,
    "y2": 327.9389953613281
  }
}, {
  "renderDpi": 150,
  "name": "4",
  "page": 7,
  "figType": "Table",
  "regionBoundary": {
    "x1": 79.67999999999999,
    "y1": 66.72,
    "x2": 515.04,
    "y2": 119.03999999999999
  },
  "caption": "Table 4. Mean Opinion Score (MOS) evaluations for naturalness with 95% confidence intervals (when training is done with 84 speakers of the VCTK dataset and cloning is done with 16 speakers of the VCTK dataset).",
  "imageText": ["Speaker", "adaptation:", "embedding-only", "3.01±0.11", "-", "3.13±0.11", "-", "3.13±0.11", "Speaker", "adaptation:", "whole-model", "2.34±0.13", "2.99±0.10", "3.07±0.09", "3.40±0.10", "3.38±0.09", "Approach", "Sample", "count", "1", "5", "10", "20", "100"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Table4-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 130.53775024414062,
    "x2": 541.4381713867188,
    "y2": 146.89898681640625
  }
}, {
  "renderDpi": 150,
  "name": "5",
  "page": 7,
  "figType": "Table",
  "regionBoundary": {
    "x1": 79.67999999999999,
    "y1": 159.84,
    "x2": 515.04,
    "y2": 213.12
  },
  "caption": "Table 5. Similarity score evaluations with 95% confidence intervals (when training is done with 84 speakers of the VCTK dataset and cloning is done with 16 speakers of the VCTK dataset).",
  "imageText": ["Speaker", "adaptation:", "embedding-only", "2.42±0.13", "-", "2.37±0.13", "-", "2.37±0.12", "Speaker", "adaptation:", "whole-model", "2.55±0.11", "2.93±0.11", "2.95±0.10", "3.01±0.10", "3.14±0.10", "Approach", "Sample", "count", "1", "5", "10", "20", "100"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Table5-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 224.27273559570312,
    "x2": 541.4402465820312,
    "y2": 240.63397216796875
  }
}, {
  "renderDpi": 150,
  "name": "15",
  "page": 16,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 126.72,
    "y1": 121.92,
    "x2": 468.47999999999996,
    "y2": 386.4
  },
  "caption": "Figure 15. Distribution of similarity scores for 1 and 10 sample counts.",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure15-1.png",
  "captionBoundary": {
    "x1": 170.71800231933594,
    "y1": 400.54974365234375,
    "x2": 426.1654052734375,
    "y2": 405.9519958496094
  }
}, {
  "renderDpi": 150,
  "name": "8",
  "page": 11,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 101.75999999999999,
    "y1": 90.72,
    "x2": 492.0,
    "y2": 352.32
  },
  "caption": "Figure 8. The sentences used to generate test samples for the voice cloning models. The white space characters / and % follow the same definition as in (Ping et al., 2017).",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure8-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 366.85675048828125,
    "x2": 541.44189453125,
    "y2": 383.2179870605469
  }
}, {
  "renderDpi": 150,
  "name": "4",
  "page": 4,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 306.71999999999997,
    "y1": 180.0,
    "x2": 546.24,
    "y2": 359.03999999999996
  },
  "caption": "Figure 4. Comparison of speaker adaptation and speaker encoding approaches in term of speaker classification accuracy with different numbers of cloning samples.",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure4-1.png",
  "captionBoundary": {
    "x1": 306.93798828125,
    "y1": 366.2437438964844,
    "x2": 541.440185546875,
    "y2": 393.5639953613281
  }
}, {
  "renderDpi": 150,
  "name": "5",
  "page": 4,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 319.68,
    "y1": 423.84,
    "x2": 522.24,
    "y2": 578.4
  },
  "caption": "Figure 5. Speaker verification (SV) EER (using 5 enrollment audio) for different numbers of cloning samples. Evaluation setup can be found in Section 3.2.5. LibriSpeech (unseen speakers) and VCTK represent EERs estimated from random pairing of utterances from ground-truth datasets respectively.",
  "imageText": ["LibriSpeech", "(unseen", "speakers)", "VCTK", "speaker", "adaption:", "embedding-only", "speaker", "adaption:", "whole-model", "speaker", "encoding:", "without", "fine-tuning", "speaker", "encoding:", "with", "fine-tuning", "%", ")", "e", "(in", "r", "r", "at", "l", "e", "rro", "Eq", "ua", "14", "12", "10", "8", "6", "4", "2", "0", "1", "2", "3", "5", "10", "20", "50", "100", "Number", "of", "samples"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure5-1.png",
  "captionBoundary": {
    "x1": 306.93798828125,
    "y1": 589.3536987304688,
    "x2": 542.931640625,
    "y2": 638.5919799804688
  }
}, {
  "renderDpi": 150,
  "name": "3",
  "page": 4,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 57.599999999999994,
    "y1": 369.59999999999997,
    "x2": 285.12,
    "y2": 559.1999999999999
  },
  "caption": "Figure 3. Performance of whole model adaptation and speaker embedding adaptation for voice cloning in terms of speaker classification accuracy for 108 VCTK speakers. Different numbers of cloning samples and fine-tuning iterations are evaluated.",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure3-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 569.1277465820312,
    "x2": 290.9255065917969,
    "y2": 607.406982421875
  }
}, {
  "renderDpi": 150,
  "name": "14",
  "page": 15,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 68.64,
    "y1": 172.79999999999998,
    "x2": 526.0799999999999,
    "y2": 519.36
  },
  "caption": "Figure 14. First two principal components of the inferred embeddings, with the ground truth labels for gender and region of accent for the VCTK speakers as in (Arik et al., 2017b).",
  "imageText": ["Great", "Britain", "North", "American", "Ireland", "South", "Hemisphere", "Asia", "Female", "Male", "Sample", "count:10", "Sample", "count:10", "Sample", "count:5", "Sample", "count:5", "Sample", "count:3", "Sample", "count:3", "Sample", "count:2", "Sample", "count:2", "Sample", "count:1", "Sample", "count:1"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/NIPS/image/fig_1802.06006v2-Figure14-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 541.0687255859375,
    "x2": 541.4396362304688,
    "y2": 557.4299926757812
  }
}]
