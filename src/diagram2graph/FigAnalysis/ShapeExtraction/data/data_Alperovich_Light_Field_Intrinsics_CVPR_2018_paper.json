[{
  "renderDpi": 150,
  "name": "1",
  "page": 0,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 310.56,
    "y1": 226.56,
    "x2": 543.36,
    "y2": 481.44
  },
  "caption": "Figure 1. Our network jointly separates an input light field into diffuse and specular components, and computes a disparity map for the center view. This figure shows output on a previously unseen light field rendered with Blender.",
  "imageText": ["diffuse", "specular", "center", "view", "disparity"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure1-1.png",
  "captionBoundary": {
    "x1": 308.86273193359375,
    "y1": 494.53521728515625,
    "x2": 545.110107421875,
    "y2": 531.4739990234375
  }
}, {
  "renderDpi": 150,
  "name": "8",
  "page": 5,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 52.8,
    "y1": 333.59999999999997,
    "x2": 284.15999999999997,
    "y2": 405.12
  },
  "caption": "Figure 8. Comparison of different error metrics for specular and diffuse components. Numbers show the average over nine previously unseen test datasets. See section 5 for a description of the metrics. Since Shi et al. [31] does not perform decomposition for the background, we multiply all results and ground truth with object mask before measuring the errors.",
  "imageText": ["Ours", "0.15", "0.11", "0.28", "0.23", "80.08", "81.37", "Alperovich", "[2]", "0.12", "0.45", "0.22", "1.04", "74.98", "48.46", "Sulc", "et", "al.", "[34]", "0.12", "0.47", "0.24", "1.01", "75.43", "47.25", "Shi", "et", "al.", "[31]", "0.34", "0.15", "0.5", "0.39", "63.02", "73.71", "LMSE", "×100", "GMSE", "×100", "SSIM", "×100", "diff.", "spec.", "diff.", "spec.", "diff.", "spec."],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure8-1.png",
  "captionBoundary": {
    "x1": 50.112098693847656,
    "y1": 416.27020263671875,
    "x2": 286.36004638671875,
    "y2": 475.12493896484375
  }
}, {
  "renderDpi": 150,
  "name": "7",
  "page": 5,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 55.68,
    "y1": 75.36,
    "x2": 547.1999999999999,
    "y2": 269.76
  },
  "caption": "Figure 7. We compare our results for disparity on challenging synthetic scenes that feature strong specularities and regions of little texture against state of the art methods for depth estimation. Epecially in regions where the specularity dominates the texture, the other EPI based methods fail, while ACC due to its strong regularization can still yield pleasing (albeit oversmoothed) results. With respect to MSE, our approach outperforms the other methods significantly.",
  "imageText": ["MSE", "×100:", "4.6", "13.4", "14.7", "17.3", "MSE", "×100:", "5.9", "23.1", "30.0", "35.5", "Center", "View", "Ground", "Truth", "Ours", "ACC", "[17]", "EPI1", "[19]", "EPI2", "[42]"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure7-1.png",
  "captionBoundary": {
    "x1": 50.111114501953125,
    "y1": 275.2961120605469,
    "x2": 545.111083984375,
    "y2": 312.23486328125
  }
}, {
  "renderDpi": 150,
  "name": "9",
  "page": 5,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 50.879999999999995,
    "y1": 488.64,
    "x2": 285.12,
    "y2": 537.12
  },
  "caption": "Figure 9. Ablation study: Quantitative comparison of separation over nine previously unseen test datasets, and depth estimation for the two scenes from Figure 7. Note that we compute error for the whole center view, without object mask.",
  "imageText": ["9", "x", "24", "x", "24", "0.28", "0.35", "0.69", "0.85", "57.72", "62.87", "55.87", "19.31", "Original", "0.25", "0.19", "0.64", "0.62", "66.66", "72.75", "5.9", "4.6", "48", "x", "48", "0.33", "0.33", "0.74", "0.73", "56.67", "59.07", "192.7", "167.9", "LMSE", "×100", "GMSE", "×100", "SSIM", "×100", "MSE", "(depth)", "×100", "diff.", "spec.", "diff.", "spec.", "diff.", "spec.", "scene", "1", "scene", "2"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure9-1.png",
  "captionBoundary": {
    "x1": 50.112098693847656,
    "y1": 548.67724609375,
    "x2": 286.3587341308594,
    "y2": 585.615966796875
  }
}, {
  "renderDpi": 150,
  "name": "2",
  "page": 1,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 51.839999999999996,
    "y1": 70.56,
    "x2": 543.36,
    "y2": 139.2
  },
  "caption": "Figure 2. The four images to the left show, from left to right, the center view of the input light field, the diffuse component, the specular component (scaled for better visibility) and the disparity. The EPIs to the right are all taken from the same scan line in the light field, marked white. From top to bottom, they again show the input, the diffuse component, the specular component and the disparity. Since the diffuse component and the disparity correspond to the same projections of the same 3D points, they share the same pattern. However, the specular component behaves differently, as it follows the specular flow [34], which depends on the local surface geometry and view point change in a complex way. In particular, the orientation of the specular lobe in the EPI is different from that of the diffuse texture.",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure2-1.png",
  "captionBoundary": {
    "x1": 50.112098693847656,
    "y1": 154.93724060058594,
    "x2": 545.11279296875,
    "y2": 213.79315185546875
  }
}, {
  "renderDpi": 150,
  "name": "11",
  "page": 6,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 55.68,
    "y1": 334.08,
    "x2": 548.16,
    "y2": 623.04
  },
  "caption": "Figure 11. Two light fields captured with the Lytro Illum plenoptic camera. The first scene consist of a highly specular saxophone and an almost Lambertian koala. Our network successfully detects more specular parts of the saxophone compared to the other methods. While we mis-detect the koala as a specular object similar to [31], our method is the only one where the diffuse part behind the large specular spot on saxophone is not blurred. The second scene has two objects with very small saturated specularity, and only our method is the only one able to separate it. For all other methods, the specularity is still present in the diffuse component. Note that the single image CNN [31] does not perform decomposition for the background, thus it appears black in the visualization.",
  "imageText": ["sp", "ec", "u", "la", "r", "our", "disparity", "map", "d", "if", "fu", "se", "center", "view", "sp", "ec", "u", "la", "r", "our", "disparity", "map", "d", "if", "fu", "se", "center", "view", "Ours", "Alperovich", "et", "al.", "[2]", "Sulc", "et", "al.", "[34]", "Shi", "et", "al.", "[31]"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure11-1.png",
  "captionBoundary": {
    "x1": 50.112098693847656,
    "y1": 638.2362060546875,
    "x2": 545.1135864257812,
    "y2": 697.093017578125
  }
}, {
  "renderDpi": 150,
  "name": "10",
  "page": 6,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 58.559999999999995,
    "y1": 72.48,
    "x2": 548.16,
    "y2": 266.4
  },
  "caption": "Figure 10. Comparison for a synthetic data set with two non-Lambertian objects with almost no texture, which is typically challenging for reflection separation. Both modeling approaches [2] and [34] fail to separate the specular component from the diffuse one. The CNNbased approach [31] successfully separates reflection components, but the diffuse one has some artifacts. In addition, the method requires an object mask, thus its application is limited to objects well separated from the background, which are rarely found in real world scenes.",
  "imageText": ["sp", "ec", "u", "la", "r", "d", "if", "fu", "se", "Ground", "truth", "Ours", "Alperovich", "et", "al.", "[2]", "Sulc", "et", "al.", "[34]", "Shi", "et", "al.", "[31]"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure10-1.png",
  "captionBoundary": {
    "x1": 50.112098693847656,
    "y1": 281.85821533203125,
    "x2": 545.1137084960938,
    "y2": 318.7969970703125
  }
}, {
  "renderDpi": 150,
  "name": "4",
  "page": 2,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 312.0,
    "y1": 70.56,
    "x2": 544.3199999999999,
    "y2": 200.16
  },
  "caption": "Figure 4. The pathways of our deep encoder-decoder network are organized in six groups of three residual blocks each. The first two blocks in each encoder group keep depth and resolution the same, the last block reduces resolution (shown on bottom, viewpoint × spatial coordinates), while increasing feature depth (shown on top) by 32. The decoder paths are exact mirrors of this chain. Disparity is only a 2D decoder, where the view point dimension of the shape is removed. To not overly clutter the figure, the visualization does not show that the encoder and 3D decoders actually operate on two EPI stacks in parallel, the horizontal and vertical one. The feature output of these is briefly joined on the bottom layer, and then decoded again into two separate chains.",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure4-1.png",
  "captionBoundary": {
    "x1": 308.8629455566406,
    "y1": 211.8221893310547,
    "x2": 545.1110229492188,
    "y2": 336.4320068359375
  }
}, {
  "renderDpi": 150,
  "name": "3",
  "page": 2,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 52.8,
    "y1": 72.0,
    "x2": 286.08,
    "y2": 175.2
  },
  "caption": "Figure 3. A single residual block of the network. After batch normalization, a first path leads through a (possibly strided) convolution layer and a leaky ReLU. A second path either keeps the input, or passes it through a strided convolution in case it needs to be resampled. Both paths are added together to produce the final output. The idea is that it is much easier for such blocks to learn the identiy transformation, or perform only small modifications to the input [10], which helps the encoder-decoder paths to gradually add details.",
  "imageText": ["Leaky", "ReLU", "ConvolutionConvolution", "+", "Batch", "normalization", "Output", "Input"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure3-1.png",
  "captionBoundary": {
    "x1": 50.112098693847656,
    "y1": 187.07420349121094,
    "x2": 286.360107421875,
    "y2": 278.80670166015625
  }
}, {
  "renderDpi": 150,
  "name": "12",
  "page": 7,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 60.48,
    "y1": 72.0,
    "x2": 541.4399999999999,
    "y2": 472.79999999999995
  },
  "caption": "Figure 12. Results on unseen light fields from various sources. We show center views of the light fields with diffuse and specular components and estimated disparities. Top: lightfield from the Stanford data set [39], where we have chosen the most challenging case with respect to reflection separation and disparity estimation. Our network, while being trained on synthetic scenes, is able to generalize to real world examples with complicated geometry and reflection. Middle: synthetic scene from light field benchmark [16], where we have selected an object with small specular regions, to evaluate how the network will cope with it. Specularity is successfully from the diffuse part, while preserving texture. Bottom: an example data set from HCI benchmark [43].",
  "imageText": ["center", "view", "disparity", "diffuse", "specular", "b", "e", "[4", "4", "]", "C", "u", "H", "C", "I", "n", "[1", "6", "]", "tt", "o", "C", "o", "ys", "t", "[3", "9", "]", "et", "h", "A", "m", "S", "ta", "n", "fo", "rd"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure12-1.png",
  "captionBoundary": {
    "x1": 50.11210250854492,
    "y1": 485.7519226074219,
    "x2": 545.1178588867188,
    "y2": 544.6087036132812
  }
}, {
  "renderDpi": 150,
  "name": "5",
  "page": 3,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 60.96,
    "y1": 79.67999999999999,
    "x2": 532.3199999999999,
    "y2": 157.44
  },
  "caption": "Figure 5. Visualization of horizontal (left) and vertical (right) EPI stacks used as input to our network. To achieve the actual spatial input resolution of 48 × 48, they need to be cut out from the above epipolar volumes. Note that although both stacks are three dimensional, they use images along different directions of view points. In effect, those two stacks assemble a crosshair of views around the center view, which is thus the only view present in both stacks.",
  "imageText": [],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure5-1.png",
  "captionBoundary": {
    "x1": 50.112098693847656,
    "y1": 171.6962127685547,
    "x2": 545.1134643554688,
    "y2": 208.6337890625
  }
}, {
  "renderDpi": 150,
  "name": "6",
  "page": 4,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 98.88,
    "y1": 71.52,
    "x2": 497.28,
    "y2": 195.35999999999999
  },
  "caption": "Figure 6. Network losses for different groups of datasets at convergence. The datasets most difficult to fit for the autoencoder are the ones from gantries, perhaps due to minimally uneven sampling of viewpoints which has not been properly corrected. Depth reconstruction on our own synthetic dataset is surprisingly easier than for the benchmark datasets, although it has much stronger specularity. However, the geometry of our objects is also substantially simpler, and the datasets have large regions of easy to fit planes. Overall, disparity MSE on the benchmark validation is around the current benchmark average, which is 6.29. However, our model is not specifically optimized for depth reconstruction, and in particular trained for non-Lambertian scenes, on which it can perform much more robustly than competing methods, see Figure 7.",
  "imageText": ["Average", "0.8702", "1.577", "1.511", "3.867", "0.8054", "1.456", "1.393", "3.6915", "Lytro", "Illum", "0.606", "–", "–", "–", "0.574", "–", "–", "–", "Stanford", "[39]", "1.045", "–", "–", "–", "0.919", "–", "–", "–", "HCI", "[43]", "1.230", "–", "–", "–", "1.150", "–", "–", "–", "Real-world", "Benchmark", "[16]", "0.860", "–", "–", "6.114", "0.816", "–", "–", "5.964", "Ours", "0.610", "1.577", "1.511", "1.620", "0.568", "1.456", "1.393", "1.419", "Synthetic", "Dataset", "AE", "diffuse", "specular", "disparity", "AE", "diffuse", "specular", "disparity", "L", "2-loss", "times", "100,", "validation", "data", "L2-loss", "times", "100,", "training", "data"],
  "renderURL": "C:/Aditi/ProjectWork/DARPA_ASKE/DATASET/pwc_edited_plt_withImage/CVPR/image/fig_Alperovich_Light_Field_Intrinsics_CVPR_2018_paper-Figure6-1.png",
  "captionBoundary": {
    "x1": 50.111297607421875,
    "y1": 206.7541961669922,
    "x2": 545.1124267578125,
    "y2": 276.5672607421875
  }
}]