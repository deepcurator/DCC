texar|rewards|discount|2d|losses|reward|py def _discount_reward_py_2d(reward, sequence_length=None, discount=1.0, dtype=None): if sequence_length is not None: reward = mask_sequences(reward, sequence_length, dtype=dtype) dtype = dtype or reward.dtype if discount == 1.0: disc_reward = np.cumsum(reward[:, ::-1], axis=1, dtype=dtype)[:, ::-1] else: disc_reward = np.copy(reward) for i in range(reward.shape[1] - 2, -1, -1): disc_reward[:, (i)] += disc_reward[:, (i + 1)] * discount return disc_reward 
meanteacher|worker|fn|LA|init|train|unlabel|certainty def worker_init_fn(worker_id): random.seed(args.seed + worker_id) 
AffineCouplingSdnEx4|det|log|and|jacobian|inverse def _inverse_and_log_det_jacobian(self, y, yy, nlf0=None, nlf1=None, iso= None, cam=None): scale = sdn_model_params_ex4(yy, iso, self.gain_init) x = y if scale is not None: x /= scale if scale is None: log_abs_det_J_inv = tf.constant(0.0, dtype=y.dtype, name='ildj') else: log_abs_det_J_inv = -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) if self._last_layer: return tf.layers.flatten(x), log_abs_det_J_inv return x, log_abs_det_J_inv 
adamax|tf|slots|create|AdamaxOptimizer|utils def _create_slots(self, var_list): for v in var_list: self._zeros_slot(v, 'm', self._name) self._zeros_slot(v, 'v', self._name) 
texar|multi|MultiAlignedData|aligned|make|data def _make_data(self): self._vocab = self.make_vocab(self._hparams.datasets) self._embedding = self.make_embedding(self._hparams.datasets, self._vocab) dataset = self._make_dataset() dataset, dataset_size = self._shuffle_dataset(dataset, self._hparams, self._hparams.datasets[0].files) self._dataset_size = dataset_size data_spec = dsutils._DataSpec(dataset=dataset, dataset_size=self. _dataset_size, vocab=self._vocab, embedding=self._embedding) dataset, data_spec = self._process_dataset(dataset, self._hparams, data_spec) self._data_spec = data_spec self._decoder = data_spec.decoder length_fn = self._make_bucket_length_fn() padded_shapes = self._make_padded_shapes(dataset, self._decoder) dataset = self._make_batch(dataset, self._hparams, length_fn, padded_shapes ) if self._hparams.prefetch_buffer_size > 0: dataset = dataset.prefetch(self._hparams.prefetch_buffer_size) self._dataset = dataset 
sidd|one|sample|utils|patch def sample_one_patch(input_image, gt_image, var_image, patch_height, patch_width): H = input_image.shape[1] W = input_image.shape[2] i = np.random.randint(0, H - patch_height + 1) j = np.random.randint(0, W - patch_width + 1) input_patch = input_image[:, i:i + patch_height, j:j + patch_width, :] gt_patch = gt_image[:, i:i + patch_height, j:j + patch_width, :] return input_patch, gt_patch, None 
evaluation|divide|translate def divide(x, y): with np.errstate(divide='ignore', invalid='ignore'): z = np.true_divide(x, y) z[~np.isfinite(z)] = 0 return z 
scannet|train def train(): trainset = input_fn(trainlist, BATCH_SIZE, 10000) train_iterator = trainset.make_initializable_iterator() next_train_element = train_iterator.get_next() testset = input_fn(vallist, BATCH_SIZE, 10000) test_iterator = testset.make_initializable_iterator() next_test_element = test_iterator.get_next() with tf.device('/gpu:0'): input_pl, label_pl, inner_label_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT) training_pl = tf.placeholder(tf.bool, shape=()) global_step = tf.Variable(0, trainable=False, name='global_step') pred, end_points = MODEL.get_model(input_pl, training_pl, config= net_config) MODEL.get_loss(pred, label_pl, end_points, inner_label_pl) if net_config.weight_decay is not None: reg_loss = tf.multiply(tf.losses.get_regularization_loss(), net_config.weight_decay, name='reg_loss') tf.add_to_collection('losses', reg_loss) losses = tf.get_collection('losses') total_loss = tf.add_n(losses, name='total_loss') tf.summary.scalar('total_loss', total_loss) for l in (losses + [total_loss]): tf.summary.scalar(l.op.name, l) correct = tf.equal(tf.argmax(pred, 2, output_type=tf.int32), tf. cast(label_pl, tf.int32)) accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float( BATCH_SIZE) tf.summary.scalar('accuracy', accuracy) print('--- Get training operator') learning_rate = get_learning_rate(global_step) tf.summary.scalar('learning_rate', learning_rate) if OPTIMIZER == 'momentum': optimizer = tf.train.MomentumOptimizer(learning_rate, momentum= MOMENTUM, use_nesterov=True) elif OPTIMIZER == 'adam': optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=0.0001) update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) with tf.control_dependencies(update_ops): train_op = optimizer.minimize(total_loss, global_step=global_step) saver = tf.train.Saver(max_to_keep=500) n = len([n.name for n in tf.get_default_graph().as_graph_def().node]) print('*****************The Graph has %d nodes*****************' % n) config = tf.ConfigProto() config.gpu_options.allow_growth = True config.allow_soft_placement = True config.log_device_placement = False init = tf.group(tf.global_variables_initializer(), tf. local_variables_initializer()) if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR) with tf.Session(config=config) as sess: merged = tf.summary.merge_all() train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph) test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph) sess.run(init) latest_ckpt = tf.train.latest_checkpoint(LOG_DIR) print(FLAGS.load_ckpt) if FLAGS.load_ckpt is not None: saver.restore(sess, FLAGS.load_ckpt) print('{}-Checkpoint loaded from {}!'.format(datetime.now(), FLAGS.load_ckpt)) elif latest_ckpt: print('{}-Found checkpoint {}'.format(datetime.now(), latest_ckpt)) saver.restore(sess, latest_ckpt) print('{}-Checkpoint loaded from {} (Iter {})'.format(datetime. now(), latest_ckpt, sess.run(global_step))) ops = {'input_pl': input_pl, 'label_pl': label_pl, 'inner_label_pl': inner_label_pl, 'training_pl': training_pl, 'pred': pred, 'loss': total_loss, 'train_op': train_op, 'merged': merged, 'global_step': global_step, 'end_points': end_points} if latest_ckpt: checkpoint_epoch = int(latest_ckpt.split('-')[-1]) + 1 elif FLAGS.load_ckpt is not None: checkpoint_epoch = 0 for epoch in range(checkpoint_epoch, MAX_EPOCH): log_string('**** EPOCH %03d ****' % epoch) sys.stdout.flush() print('learning rate:', sess.run(learning_rate)) print('global step:', sess.run(global_step)) sess.run(train_iterator.initializer) train_one_epoch(sess, ops, next_train_element, train_writer) log_string(str(datetime.now())) log_string('---- EPOCH %03d EVALUATION ----' % epoch) sess.run(test_iterator.initializer) eval_one_epoch(sess, ops, next_test_element, test_writer) save_path = saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt' ), global_step=epoch) log_string('Model saved in file: %s' % save_path) 
hparams|from|overrides|hyperparameters|load def load_from_hparams_overrides(params, params_source, hparams_overrides): """Given a dictionary of hyperparameters and a list of overrides, merge them.  Args: params: Python dict containing a base hyperparameters set. params_source: Python dictionary to record source of hyperparameters. hparams_overrides: Python list of strings. This is a set of k=v overrides for the hyperparameters in `params`; if `k=v1` in `params` but `k=v2` in `hparams_overrides`, the second value wins and the value for `k` is `v2`.  Returns: Python dict of hyperparameters. """ if params is None: raise ValueError( 'Input dictionary is empty. It is expected to be loaded with default values' ) if not isinstance(params, dict): raise ValueError( 'The base hyperparameters set must be a Python dict, was: {}'. format(type(params))) if hparams_overrides is None: return params, params_source if isinstance(hparams_overrides, six.string_types): hparams_overrides = [hparams_overrides] if not isinstance(hparams_overrides, list): raise ValueError( 'Expected that hparams_overrides would be `None`, a single string, or a list of strings, was: {}' .format(type(hparams_overrides))) for kv_pair in hparams_overrides: if not isinstance(kv_pair, six.string_types): raise ValueError( 'Expected that hparams_overrides would contain Python list of strings, but encountered an item: {}' .format(type(kv_pair))) key, value = kv_pair.split('=') parser = type(params[key]) if parser is bool: params[key] = value not in ('0', 'False', 'false') else: params[key] = parser(value) params_source[key] = 'Command-line `hparams` flag' return params, params_source 
minibatch|sidd|load|utils def load_minibatch(dir1, id1): mb = np.load(os.path.join(dir1, 'mb_%08d.npy' % id1)).item() meta = np.load(os.path.join(dir1, 'mb_%08d_meta.npy' % id1)) mb['metadata'] = meta return mb 
preprocessing|add|label|DataPreprocess|image|no|data def add_image_no_label(self, test_data_class): for key in test_data_class: for idx in test_data_class[key]: test_data_class[key][idx] = reshape_func(test_data_class[key][ idx], self.subcarrier_spacing) self.no_label_test = test_data_class 
dense|ops def dense(x, inputFeatures, outputFeatures, scope=None, with_w=False): with tf.variable_scope(scope or 'Linear'): matrix = tf.get_variable('Matrix', [inputFeatures, outputFeatures], tf.float32, tf.random_normal_initializer(stddev=0.02)) bias = tf.get_variable('bias', [outputFeatures], initializer=tf. constant_initializer(0.0)) if with_w: return tf.matmul(x, matrix) + bias, matrix, bias else: return tf.matmul(x, matrix) + bias 
deepMOT|SiamRPNBIG|models|init|master|siamrpn def __init__(self): super(SiamRPNBIG, self).__init__(size=2) self.cfg = {'lr': 0.295, 'window_influence': 0.42, 'penalty_k': 0.055, 'instance_size': 271, 'adaptive': True} 
GRUCell|rnn|init|translate def __init__(self, num_units, activation=None, reuse=None, kernel_initializer=None, bias_initializer=None, layer_norm=False): super(GRUCell, self).__init__(_reuse=reuse) self._num_units = num_units self._activation = activation or tf.nn.tanh self._kernel_initializer = kernel_initializer self._bias_initializer = bias_initializer self._layer_norm = layer_norm 
tests|get|test|data|utils def get_test_data(sample_size=1000, sparse_feature_num=1, dense_feature_num =1, sequence_feature=('max', 'mean', 'sum'), classification=True, include_length=False, hash_flag=False): feature_dim_dict = {'sparse': [], 'dense': [], 'sequence': []} for i in range(sparse_feature_num): dim = np.random.randint(1, 10) feature_dim_dict['sparse'].append(SingleFeat('sparse_' + str(i), dim, hash_flag, tf.int32)) for i in range(dense_feature_num): feature_dim_dict['dense'].append(SingleFeat('dense_' + str(i), 0, False, tf.float32)) for i, mode in enumerate(sequence_feature): dim = np.random.randint(1, 10) maxlen = np.random.randint(1, 10) feature_dim_dict['sequence'].append(VarLenFeat('sequence_' + str(i), dim, maxlen, mode)) sparse_input = [np.random.randint(0, dim, sample_size) for feat, dim, _, _ in feature_dim_dict['sparse']] dense_input = [np.random.random(sample_size) for _ in feature_dim_dict[ 'dense']] sequence_input = [] sequence_len_input = [] for var in feature_dim_dict['sequence']: s_input, s_len_input = gen_sequence(var.dimension, var.maxlen, sample_size) sequence_input.append(s_input) sequence_len_input.append(s_len_input) if classification: y = np.random.randint(0, 2, sample_size) else: y = np.random.random(sample_size) x = sparse_input + dense_input + sequence_input if include_length: x += sequence_len_input return x, y, feature_dim_dict 
sent|topK|init|find def init_topK(K, emb_in, emb_out): source_index = tf.placeholder(tf.int32) source_vec = tf.nn.l2_normalize(tf.nn.embedding_lookup(emb_in, source_index), axis=0) source_vec = tf.expand_dims(source_vec, axis=0) dot_prod = tf.reduce_sum(tf.multiply(source_vec, tf.nn.l2_normalize( emb_out, axis=1)), axis=1) values, indices = tf.nn.top_k(dot_prod, k=K) return source_index, indices 
xlnet|estimator|tpu|Inputs|master|dataset @property def dataset(self): return self._dataset 
npy2ckpt|save def save(saver, sess, logdir): model_name = 'model.ckpt' checkpoint_path = os.path.join(logdir, model_name) if not os.path.exists(logdir): os.makedirs(logdir) saver.save(sess, checkpoint_path, write_meta_graph=False) print('The weights have been converted to {}.'.format(checkpoint_path)) 
devtools|cleverhans|NumpyDocString|tests|docscrape|index|str def _str_index(self): idx = self['index'] out = [] out += ['.. index:: %s' % idx.get('default', '')] for section, references in six.iteritems(idx): if section == 'default': continue out += ['   :%s: %s' % (section, ', '.join(references))] return out 
get|fasterai|visualize|video|colorizer def get_video_colorizer(render_factor: int=21) ->VideoColorizer: return get_stable_video_colorizer(render_factor=render_factor) 
gym|actual|lookup|pycolab|Palette|engine def _actual_lookup(self, key, error): """Helper: perform character validation and conversion to numeric value.""" if key in self._ALIASES: key = self._ALIASES[key] if key in self._legal_characters: return ord(key) raise error( '{} is not a legal character in this Palette; legal characters are {}.' .format(key, list(self._legal_characters))) 
rho|BinaryFullyConnectedLayer|layers|set def set_rho(self, session, new_rate): """Op to set rho  Arguments: session {tf session} -- Session new_rate {np float} -- new rhos """ session.run(self.set_rho_op, {self.rho_ph: new_rate}) 
term|topk|prepare|preprocess|d2d def topk_term(df_file, corpus_file, output_file, nb_docs, topk): """Get top-k tf-idf weighting terms for each document  Args: df_file: format: docno	df\\cf corpus_file: output_file: format: docno	doclen	term nb_docs: number of total documents in the collection topk: number of top terms  Returns:  """  def representsInt(s): try: int(s) return True except ValueError: return False df_map = df_map_from_file(df_file)  def formatted(docno, doclen, term_list): term_string = ' '.join(term_list) line = '{0}\t{1}\t{2}'.format(docno, doclen, term_string) return line info_list = [] i = 0 with open(corpus_file, 'r') as f: for line in f: if i % 1000 == 0: logging.info('doc {0}'.format(i)) i += 1 term_list = [] score_list = [] docno, doclen, raw_content = line.strip().split('\t') term_count_pair = Counter(raw_content.split()).most_common() for term, count in term_count_pair: count = int(count) if df_map.get(term) == None: pass else: df = df_map.get(term) idf = np.log((nb_docs - df + 0.5) / (df + 0.5)) tfidf = count * idf score_list.append(tfidf) term_list.append(term) tuple_list = zip(term_list, score_list) sorted_tuple_list = sorted(tuple_list, key=lambda x: x[1], reverse=True) topk_terms = zip(*sorted_tuple_list)[0] qualified_terms = [] index = 0 while len(qualified_terms) < topk and index < len(topk_terms): t = topk_terms[index] if not representsInt(t) and len(t) >= 2: qualified_terms.append(t) index += 1 if len(qualified_terms) < topk: logging.warn( 'document {0} does not contain enough terms, totally {1}' .format(docno, len(qualified_terms))) posting = formatted(docno, doclen, qualified_terms) info_list.append(posting) res = '\n'.join(info_list) with open(output_file, 'w') as f: f.write(res) 
string|14|log|tfrecord|make|ruemonge2 def log_string(LOG_FOUT, out_str): LOG_FOUT.write(out_str + '\n') LOG_FOUT.flush() print(out_str) 
train|Trainer|trainer def train(self): log.infov('Training Starts!') output_save_step = 1000 for n_updates in range(1, 1 + self.config.max_steps): step, summary, loss, step_time = self.run_single_step() self.summary_writer.add_summary(summary, global_step=step) if n_updates % 100 == 0: self.evaluate_step(n_updates) if n_updates == 1 or n_updates % output_save_step == 0: if self.config.save: save_path = self.saver.save(self.session, os.path.join(self .train_dir, 'model'), global_step=step) ground_truth, samples = self.sample_step(1000) ax = sns.kdeplot(ground_truth[:, (0)], ground_truth[:, (1)], cmap='Reds') plt.scatter(samples[:, (0)], samples[:, (1)], color='b', alpha=0.5, s=10) plt.savefig('%s/step-%d.png' % (self.fig_dir, n_updates)) plt.close() self.eval_run() self.saver.save(self.session, os.path.join(self.train_dir, 'model'), global_step=step) self.eval_run() 
preprocess|Patch def Patch(height_index, width_index): height_slice = slice(height_index, height_index + PATCH_SIZE) width_slice = slice(width_index, width_index + PATCH_SIZE) patch = input_mat[:, (height_slice), (width_slice)] mean_normalized_patch = [] for i in range(patch.shape[0]): mean_normalized_patch.append(patch[i] - MEAN_ARRAY[i]) return np.array(mean_normalized_patch).astype(datatype) 
normalize|scannet|xyz|SPH3D def normalize_xyz(points): min_xyz = tf.reduce_min(points, axis=1, keepdims=True) max_xyz = tf.reduce_max(points, axis=1, keepdims=True) center = (max_xyz + min_xyz) / 2 xy = points[:, :, 0:2] - center[:, :, 0:2] z = points[:, :, 2:] points = tf.concat((xy, z), axis=2) return points 
tangents|stax|fn|neural|init|GlobalAvgPool def init_fn(rng, input_shape): output_shape = input_shape[0], input_shape[-1] return output_shape, () 
process|main def main(): with open('train') as finp: lines = finp.read().strip().replace('\n', '<eos>') words = lines.split(' ') vocab, index = {}, {} for word in sorted(words): if word not in vocab: index[len(vocab)] = word vocab[word] = len(vocab) print('vocab size: {}'.format(len(vocab))) x_train = [vocab[word] for word in words] + [vocab['<eos>']] x_train = np.array(x_train, dtype=np.int32) with open('valid') as finp: lines = finp.read().strip().replace('\n', '<eos>') words = lines.split(' ') x_valid = [vocab[word] for word in words] + [vocab['<eos>']] x_valid = np.array(x_valid, dtype=np.int32) with open('test') as finp: lines = finp.read().strip().replace('\n', '<eos>') words = lines.split(' ') x_test = [vocab[word] for word in words] + [vocab['<eos>']] x_test = np.array(x_test, dtype=np.int32) print('train size: {}'.format(np.size(x_train))) print('valid size: {}'.format(np.size(x_valid))) print('test size: {}'.format(np.size(x_test))) with open('ptb.pkl', 'w') as fout: pickle.dump((x_train, x_valid, x_test, vocab, index), fout, protocol=2) 
models|afm|deepctr|AFM def AFM(feature_dim_dict, embedding_size=8, use_attention=True, attention_factor=8, l2_reg_linear=1e-05, l2_reg_embedding=1e-05, l2_reg_att=1e-05, afm_dropout=0, init_std=0.0001, seed=1024, task='binary' ): """Instantiates the Attentonal Factorization Machine architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param use_attention: bool,whether use attention or not,if set to ``False``.it is the same as **standard Factorization Machine** :param attention_factor: positive integer,units in attention net :param l2_reg_linear: float. L2 regularizer strength applied to linear part :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_att: float. L2 regularizer strength applied to attention net :param afm_dropout: float in [0,1), Fraction of the attention net output units to dropout. :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) deep_emb_list, linear_emb_list, dense_input_dict, inputs_list = ( preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True)) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) fm_input = concat_fun(deep_emb_list, axis=1) if use_attention: fm_logit = AFMLayer(attention_factor, l2_reg_att, afm_dropout, seed)( deep_emb_list) else: fm_logit = FM()(fm_input) final_logit = tf.keras.layers.add([linear_logit, fm_logit]) output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
node|eval|get|visualization|image|valid|metric|is def is_valid_node(node): if node == constants.STOP_NODE_NAME or node == constants.INVALID_NODE_NAME: return False return True 
construct|update|graph|delta|build|patch def update_delta(delta, i): tf.import_graph_def(gdef_1, input_map={input_name: tf.get_default_graph ().get_tensor_by_name(input_name) + delta}) graph = tf.get_default_graph() image = graph.get_tensor_by_name(input_name) fx = graph.get_tensor_by_name(func_name) fxd = graph.get_tensor_by_name('while/import/' + func_name) loss = tf.norm(fxd - fx) loss = loss - alpha * tf.norm(tf.reshape(delta, [-1])) relu = tf.reduce_sum(tf.nn.relu(image + delta - image_bounds[0]) + tf. nn.relu(tf.fill(tf.shape(image), image_bounds[1]) - image - delta)) loss = loss + beta * relu new_delta = delta - step_size * tf.squeeze(tf.gradients(loss, delta), 0) new_delta = tf.pad(new_delta[:, loc[0]:loc[2], loc[1]:loc[3], :] * dog_mask_resized, tf.constant([[0, 0], [loc[0], 224 - loc[2]], [loc [1], 224 - loc[3]], [0, 0]])) return new_delta, i + 1 
get|official|flags|scale|loss|utils|performance def get_loss_scale(flags_obj): if flags_obj.loss_scale is not None: return flags_obj.loss_scale return DTYPE_MAP[flags_obj.dtype][1] 
construct|linear|preprocessing|feed|gae|dict def construct_feed_dict(adj_normalized, adj, features, placeholders): feed_dict = dict() feed_dict.update({placeholders['features']: features}) feed_dict.update({placeholders['adj']: adj_normalized}) feed_dict.update({placeholders['adj_orig']: adj}) return feed_dict 
HVEDApplication|initialise|extensions|evaluator|U|hved|application|u def initialise_evaluator(self, eval_param): self.eval_param = eval_param self.evaluator = SegmentationEvaluator(self.readers[0], self. segmentation_param, eval_param) 
python|grpc|OpsTest|device|ops|test|tf|same|tpu|seed|rl|master|function|foo @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): with tf.device('/device:GPU:0'): b = x + get_a_plus_one() return b + 1 
actor|problem|select|action|R2RProblem|r2r def select_actor_action(self, env_output, agent_output): oracle_next_action = env_output.observation[constants.ORACLE_NEXT_ACTION] oracle_next_action_indices = tf.where(tf.equal(env_output.observation[ constants.CONN_IDS], oracle_next_action)) oracle_next_action_idx = tf.reduce_min(oracle_next_action_indices) assert self._mode, 'mode must be set.' if self._mode == 'train': if self._loss_type == common.CE_LOSS: action_idx = oracle_next_action_idx elif self._loss_type == common.AC_LOSS: action_idx = tfp.distributions.Categorical(logits=agent_output. policy_logits).sample() else: raise ValueError('Unsupported loss type {}'.format(self._loss_type) ) else: action_idx = tf.argmax(agent_output.policy_logits, axis=-1) action_val = env_output.observation[constants.CONN_IDS][action_idx] return common.ActorAction(chosen_action_idx=int(action_idx.numpy()), oracle_next_action_idx=int(oracle_next_action_idx.numpy())), int( action_val.numpy()) 
image|borealisflows|save|graphics def save_image(x, path): im = Image.fromarray(x) im.save(path, optimize=True) return 
generator|get|mnist|tflib|epoch def get_epoch(): rng_state = numpy.random.get_state() numpy.random.shuffle(images) numpy.random.set_state(rng_state) numpy.random.shuffle(targets) if n_labelled is not None: numpy.random.set_state(rng_state) numpy.random.shuffle(labelled) image_batches = images.reshape(-1, batch_size, 784) target_batches = targets.reshape(-1, batch_size) if n_labelled is not None: labelled_batches = labelled.reshape(-1, batch_size) for i in xrange(len(image_batches)): yield numpy.copy(image_batches[i]), numpy.copy(target_batches[i] ), numpy.copy(labelled) else: for i in xrange(len(image_batches)): yield numpy.copy(image_batches[i]), numpy.copy(target_batches[i]) 
utils|circmedian def circmedian(angs, unit='rad'): pdists = angs[(np.newaxis), :] - angs[:, (np.newaxis)] if unit == 'rad': pdists = (pdists + np.pi) % (2 * np.pi) - np.pi elif unit == 'deg': pdists = (pdists + 180) % 360.0 - 180.0 pdists = np.abs(pdists).sum(1) if len(angs) % 2 != 0: return angs[np.argmin(pdists)] else: index_of_min = np.argmin(pdists) min1 = angs[index_of_min] new_pdists = np.delete(pdists, index_of_min) new_angs = np.delete(angs, index_of_min) min2 = new_angs[np.argmin(new_pdists)] if unit == 'rad': return scipy.stats.circmean([min1, min2], high=np.pi, low=-np.pi) elif unit == 'deg': return scipy.stats.circmean([min1, min2], high=180.0, low=-180.0) 
lrp|models|model|graph|rnnsearch|thumt def model_graph(features, labels, params): src_vocab_size = len(params.vocabulary['source']) tgt_vocab_size = len(params.vocabulary['target']) with tf.variable_scope('source_embedding'): src_emb = tf.get_variable('embedding', [src_vocab_size, params. embedding_size]) src_bias = tf.get_variable('bias', [params.embedding_size]) src_inputs = tf.nn.embedding_lookup(src_emb, features['source']) with tf.variable_scope('target_embedding'): tgt_emb = tf.get_variable('embedding', [tgt_vocab_size, params. embedding_size]) tgt_bias = tf.get_variable('bias', [params.embedding_size]) tgt_inputs = tf.nn.embedding_lookup(tgt_emb, features['target']) src_inputs = tf.nn.bias_add(src_inputs, src_bias) tgt_inputs = tf.nn.bias_add(tgt_inputs, tgt_bias) if params.dropout and not params.use_variational_dropout: src_inputs = tf.nn.dropout(src_inputs, 1.0 - params.dropout) tgt_inputs = tf.nn.dropout(tgt_inputs, 1.0 - params.dropout) cell_fw = lrp.LegacyGRUCell_encoder_v2n(params.hidden_size) cell_bw = lrp.LegacyGRUCell_encoder_v2n(params.hidden_size) if params.use_variational_dropout: cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, input_keep_prob= 1.0 - params.dropout, output_keep_prob=1.0 - params.dropout, state_keep_prob=1.0 - params.dropout, variational_recurrent= True, input_size=params.embedding_size, dtype=tf.float32) cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, input_keep_prob= 1.0 - params.dropout, output_keep_prob=1.0 - params.dropout, state_keep_prob=1.0 - params.dropout, variational_recurrent= True, input_size=params.embedding_size, dtype=tf.float32) encoder_output = _encoder(cell_fw, cell_bw, src_inputs, features[ 'source_length'], params) w_x_h_fw, w_x_h_bw = encoder_output['weight_ratios'] w_x_h_bw = w_x_h_bw[::-1, :, ::-1] w_x_enc = tf.concat([w_x_h_fw, w_x_h_bw], -1) cell = lrp.LegacyGRUCell_decoder_v2n(params.hidden_size) if params.use_variational_dropout: cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1.0 - params.dropout, output_keep_prob=1.0 - params.dropout, state_keep_prob=1.0 - params.dropout, variational_recurrent= True, input_size=params.embedding_size + 2 * params.hidden_size, dtype=tf.float32) length = {'source': features['source_length'], 'target': features[ 'target_length']} initial_state = encoder_output['final_states']['backward'] decoder_output = _decoder(cell, tgt_inputs, encoder_output['annotation' ], length, initial_state, w_x_enc, w_x_h_bw, params) w_x_dec, w_x_ctx, w_x_init = decoder_output['weight_ratios'] shifted_tgt_inputs = tf.pad(tgt_inputs, [[0, 0], [1, 0], [0, 0]]) shifted_tgt_inputs = shifted_tgt_inputs[:, :-1, :] all_outputs = tf.concat([tf.expand_dims(decoder_output['initial_state'], axis=1), decoder_output['outputs']], axis=1) shifted_outputs = all_outputs[:, :-1, :] maxout_features = [shifted_tgt_inputs, shifted_outputs, decoder_output[ 'values']] maxout_size = params.hidden_size // params.maxnum if labels is None: maxout_features = [shifted_tgt_inputs[:, (-1), :], shifted_outputs[ :, (-1), :], decoder_output['values'][:, (-1), :]] maxhid = layers.nn.maxout(maxout_features, maxout_size, params. maxnum, params, concat=False) readout = layers.nn.linear(maxhid, params.embedding_size, False, False, scope='deepout') logits = layers.nn.linear(readout, tgt_vocab_size, True, False, scope='softmax') return logits maxhid_maxout = lrp.maxout_v2n(maxout_features, maxout_size, params. maxnum, [w_x_dec, w_x_ctx], params, concat=False) maxhid = maxhid_maxout['output'] w_x_maxout = maxhid_maxout['weight_ratios'][0] w_x_maxout = tf.transpose(w_x_maxout, [0, 2, 1, 3]) readout = lrp.linear_v2n(maxhid, params.embedding_size, False, [ w_x_maxout], params, False, scope='deepout') w_x_readout = readout['weight_ratios'][0] readout = readout['output'] if params.dropout and not params.use_variational_dropout: readout = tf.nn.dropout(readout, 1.0 - params.dropout) logits = lrp.linear_v2n(readout, tgt_vocab_size, True, [w_x_readout], params, False, scope='softmax') w_x_true = logits['weight_ratios'][0] logits = logits['output'] logits = tf.reshape(logits, [-1, tgt_vocab_size]) w_x_true = tf.transpose(w_x_true, [0, 2, 1, 3]) w_x_true = tf.reshape(w_x_true, [-1, tf.shape(w_x_true)[-2], tf.shape( w_x_true)[-1]]) w_x_true = tf.transpose(w_x_true, [0, 2, 1]) labels_lrp = labels bs = tf.shape(labels_lrp)[0] idx = tf.range(tf.shape(labels_lrp)[-1]) idx = tf.cast(idx, tf.int64) idx = tf.reshape(idx, [1, -1]) labels_lrp = tf.concat([idx, labels_lrp], axis=0) labels_lrp = tf.transpose(labels_lrp, [1, 0]) w_x_true = tf.gather_nd(w_x_true, labels_lrp) w_x_true = tf.reshape(w_x_true, [bs, -1, tf.shape(w_x_true)[-1]]) ce = losses.smoothed_softmax_cross_entropy_with_logits(logits=logits, labels=labels, smoothing=params.label_smoothing, normalize=True) ce = tf.reshape(ce, tf.shape(labels)) tgt_mask = tf.to_float(tf.sequence_mask(features['target_length'], maxlen=tf.shape(features['target'])[1])) rlv_info = {} rlv_info['result'] = w_x_true loss = tf.reduce_sum(ce * tgt_mask) / tf.reduce_sum(tgt_mask) return loss, rlv_info 
envs|test|multi|agent|eight|figure|TestMixedAutonomy def test_multi_agent_figure_eight(self): env = FlowEnv(env_name='figure_eight', env_params={'num_automated': 1, 'horizon': 1500, 'simulator': 'traci', 'multiagent': True}, version=0) env.reset() pass pass env.wrapped_env.terminate() env = FlowEnv(env_name='figure_eight', env_params={'num_automated': 14, 'horizon': 1500, 'simulator': 'traci', 'multiagent': True}, version=1) env.reset() pass pass env.wrapped_env.terminate() 
bert|tokenization|matches|checkpoint|case|validate def validate_case_matches_checkpoint(do_lower_case, init_checkpoint): """Checks whether the casing config is consistent with the checkpoint name.""" if not init_checkpoint: return m = re.match('^.*?([A-Za-z0-9_-]+)/bert_model.ckpt', init_checkpoint) if m is None: return model_name = m.group(1) lower_models = ['uncased_L-24_H-1024_A-16', 'uncased_L-12_H-768_A-12', 'multilingual_L-12_H-768_A-12', 'chinese_L-12_H-768_A-12'] cased_models = ['cased_L-12_H-768_A-12', 'cased_L-24_H-1024_A-16', 'multi_cased_L-12_H-768_A-12'] is_bad_config = False if model_name in lower_models and not do_lower_case: is_bad_config = True actual_flag = 'False' case_name = 'lowercased' opposite_flag = 'True' if model_name in cased_models and do_lower_case: is_bad_config = True actual_flag = 'True' case_name = 'cased' opposite_flag = 'False' if is_bad_config: raise ValueError( 'You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. However, `%s` seems to be a %s model, so you should pass in `--do_lower_case=%s` so that the fine-tuning matches how the model was pre-training. If this error is wrong, please just comment out this check.' % (actual_flag, init_checkpoint, model_name, case_name, opposite_flag)) 
codecs|test|craystack|unflatten|flatten @pytest.mark.parametrize('shape', [(100,), (1, 23), (2, 4, 5)]) def test_flatten_unflatten(shape, depth=1000): np.random.seed(0) p = 8 bits = np.random.randint(1 << p, size=(depth,) + shape, dtype=np.uint64) message = cs.empty_message(shape) other_bits_push, _ = cs.repeat(cs.Uniform(p), depth) message = other_bits_push(message, bits) flattened = cs.flatten(message) reconstructed = cs.unflatten(flattened, shape) init_head, init_tail = message recon_head, recon_tail = reconstructed np.testing.assert_equal(init_head, recon_head) while init_tail: el, init_tail = init_tail el_, recon_tail = recon_tail assert el == el_ 
batches|gen|preprocessing|scripts|mini|do def do_preprocessing(dataset, indices): mini_batch_utils = dataset.kitti_utils.mini_batch_utils print('Generating mini batches in {}'.format(mini_batch_utils. mini_batch_dir)) mini_batch_utils.preprocess_rpn_mini_batches(indices) print('Mini batches generated') 
embedding|fn|generate|train|main|vae def _embedding_fn(ids, times): w_embed = decoder_w_embedder(ids) p_embed = decoder_p_embedder(times) return w_embed * config.hidden_size ** 0.5 + p_embed 
bias|optimizer|ops|corr|regression|init|MVGOptimizer|mvg def _init_bias_corr(self): self._bias_corr = tf.get_variable(self.w_name + 'bias_corr', initializer=0.0, trainable=False) 
tf|init|common|utils|CheckpointLoader def __init__(self, saver, global_step, logdir): self.saver = saver self.global_step_tensor = global_step self.logdir = logdir self.last_global_step = 0 
sobolev|resblock|cifar1 def resblock(x, filters, resample=None, normalize=False): if normalize: norm_fn = bn else: norm_fn = tf.identity if resample == 'down': conv_1 = functools.partial(apply_conv, filters=filters) conv_2 = functools.partial(conv_meanpool, filters=filters) conv_shortcut = functools.partial(conv_meanpool, filters=filters, kernel_size=1, he_init=False) elif resample == 'up': conv_1 = functools.partial(upsample_conv, filters=filters) conv_2 = functools.partial(apply_conv, filters=filters) conv_shortcut = functools.partial(upsample_conv, filters=filters, kernel_size=1, he_init=False) elif resample == None: conv_1 = functools.partial(apply_conv, filters=filters) conv_2 = functools.partial(apply_conv, filters=filters) conv_shortcut = tf.identity with tf.name_scope('resblock'): x = tf.identity(x) update = conv_1(activation(norm_fn(x))) update = conv_2(activation(norm_fn(update))) skip = conv_shortcut(x) return skip + update 
get|xlnet|master|model|train|utils|op def get_train_op(FLAGS, total_loss, grads_and_vars=None): global_step = tf.train.get_or_create_global_step() if FLAGS.warmup_steps > 0: warmup_lr = tf.cast(global_step, tf.float32) / tf.cast(FLAGS. warmup_steps, tf.float32) * FLAGS.learning_rate else: warmup_lr = 0.0 if FLAGS.decay_method == 'poly': decay_lr = tf.train.polynomial_decay(FLAGS.learning_rate, global_step=global_step - FLAGS.warmup_steps, decay_steps=FLAGS .train_steps - FLAGS.warmup_steps, end_learning_rate=FLAGS. learning_rate * FLAGS.min_lr_ratio) elif FLAGS.decay_method == 'cos': decay_lr = tf.train.cosine_decay(FLAGS.learning_rate, global_step= global_step - FLAGS.warmup_steps, decay_steps=FLAGS.train_steps - FLAGS.warmup_steps, alpha=FLAGS.min_lr_ratio) else: raise ValueError(FLAGS.decay_method) learning_rate = tf.where(global_step < FLAGS.warmup_steps, warmup_lr, decay_lr) if FLAGS.weight_decay == 0: optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=FLAGS.adam_epsilon) elif FLAGS.weight_decay > 0 and FLAGS.num_core_per_host == 1: optimizer = AdamWeightDecayOptimizer(learning_rate=learning_rate, epsilon=FLAGS.adam_epsilon, exclude_from_weight_decay=[ 'LayerNorm', 'layer_norm', 'bias'], weight_decay_rate=FLAGS. weight_decay) else: raise ValueError( 'Do not support `weight_decay > 0` with multi-gpu training so far.' ) if FLAGS.use_tpu: optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer) if grads_and_vars is None: grads_and_vars = optimizer.compute_gradients(total_loss) gradients, variables = zip(*grads_and_vars) clipped, gnorm = tf.clip_by_global_norm(gradients, FLAGS.clip) if getattr(FLAGS, 'lr_layer_decay_rate', 1.0) != 1.0: n_layer = 0 for i in range(len(clipped)): m = re.search('model/transformer/layer_(\\d+?)/', variables[i].name ) if not m: continue n_layer = max(n_layer, int(m.group(1)) + 1) for i in range(len(clipped)): for l in range(n_layer): if 'model/transformer/layer_{}/'.format(l) in variables[i ].name: abs_rate = FLAGS.lr_layer_decay_rate ** (n_layer - 1 - l) clipped[i] *= abs_rate tf.logging.info('Apply mult {:.4f} to layer-{} grad of {}' .format(abs_rate, l, variables[i].name)) break train_op = optimizer.apply_gradients(zip(clipped, variables), global_step=global_step) if isinstance(optimizer, AdamWeightDecayOptimizer): new_global_step = global_step + 1 train_op = tf.group(train_op, [global_step.assign(new_global_step)]) return train_op, learning_rate, gnorm 
models|cae|forward|Autoencoder def forward(self, sess, input_feature): """Feed-forward pass in the autoencoder @param sess (tf.Session) the current session @param input_feature (np.array) matrix of features @return (np.array) the output of the autoencoder (reconstruction) """ output = sess.run([self.output], feed_dict={self.x: input_feature}) return output 
gym|action|connect|pycolab|worlds|correct|GameManagerSprite|four def correct_action(self, board, actions): return board[0][actions] == ord(C_BACKGROUND) 
layerbn|f2|basenet|cnn|CNNBaseModel def f2(): """  :return: """ return tf_layer.batch_norm(inputdata, is_training=False, center=True, scale=True, updates_collections=None, scope=name, reuse=True) 
deepMOT|add|models|dim|SST|master|unmatched|DAN def add_unmatched_dim(self, x): if self.false_objects_column is None: self.false_objects_column = Variable(torch.ones(x.shape[0], x.shape [1], x.shape[2], 1)) * self.false_constant if self.use_gpu: self.false_objects_column = self.false_objects_column.cuda() x = torch.cat([x, self.false_objects_column], 3) if self.false_objects_row is None: self.false_objects_row = Variable(torch.ones(x.shape[0], x.shape[1], 1, x.shape[3])) * self.false_constant if self.use_gpu: self.false_objects_row = self.false_objects_row.cuda() x = torch.cat([x, self.false_objects_row], 2) return x 
fisher|sources|ops|NaiveDiagonalFactor|classification|factors|num @property def _num_sources(self): return len(self._params_grads) 
set|basic|dir|up|amb|utils def set_up_dir(directory, clean=False): if os.path.exists(directory): if clean: shutil.rmtree(directory) else: os.makedirs(directory) 
nmt|model|helper|Dense|call def call(self, inputs): if self.use_bias: raise ValueError() rank = common_shapes.rank(inputs) outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]]) shape = inputs.get_shape().as_list() output_shape = shape[:-1] + [self.units] outputs.set_shape(output_shape) return outputs 
STS|STSTask|create|evaluate|model|CNN def create_model(self): K.clear_session() input0 = Input(shape=(self.c['sentencepad'], self.c['wordvectdim'])) input1 = Input(shape=(self.c['sentencepad'], self.c['wordvectdim'])) Convolt_Layer = [] MaxPool_Layer = [] Flatten_Layer = [] for kernel_size, filters in self.c['cnnfilters'].items(): Convolt_Layer.append(Convolution1D(filters=filters, kernel_size= kernel_size, padding='valid', activation=self.c['cnnactivate'], kernel_initializer=self.c['cnninitial'])) MaxPool_Layer.append(MaxPooling1D(pool_size=int(self.c[ 'sentencepad'] - kernel_size + 1))) Flatten_Layer.append(Flatten()) Convolted_tensor0 = [] Convolted_tensor1 = [] for channel in range(len(self.c['cnnfilters'])): Convolted_tensor0.append(Convolt_Layer[channel](input0)) Convolted_tensor1.append(Convolt_Layer[channel](input1)) MaxPooled_tensor0 = [] MaxPooled_tensor1 = [] for channel in range(len(self.c['cnnfilters'])): MaxPooled_tensor0.append(MaxPool_Layer[channel](Convolted_tensor0[ channel])) MaxPooled_tensor1.append(MaxPool_Layer[channel](Convolted_tensor1[ channel])) Flattened_tensor0 = [] Flattened_tensor1 = [] for channel in range(len(self.c['cnnfilters'])): Flattened_tensor0.append(Flatten_Layer[channel](MaxPooled_tensor0[ channel])) Flattened_tensor1.append(Flatten_Layer[channel](MaxPooled_tensor1[ channel])) if len(self.c['cnnfilters']) > 1: Flattened_tensor0 = concatenate(Flattened_tensor0) Flattened_tensor1 = concatenate(Flattened_tensor1) else: Flattened_tensor0 = Flattened_tensor0[0] Flattened_tensor1 = Flattened_tensor1[0] absDifference = Lambda(lambda X: K.abs(X[0] - X[1]))([Flattened_tensor0, Flattened_tensor1]) mulDifference = multiply([Flattened_tensor0, Flattened_tensor1]) allDifference = concatenate([absDifference, mulDifference]) for ilayer, densedimension in enumerate(self.c['densedimension']): allDifference = Dense(units=int(densedimension), activation=self.c[ 'denseactivate'], kernel_initializer=self.c['denseinitial'])( allDifference) output = Dense(name='output', units=self.c['num_classes'], activation= 'softmax', kernel_initializer=self.c['denseinitial'])(allDifference) self.model = Model(inputs=[input0, input1], outputs=output) self.model.compile(loss={'output': self._lossfunction}, optimizer=self. c['optimizer']) 
tangents|kernel|cpu|neural|move|batch|utils|to def _move_kernel_to_cpu(k): """Moves data in a kernel from an accelerator to the CPU.""" if hasattr(k, '_asdict') and hasattr(k, '_replace'): return k._replace(**dict([((k, v) if not isinstance(v, np.ndarray) else (k, device_get(v))) for k, v in k._asdict().items()])) elif isinstance(k, np.ndarray): return device_get(k) else: raise TypeError( 'Expected kernel to be either a namedtuple or a `np.ndarray`, got %s.' % type(k)) 
minibatch|shuffle|EdgeMinibatchIterator|graphsage def shuffle(self): """ Re-shuffle the training set. Also reset the batch number. """ self.train_edges = np.random.permutation(self.train_edges) self.nodes = np.random.permutation(self.nodes) self.batch_num = 0 
lfw|pairs|src|master|facenet|read def read_pairs(pairs_filename): pairs = [] with open(pairs_filename, 'r') as f: for line in f.readlines()[1:]: pair = line.strip().split() pairs.append(pair) return np.array(pairs) 
CapturingContext|enter|xlnet|estimator|tpu|master def __enter__(self): self._g = ops.get_default_graph() self._old = self._g._get_control_flow_context() self._g._set_control_flow_context(self) 
DeepGCNModelAE|linear|model|gae|build def _build(self): self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim, output_dim=FLAGS.hidden, adj=self.adj, features_nonzero=self. features_nonzero, act=tf.nn.relu, dropout=self.dropout, logging= self.logging)(self.inputs) self.hidden2 = GraphConvolution(input_dim=FLAGS.hidden, output_dim= FLAGS.hidden, adj=self.adj, act=tf.nn.relu, dropout=self.dropout, logging=self.logging)(self.hidden1) self.z_mean = GraphConvolution(input_dim=FLAGS.hidden, output_dim=FLAGS .dimension, adj=self.adj, act=lambda x: x, dropout=self.dropout, logging=self.logging)(self.hidden2) self.reconstructions = InnerProductDecoder(act=lambda x: x, logging= self.logging)(self.z_mean) 
decoder|VNet|vnet def decoder(self, features): x1 = features[0] x2 = features[1] x3 = features[2] x4 = features[3] x5 = features[4] x5_up = self.block_five_up(x5) x5_up = x5_up + x4 x6 = self.block_six(x5_up) x6_up = self.block_six_up(x6) x6_up = x6_up + x3 x7 = self.block_seven(x6_up) x7_up = self.block_seven_up(x7) x7_up = x7_up + x2 x8 = self.block_eight(x7_up) x8_up = self.block_eight_up(x8) x8_up = x8_up + x1 x9 = self.block_nine(x8_up) if self.has_dropout: x9 = self.dropout(x9) out = self.out_conv(x9) return out 
mrcnn|coord|loss|model|bins|graph def mrcnn_coord_bins_loss_graph(target_masks, target_coord, target_class_ids, pred_coord): """Mask L2 loss for the coordinates head.  target_masks: [batch, num_rois, height, width]. A float32 tensor of values 0 or 1. Uses zero padding to fill array. target_coord: [batch, num_rois, height, width]. Might be for x, y or z channel. A float32 tensor of values in the range of [0, 1]. Uses zero padding to fill array. target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded. pred_coord: [batch, proposals, height, width, num_classes, num_bins] float32 tensor with values from 0 to 1. """ num_bins = tf.shape(pred_coord)[-1] target_class_ids = K.reshape(target_class_ids, (-1,)) mask_shape = tf.shape(target_masks) target_masks = K.reshape(target_masks, (-1, mask_shape[2], mask_shape[3])) target_coord = K.reshape(target_coord, (-1, mask_shape[2], mask_shape[3])) coord_shape = tf.shape(target_coord) target_coord_bins = target_coord * tf.cast(num_bins, tf.float32) - 1e-06 target_coord_bins = tf.floor(target_coord_bins) target_coord_bins = tf.cast(target_coord_bins, dtype=tf.int32) target_coord_bins_flatten = K.flatten(target_coord_bins) target_coord_one_hot = tf.one_hot(target_coord_bins_flatten, num_bins) target_coord_one_hot = K.reshape(target_coord_one_hot, (coord_shape[0], coord_shape[1], coord_shape[2], num_bins)) pred_shape = tf.shape(pred_coord) pred_coord = K.reshape(pred_coord, (-1, pred_shape[2], pred_shape[3], pred_shape[4], pred_shape[5])) pred_coord = tf.transpose(pred_coord, [0, 3, 1, 2, 4]) positive_ix = tf.where(target_class_ids > 0)[:, (0)] positive_class_ids = tf.cast(tf.gather(target_class_ids, positive_ix), tf.int64) indices = tf.stack([positive_ix, positive_class_ids], axis=1) y_true = tf.gather(target_coord_one_hot, positive_ix) mask = tf.gather(target_masks, positive_ix) mask = tf.cast(mask, dtype=tf.bool) y_true_in_mask = tf.boolean_mask(y_true, mask) y_pred = tf.gather_nd(pred_coord, indices) y_pred_in_mask = tf.boolean_mask(y_pred, mask) coord_loss_in_mask = K.categorical_crossentropy(y_true_in_mask, y_pred_in_mask) mean_loss = K.mean(coord_loss_in_mask) loss = K.switch(tf.size(y_true) > 0, mean_loss, tf.constant(0.0)) loss = K.reshape(loss, [1, 1]) return loss 
xlnet|create|master|config|run def create_run_config(is_training, is_finetune, FLAGS): kwargs = dict(is_training=is_training, use_tpu=FLAGS.use_tpu, use_bfloat16=FLAGS.use_bfloat16, dropout=FLAGS.dropout, dropatt= FLAGS.dropatt, init=FLAGS.init, init_range=FLAGS.init_range, init_std=FLAGS.init_std, clamp_len=FLAGS.clamp_len) if not is_finetune: kwargs.update(dict(mem_len=FLAGS.mem_len, reuse_len=FLAGS.reuse_len, bi_data=FLAGS.bi_data, clamp_len=FLAGS.clamp_len, same_length= FLAGS.same_length)) return RunConfig(**kwargs) 
testInfiniteTimeAgreement|predict|test|PredictTest @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train={}_test={}_network={}_logits={}_get={}'.format(train, test, network, out_logits, get), 'train_shape': train, 'test_shape': test, 'network': network, 'out_logits': out_logits, 'get': get} for train, test, network in zip(TRAIN_SHAPES[:-1], TEST_SHAPES[:-1], NETWORK[:-1]) for out_logits in OUTPUT_LOGITS for get in GETS)) def testInfiniteTimeAgreement(self, train_shape, test_shape, network, out_logits, get): key = random.PRNGKey(0) key, split = random.split(key) x_train = np.cos(random.normal(split, train_shape)) key, split = random.split(key) y_train = np.array(random.bernoulli(split, shape=(train_shape[0], out_logits)), np.float32) key, split = random.split(key) x_test = np.cos(random.normal(split, test_shape)) _, _, kernel_fn = _build_network(train_shape[1:], network, out_logits) reg = 1e-07 prediction = predict.gradient_descent_mse_gp(kernel_fn, x_train, y_train, x_test, get, diag_reg=reg, compute_cov=True) finite_prediction = prediction(np.inf) finite_prediction_none = prediction(None) gp_inference = predict.gp_inference(kernel_fn, x_train, y_train, x_test, get, reg, True) self.assertAllClose(finite_prediction_none, finite_prediction, True) self.assertAllClose(finite_prediction_none, gp_inference, True) 
sampling|negative|PopularSampler|popular|samplers|sampler def _negative_sampling(self, user_ids, pos_ids, neg_ids): neg_samples = np.random.choice(neg_ids, size=(len(pos_ids), self. n_negatives), replace=False, p=self.item_popularities) for i, uid, negatives in zip(range(len(user_ids)), user_ids, neg_samples): for j, neg in enumerate(negatives): while neg in self.user_items[uid]: neg_samples[i, j] = neg = np.random.choice(neg_ids, p=self. item_popularities) return neg_samples 
compute|sigmoid|loss|model|helper def compute_loss_sigmoid(logits, targets, final_sequence_length, target_sequence_length, mode): assert mode != tf.estimator.ModeKeys.PREDICT if mode == tf.estimator.ModeKeys.TRAIN: target_weights = tf.sequence_mask(target_sequence_length, dtype=tf. float32) loss = sequence_loss_sigmoid(logits, targets, target_weights) else: max_ts = tf.reduce_max(target_sequence_length) max_fs = tf.reduce_max(final_sequence_length) max_sequence_length = tf.to_int32(tf.maximum(max_ts, max_fs)) logits = tf.slice(logits, begin=[0, 0, 0], size=[-1, max_fs, -1]) targets = tf.pad(targets, [[0, 0], [0, tf.maximum(0, max_sequence_length - tf.shape(targets)[1])], [0, 0]], constant_values=0) logits = tf.pad(logits, [[0, 0], [0, tf.maximum(0, max_sequence_length - tf.shape(logits)[1])], [0, 0]], constant_values=0) sequence_length = tf.reduce_max([target_sequence_length, final_sequence_length], 0) target_weights = tf.sequence_mask(sequence_length, maxlen= max_sequence_length, dtype=tf.float32) loss = sequence_loss_sigmoid(logits, targets, target_weights) return loss 
instance|texar|utils|get def get_instance(class_or_name, kwargs, module_paths=None): """Creates a class instance.  Args: class_or_name: A class, or its name or full path to a class to instantiate. kwargs (dict): Keyword arguments for the class constructor. module_paths (list, optional): Paths to candidate modules to search for the class. This is used if the class cannot be located solely based on :attr:`class_name`. The first module in the list that contains the class is used.  Returns: A class instance.  Raises: ValueError: If class is not found based on :attr:`class_or_name` and :attr:`module_paths`. ValueError: If :attr:`kwargs` contains arguments that are invalid for the class construction. """ class_ = class_or_name if is_str(class_): class_ = get_class(class_, module_paths) class_args = set(get_args(class_.__init__)) if kwargs is None: kwargs = {} for key in kwargs.keys(): if key not in class_args: raise ValueError( 'Invalid argument for class %s.%s: %s, valid args: %s' % ( class_.__module__, class_.__name__, key, list(class_args))) return class_(**kwargs) 
get|xlnet|variable|AdamWeightDecayOptimizer|name|master|model|utils def _get_variable_name(self, param_name): """Get the variable name from the tensor name.""" m = re.match('^(.*):\\d+$', param_name) if m is not None: param_name = m.group(1) return param_name 
conv2d|VGG|VGG16 def conv2d(self, x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') 
len|dataset|train|Dataset def __len__(self): return len(self.darc) // 2 
neural|tangents|erf|stax def _erf(x, **kwargs): return erf(x) 
vkge|build|LIM def build_LIM(self, nb_entities, nb_predicates, embedding_size, optimizer): """ Construct full computation graph for Latent Information Model """ self.build_encoder(nb_entities, nb_predicates, embedding_size) self.build_decoder() self.y_pos = tf.gather(self.y_inputs, self.idx_pos) self.y_neg = tf.gather(self.y_inputs, self.idx_neg) self.p_x_i_pos = tf.gather(self.p_x_i, self.idx_pos) self.p_x_i_neg = tf.gather(self.p_x_i, self.idx_neg) self.reconstruction_loss_p = -tf.reduce_sum(tf.log(tf.where(condition= self.y_pos, x=self.p_x_i_pos, y=1 - self.p_x_i_pos) + 1e-10)) self.reconstruction_loss_n = -tf.reduce_sum(tf.log(tf.where(condition= self.y_neg, x=self.p_x_i_neg, y=1 - self.p_x_i_neg) + 1e-10)) self.nreconstruction_loss = (self.reconstruction_loss_p + self. reconstruction_loss_n * self.ELBOBS) prior = util.make_prior(code_size=embedding_size, distribution=self. distribution, alt_prior=self.alt_prior) if self.distribution == 'normal': if self.s_o: s_posterior = tfd.MultivariateNormalDiag(self.s_embedding_mean, util.distribution_scale(self.s_embedding_sigma)) o_posterior = tfd.MultivariateNormalDiag(self.o_embedding_mean, util.distribution_scale(self.o_embedding_sigma)) self.kl0 = tf.reduce_sum(tfd.kl_divergence(s_posterior, prior)) self.kl1 = tf.reduce_sum(tfd.kl_divergence(o_posterior, prior)) else: entity_posterior = tfd.MultivariateNormalDiag(self. entity_embedding_mean, util.distribution_scale(self. entity_embedding_sigma)) self.kl0 = 0 self.kl1 = tf.reduce_sum(tfd.kl_divergence(entity_posterior, prior) ) predicate_posterior = tfd.MultivariateNormalDiag(self. predicate_embedding_mean, util.distribution_scale(self. predicate_embedding_sigma)) self.kl2 = tf.reduce_sum(tfd.kl_divergence(predicate_posterior, prior)) else: raise NotImplemented self.nkl = (self.kl0 + self.kl1 + self.kl2) * self.KL_discount self.nelbo = self.nkl + self.nreconstruction_loss self._setup_training(loss=self.nelbo, optimizer=optimizer) 
fn|2|map|iterator|nmt|make|pipeline|utils|distributed def map_fn_2(src, tgt, unused_filter_bool): if use_char_encode: src = tf.reshape(vocab_utils.tokens_to_bytes(src), [-1]) tgt = tf.cast(tgt_vocab_table.lookup(tgt), tf.int32) else: src = tf.cast(src_vocab_table.lookup(src), tf.int32) tgt = tf.cast(tgt_vocab_table.lookup(tgt), tf.int32) tgt_in = tf.concat(([tgt_sos_id], tgt), 0) tgt_out = tf.concat((tgt, [tgt_eos_id]), 0) if use_char_encode: src_len = tf.to_int32(tf.size(src) / vocab_utils.DEFAULT_CHAR_MAXLEN) else: src_len = tf.size(src) tgt_len = tf.size(tgt_in) return src, tgt_in, tgt_out, src_len, tgt_len 
get|R2REnv|heading|pitch|env def _get_heading_pitch(self, pano_id, scan_id, time_step): if time_step == 0: return np.array([self._paths[self._current_idx]['heading']]).astype(np .float32), np.array([0.0]).astype(np.float32) else: prev_pano_id = self.get_current_env_output().observation[constants. PANO_ID] conn_idx = np.asscalar(np.argwhere(self._scan_info[scan_id]. conn_ids[prev_pano_id] == pano_id)) conn_enc = self._scan_info[scan_id].conn_enc[prev_pano_id][conn_idx] return _get_heading(conn_enc), _get_pitch(conn_enc) 
tangents|predict|gp|inference|neural|mat @get_namedtuple('Gaussians') def _gp_inference_mat(kdd, ktd, ktt, y_train, get, diag_reg=0.0): """Compute the mean and variance of the `posterior` of NNGP and NTK.  Args: kdd: A train-train `Kernel` namedtuple. ktd: A test-train `Kernel` namedtuple. ktt: A test-test `Kernel` namedtuple. y_train: A `np.ndarray`, representing the train targets. get: string, the mode of the Gaussian process, either "nngp" or "ntk", or a tuple, or `None`. If `None` then both `nngp` and `ntk` predictions are returned. diag_reg: A float, representing the strength of the regularization.  Returns: Either a Gaussian(`mean`, `variance`) namedtuple or `mean` of the GP posterior. """ out = {} if get is None: get = 'nngp', 'ntk' if 'nngp' in get: op = _inv_operator(kdd.nngp, diag_reg) pred_mean = _mean_prediction(op, ktd.nngp, y_train) if ktt is not None: pred_cov = _nngp_cov(op, ktd.nngp, ktt) out['nngp'] = Gaussian(pred_mean, pred_cov ) if ktt is not None else pred_mean if 'ntk' in get: op = _inv_operator(kdd.ntk, diag_reg) pred_mean = _mean_prediction(op, ktd.ntk, y_train) if ktt is not None: pred_cov = _ntk_cov(op, ktd.ntk, kdd.nngp, ktd.nngp, ktt) out['ntk'] = Gaussian(pred_mean, pred_cov ) if ktt is not None else pred_mean return out 
v1|rate|get|train|nets|learning|mobilenet def get_learning_rate(): if FLAGS.fine_tune_checkpoint: return 0.0001 else: return 0.045 
activation|layers|deepctr|layer def activation_layer(activation): if activation == 'dice' or activation == 'Dice': act_layer = Dice() elif isinstance(activation, str ) or sys.version_info.major == 2 and isinstance(activation, (str, unicode)): act_layer = tf.keras.layers.Activation(activation) elif issubclass(activation, Layer): act_layer = activation() else: raise ValueError( 'Invalid activation,found %s.You should use a str or a Activation Layer Class.' % activation) return act_layer 
get|test|examples|data|utils|MrpcProcessor def get_test_examples(self, data_dir): """See base class.""" return self._create_examples(self._read_tsv(os.path.join(data_dir, 'test.tsv')), 'test') 
gen|wgan|initialized|weight|variable|celebA|xavier|utils def weight_variable_xavier_initialized(shape, constant=1, name=None): stddev = constant * np.sqrt(2.0 / (shape[2] + shape[3])) return weight_variable(shape, stddev=stddev, name=name) 
gen|models|64|SRGANs|init|master|ResNetGenerator|Regularization|GANs|Spectral|resnet def __init__(self, ch=64, dim_z=128, bottom_width=4, activation=F.relu, n_classes=0, distribution='normal'): super(ResNetGenerator, self).__init__() initializer = chainer.initializers.GlorotUniform() self.bottom_width = bottom_width self.activation = activation self.distribution = distribution self.dim_z = dim_z self.n_classes = n_classes with self.init_scope(): self.l1 = L.Linear(dim_z, bottom_width ** 2 * ch * 16, initialW= initializer) self.block2 = Block(ch * 16, ch * 8, activation=activation, upsample=True, n_classes=n_classes) self.block3 = Block(ch * 8, ch * 4, activation=activation, upsample =True, n_classes=n_classes) self.block4 = Block(ch * 4, ch * 2, activation=activation, upsample =True, n_classes=n_classes) self.block5 = Block(ch * 2, ch, activation=activation, upsample= True, n_classes=n_classes) self.b6 = L.BatchNormalization(ch) self.l6 = L.Convolution2D(ch, 3, ksize=3, stride=1, pad=1, initialW =initializer) 
utils|modcrop def modcrop(imgs, modulo): sz = imgs.shape sz = np.asarray(sz) if len(sz) == 2: sz = sz - sz % modulo out = imgs[0:sz[0], 0:sz[1]] elif len(sz) == 3: szt = sz[0:2] szt = szt - szt % modulo out = imgs[0:szt[0], 0:szt[1], :] return out 
test|official|5|resnet|BaseTest|tensor|v2|shapes|imagenet def test_tensor_shapes_resnet_50_v2(self): self.tensor_shapes_helper(50, resnet_version=2) 
utils|AvgPool def AvgPool(ip, k=2): return tf.nn.avg_pool(ip, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID') 
gen|test|src|data|util def test_gen(): print('testing gen_valid') s_gen = EvalGen('../../data/evaluation_data/icdar13', 'test.txt') count = 0 for batch in s_gen.gen(1): count += 1 print(str(batch['bucket_id']) + ' ' + str(batch['data'].shape[2:])) assert batch['data'].shape[2] == img_height print(count) 
test|testCheckVocab|VocabUtilsTest|nmt|vocab|utils def testCheckVocab(self): vocab_dir = os.path.join(tf.test.get_temp_dir(), 'vocab_dir') os.makedirs(vocab_dir) vocab_file = os.path.join(vocab_dir, 'vocab_file') vocab = ['a', 'b', 'c'] with codecs.getwriter('utf-8')(tf.gfile.GFile(vocab_file, 'wb')) as f: for word in vocab: f.write('%s\n' % word) out_dir = os.path.join(tf.test.get_temp_dir(), 'out_dir') os.makedirs(out_dir) vocab_size, new_vocab_file = vocab_utils.check_vocab(vocab_file, out_dir) self.assertEqual(len(vocab) + 3, vocab_size) self.assertEqual(os.path.join(out_dir, 'vocab_file'), new_vocab_file) new_vocab, _ = vocab_utils.load_vocab(new_vocab_file) self.assertEqual([vocab_utils.UNK, vocab_utils.SOS, vocab_utils.EOS] + vocab, new_vocab) 
BertModelTest|bert|output|test|question|answering|check|BertModelTester|for|modeling def check_bert_for_question_answering_output(self, result): self.parent.assertListEqual(list(result['start_logits'].size()), [self. batch_size, self.seq_length]) self.parent.assertListEqual(list(result['end_logits'].size()), [self. batch_size, self.seq_length]) 
init|loss|FeatureLoss|fasterai def __init__(self, layer_wgts=[20, 70, 10]): super().__init__() self.m_feat = models.vgg16_bn(True).features.cuda().eval() requires_grad(self.m_feat, False) blocks = [(i - 1) for i, o in enumerate(children(self.m_feat)) if isinstance(o, nn.MaxPool2d)] layer_ids = blocks[2:5] self.loss_features = [self.m_feat[i] for i in layer_ids] self.hooks = hook_outputs(self.loss_features, detach=False) self.wgts = layer_wgts self.metric_names = ['pixel'] + [f'feat_{i}' for i in range(len(layer_ids)) ] self.base_loss = F.l1_loss 
testAnalyticKernelComposeAutomatic|test|batch|BatchTest @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_on_device={}'.format(store_on_device), 'store_on_device': store_on_device} for store_on_device in [True, False])) def testAnalyticKernelComposeAutomatic(self, store_on_device): utils.stub_out_pmap(batch, 2) self._test_analytic_kernel_composition(partial(batch.batch, batch_size= 2, store_on_device=store_on_device)) 
generator|tflib|mnist def mnist_generator(data, batch_size, n_labelled, limit=None): images, targets = data rng_state = numpy.random.get_state() numpy.random.shuffle(images) numpy.random.set_state(rng_state) numpy.random.shuffle(targets) if limit is not None: print('WARNING ONLY FIRST {} MNIST DIGITS'.format(limit)) images = images.astype('float32')[:limit] targets = targets.astype('int32')[:limit] if n_labelled is not None: labelled = numpy.zeros(len(images), dtype='int32') labelled[:n_labelled] = 1  def get_epoch(): rng_state = numpy.random.get_state() numpy.random.shuffle(images) numpy.random.set_state(rng_state) numpy.random.shuffle(targets) if n_labelled is not None: numpy.random.set_state(rng_state) numpy.random.shuffle(labelled) image_batches = images.reshape(-1, batch_size, 784) target_batches = targets.reshape(-1, batch_size) if n_labelled is not None: labelled_batches = labelled.reshape(-1, batch_size) for i in xrange(len(image_batches)): yield numpy.copy(image_batches[i]), numpy.copy(target_batches [i]), numpy.copy(labelled) else: for i in xrange(len(image_batches)): yield numpy.copy(image_batches[i]), numpy.copy(target_batches [i]) return get_epoch 
devtools|cleverhans|tests|test|docstrings|format def test_format_docstrings(): """ Test if docstrings are well formatted. """ return True try: verify_format_docstrings() except SkipTest as e: import traceback traceback.print_exc(e) raise AssertionError( 'Some file raised SkipTest on import, and inadvertently canceled the documentation testing.' ) 
predict|triple|embeddings|OpenKE|Config|config def predict_triple(self, h, t, r, thresh=None, returnValue=False): """This method tells you whether the given triple (h, t, r) is correct of wrong  Args: h (int): head entity id t (int): tail entity id r (int): relation id thresh (fload): threshold for the triple """ self.init_triple_classification() if self.importName != None: self.restore_tensorflow() res = self.test_step(np.array([h]), np.array([t]), np.array([r])) if thresh != None: if res < thresh: print('triple (%d,%d,%d) is correct' % (h, t, r)) else: print('triple (%d,%d,%d) is wrong' % (h, t, r)) return self.lib.getValidBatch(self.valid_pos_h_addr, self.valid_pos_t_addr, self.valid_pos_r_addr, self.valid_neg_h_addr, self.valid_neg_t_addr, self.valid_neg_r_addr) res_pos = self.test_step(self.valid_pos_h, self.valid_pos_t, self. valid_pos_r) res_neg = self.test_step(self.valid_neg_h, self.valid_neg_t, self. valid_neg_r) self.lib.getBestThreshold(self.relThresh_addr, res_pos. __array_interface__['data'][0], res_neg.__array_interface__['data'][0]) if res < self.relThresh[r]: print('triple (%d,%d,%d) is correct' % (h, t, r)) else: print('triple (%d,%d,%d) is wrong' % (h, t, r)) 
texar|test|embedders|modules|EmbedderTest|word|embedder def _test_word_embedder(self, hparams): """Tests :class:`texar.modules.WordEmbedder`. """ embedder = WordEmbedder(vocab_size=100, hparams=hparams) inputs = tf.ones([64, 16], dtype=tf.int32) outputs = embedder(inputs) inputs_soft = tf.ones([64, 16, embedder.vocab_size], dtype=tf.float32) outputs_soft = embedder(soft_ids=inputs_soft) emb_dim = embedder.dim if not isinstance(emb_dim, (list, tuple)): emb_dim = [emb_dim] hparams_dim = hparams['dim'] if not isinstance(hparams['dim'], (list, tuple)): hparams_dim = [hparams['dim']] self.assertEqual(outputs.shape, [64, 16] + emb_dim) self.assertEqual(outputs_soft.shape, [64, 16] + emb_dim) self.assertEqual(emb_dim, hparams_dim) self.assertEqual(embedder.vocab_size, 100) self.assertEqual(len(embedder.trainable_variables), 1) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) outputs_, outputs_soft_ = sess.run([outputs, outputs_soft], feed_dict={global_mode(): tf.estimator.ModeKeys.TRAIN}) self.assertEqual(outputs_.shape, (64, 16) + tuple(emb_dim)) self.assertEqual(outputs_soft_.shape, (64, 16) + tuple(emb_dim)) inputs = tf.placeholder(dtype=tf.int64, shape=[None, None]) outputs = embedder(inputs) self.assertEqual(len(outputs.get_shape()), 2 + len(hparams_dim)) inputs_soft = tf.placeholder(dtype=tf.int64, shape=[None, None, None]) outputs_soft = embedder(soft_ids=inputs_soft) self.assertEqual(len(outputs_soft.get_shape()), 2 + len(hparams_dim)) 
size|output|conv|lstm|translate|BasicConvLSTMCell @property def output_size(self): return self._num_units 
init|train|Train def __init__(self, trial, step, size, batch_size, learning_rate, max_epoch, tfrecord_path, checkpoint_dir, num_of_data, conf): print('[*] Initialize Training') self.trial = trial self.step = step self.HEIGHT = size[0] self.WIDTH = size[1] self.CHANNEL = size[2] self.BATCH_SIZE = batch_size self.learning_rate = learning_rate self.EPOCH = max_epoch self.tfrecord_path = tfrecord_path self.checkpoint_dir = checkpoint_dir self.num_of_data = num_of_data self.conf = conf self.image = tf.placeholder(tf.float32, shape=[None, self.HEIGHT, self. WIDTH, self.CHANNEL]) self.label = tf.placeholder(tf.float32, shape=[None, None]) self.MODEL = model.NMD(self.image) 
statement|convert|qajson|entailment|to def convert_qajson_to_entailment(qa_json: JsonDict): question_text = qa_json['question']['stem'] choice = qa_json['question']['choice']['text'] support = qa_json['question']['support']['text'] hypothesis = create_hypothesis(get_fitb_from_question(question_text), choice) output_dict = create_output_dict(qa_json, support, hypothesis) return output_dict 
gan|DSpritesInceptionScore|load|metric def load(self): self.shape_network.load(self.sess, self.shape_network_path) 
CamRestDST|update|state|applications|camrest|db|dst|restaurants|cambridge def update_state_db(self, db_result, sys_req_slot_entropies=None): """ Updates the current database results in the dialogue state.  :param db_result: a dictionary containing the database query results :param sys_req_slot_entropies: entropy values for each slot :return: """ if db_result: self.DState.db_matches_ratio = len(db_result) self.DState.item_in_focus = db_result[0] if sys_req_slot_entropies: self.DState.system_requestable_slot_entropies = deepcopy( sys_req_slot_entropies) return self.DState 
pnasnet|test|testOverrideHParamsMobileModel|nasnet|PNASNetTest|nets def testOverrideHParamsMobileModel(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) tf.train.create_global_step() config = pnasnet.mobile_imagenet_config() config.set_hparam('data_format', 'NCHW') with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()): _, end_points = pnasnet.build_pnasnet_mobile(inputs, num_classes, config=config) self.assertListEqual(end_points['Stem'].shape.as_list(), [batch_size, 135, 28, 28]) 
rnn|texar|UnidirectionalRNNEncoderTest|variables|test|modules|trainable|encoders def test_trainable_variables(self): """Tests the functionality of automatically collecting trainable variables. """ inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 100]) encoder = UnidirectionalRNNEncoder() _, _ = encoder(inputs) self.assertEqual(len(encoder.trainable_variables), 2) hparams = {'rnn_cell': {'dropout': {'input_keep_prob': 0.5}}} encoder = UnidirectionalRNNEncoder(hparams=hparams) _, _ = encoder(inputs) self.assertEqual(len(encoder.trainable_variables), 2) hparams = {'output_layer': {'num_layers': 2, 'layer_size': [100, 6], 'activation': 'relu', 'final_layer_activation': 'identity', 'dropout_layer_ids': [0, 1, 2], 'variational_dropout': False}} encoder = UnidirectionalRNNEncoder(hparams=hparams) _, _ = encoder(inputs) self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 2) _, _ = encoder(inputs) self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 2) 
get|attacks|loss|fda|FDA def get_fda_loss(self, opt_operations): loss = 0 for layer in opt_operations: layer = layer[0] batch_size = int(int(layer.shape[0]) / 2) tensor = layer[:batch_size] mean_tensor = tf.stack([tf.reduce_mean(tensor, -1)] * tensor.shape[ -1], -1) wts_good = tensor < mean_tensor wts_good = tf.to_float(wts_good) wts_bad = tensor >= mean_tensor wts_bad = tf.to_float(wts_bad) loss += tf.log(tf.nn.l2_loss(wts_good * layer[batch_size:] / tf. cast(tf.size(layer), tf.float32))) loss -= tf.log(tf.nn.l2_loss(wts_bad * layer[batch_size:] / tf.cast (tf.size(layer), tf.float32))) loss = loss / len(opt_operations) return loss 
get|localization|cues|image|reader|deeplab|segment|resnet def get_localization_cues(attn_file, saliency_file, catg_file, n_classes, adapt ): """ Generates the localization cues.  Args: attn_file: Path to file containing attention. saliency_file: Path to file containing saliency. catg_file: Path to file containing category information. n_classes: Total number of classes (including background) in the dataset. adapt: Whether to obtain adapted ground truth cues (True/ False). """ attn_arr = np.load(attn_file) if adapt: img_label = attn_arr['actv'].astype(np.int32) else: attn = attn_arr['actv'].astype(np.float64) saliency_arr = np.load(saliency_file) saliency = saliency_arr['actv'].astype(np.float64) catg_IDs = get_multiclass_labels(catg_file) saliency = saliency.reshape((saliency.shape[0], saliency.shape[1], 1)) hm = 2 / (1 / (attn + EPSILON) + 1 / (saliency + EPSILON)) maxs = np.amax(hm, axis=2) bckg_prob = np.zeros((maxs.shape[0], maxs.shape[1])) bckg_prob[maxs < 0.4] = 1 maxs = np.reshape(maxs, (maxs.shape[0], maxs.shape[1], 1)) bckg_prob = np.reshape(bckg_prob, (bckg_prob.shape[0], bckg_prob. shape[1], 1)) hm_max = np.equal(hm, maxs).astype(np.int32) * hm concat_hm = np.concatenate((bckg_prob, hm_max), axis=2) final_label = np.zeros((saliency.shape[0], saliency.shape[1], n_classes)).astype(np.float64) final_label[:, :, (catg_IDs)] = concat_hm img_label = np.argmax(final_label, axis=2).astype(np.int32) img_label = np.reshape(img_label, (final_label.shape[0], final_label.shape[1], 1)).astype(np.int32) return img_label 
gen|src|data|clear|DataGen|util def clear(self): self.bucket_data = {i: BucketData() for i in range(self. bucket_max_width + 1)} 
aggregator|cifar|main def main(): inception_root_dir = './results/hparams/cifar10/' pkl_filepaths = get_pkl_filepaths(inception_root_dir) hparams_list = [] metrics_list = [] for pkl_filepath in pkl_filepaths: hparams = basic_utils.read_hparams(pkl_filepath) metrics = get_metrics(hparams) if metrics is not None: hparams_list.append(hparams) metrics_list.append(metrics) df = get_df(hparams_list, metrics_list) df_pkl_path = './results/df_cifar.pkl' basic_utils.save_to_pickle(df, df_pkl_path) 
rows|core|official|test|io|file|BaseTest|data|low|utils|tiny def test_tiny_rows_low_core(self): self._test_sharding(**_TEST_CASES[0]) 
GAN|model|TransitionDown def TransitionDown(self, input_, name): with tf.variable_scope(name): reduction = 0.5 reduced_output_size = int(int(input_.get_shape()[-1]) * reduction) next_layer = BatchNorm(input_, isTrain=self.isTrain, decay=self.decay) next_layer = Conv(next_layer, kernel_size=1, stride=1, output_channels=reduced_output_size) next_layer = DropOut(next_layer, isTrain=self.isTrain, rate=0.2) next_layer = AvgPool(next_layer) return next_layer 
NetworkClass|error|compute|BuildNetwork def compute_error(self, fitting, y_val_pred, y_val, y_train_pred, y_train): error = 10 print('Selection method: ', self.network_info.selection) if self.network_info.selection == 'validation_loss': error = np.mean(fitting.history['val_loss'][-1:]) elif self.network_info.selection == 'variance_prediction_error': error = utils.compute_prediction_error_variance(y_val, y_val_pred, 2) elif self.network_info.selection == 'mean_prediction_error': error = utils.compute_mean_prediction_error(y_val, y_val_pred, 2) elif self.network_info.selection == 'train_loss': error = np.mean(fitting.history['loss'][-1:]) elif self.network_info.selection == 'wasserstein_train': error = scipy.stats.wasserstein_distance(y_train, y_train_pred) return error 
embed|minibatch|feed|incremental|graphsage|EdgeMinibatchIterator|dict def incremental_embed_feed_dict(self, size, iter_num): node_list = self.nodes val_nodes = node_list[iter_num * size:min((iter_num + 1) * size, len( node_list))] val_edges = [(n, n) for n in val_nodes] return self.batch_feed_dict(val_edges), (iter_num + 1) * size >= len( node_list), val_edges 
forward|det|log|and|jacobian|AffineCouplingFitSdnGain2 def _forward_and_log_det_jacobian(self, x, yy, nlf0=None, nlf1=None, iso= None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) beta1, beta2 = sdn_iso_model_params_2(iso) scale = tf.sqrt(beta1 * yy + beta2) y = x if scale is not None: y *= scale if scale is None: log_abs_det_J = tf.constant(0.0, dtype=x.dtype, name='fldj') else: log_abs_det_J = tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) return y, log_abs_det_J 
predict|testMaxLearningRate|test|PredictTest @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train={}_network={}_logits={}_{}'.format(train, network, out_logits, name), 'train_shape': train, 'network': network, 'out_logits': out_logits, 'fn_and_kernel': fn, 'name': name} for train, network in zip(TRAIN_SHAPES, NETWORK) for out_logits in OUTPUT_LOGITS for name, fn in KERNELS.items())) def testMaxLearningRate(self, train_shape, network, out_logits, fn_and_kernel, name): key = random.PRNGKey(0) key, split = random.split(key) if len(train_shape) == 2: train_shape = train_shape[0] * 5, train_shape[1] * 10 else: train_shape = 16, 8, 8, 3 x_train = random.normal(split, train_shape) key, split = random.split(key) y_train = np.array(random.bernoulli(split, shape=(train_shape[0], out_logits)), np.float32) for lr_factor in [0.5, 3.0]: params, f, ntk = fn_and_kernel(key, train_shape[1:], network, out_logits) loss = lambda params, x: 0.5 * np.mean((f(params, x) - y_train) ** 2) grad_loss = jit(grad(loss)) g_dd = ntk(x_train, None, 'ntk') steps = 20 if name == 'theoretical': step_size = predict.max_learning_rate(g_dd, num_outputs=out_logits ) * lr_factor else: step_size = predict.max_learning_rate(g_dd, num_outputs=-1 ) * lr_factor opt_init, opt_update, get_params = optimizers.sgd(step_size) opt_state = opt_init(params)  def get_loss(opt_state): return loss(get_params(opt_state), x_train) init_loss = get_loss(opt_state) for i in range(steps): params = get_params(opt_state) opt_state = opt_update(i, grad_loss(params, x_train), opt_state) trained_loss = get_loss(opt_state) loss_ratio = trained_loss / (init_loss + 1e-12) if lr_factor == 3.0: if not math.isnan(loss_ratio): self.assertGreater(loss_ratio, 10.0) else: self.assertLess(loss_ratio, 0.1) 
CIN|tests|test|interaction|layers|invalid @pytest.mark.parametrize('layer_size', [(), (3, 10)]) def test_test_CIN_invalid(layer_size): with pytest.raises(ValueError): with CustomObjectScope({'CIN': layers.CIN}): layer_test(layers.CIN, kwargs={'layer_size': layer_size}, input_shape=(BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE)) 
get|plato|multi|ConversationalMultiAgent|state|agent|conversational def get_state(self): """ Get this agent's state  :return: a DialogueState """ return self.dialogue_manager.get_state() 
GAN|generator|model def generator(self, input_): """ 54 Layer Tiramisu """ with tf.variable_scope('InputConv') as scope: input_ = Conv(input_, kernel_size=3, stride=1, output_channels=self .growth_rate * 4) collect_conv = [] for i in range(1, 6): input_ = self.DenseBlock(input_, name='Encoder' + str(i), layers= self.layers) collect_conv.append(input_) input_ = self.TransitionDown(input_, name='TD' + str(i)) input_ = self.DenseBlock(input_, name='BottleNeck', layers=15) for i in range(1, 6): input_ = self.TransitionUp(input_, output_channels=self.growth_rate * 4, name='TU' + str(6 - i)) input_ = tf.concat([input_, collect_conv[6 - i - 1]], axis=3, name= 'Decoder' + str(6 - i) + '/Concat') input_ = self.DenseBlock(input_, name='Decoder' + str(6 - i), layers=self.layers) with tf.variable_scope('OutputConv') as scope: output = Conv(input_, kernel_size=1, stride=1, output_channels=3) return tf.nn.tanh(output) 
texar|DataBase|epochs|base|data|num @property def num_epochs(self): """Number of epochs. """ return self._hparams.num_epochs 
official|flags|wrap|conventions|help|utils def help_wrap(text, *args, **kwargs): return _help_wrap(text, *args, **kwargs).replace('\ufeff', '') 
bayesian|y|regression|controller|learning|BayesianLearning @property def y(self): return self._y 
sparse|entropy|utils|softmax|cross|imagenet def sparse_softmax_cross_entropy(labels, logits, weights=1.0, label_smoothing=0.0, scope=None, loss_collection=ops.GraphKeys.LOSSES, reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS): """Cross-entropy loss using `tf.nn.sparse_softmax_cross_entropy_with_logits`. `weights` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `weights` is a tensor of shape [`batch_size`], then the loss weights apply to each corresponding sample. Args: labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0, num_classes)`. Other values will raise an exception when this op is run on CPU, and return `NaN` for corresponding loss and gradient rows on GPU. logits: Unscaled log probabilities of shape `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float32` or `float64`. weights: Coefficients for the loss. This must be scalar or broadcastable to `labels` (i.e. same rank and each dimension is either 1 or the same). scope: the scope for the operations performed in computing the loss. loss_collection: collection to which the loss will be added. reduction: Type of reduction to apply to loss. Returns: Weighted loss `Tensor` of the same type as `logits`. If `reduction` is `NONE`, this has the same shape as `labels`; otherwise, it is scalar. Raises: ValueError: If the shapes of `logits`, `labels`, and `weights` are incompatible, or if any of them are None. """ if labels is None: raise ValueError('labels must not be None.') if logits is None: raise ValueError('logits must not be None.') with tf.name_scope(scope, 'sparse_softmax_cross_entropy_loss', (logits, labels, weights)) as scope: if labels.shape.ndims == 1: loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits= logits, labels=labels, name='xentropy') else: loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy') loss = tf.losses.compute_weighted_loss(loss, weights, scope, loss_collection, reduction=reduction) smooth_loss = 0.0 if label_smoothing > 0: loss = tf.scalar_mul(1.0 - label_smoothing, loss) aux_log_softmax = -tf.nn.log_softmax(logits) smooth_loss = tf.losses.compute_weighted_loss(aux_log_softmax, label_smoothing * weights, 'label_smoothing', loss_collection, reduction=reduction) return loss + smooth_loss 
dwconv|cases|generate|test def generate_test_cases(ukernel, cr, kr, c_block, is_pipelined, isa): """Generates all tests cases for a DWCONV micro-kernel.  Args: ukernel: C name of the micro-kernel function. cr: CR parameter of the DWCONV micro-kernel. kr: KR parameter of the DWCONV micro-kernel. k_block: Number of C values processed per one iteration of the main loop of the micro-kernel. is_pipelined: Indicates if the micro-kernel is implemented with software pipelining. Additional test cases are generated for software pipelined micro-kernels to separately test prologue + epiloque of the pipelined loop and iteration of the pipelined loop. isa: instruction set required to run the micro-kernel. Generated unit test will skip execution if the host processor doesn't support this ISA.  Returns: Code for the test case. """ _, test_name = ukernel.split('_', 1) _, datatype, ukernel_type, _ = ukernel.split('_', 3) test_args = [ukernel] if not isa or isa == 'psimd': test_args.append('DWConvMicrokernelTester::Variant::Scalar') return xngen.preprocess(DWCONV_TEST_CODE, {'TEST_NAME': test_name.upper ().replace('UKERNEL_', ''), 'TEST_ARGS': test_args, 'UKERNEL_TYPE': ukernel_type.upper(), 'DATATYPE': datatype, 'CR': cr, 'KR': kr, 'CBLOCK': c_block, 'ADJCBLOCK': 2 * c_block if is_pipelined else c_block, 'IS_PIPELINED': is_pipelined, 'ISA_CHECK': xnncommon. generate_isa_check_macro(isa), 'next_prime': next_prime, 'sqrt': math.sqrt}) 
detokenizer|lambada|LambadaTask def detokenizer(self): with open(vocab_file, 'r', encoding='utf-8') as file: lines = file.read().splitlines() vocab = {index: value for index, value in enumerate(lines)} return Detokenizer(vocab) 
attack|improve|l|invariant|score|transform def score(image, label): zs = np.random.normal(0, 1, size=(128, 74)) zs[:, :10] = 0 zs[:, (label)] = 1 for _ in range(30): ell, l_sim, l_z, nimg, delta = sess.run((total_loss, similarity_loss, z_loss, generated_images, grads), {zin: zs, x_target: image[(np.newaxis), :, :, :]}) zs[:, 10:] -= delta[:, 10:] * 0.01 return np.min(ell) 
distance|utils|metrics|edit def edit_distance(hypothesis, truth, eos_id, mapping=None): if mapping: mapping = tf.convert_to_tensor(mapping) hypothesis = tf.nn.embedding_lookup(mapping, hypothesis) truth = tf.nn.embedding_lookup(mapping, truth) hypothesis = dense_to_sparse(hypothesis, eos_id, merge_repeated=True) truth = dense_to_sparse(truth, eos_id, merge_repeated=True) return tf.edit_distance(hypothesis, truth, normalize=True) 
tests|test|LayerNormalization|layers|normalization @pytest.mark.parametrize('axis', [-1, -2]) def test_LayerNormalization(axis): with CustomObjectScope({'LayerNormalization': layers.LayerNormalization}): layer_test(layers.LayerNormalization, kwargs={'axis': axis}, input_shape=(BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE)) 
format|common|times|density def _format_times_density(x): return '${:.2f}\\times$'.format(1 / x) 
Trainer|epoch|train|classification def train_epoch(self): loss_list = [] acc_list = [] for itr, (x, y) in enumerate(tqdm(self.train_loader)): feed_dict = {self.model.inputs: x, self.model.targets: y, self. model.n_particles: self.config.train_particles} feed_dict.update({self.model.is_training: True}) self.sess.run([self.model.train_op], feed_dict=feed_dict) feed_dict.update({self.model.is_training: False}) loss, acc = self.sess.run([self.model.loss, self.model.acc], feed_dict=feed_dict) loss_list.append(loss) acc_list.append(acc) cur_iter = self.model.global_step_tensor.eval(self.sess) if cur_iter % self.config.get('TCov', 10 ) == 0 and self.model.cov_update_op is not None: self.sess.run([self.model.cov_update_op], feed_dict=feed_dict) if self.config.optimizer == 'diag': self.sess.run([self.model.var_update_op], feed_dict=feed_dict) if cur_iter % self.config.get('TInv', 200 ) == 0 and self.model.inv_update_op is not None: self.sess.run([self.model.inv_update_op, self.model. var_update_op], feed_dict=feed_dict) if cur_iter % self.config.get('TEigen', 200 ) == 0 and self.model.eigen_basis_update_op is not None: self.sess.run([self.model.eigen_basis_update_op, self.model. var_update_op], feed_dict=feed_dict) if self.config.kfac_init_after_basis: self.sess.run(self.model.re_init_kfac_scale_op) if cur_iter % self.config.get('TScale', 10 ) == 0 and self.model.scale_update_op is not None: self.sess.run([self.model.scale_update_op, self.model. var_scale_update_op], feed_dict=feed_dict) avg_loss = np.mean(loss_list) avg_acc = np.mean(acc_list) self.logger.info('train | loss: %5.6f | accuracy: %5.6f' % (float( avg_loss), float(avg_acc))) summaries_dict = dict() summaries_dict['train_loss'] = avg_loss summaries_dict['train_acc'] = avg_acc cur_iter = self.model.global_step_tensor.eval(self.sess) self.summarizer.summarize(cur_iter, summaries_dict=summaries_dict) 
gen|resblocks|models|conv|SRGANs|upsample|master|Regularization|GANs|Spectral def upsample_conv(x, conv): return conv(_upsample(x)) 
deepg|main def main(): parser = argparse.ArgumentParser(description='Analyze NN.') parser.add_argument('--net', type=str, help='Neural network to analyze') parser.add_argument('--dataset', type=str, default='mnist', help='Dataset') parser.add_argument('--data_dir', type=str, help= 'Directory which contains data') parser.add_argument('--data_root', type=str, help= 'Directory which contains data') parser.add_argument('--num_params', type=int, default=0, help= 'Number of transformation parameters') parser.add_argument('--num_tests', type=int, default=None, help= 'Number of images to test') parser.add_argument('--from_test', type=int, default=0, help= 'Number of images to test') parser.add_argument('--test_idx', type=int, default=None, help= 'Index to test') parser.add_argument('--debug', action='store_true', help= 'Whether to display debug info') parser.add_argument('--attack', action='store_true', help= 'Whether to attack') parser.add_argument('--timeout_lp', type=float, default=1, help= 'timeout for the LP solver') parser.add_argument('--timeout_milp', type=float, default=1, help= 'timeout for the MILP solver') parser.add_argument('--use_area_heuristic', type=str2bool, default=True, help= 'whether to use area heuristic for the DeepPoly ReLU approximation') args = parser.parse_args() global n_rows, n_cols, n_channels if args.dataset == 'cifar10': n_rows, n_cols, n_channels = 32, 32, 3 else: n_rows, n_cols, n_channels = 28, 28, 1 filename, file_extension = os.path.splitext(args.net) is_trained_with_pytorch = False is_saved_tf_model = False if file_extension == '.net' or file_extension == '.pyt': is_trained_with_pytorch = True elif file_extension == '.meta': is_saved_tf_model = True elif file_extension != '.tf': print('file extension not supported') exit(1) is_conv = False if is_saved_tf_model: netfolder = os.path.dirname(args.net) tf.logging.set_verbosity(tf.logging.ERROR) sess = tf.Session() saver = tf.train.import_meta_graph(args.net) saver.restore(sess, tf.train.latest_checkpoint(netfolder + '/')) eran = ERAN(sess.graph.get_tensor_by_name('logits:0'), sess) else: if args.dataset == 'mnist' or args.dataset == 'fashion': num_pixels = 784 else: num_pixels = 3072 model, is_conv, means, stds = read_net(args.net, num_pixels, is_trained_with_pytorch) eran = ERAN(model) csvfile = open('../../code/datasets/{}_test.csv'.format(args.dataset), 'r') tests = csv.reader(csvfile, delimiter=',') total, attacked, standard_correct, tot_time = 0, 0, 0, 0 correct_box, correct_poly = 0, 0 cver_box, cver_poly = [], [] for i, test in enumerate(tests): if args.test_idx is not None and i != args.test_idx: continue attacks_file = os.path.join(args.data_dir, 'attack_{}.csv'.format(i)) if args.num_tests is not None and i >= args.num_tests: break print('Test {}:'.format(i)) if args.dataset == 'mnist' or args.dataset == 'fashion': image = np.float64(test[1:len(test)]) elif is_trained_with_pytorch: image = np.float64(test[1:len(test)]) else: image = np.float64(test[1:len(test)]) - 0.5 spec_lb = np.copy(image) spec_ub = np.copy(image) if is_trained_with_pytorch: normalize(spec_lb, means, stds, args.dataset, is_conv) normalize(spec_ub, means, stds, args.dataset, is_conv) label, nn, nlb, nub = eran.analyze_box(spec_lb, spec_ub, 'deeppoly', args.timeout_lp, args.timeout_milp, args.use_area_heuristic) print('Label: ', label) if label != int(test[0]): print('Label {}, but true label is {}, skipping...'.format( label, int(test[0]))) print('Standard accuracy: {} percent'.format(standard_correct / float(i + 1) * 100)) continue else: standard_correct += 1 print('Standard accuracy: {} percent'.format(standard_correct / float(i + 1) * 100)) dim = n_rows * n_cols * n_channels ok_box, ok_poly = True, True k = args.num_params + 1 + 1 + dim attack_imgs, checked, attack_pass = [], [], 0 cex_found = False if args.attack: with open(attacks_file, 'r') as fin: lines = fin.readlines() for j in range(0, len(lines), args.num_params + 1): params = [float(line[:-1]) for line in lines[j:j + args .num_params]] tokens = lines[j + args.num_params].split(',') values = np.array(list(map(float, tokens))) attack_lb = values[::2] attack_ub = values[1::2] if is_trained_with_pytorch: normalize(attack_lb, means, stds, args.dataset, is_conv ) normalize(attack_ub, means, stds, args.dataset, is_conv ) else: attack_lb -= 0.5 attack_ub -= 0.5 attack_imgs.append((params, attack_lb, attack_ub)) checked.append(False) predict_label, _, _, _ = eran.analyze_box(attack_lb[: dim], attack_ub[:dim], 'deeppoly', args.timeout_lp, args.timeout_milp, args.use_area_heuristic, 0) if predict_label != int(test[0]): print('counter-example, params: ', params, ', predicted label: ', predict_label) cex_found = True break else: attack_pass += 1 print('tot attacks: ', len(attack_imgs)) specs_file = os.path.join(args.data_dir, '{}.csv'.format(i)) begtime = time.time() with open(specs_file, 'r') as fin: lines = fin.readlines() print('Number of lines: ', len(lines)) assert len(lines) % k == 0 spec_lb = np.zeros(args.num_params + dim) spec_ub = np.zeros(args.num_params + dim) expr_size = args.num_params lexpr_cst, uexpr_cst = [], [] lexpr_weights, uexpr_weights = [], [] lexpr_dim, uexpr_dim = [], [] ver_chunks_box, ver_chunks_poly, tot_chunks = 0, 0, 0 for i, line in enumerate(lines): if i % k < args.num_params: values = np.array(list(map(float, line[:-1].split(' ')))) assert values.shape[0] == 2 param_idx = i % k spec_lb[dim + param_idx] = values[0] spec_ub[dim + param_idx] = values[1] if args.debug: print('parameter %d: [%.4f, %.4f]' % (param_idx, values[0], values[1])) elif i % k == args.num_params: values = np.array(list(map(float, line[:-1].split(',')))) spec_lb[:dim] = values[::2] spec_ub[:dim] = values[1::2] elif i % k < k - 1: tokens = line[:-1].split(' ') assert len(tokens) == 2 + 2 * args.num_params + 1 bias_lower, weights_lower = float(tokens[0]), list(map( float, tokens[1:1 + args.num_params])) assert tokens[args.num_params + 1] == '|' bias_upper, weights_upper = float(tokens[args. num_params + 2]), list(map(float, tokens[3 + args. num_params:])) assert len(weights_lower) == args.num_params assert len(weights_upper) == args.num_params lexpr_cst.append(bias_lower) uexpr_cst.append(bias_upper) for j in range(args.num_params): lexpr_dim.append(dim + j) uexpr_dim.append(dim + j) lexpr_weights.append(weights_lower[j]) uexpr_weights.append(weights_upper[j]) else: assert line == 'SPEC_FINISHED\n' for p_idx in range(args.num_params): lexpr_cst.append(spec_lb[dim + p_idx]) for l in range(args.num_params): lexpr_weights.append(0) lexpr_dim.append(dim + l) uexpr_cst.append(spec_ub[dim + p_idx]) for l in range(args.num_params): uexpr_weights.append(0) uexpr_dim.append(dim + l) if is_trained_with_pytorch: normalize(spec_lb[:dim], means, stds, args.dataset, is_conv) normalize(spec_ub[:dim], means, stds, args.dataset, is_conv) normalize_poly(args.num_params, lexpr_cst, lexpr_weights, lexpr_dim, uexpr_cst, uexpr_weights, uexpr_dim, means, stds, args.dataset) for attack_idx, (attack_params, attack_lb, attack_ub ) in enumerate(attack_imgs): ok_attack = True for j in range(num_pixels): low, up = lexpr_cst[j], uexpr_cst[j] for idx in range(args.num_params): low += lexpr_weights[j * args.num_params + idx ] * attack_params[idx] up += uexpr_weights[j * args.num_params + idx ] * attack_params[idx] if low > attack_lb[j] + EPS or attack_ub[j ] > up + EPS: ok_attack = False if ok_attack: checked[attack_idx] = True if args.debug: print('Running the analysis...') t_begin = time.time() perturbed_label_poly, _, _, _ = eran.analyze_box(spec_lb, spec_ub, 'deeppoly', args.timeout_lp, args. timeout_milp, args.use_area_heuristic, 0, lexpr_weights, lexpr_cst, lexpr_dim, uexpr_weights, uexpr_cst, uexpr_dim, expr_size) perturbed_label_box, _, _, _ = eran.analyze_box(spec_lb [:dim], spec_ub[:dim], 'deeppoly', args.timeout_lp, args.timeout_milp, args.use_area_heuristic, 0) t_end = time.time() print('DeepG: ', perturbed_label_poly, '\tInterval: ', perturbed_label_box, '\tlabel: ', label, '[Time: %.4f]' % (t_end - t_begin)) tot_chunks += 1 if perturbed_label_box != label: ok_box = False else: ver_chunks_box += 1 if perturbed_label_poly != label: ok_poly = False else: ver_chunks_poly += 1 lexpr_cst, uexpr_cst = [], [] lexpr_weights, uexpr_weights = [], [] lexpr_dim, uexpr_dim = [], [] total += 1 if ok_box: correct_box += 1 if ok_poly: correct_poly += 1 if cex_found: assert not ok_box and not ok_poly attacked += 1 cver_poly.append(ver_chunks_poly / float(tot_chunks)) cver_box.append(ver_chunks_box / float(tot_chunks)) tot_time += time.time() - begtime print('Verified[box]: {}, Verified[poly]: {}, CEX found: {}'.format (ok_box, ok_poly, cex_found)) assert not cex_found or not ok_box, 'ERROR! Found counter-example, but image was verified with box!' assert not cex_found or not ok_poly, 'ERROR! Found counter-example, but image was verified with poly!' print('Attacks found: %.2f percent, %d/%d' % (100.0 * attacked / total, attacked, total)) print('[Box]  Provably robust: %.2f percent, %d/%d' % (100.0 * correct_box / total, correct_box, total)) print('[Poly] Provably robust: %.2f percent, %d/%d' % (100.0 * correct_poly / total, correct_poly, total)) print('Empirically robust: %.2f percent, %d/%d' % (100.0 * (total - attacked) / total, total - attacked, total)) print('[Box]  Average chunks verified: %.2f percent' % (100.0 * np. mean(cver_box))) print('[Poly]  Average chunks verified: %.2f percent' % (100.0 * np .mean(cver_poly))) print('Average time: ', tot_time / total) 
transfo|test|tokenization|punctuation|TransfoXLTokenizationTest|is|xl def test_is_punctuation(self): self.assertTrue(_is_punctuation('-')) self.assertTrue(_is_punctuation('$')) self.assertTrue(_is_punctuation('`')) self.assertTrue(_is_punctuation('.')) self.assertFalse(_is_punctuation('A')) self.assertFalse(_is_punctuation(' ')) 
block|block2scene|scannet|scene|parse|index def parse_block_scene(datapath, scene_names): f = open(os.path.join(datapath, 'log_block.txt'), 'r') blocklist = f.read().splitlines() block2scene = [] ordered_testlist = [] for line in blocklist: str = line.split(', ') if str[0] in scene_names: block2scene.append(tuple(str)) tfrecord_name = os.path.join(datapath, '%s.tfrecord' % str[0]) if not tfrecord_name in ordered_testlist: ordered_testlist.append(tfrecord_name) return block2scene, ordered_testlist 
loss|get def get_loss(y_true, y_pred): return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred) 
top|LayerwiseCVAE|run|model|layerwise|prior def run_top_prior(self, sess): return sess.run(self.top_prior_params_and_inputs, dict(zip(self. top_context_inputs, prior_contexts(self.model.hps)))) 
celeba|Attention|u|t|sagan|build def build(self, input_shape): self.gamma = self.add_weight(name='gamma', shape=[1], initializer= 'zeros', trainable=True) 
amb|BlurAddNoise|measure|init def __init__(self, hparams): MeasurementDevice.__init__(self, hparams) self.output_type = 'image' 
rnn|texar|BidirectionalRNNEncoder|modules|build|encoders def _build(self, inputs, sequence_length=None, initial_state_fw=None, initial_state_bw=None, time_major=False, mode=None, return_cell_output= False, return_output_size=False, **kwargs): """Encodes the inputs.  Args: inputs: A 3D Tensor of shape `[batch_size, max_time, dim]`. The first two dimensions `batch_size` and `max_time` may be exchanged if `time_major=True` is specified. sequence_length (optional): A 1D int tensor of shape `[batch_size]`. Sequence lengths of the batch inputs. Used to copy-through state and zero-out outputs when past a batch element's sequence length. initial_state (optional): Initial state of the RNN. time_major (bool): The shape format of the :attr:`inputs` and :attr:`outputs` Tensors. If `True`, these tensors are of shape `[max_time, batch_size, depth]`. If `False` (default), these tensors are of shape `[batch_size, max_time, depth]`. mode (optional): A tensor taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including `TRAIN`, `EVAL`, and `PREDICT`. Controls output layer dropout if the output layer is specified with :attr:`hparams`. If `None` (default), :func:`texar.global_mode()` is used. return_cell_output (bool): Whether to return the output of the RNN cell. This is the results prior to the output layer. **kwargs: Optional keyword arguments of :tf_main:`tf.nn.dynamic_rnn <nn/dynamic_rnn>`, such as `swap_memory`, `dtype`, `parallel_iterations`, etc.  Returns: - By default (both `return_cell_output` and `return_output_size`             are False), returns a pair :attr:`(outputs, final_state)`  - :attr:`outputs`: A tuple `(outputs_fw, outputs_bw)`                 containing                 the forward and the backward RNN outputs, each of which is of                 shape `[batch_size, max_time, output_dim]` if                 `time_major` is False, or                 `[max_time, batch_size, output_dim]` if                 `time_major` is True.                 If RNN cell output is a (nested) tuple of Tensors, then                 `outputs_fw` and `outputs_bw` will be a (nested) tuple having                 the same structure as the cell output.  - :attr:`final_state`: A tuple                 `(final_state_fw, final_state_bw)`                 containing the final states of the forward and backward                 RNNs, each of which is a                 Tensor of shape `[batch_size] + cell.state_size`, or                 a (nested) tuple of Tensors if `cell.state_size` is a (nested)                tuple.  - If `return_cell_output` is True, returns a triple             :attr:`(outputs, final_state, cell_outputs)` where  - :attr:`cell_outputs`: A tuple                 `(cell_outputs_fw, cell_outputs_bw)` containting the outputs                 by the forward and backward RNN cells prior to the                 output layers, having the same structure with :attr:`outputs`                 except for the `output_dim`.  - If `return_output_size` is True, returns a tuple             :attr:`(outputs, final_state, output_size)` where  - :attr:`output_size`: A tupple                 `(output_size_fw, output_size_bw)` containing the size of                 `outputs_fw` and `outputs_bw`, respectively.                 Take `*_fw` for example,                 `output_size_fw` is a (possibly nested tuple of) int.                 If a single int or an int array, then `outputs_fw` has shape                 `[batch/time, time/batch] + output_size_fw`. If                 a (nested) tuple, then `output_size_fw` has the same                 structure as with `outputs_fw`. The same applies to                  `output_size_bw`.  - If both `return_cell_output` and             `return_output_size` are True, returns             :attr:`(outputs, final_state, cell_outputs, output_size)`. """ no_initial_state = initial_state_fw is None and initial_state_bw is None if 'dtype' not in kwargs and no_initial_state: cell_outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=self ._cell_fw, cell_bw=self._cell_bw, inputs=inputs, sequence_length=sequence_length, initial_state_fw= initial_state_fw, initial_state_bw=initial_state_bw, time_major =time_major, dtype=tf.float32, **kwargs) else: cell_outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=self ._cell_fw, cell_bw=self._cell_bw, inputs=inputs, sequence_length=sequence_length, initial_state_fw= initial_state_fw, initial_state_bw=initial_state_bw, time_major =time_major, **kwargs) outputs_fw, output_size_fw = _apply_rnn_encoder_output_layer(self. _output_layer_fw, time_major, self._output_layer_hparams_fw, mode, cell_outputs[0], self._cell_fw.output_size) outputs_bw, output_size_bw = _apply_rnn_encoder_output_layer(self. _output_layer_bw, time_major, self._output_layer_hparams_bw, mode, cell_outputs[1], self._cell_bw.output_size) outputs = outputs_fw, outputs_bw output_size = output_size_fw, output_size_bw if not self._built: self._add_internal_trainable_variables() self._add_trainable_variable(layers. get_rnn_cell_trainable_variables(self._cell_fw)) self._add_trainable_variable(layers. get_rnn_cell_trainable_variables(self._cell_bw)) if self._output_layer_fw and not isinstance(self._output_layer_fw, (list, tuple)): self._add_trainable_variable(self._output_layer_fw. trainable_variables) if self._output_layer_bw and not isinstance(self._output_layer_bw, (list, tuple)): self._add_trainable_variable(self._output_layer_bw. trainable_variables) self._built = True returns = outputs, states if return_cell_output: returns += cell_outputs, if return_output_size: returns += output_size, return returns 
contributed|and|align|embeddings|master|facenet|load|data|export def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction): minsize = 20 threshold = [0.6, 0.7, 0.7] factor = 0.709 print('Creating networks and loading parameters') with tf.Graph().as_default(): gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction= gpu_memory_fraction) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)) with sess.as_default(): pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None) nrof_samples = len(image_paths) img_list = [None] * nrof_samples for i in xrange(nrof_samples): print(image_paths[i]) img = misc.imread(os.path.expanduser(image_paths[i])) img_size = np.asarray(img.shape)[0:2] bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor) det = np.squeeze(bounding_boxes[(0), 0:4]) bb = np.zeros(4, dtype=np.int32) bb[0] = np.maximum(det[0] - margin / 2, 0) bb[1] = np.maximum(det[1] - margin / 2, 0) bb[2] = np.minimum(det[2] + margin / 2, img_size[1]) bb[3] = np.minimum(det[3] + margin / 2, img_size[0]) cropped = img[bb[1]:bb[3], bb[0]:bb[2], :] aligned = misc.imresize(cropped, (image_size, image_size), interp= 'bilinear') prewhitened = facenet.prewhiten(aligned) img_list[i] = prewhitened images = np.stack(img_list) return images 
bert|features|fn|builder|extract|master|model def model_fn_builder(bert_config, init_checkpoint, layer_indexes, use_tpu, use_one_hot_embeddings): """Returns `model_fn` closure for TPUEstimator."""  def model_fn(features, labels, mode, params): """The `model_fn` for TPUEstimator.""" unique_ids = features['unique_ids'] input_ids = features['input_ids'] input_mask = features['input_mask'] input_type_ids = features['input_type_ids'] model = modeling.BertModel(config=bert_config, is_training=False, input_ids=input_ids, input_mask=input_mask, token_type_ids= input_type_ids, use_one_hot_embeddings=use_one_hot_embeddings) if mode != tf.estimator.ModeKeys.PREDICT: raise ValueError('Only PREDICT modes are supported: %s' % mode) tvars = tf.trainable_variables() scaffold_fn = None assignment_map, initialized_variable_names = (modeling. get_assignment_map_from_checkpoint(tvars, init_checkpoint)) if use_tpu:  def tpu_scaffold(): tf.train.init_from_checkpoint(init_checkpoint, assignment_map) return tf.train.Scaffold() scaffold_fn = tpu_scaffold else: tf.train.init_from_checkpoint(init_checkpoint, assignment_map) tf.logging.info('**** Trainable Variables ****') for var in tvars: init_string = '' if var.name in initialized_variable_names: init_string = ', *INIT_FROM_CKPT*' tf.logging.info('  name = %s, shape = %s%s', var.name, var. shape, init_string) all_layers = model.get_all_encoder_layers() predictions = {'unique_id': unique_ids} for i, layer_index in enumerate(layer_indexes): predictions['layer_output_%d' % i] = all_layers[layer_index] output_spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, predictions=predictions, scaffold_fn=scaffold_fn) return output_spec return model_fn 
sampleImages|utils|plot def sampleImages(imgs_lr, fake_hr, imgs_hr, dir_, step): imgs_lr = 0.5 * imgs_lr + 0.5 fake_hr = 0.5 * fake_hr + 0.5 imgs_hr = 0.5 * imgs_hr + 0.5 r, c = 2, 2 titles = ['Generated', 'Original'] fig, axs = plt.subplots(r, c) cnt = 0 for row in range(r): for col, image in enumerate([fake_hr, imgs_hr]): axs[row, col].imshow(image[row]) axs[row, col].set_title(titles[col]) axs[row, col].axis('off') cnt += 1 fig.savefig(os.path.join(dir_, '%d.png' % step)) plt.close() for i in range(r): fig = plt.figure() plt.imshow(imgs_lr[i]) fig.savefig(os.path.join(dir_, '%d_lowres_%d.png' % (step, i))) plt.close() 
cpplint|CheckSpacing def CheckSpacing(filename, clean_lines, linenum, nesting_state, error): """Checks for the correctness of various spacing issues in the code.  Things we check for: spaces around operators, spaces after if/for/while/switch, no spaces around parens in function calls, two spaces between code and comment, don't start a block with a blank line, don't end a function with a blank line, don't add a blank line after public/protected/private, don't have too many blank lines in a row.  Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. nesting_state: A NestingState instance which maintains information about the current stack of nested blocks being parsed. error: The function to call with any errors found. """ raw = clean_lines.lines_without_raw_strings line = raw[linenum] if IsBlankLine(line) and not nesting_state.InNamespaceBody( ) and not nesting_state.InExternC(): elided = clean_lines.elided prev_line = elided[linenum - 1] prevbrace = prev_line.rfind('{') if prevbrace != -1 and prev_line[prevbrace:].find('}') == -1: exception = False if Match(' {6}\\w', prev_line): search_position = linenum - 2 while search_position >= 0 and Match(' {6}\\w', elided[ search_position]): search_position -= 1 exception = search_position >= 0 and elided[search_position][:5 ] == '    :' else: exception = Match( ' {4}\\w[^\\(]*\\)\\s*(const\\s*)?(\\{\\s*$|:)', prev_line ) or Match(' {4}:', prev_line) if not exception: error(filename, linenum, 'whitespace/blank_line', 2, 'Redundant blank line at the start of a code block should be deleted.' ) if linenum + 1 < clean_lines.NumLines(): next_line = raw[linenum + 1] if next_line and Match('\\s*}', next_line) and next_line.find( '} else ') == -1: error(filename, linenum, 'whitespace/blank_line', 3, 'Redundant blank line at the end of a code block should be deleted.' ) matched = Match('\\s*(public|protected|private):', prev_line) if matched: error(filename, linenum, 'whitespace/blank_line', 3, 'Do not leave a blank line after "%s:"' % matched.group(1)) next_line_start = 0 if linenum + 1 < clean_lines.NumLines(): next_line = raw[linenum + 1] next_line_start = len(next_line) - len(next_line.lstrip()) CheckComment(line, filename, linenum, next_line_start, error) line = clean_lines.elided[linenum] if Search('\\w\\s+\\[', line) and not Search('(?:delete|return)\\s+\\[', line): error(filename, linenum, 'whitespace/braces', 5, 'Extra space before [' ) if Search('for *\\(.*[^:]:[^: ]', line) or Search('for *\\(.*[^: ]:[^:]', line): error(filename, linenum, 'whitespace/forcolon', 2, 'Missing space around colon in range-based for loop') 
main def main(): args = parse_args() if args is None: exit() models = [ACWGANGP] with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess: gan = None for model in models: if args.gan_type == model.model_name: gan = model(sess, epoch=args.epoch, batch_size=args. batch_size, z_dim=args.z_dim, dataset_name=args.dataset, checkpoint_dir=args.checkpoint_dir, result_dir=args. result_dir, log_dir=args.log_dir) if gan is None: raise Exception('[!] There is no option for ' + args.gan_type) gan.build_model() show_all_variables() gan.train() print(' [*] Training finished!') gan.visualize_results(args.epoch - 1) print(' [*] Testing finished!') 
softmax|compute|xlnet|master|squad|run def _compute_softmax(scores): """Compute softmax probability over raw logits.""" if not scores: return [] max_score = None for score in scores: if max_score is None or score > max_score: max_score = score exp_scores = [] total_sum = 0.0 for score in scores: x = math.exp(score - max_score) exp_scores.append(x) total_sum += x probs = [] for score in exp_scores: probs.append(score / total_sum) return probs 
domain|plato|ontology|Ontology|init def __init__(self, filename): """ Initialize the internal structures of the domain :param filename: path to load the ontolgoy from """ self.ontology_file_name = None self.ontology = None if isinstance(filename, str): if os.path.isfile(filename): self.ontology_file_name = filename self.load_ontology() else: raise FileNotFoundError('domain file %s not found' % filename) else: raise ValueError('Unacceptable value for ontology file name: %s ' % filename) 
DropDevice|get|ph|theta|measure|amb def get_theta_ph(self, hparams, name): theta_ph = tf.placeholder(tf.float32, shape=self.batch_dims, name=name) return theta_ph 
texar|models|Seq2seqBase|connect|base|seq2seq def _connect(self, encoder_results, features, labels, mode): """Transforms encoder final state into decoder initial state. """ raise NotImplementedError 
bert|BertModel|pytorch|init|modeling|pretrained def __init__(self, config): super(BertModel, self).__init__(config) self.embeddings = BertEmbeddings(config) self.encoder = BertEncoder(config) self.pooler = BertPooler(config) self.apply(self.init_bert_weights) 
division|models|dsin|deepctr|sess|interest def sess_interest_division(sparse_embedding_dict, user_behavior_input_dict, sparse_fg_list, sess_feture_list, sess_max_count, bias_encoding=True): tr_input = [] for i in range(sess_max_count): sess_name = 'sess_' + str(i) keys_emb_list = get_embedding_vec_list(sparse_embedding_dict, user_behavior_input_dict[sess_name], sparse_fg_list, sess_feture_list, sess_feture_list) keys_emb = concat_fun(keys_emb_list) tr_input.append(keys_emb) if bias_encoding: tr_input = BiasEncoding(sess_max_count)(tr_input) return tr_input 
ABReluTest|test|stax|relu|id|ab def test_ab_relu_id(self, same_inputs): key = random.PRNGKey(1) X0_1 = random.normal(key, (5, 7)) fc = stax.Dense(10, 1, 0) X0_2 = None if same_inputs else random.normal(key, (9, 7)) init_fn, apply_id, kernel_fn_id = stax.serial(fc, stax.Identity()) params = init_fn(key, input_shape=(-1, 7)) for a in [-5, -1, -0.5, 0, 0.5, 1, 5]: with self.subTest(a=a): _, apply_ab_relu, kernel_fn_ab_relu = stax.serial(fc, stax. ABRelu(a, a)) X1_1_id = a * apply_id(params, X0_1) X1_1_ab_relu = apply_ab_relu(params, X0_1) self.assertAllClose(X1_1_id, X1_1_ab_relu, True) kernels_id = kernel_fn_id(X0_1 * a, None if X0_2 is None else a * X0_2) kernels_ab_relu = kernel_fn_ab_relu(X0_1, X0_2) self.assertAllClose(kernels_id, kernels_ab_relu, True) 
gym|grid|pycolab|main|worlds def main(argv=()): WALL_LEVEL = ['    #   ', '    #   ', '##  #   ', '        ', '        ', '   #  ##', '   #    ', '   #    '] game = make_game(raw_level=WALL_LEVEL, terminal_reward=1.0, doors=True) repainter = rendering.ObservationCharacterRepainter(INVERSE_REPAINT_MAPPING ) ui = human_ui.CursesUi(keys_to_actions={curses.KEY_UP: 0, curses. KEY_DOWN: 1, curses.KEY_LEFT: 2, curses.KEY_RIGHT: 3, 'q': 4, (-1): 5}, repainter=repainter, delay=2000) ui.play(game) 
loss|get def get_loss(args): if args.opt == 'dfw': loss_fn = MultiClassHingeLoss() if 'cifar' in args.dataset: args.smooth_svm = True else: loss_fn = nn.CrossEntropyLoss() print('L2 regularization: \t {}'.format(args.weight_decay)) print('\nLoss function:') print(loss_fn) if args.cuda: loss_fn = loss_fn.cuda() return loss_fn 
block2scene|fn|scannet|input|index def input_fn(filelist, batch_size=16): dataset = tf.data.TFRecordDataset(filelist) dataset = dataset.map(parse_fn, num_parallel_calls=4) dataset = dataset.padded_batch(batch_size, padded_shapes=(None, 1), padding_values=-1.0, drop_remainder=False) return dataset 
models|model|CNN|cnn|compile def compile_model(self, dataset='mnist'): """ Initialize the model """ if dataset.lower() == 'cifar10': self.model.compile(loss='categorical_crossentropy', optimizer=Adam( lr=self.lr_schedule(0)), metrics=['accuracy']) else: self.model.compile(loss='categorical_crossentropy', optimizer= 'adadelta', metrics=['accuracy']) 
test|tokenization|lower|no|TokenizationTest|basic|tokenizer def test_basic_tokenizer_no_lower(self): tokenizer = BasicTokenizer(do_lower_case=False) self.assertListEqual(tokenizer.tokenize(' \tHeLLo!how  \n Are yoU?  '), ['HeLLo', '!', 'how', 'Are', 'yoU', '?']) 
strip|texar|test|UtilsTest|special|tokens|utils def test_strip_special_tokens(self): """Test :func:`texar.utils.strip_special_tokens` """ str_ = '<BOS> i am <EOS> <PAD> <PAD>' self.assertEqual(utils.strip_special_tokens(str_), 'i am') self.assertEqual(utils.strip_special_tokens([str_]), ['i am']) str_ = str_.split() self.assertEqual(utils.strip_special_tokens(str_, is_token_list=True), ['i', 'am']) self.assertEqual(utils.strip_special_tokens([str_], is_token_list=True), [['i', 'am']]) 
bert|transfo|LMShuffledIterator|tokenization|iter|pytorch|xl|pretrained def __iter__(self): sent_stream = self.get_sent_stream() for batch in self.stream_iterator(sent_stream): yield batch 
get|size|test|input|cvusa|data|InputData|dataset def get_test_dataset_size(self): return self.test_data_size 
checker|avod|4c|check|core|format|box def check_box_4c_format(input_data): """Checks for correct box_4c format. If not proper type, raises error.  Args: input_data: input numpy array or tensor to check for valid box_8c format  Raises: ValueError: if input_data with invalid dimensions is given. """ if isinstance(input_data, np.ndarray): if input_data.ndim > 2 or input_data.shape[-1] != 10: raise TypeError( 'Given input does not have valid number of attributes. Should be N x 10 for box_4c.' ) elif isinstance(input_data, tf.Tensor): if isinstance(input_data, tf.Tensor): if input_data.shape[1] != 10: raise TypeError( 'Given input does not have valid number of attributes. Should be N x 10 for box_4c.' ) else: raise TypeError( 'Given input is not of valid types.(i.e. np.ndarray or tf.Tensor)') 
conv|net|src|factory|dw def conv_dw(inputs, out_channels, kernel_size=3, stride=1, dilation=1, training=True, name=''): net = tf.layers.separable_conv2d(inputs, filters=out_channels, kernel_size=kernel_size, strides=stride, dilation_rate=dilation, padding='same', use_bias=False, name=name) net = tf.layers.batch_normalization(net, training=training) net = tf.nn.relu(net) return net 
bert|fn|based|file|builder|input|master|run|classifier def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder): """Creates an `input_fn` closure to be passed to TPUEstimator.""" name_to_features = {'input_ids': tf.FixedLenFeature([seq_length], tf. int64), 'input_mask': tf.FixedLenFeature([seq_length], tf.int64), 'segment_ids': tf.FixedLenFeature([seq_length], tf.int64), 'label_ids': tf.FixedLenFeature([], tf.int64), 'is_real_example': tf.FixedLenFeature([], tf.int64)}  def _decode_record(record, name_to_features): """Decodes a record to a TensorFlow example.""" example = tf.parse_single_example(record, name_to_features) for name in list(example.keys()): t = example[name] if t.dtype == tf.int64: t = tf.to_int32(t) example[name] = t return example  def input_fn(params): """The actual input function.""" batch_size = params['batch_size'] d = tf.data.TFRecordDataset(input_file) if is_training: d = d.repeat() d = d.shuffle(buffer_size=100) d = d.apply(tf.contrib.data.map_and_batch(lambda record: _decode_record(record, name_to_features), batch_size=batch_size, drop_remainder=drop_remainder)) return d return input_fn 
environments|VideoWrapper|wrapper|frames|video|slac @property def frames(self): return self._frames 
texar|modules|BertBase|berts|init def __init__(self, pretrained_model_name=None, cache_dir=None, hparams=None): ModuleBase.__init__(self, hparams) if pretrained_model_name: self.pretrained_model = bert_utils.load_pretrained_model( pretrained_model_name, cache_dir) elif self._hparams.pretrained_model_name is not None: self.pretrained_model = bert_utils.load_pretrained_model(self. _hparams.pretrained_model_name, cache_dir) else: self.pretrained_model = None if self.pretrained_model: self.pretrained_model_hparams = (bert_utils. transform_bert_to_texar_config(self.pretrained_model)) 
params|common|BigFixedLocator|set def set_params(self, nbins=None): if nbins is not None: self.nbins = nbins 
get|create|map|paf|src def create_paf_map(centerA, centerB, size_x, size_y, thresh): """ creat paf vector :param centerA:  start point coord :param centerB: end point coord :param size_x:  map size x :param size_y:  map size y :param thresh:  width of paf vector :return: paf map: mask: mask indicate where should have data """ centerA = centerA.astype(float) centerB = centerB.astype(float) paf_map = np.zeros((2, size_y, size_x)) norm = np.linalg.norm(centerB - centerA) if norm == 0.0: return paf_map limb_vec_unit = (centerB - centerA) / norm min_x = max(int(round(min(centerA[0], centerB[0]) - thresh)), 0) max_x = min(int(round(max(centerA[0], centerB[0]) + thresh)), size_x) min_y = max(int(round(min(centerA[1], centerB[1]) - thresh)), 0) max_y = min(int(round(max(centerA[1], centerB[1]) + thresh)), size_y) range_x = list(range(int(min_x), int(max_x), 1)) range_y = list(range(int(min_y), int(max_y), 1)) xx, yy = np.meshgrid(range_x, range_y) xx = xx.astype(np.int32) yy = yy.astype(np.int32) ba_x = xx - centerA[0] ba_y = yy - centerA[1] limb_width = np.abs(ba_x * limb_vec_unit[1] - ba_y * limb_vec_unit[0]) mask = limb_width < thresh paf_map[:, (yy), (xx)] = np.repeat(mask[(np.newaxis), :, :], 2, axis=0) paf_map[:, (yy), (xx)] *= limb_vec_unit[:, (np.newaxis), (np.newaxis)] mask = np.logical_or.reduce((np.abs(paf_map[(0), :, :]) > 0, np.abs( paf_map[(0), :, :]) > 0)) return paf_map, mask 
models|encoder|transformer|thumt def transformer_encoder(inputs, bias, params, dtype=None, scope=None): with tf.variable_scope(scope, default_name='encoder', dtype=dtype, values=[inputs, bias]): x = inputs for layer in range(params.num_encoder_layers): with tf.variable_scope('layer_%d' % layer): with tf.variable_scope('self_attention'): max_relative_dis = (params.max_relative_dis if params. position_info_type == 'relative' else None) y = layers.attention.multihead_attention(_layer_process (x, params.layer_preprocess), None, bias, params. num_heads, params.attention_key_channels or params. hidden_size, params.attention_value_channels or params.hidden_size, params.hidden_size, 1.0 - params.attention_dropout, max_relative_dis= max_relative_dis) y = y['outputs'] x = _residual_fn(x, y, 1.0 - params.residual_dropout) x = _layer_process(x, params.layer_postprocess) with tf.variable_scope('feed_forward'): y = _ffn_layer(_layer_process(x, params. layer_preprocess), params.filter_size, params. hidden_size, 1.0 - params.relu_dropout) x = _residual_fn(x, y, 1.0 - params.residual_dropout) x = _layer_process(x, params.layer_postprocess) outputs = _layer_process(x, params.layer_preprocess) return outputs 
bert|transfo|get|tokenization|sym|pytorch|xl|pretrained|TransfoXLTokenizer def get_sym(self, idx): assert 0 <= idx < len(self), 'Index {} out of vocabulary range'.format(idx) return self.idx2sym[idx] 
nmt|get|metric|estimator def get_metric(hparams, predictions, current_step): """Run inference and compute metric.""" predicted_ids = [] for prediction in predictions: predicted_ids.append(prediction['predictions']) mlperf_log.gnmt_print(key=mlperf_log.EVAL_SIZE, value=hparams. examples_to_infer) if hparams.examples_to_infer < len(predicted_ids): predicted_ids = predicted_ids[0:hparams.examples_to_infer] translations = _convert_ids_to_strings(hparams.tgt_vocab_file, predicted_ids) trans_file = os.path.join(hparams.out_dir, 'newstest2014_out_{}.tok.de' .format(current_step)) trans_dir = os.path.dirname(trans_file) if not tf.gfile.Exists(trans_dir): tf.gfile.MakeDirs(trans_dir) tf.logging.info('Writing to file %s' % trans_file) with codecs.getwriter('utf-8')(tf.gfile.GFile(trans_file, mode='wb') ) as trans_f: trans_f.write('') for translation in translations: sentence = nmt_utils.get_translation(translation, tgt_eos= hparams.eos, subword_option=hparams.subword_option) trans_f.write((sentence + b'\n').decode('utf-8')) output_dir = os.path.join(hparams.out_dir, 'eval_{}'.format(hparams. test_year)) tf.gfile.MakeDirs(output_dir) summary_writer = tf.summary.FileWriter(output_dir) ref_file = '%s.%s' % (hparams.test_prefix, hparams.tgt) metric = 'bleu' if hparams.use_borg: score = evaluation_utils.evaluate(ref_file, trans_file, metric, hparams.subword_option) else: score = get_sacrebleu(trans_file, hparams.detokenizer_file, hparams .test_year) with tf.Graph().as_default(): summaries = [] summaries.append(tf.Summary.Value(tag=metric, simple_value=score)) tf_summary = tf.Summary(value=list(summaries)) summary_writer.add_summary(tf_summary, current_step) with tf.gfile.Open(os.path.join(output_dir, 'bleu'), 'w') as f: f.write('{}\n'.format(score)) misc_utils.print_out('  %s: %.1f' % (metric, score)) summary_writer.close() return score 
rotate|point|cloud|angle|by|data|util def rotate_point_cloud_by_angle(batch_data, rotation_angle): """ Rotate the point cloud along up direction with certain angle. Input: BxNx3 array, original batch of point clouds Return: BxNx3 array, rotated batch of point clouds """ rotated_data = np.zeros(batch_data.shape, dtype=np.float32) for k in range(batch_data.shape[0]): rotation_matrix = rot_z(rotation_angle) shape_pc = batch_data[(k), :, 0:3] rotated_data[(k), :, 0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix) return rotated_data 
residual|gnmt|fn|nmt|model def gnmt_residual_fn(inputs, outputs): """Residual function that handles different inputs and outputs inner dims.  Args: inputs: cell inputs, this is actual inputs concatenated with the attention vector. outputs: cell outputs  Returns: outputs + actual inputs """  def split_input(inp, out): out_dim = out.get_shape().as_list()[-1] inp_dim = inp.get_shape().as_list()[-1] return tf.split(inp, [out_dim, inp_dim - out_dim], axis=-1) actual_inputs, _ = tf.contrib.framework.nest.map_structure(split_input, inputs, outputs)  def assert_shape_match(inp, out): inp.get_shape().assert_is_compatible_with(out.get_shape()) tf.contrib.framework.nest.assert_same_structure(actual_inputs, outputs) tf.contrib.framework.nest.map_structure(assert_shape_match, actual_inputs, outputs) return tf.contrib.framework.nest.map_structure(lambda inp, out: inp + out, actual_inputs, outputs) 
ResidualGraphConvolutionalNetwork|model|network def network(self, x, adj, regularizer): residual = None for i in range(1, self.num_layers + 1): pre_nonlinearity, x = self.gcn_layer(x, adj, i, regularizer) if residual is not None: x = residual + self.layer_decay * x residual = pre_nonlinearity output = self.decoder(x) return output 
texar|embedders|modules|embeds|base|EmbedderBase|num|embedder @property def num_embeds(self): """The number of embedding elements. """ return self._num_embeds 
loaders|create|data def create_loaders(dataset_train, dataset_val, dataset_test, train_size, val_size, test_size, batch_size, test_batch_size, cuda, num_workers, split=True): kwargs = {'num_workers': num_workers, 'pin_memory': True} if cuda else {} if split: train_indices, val_indices = random_subsets((train_size, val_size), len(dataset_train), seed=1234) else: train_size = train_size if train_size is not None else len( dataset_train) train_indices, = random_subsets((train_size,), len(dataset_train), seed=1234) val_size = val_size if val_size is not None else len(dataset_val) val_indices, = random_subsets((val_size,), len(dataset_val), seed=1234) test_size = test_size if test_size is not None else len(dataset_test) test_indices, = random_subsets((test_size,), len(dataset_test), seed=1234) dataset_train = Subset(dataset_train, train_indices) dataset_val = Subset(dataset_val, val_indices) dataset_test = Subset(dataset_test, test_indices) print('Dataset sizes: \t train: {} \t val: {} \t test: {}'.format(len( dataset_train), len(dataset_val), len(dataset_test))) print('Batch size: \t {}'.format(batch_size)) train_loader = data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, **kwargs) val_loader = data.DataLoader(dataset_val, batch_size=test_batch_size, shuffle=False, **kwargs) test_loader = data.DataLoader(dataset_test, batch_size=test_batch_size, shuffle=False, **kwargs) train_loader.tag = 'train' val_loader.tag = 'val' test_loader.tag = 'test' return train_loader, val_loader, test_loader 
loss|layers|log def log_loss(x, xh): return tf.reduce_mean(GaussianLogDensity(slim.flatten(x), slim.flatten( xh), tf.zeros_like(slim.flatten(xh)))) 
checkpoints|average|main def main(): tf.logging.set_verbosity(tf.logging.INFO) parser = argparse.ArgumentParser(formatter_class=argparse. ArgumentDefaultsHelpFormatter) parser.add_argument('--model_dir', required=True, help= 'The model directory containing the checkpoints.') parser.add_argument('--output_dir', required=True, help= 'The output directory where the averaged checkpoint will be saved.') parser.add_argument('--max_count', type=int, default=8, help= 'The maximal number of checkpoints to average.') args = parser.parse_args() if args.model_dir == args.output_dir: raise ValueError('Model and output directory must be different') checkpoints_path = tf.train.get_checkpoint_state(args.model_dir ).all_model_checkpoint_paths if len(checkpoints_path) > args.max_count: checkpoints_path = checkpoints_path[-args.max_count:] num_checkpoints = len(checkpoints_path) tf.logging.info('Averaging %d checkpoints...' % num_checkpoints) tf.logging.info('Listing variables...') var_list = tf.train.list_variables(checkpoints_path[0]) avg_values = {} for name, shape in var_list: if not name.startswith('global_step'): avg_values[name] = np.zeros(shape) for checkpoint_path in checkpoints_path: tf.logging.info('Loading checkpoint %s' % checkpoint_path) reader = tf.train.load_checkpoint(checkpoint_path) for name in avg_values: avg_values[name] += reader.get_tensor(name) / num_checkpoints tf_vars = [] for name, value in six.iteritems(avg_values): tf_vars.append(tf.get_variable(name, shape=value.shape)) placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars] assign_ops = [tf.assign(v, p) for v, p in zip(tf_vars, placeholders)] latest_step = int(checkpoints_path[-1].split('-')[-1]) out_base_file = os.path.join(args.output_dir, 'model.ckpt') global_step = tf.get_variable('global_step', initializer=tf.constant( latest_step, dtype=tf.int64), trainable=False) saver = tf.train.Saver(tf.global_variables()) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for p, assign_op, (name, value) in zip(placeholders, assign_ops, six.iteritems(avg_values)): sess.run(assign_op, {p: value}) tf.logging.info('Saving averaged checkpoint to %s-%d' % ( out_base_file, latest_step)) saver.save(sess, out_base_file, global_step=global_step) 
darkflow|cli|cliHandler def cliHandler(args): FLAGS = argHandler() FLAGS.setDefaults() FLAGS.parseArgs(args)  def _get_dir(dirs): for d in dirs: this = os.path.abspath(os.path.join(os.path.curdir, d)) if not os.path.exists(this): os.makedirs(this) requiredDirectories = [FLAGS.imgdir, FLAGS.binary, FLAGS.backup, os. path.join(FLAGS.imgdir, 'out')] if FLAGS.summary: requiredDirectories.append(FLAGS.summary) _get_dir(requiredDirectories) try: FLAGS.load = int(FLAGS.load) except: pass tfnet = TFNet(FLAGS) if FLAGS.demo: tfnet.camera() exit('Demo stopped, exit.') if FLAGS.train: print('Enter training ...') tfnet.train() if not FLAGS.savepb: exit('Training finished, exit.') if FLAGS.savepb: print('Rebuild a constant version ...') tfnet.savepb() exit('Done') tfnet.predict() 
neural|tangents|Relu|stax @_layer def Relu(do_backprop=False, do_stabilize=False): return _elementwise(_ab_relu, a=0, b=1, do_backprop=do_backprop, do_stabilize=do_stabilize) 
texar|dtypes|is|str|utils def is_str(x): """Returns `True` if :attr:`x` is either a str or unicode. Returns `False` otherwise. """ return isinstance(x, six.string_types) 
stack|done|AgentsTest|test|agents|frames def test_stack_frames_done(self): zero_state = agents.DuelingLSTMDQNNet(2, [1], stack_size=4).initial_state(1 ).frame_stacking_state output, state = stack_frames(frames=[[[1]]], done=[[False]], frame_stacking_state=zero_state, stack_size=4) self.assertAllEqual(output, [[[1, 0, 0, 0]]]) output, state = stack_frames(frames=[[[2]]], done=[[True]], frame_stacking_state=state, stack_size=4) self.assertAllEqual(output, [[[2, 0, 0, 0]]]) output, state = stack_frames(frames=[[[3]], [[4]], [[5]], [[6]], [[7]], [[8]]], done=[[False], [False], [False], [False], [True], [False]], frame_stacking_state=state, stack_size=4) self.assertEqual(output.shape[0], 6) self.assertAllEqual(output[0], [[3, 2, 0, 0]]) self.assertAllEqual(output[5], [[8, 7, 0, 0]]) 
strip|avod|evaluator|checkpoint|core|id|utils def strip_checkpoint_id(checkpoint_dir): """Helper function to return the checkpoint index number.  Args: checkpoint_dir: Path directory of the checkpoints  Returns: checkpoint_id: An int representing the checkpoint index """ checkpoint_name = checkpoint_dir.split('/')[-1] return int(checkpoint_name.split('-')[-1]) 
gnmt|GNMTModel|nmt|encoder|model|build def _build_encoder(self, hparams): """Build a GNMT encoder.""" if hparams.encoder_type == 'uni' or hparams.encoder_type == 'bi': return super(GNMTModel, self)._build_encoder(hparams) if hparams.encoder_type != 'gnmt': raise ValueError('Unknown encoder_type %s' % hparams.encoder_type) num_bi_layers = 1 num_uni_layers = self.num_encoder_layers - num_bi_layers utils.print_out('# Build a GNMT encoder') utils.print_out('  num_bi_layers = %d' % num_bi_layers) utils.print_out('  num_uni_layers = %d' % num_uni_layers) source = self.features['source'] if self.time_major: source = tf.transpose(source) with tf.variable_scope('encoder'): self.encoder_emb_inp = tf.cast(self.encoder_emb_lookup_fn(self. embedding_encoder, source), self.dtype) bi_encoder_outputs, bi_encoder_state = self._build_bidirectional_rnn( inputs=self.encoder_emb_inp, sequence_length=self.features[ 'source_sequence_length'], dtype=self.dtype, hparams=hparams, num_bi_layers=num_bi_layers, num_bi_residual_layers=0) encoder_state, encoder_outputs = self._build_all_encoder_layers( bi_encoder_outputs, num_uni_layers, self.dtype, hparams) encoder_state = (bi_encoder_state[1],) + ((encoder_state,) if num_uni_layers == 1 else encoder_state) return encoder_outputs, encoder_state 
MockAgent|utils|head|testing def _head(self, neck_output): return common.AgentOutput(policy_logits=self._logits_layer(neck_output), baseline=tf.ones(shape=[tf.shape(neck_output)[0]])) 
BertForSequenceClassification|bert|pytorch|init|modeling|pretrained def __init__(self, config, num_labels): super(BertForSequenceClassification, self).__init__(config) self.num_labels = num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, num_labels) self.apply(self.init_bert_weights) 
from|images|carlini def from_carlini_images(x_ca): x = (x_ca * 2 + 1) / 2 x = x.transpose(0, 3, 1, 2) return np.asarray(x, dtype=np.float32) 
download|google|and|file|from|extract|src|master|facenet|drive def download_file_from_google_drive(file_id, destination): URL = 'https://drive.google.com/uc?export=download' session = requests.Session() response = session.get(URL, params={'id': file_id}, stream=True) token = get_confirm_token(response) if token: params = {'id': file_id, 'confirm': token} response = session.get(URL, params=params, stream=True) save_response_content(response, destination) 
KnowledgeBaseParser|knowledgebase|init|base|vkge def __init__(self, facts): self.entity_vocabulary, self.predicate_vocabulary = set(), set() for fact in facts: self.predicate_vocabulary.add(fact.predicate_name) for arg in fact.argument_names: self.entity_vocabulary.add(arg) self.entity_to_index, self.predicate_to_index = KnowledgeBaseParser._fit( self.entity_vocabulary, self.predicate_vocabulary) self.index_to_entity = {idx: e for e, idx in self.entity_to_index.items()} self.index_to_predicate = {idx: p for p, idx in self.predicate_to_index .items()} 
finetuning|lm|pair|seq|truncate|run def _truncate_seq_pair(tokens_a, tokens_b, max_length): """Truncates a sequence pair in place to the maximum length.""" while True: total_length = len(tokens_a) + len(tokens_b) if total_length <= max_length: break if len(tokens_a) > len(tokens_b): tokens_a.pop() else: tokens_b.pop() 
validate|cvact|test def validate(dist_array, top_k): accuracy = 0.0 data_amount = 0.0 for i in range(dist_array.shape[0]): gt_dist = dist_array[i, i] prediction = np.sum(dist_array[:, (i)] < gt_dist) if prediction < top_k: accuracy += 1.0 data_amount += 1.0 accuracy /= data_amount return accuracy 
update|methods|query|QueryMethod|model def update_model(self, new_model): del self.model gc.collect() self.model = new_model 
seld|architectures|get|crnn|tagger|model def get_model_crnn_seld_tagger(params_crnn=None, params_learn=None, params_extract=None): K.set_image_data_format('channels_first') n_class = params_learn.get('n_classes') input_shape = 1, params_extract.get('patch_len'), params_extract.get( 'n_mels') spec_start = Input(shape=input_shape) spec_cnn = spec_start for i, convCnt in enumerate(params_crnn.get('cnn_pool_size')): spec_cnn = Conv2D(filters=params_crnn.get('cnn_nb_filt'), kernel_size=(3, 3), padding='same')(spec_cnn) spec_cnn = Activation('relu')(spec_cnn) spec_cnn = MaxPooling2D(pool_size=(1, params_crnn.get( 'cnn_pool_size')[i]))(spec_cnn) spec_cnn = Dropout(params_crnn.get('dropout_rate'))(spec_cnn) spec_cnn = Permute((2, 1, 3))(spec_cnn) spec_rnn = Reshape((params_extract.get('patch_len'), -1))(spec_cnn) for i, nb_rnn_filt in enumerate(params_crnn.get('rnn_nb')): if len(params_crnn.get('rnn_nb')) == 1 or len(params_crnn.get('rnn_nb') ) == 2 and i == 1: spec_rnn = Bidirectional(GRU(nb_rnn_filt, activation='tanh', dropout=params_crnn.get('dropout_rate'), recurrent_dropout= params_crnn.get('dropout_rate'), return_sequences=False), merge_mode='mul')(spec_rnn) else: spec_rnn = Bidirectional(GRU(nb_rnn_filt, activation='tanh', dropout=params_crnn.get('dropout_rate'), recurrent_dropout= params_crnn.get('dropout_rate'), return_sequences=True), merge_mode='mul')(spec_rnn) for nb_fnn_filt in params_crnn.get('fc_nb'): spec_rnn = Dense(nb_fnn_filt)(spec_rnn) spec_rnn = Dropout(params_crnn.get('dropout_rate'))(spec_rnn) spec_x = Dense(n_class)(spec_rnn) out = Activation('softmax')(spec_x) _model = Model(inputs=spec_start, outputs=out) return _model 
partial|texar|get|seq|agent|agents|feeds|pg|run|SeqPGAgent def _get_partial_run_feeds(self, feeds=None): if feeds is None: feeds = [] feeds += [self._qvalue_inputs] return feeds 
get|deepctr|config|core|PredictionLayer|layers def get_config(self): config = {'task': self.task, 'use_bias': self.use_bias} base_config = super(PredictionLayer, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
residual|assert|match|gnmt|fn|nmt|model|shape def assert_shape_match(inp, out): inp.get_shape().assert_is_compatible_with(out.get_shape()) 
prefab|gym|Scrolly|drapes|parts|maybe|move|pycolab def _maybe_move(self, the_plot, motion): """Handle all aspects of single-row and/or single-column scrolling.  Implements every aspect of deciding whether to scroll one step in any of the nine possible gridworld directions (includes staying put). This amounts to:  1. Checking for scrolling orders from other entities (see `protocols/scrolling.py`), and, if present, applying them indiscriminately and returning. 2. Determining whether this `Scrolly` should scroll (e.g. one of the sprites is encroaching on the board margins). If not, returning. 3. Determining whether this `Scrolly` can scroll---that is, it's not constrained by egocentric entities, it wouldn't wind up scrolling the board off of the pattern, and so on. If not, returning. 4. Issuing a scrolling order for the scroll, and updating the curtain from the pattern.  Args: the_plot: this pycolab game's `Plot` object. motion: a 2-tuple indicating the number of rows and columns that the game board should move over the pattern if scrolling is both appropriate and possible. See class docstring for more details.  Raises: scrolling.Error: another game entity has issued a scrolling order which does not have any component in common with `motion`. """ if self._last_maybe_move_frame < the_plot.frame: self._last_maybe_move_frame = the_plot.frame self._prescroll_northwest_corner = self._northwest_corner scrolling_order = scrolling.get_order(self, the_plot, self._scrolling_group ) if scrolling_order: if motion[0] != scrolling_order[0] and motion[1] != scrolling_order[1]: raise scrolling.Error( 'The Scrolly corresponding to character {} received a fresh scrolling order, {}, which has no component in common with thecurrent action-selected motion, which is {}.' .format(self.character, scrolling_order, motion)) self._northwest_corner = things.Sprite.Position(row=scrolling_order [0] + self._northwest_corner[0], col=scrolling_order[1] + self. _northwest_corner[1]) self._update_curtain() return if motion == self._STAY: self._update_curtain() return if not self._have_margins: if scrolling.is_possible(self, the_plot, motion, self._scrolling_group ): possible_board_edge_north = self._northwest_corner[0] + motion[0] possible_board_edge_west = self._northwest_corner[1] + motion[1] can_scroll_vertically = (0 <= possible_board_edge_north <= self ._northwest_corner_limit[0]) can_scroll_horizontally = (0 <= possible_board_edge_west <= self._northwest_corner_limit[1]) scrolling_order = motion[0 ] if can_scroll_vertically else 0, motion[1 ] if can_scroll_horizontally else 0 self._northwest_corner = things.Sprite.Position(row= scrolling_order[0] + self._northwest_corner[0], col= scrolling_order[1] + self._northwest_corner[1]) scrolling.order(self, the_plot, scrolling_order, self. _scrolling_group, check_possible=False) self._update_curtain() return action_demands_vertical_scrolling = False action_demands_horizontal_scrolling = False egocentric_participants = scrolling.egocentric_participants(self, the_plot, self._scrolling_group) for entity in egocentric_participants: if (action_demands_vertical_scrolling and action_demands_horizontal_scrolling): break if not isinstance(entity, things.Sprite): continue burrowing_vertical, burrowing_horizontal = (self. _sprite_burrows_into_a_margin(entity, motion)) action_demands_vertical_scrolling |= burrowing_vertical action_demands_horizontal_scrolling |= burrowing_horizontal if not (action_demands_vertical_scrolling or action_demands_horizontal_scrolling): self._update_curtain() return scrolling_order = motion[0 ] if action_demands_vertical_scrolling else 0, motion[1 ] if action_demands_horizontal_scrolling else 0 possible_northwest_corner = things.Sprite.Position(row=scrolling_order[ 0] + self._northwest_corner[0], col=scrolling_order[1] + self. _northwest_corner[1]) we_can_actually_scroll = 0 <= possible_northwest_corner[0 ] <= self._northwest_corner_limit[0] we_can_actually_scroll &= 0 <= possible_northwest_corner[1 ] <= self._northwest_corner_limit[1] we_can_actually_scroll &= scrolling.is_possible(self, the_plot, motion, self._scrolling_group) if we_can_actually_scroll: self._northwest_corner = possible_northwest_corner scrolling.order(self, the_plot, scrolling_order, self. _scrolling_group, check_possible=False) self._update_curtain() 
layers|init|borealisflows|AffineCoupling def __init__(self, x_shape, shift_and_log_scale_fn, layer_id=0, last_layer= False, validate_args=False, name='real_nvp'): super(AffineCoupling, self).__init__(forward_min_event_ndims=1, is_constant_jacobian=False, validate_args=validate_args, name=name) self.x_shape = x_shape self.i0, self.i1, self.ic = x_shape self._last_layer = last_layer self.id = layer_id self._shift_and_log_scale_fn = shift_and_log_scale_fn self.scale = tf.get_variable('rescaling_scale{}'.format(self.id), [], dtype=DTYPE, initializer=tf.constant_initializer(0.0001)) 
tensor|official|receiver|fn|input|export|utils|build|serving def serving_input_receiver_fn(): features = tf.placeholder(dtype=dtype, shape=[batch_size] + shape, name ='input_tensor') return tf.estimator.export.TensorServingInputReceiver(features=features, receiver_tensors=features) 
gan|cr|JointLatent|latent|sample def sample_cr(self, batch_size, group_num, alpha_same, alpha_different, select_id): samples = [np.zeros((batch_size, self._in_dim)) for _ in range(group_num)] col_id = 0 reg_id = 0 for i, latent_i in enumerate(self.latent_list): if not latent_i.apply_reg: sub_samples = latent_i.sample_same(batch_size, group_num, 0.0) else: sub_samples = [np.zeros((batch_size, latent_i.in_dim)) for _ in range(group_num)] rows_select = np.where(select_id == reg_id)[0] if rows_select.shape[0] > 0: sub_samples_select = latent_i.sample_same(rows_select.shape [0], group_num, alpha_same[rows_select]) for j in range(group_num): sub_samples[j][(rows_select), :] = sub_samples_select[j] rows_not_select = np.where(select_id != reg_id)[0] if rows_not_select.shape[0] > 0: sub_samples_not_select = latent_i.sample_different( rows_not_select.shape[0], group_num, alpha_different[ rows_not_select]) for j in range(group_num): sub_samples[j][(rows_not_select), : ] = sub_samples_not_select[j] reg_id += 1 for j in range(group_num): samples[j][:, col_id:col_id + latent_i.in_dim] = sub_samples[j] col_id += latent_i.in_dim return samples 
codecs|substack|craystack def substack(codec, view_fun): """ Apply a codec on a subset of a message head.  view_fun should be a function: head -> subhead, for example view_fun = lambda head: head[0] to run the codec on only the first element of the head """ push_, pop_ = codec  def push(message, data, *args, **kwargs): head, tail = message subhead, update = util.view_update(head, view_fun) subhead, tail = push_((subhead, tail), data, *args, **kwargs) return update(subhead), tail  def pop(message, *args, **kwargs): head, tail = message subhead, update = util.view_update(head, view_fun) (subhead, tail), data = pop_((subhead, tail), *args, **kwargs) return (update(subhead), tail), data return Codec(push, pop) 
embeddings|plot|wiki|line def plot_embeddings(embeddings): X, Y = read_node_label('../data/wiki/wiki_labels.txt') emb_list = [] for k in X: emb_list.append(embeddings[k]) emb_list = np.array(emb_list) model = TSNE(n_components=2) node_pos = model.fit_transform(emb_list) color_idx = {} for i in range(len(X)): color_idx.setdefault(Y[i][0], []) color_idx[Y[i][0]].append(i) for c, idx in color_idx.items(): plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c) plt.legend() plt.show() 
write|operation|result|file|format|utils|trec|to def write_result_to_trec_format(result_dict, write_path, docnolist_file): docnolist = parse_corpus(docnolist_file) f = open(write_path, 'w') for qid, result in result_dict.items(): docid_list = result.get_docid_list() score_list = result.get_score_list() rank = 0 for docid, score in zip(docid_list, score_list): f.write('{0}\tQ0\t{1}\t{2}\t{3}\t{4}\n'.format(qid, docnolist[ docid], rank, score, result.get_runid())) rank += 1 f.close() 
list|sent|controller|id|to def _id_list_to_sent(id_list, id2word): s = [] for i in id_list: s.append(id2word[i]) return s 
kernels|isclose|assert|bo|dc|svae|utils def assert_isclose(tnsr1, tnsr2, eps=1e-06): ok = torch.all(torch.lt(torch.abs(torch.add(tnsr1, -tnsr2)), eps)) if not ok: torch.set_printoptions(precision=12, threshold=sys.maxsize, sci_mode=False) print('assert_isclose(): tnsr1\n', tnsr1) print('vs tnsr2\n', tnsr2) print('diff', torch.abs(torch.add(tnsr1, -tnsr2))) assert False 
get|test|Micro|and|no|master|city|utils|Net def test_get_city_and_no(): """ get_city_and_no() test case  :return: """ get_city_and_no(image_path= '/Users/chen/TIFF_DATA/AerialImageDataset/train/images/austin26.png') 
minibatch|fisher|ops|ConvDiagonalFB|classification|blocks|register|additional def register_additional_minibatch(self, inputs, outputs): """Registers an additional minibatch to the FisherBlock. Args: inputs: Tensor of shape [batch_size, height, width, input_size]. Inputs to the convolution. outputs: Tensor of shape [batch_size, height, width, output_size]. Layer preactivations. """ self._inputs.append(inputs) self._outputs.append(outputs) 
pdist|losses def _pdist(a, b=None): sq_sum_a = tf.reduce_sum(tf.square(a), reduction_indices=[1]) if b is None: return -2 * tf.matmul(a, tf.transpose(a)) + tf.reshape(sq_sum_a, (- 1, 1)) + tf.reshape(sq_sum_a, (1, -1)) sq_sum_b = tf.reduce_sum(tf.square(b), reduction_indices=[1]) return -2 * tf.matmul(a, tf.transpose(b)) + tf.reshape(sq_sum_a, (-1, 1) ) + tf.reshape(sq_sum_b, (1, -1)) 
layernorm|tflib|ops|Layernorm def Layernorm(name, norm_axes, inputs): mean, var = tf.nn.moments(inputs, norm_axes, keep_dims=True) n_neurons = inputs.get_shape().as_list()[norm_axes[0]] offset = lib.param(name + '.offset', np.zeros(n_neurons, dtype='float32')) scale = lib.param(name + '.scale', np.ones(n_neurons, dtype='float32')) offset = tf.reshape(offset, [-1] + [(1) for i in range(len(norm_axes) - 1)] ) scale = tf.reshape(scale, [-1] + [(1) for i in range(len(norm_axes) - 1)]) result = tf.nn.batch_normalization(inputs, mean, var, offset, scale, 1e-05) return result 
nets|pool|mobilenet|global def global_pool(input_tensor, pool_op=tf.nn.avg_pool): """Applies avg pool to produce 1x1 output.  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has baked in average pool which has better support across hardware.  Args: input_tensor: input tensor pool_op: pooling op (avg pool is default) Returns: a tensor batch_size x 1 x 1 x depth. """ shape = input_tensor.get_shape().as_list() if shape[1] is None or shape[2] is None: kernel_size = tf.convert_to_tensor([1, tf.shape(input_tensor)[1], tf.shape(input_tensor)[2], 1]) else: kernel_size = [1, shape[1], shape[2], 1] output = pool_op(input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID') output.set_shape([None, 1, 1, None]) return output 
rnn|texar|modules|init|UnidirectionalRNNEncoder|encoders def __init__(self, cell=None, cell_dropout_mode=None, output_layer=None, hparams=None): RNNEncoderBase.__init__(self, hparams) with tf.variable_scope(self.variable_scope): if cell is not None: self._cell = cell else: self._cell = layers.get_rnn_cell(self._hparams.rnn_cell, cell_dropout_mode) with tf.variable_scope(self.variable_scope): if output_layer is not None: self._output_layer = output_layer self._output_layer_hparams = None else: self._output_layer = _build_dense_output_layer(self._hparams. output_layer) self._output_layer_hparams = self._hparams.output_layer 
bbans|pixelvae|pvae|vae|view def vae_view(head): return ag_tuple((np.reshape(head[:latent_size], latent_shape), np. reshape(head[latent_size:], (batch_size,)))) 
csv|evaluate|export|similarity|to def export_to_csv(txtResults, txtCov, name=None, filenameResults= 'Results.csv', filenameCoverage='Coverage.csv'): if name: txtResults = str(name) + ',' + txtResults txtCov = str(name) + ',' + txtCov with open(filenameResults, 'a+') as file: print('%s' % str(txtResults), file=file) with open(filenameCoverage, 'a+') as file: print('%s' % str(txtCov), file=file) 
DropOut|utils def DropOut(x, rate=0.2): return tf.nn.dropout(x, keep_prob=1 - rate) 
modelnet|log|string|train def log_string(out_str): LOG_FOUT.write(out_str + '\n') LOG_FOUT.flush() print(out_str) 
sdn|params|ex4|model|cond|utils def sdn_model_params_ex4(yy, iso, gain_init): c = 1 with tf.variable_scope('sdn_gain', reuse=tf.AUTO_REUSE): gain = tf.get_variable('gain_val', [1], tf.float32, initializer=tf. constant_initializer(1.0)) iso_vals = tf.constant([100, 400, 800, 1600, 3200], dtype=tf.float32) gain_params = tf.get_variable('gain_params', [iso_vals.shape[0]], tf.float32, initializer=tf.constant_initializer(gain_init / c)) iso_idx = tf.where(tf.equal(iso_vals, iso)) gain_one_hot = tf.one_hot(iso_idx, iso_vals.shape[0]) g = tf.reduce_sum(gain_one_hot * gain_params) gain = tf.exp(c * g) * iso beta1 = tf.get_variable('beta1', [1], tf.float32, initializer=tf. constant_initializer(gain_init / c)) beta2 = tf.get_variable('beta2', [1], tf.float32, initializer=tf. constant_initializer(0.0)) beta1 = tf.exp(c * beta1) beta2 = tf.exp(c * beta2) scale = tf.sqrt(beta1 * yy / gain + beta2) return scale 
minibatch|fisher|ops|additional|classification|blocks|register|FullyConnectedDiagonalFB def register_additional_minibatch(self, inputs, outputs): """Registers an additional minibatch to the FisherBlock. Args: inputs: Tensor of shape [batch_size, input_size]. Inputs to the matrix-multiply. outputs: Tensor of shape [batch_size, output_size]. Layer preactivations. """ self._inputs.append(inputs) self._outputs.append(outputs) 
utils|sidd|que|load|minibatches def load_minibatches_que(mb_dir, ids, que, requeue=False, calc_stds=False): if calc_stds: cam_ids = ['GP', 'IP', 'S6', 'N6', 'G4'] iso_ids = ['00100', '00400', '00800', '01600', '03200'] n_cam = 5 n_iso = 5 stds = np.ndarray([n_cam, n_iso]) std_cnts = np.ndarray([n_cam, n_iso]) stds[:] = 0.0 mb = None meta = None first = True while first or requeue: first = False for i in ids: mb = np.load(os.path.join(mb_dir, 'mb_%08d.npy' % i)).item() meta = np.load(os.path.join(mb_dir, 'mb_%08d_meta.npy' % i)) mb['metadata'] = meta mb['id'] = i que.put(mb) if calc_stds: cam_idx = cam_ids.index(mb['fn'][9:11]) iso_idx = iso_ids.index(mb['fn'][12:17]) stds[cam_idx, iso_idx] += np.sqrt(np.mean((mb['_x'] / (np. sqrt(mb['_y']) + sys.float_info.epsilon)) ** 2)) std_cnts[cam_idx, iso_idx] += 1 if calc_stds: stds /= std_cnts stds = np.sqrt(stds) np.savetxt('stds.txt', stds) 
generate|search|atomic|make|batch|beam def make_batch(X): X = np.array(X) assert X.ndim in [1, 2] if X.ndim == 1: X = np.expand_dims(X, axis=0) pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1]) pos_enc = np.expand_dims(pos_enc, axis=0) batch = np.stack([X, pos_enc], axis=-1) batch = torch.tensor(batch, dtype=torch.long).to(device) return batch 
rnn|build|nmt|model|bidirectional|Model def _build_bidirectional_rnn(self, inputs, sequence_length, dtype, hparams, num_bi_layers, num_bi_residual_layers): """Create and call biddirectional RNN cells.""" fast_reverse = not hparams.use_dynamic_rnn fw_cell = self._build_encoder_cell(hparams, num_bi_layers, num_bi_residual_layers, fast_reverse) bw_cell = self._build_encoder_cell(hparams, num_bi_layers, num_bi_residual_layers, fast_reverse, True) if hparams.use_dynamic_rnn: bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, inputs, dtype=dtype, sequence_length=sequence_length, time_major=self.time_major, swap_memory=True) else: bi_outputs, bi_state = (tf.contrib.recurrent. bidirectional_functional_rnn(fw_cell, bw_cell, inputs, dtype= dtype, sequence_length=sequence_length, time_major=self. time_major, use_tpu=hparams.use_tpu, fast_reverse=True)) if self.mode == tf.contrib.learn.ModeKeys.INFER: bi_state = tuple(s[0] for s in bi_state) return tf.concat(bi_outputs, -1), bi_state 
gym|update|connect|pycolab|worlds|GameManagerSprite|four def update(self, actions, board, layers, backdrop, things, the_plot): if actions == self.shape[1]: the_plot.terminate_episode() return if int(self.shape[1] + 1 if actions is None else actions) > self.shape[1]: return if self.correct_action(board, actions): pos = self.get_postion_for_actions(board, actions) the_plot['player'] = pos, actions board[pos][actions] = ord(C_PLAYER) self.moves += 1 if self.game_over(board, the_plot['player']): the_plot.add_reward(self.won_reward) the_plot.terminate_episode() return elif self.tie(): the_plot.terminate_episode() return enemy_actions = self.enemy_action(board) random.shuffle(enemy_actions) for a in enemy_actions: if self.correct_action(board, a): pos = self.get_postion_for_actions(board, a) the_plot['enemy'] = pos, a board[pos][a] = ord(C_ENEMY) self.moves += 1 if self.game_over(board, the_plot['enemy']): the_plot.add_reward(self.lost_reward) the_plot.terminate_episode() return elif self.tie(): the_plot.terminate_episode() return break else: the_plot.add_reward(self.lost_reward) the_plot.terminate_episode() return 
tf|conv1d|for|batch|norm|util def batch_norm_for_conv1d(inputs, is_training, bn_decay, scope, is_dist=False): """ Batch normalization on 1D convolutional maps.  Args: inputs:      Tensor, 3D BLC input maps is_training: boolean tf.Varialbe, true indicates training phase bn_decay:    float or float tensor variable, controling moving average weight scope:       string, variable scope is_dist:     true indicating distributed training scheme Return: normed:      batch-normalized maps """ if is_dist: return batch_norm_dist_template(inputs, is_training, scope, [0, 1], bn_decay) else: return batch_norm_template(inputs, is_training, scope, [0, 1], bn_decay ) 
strip|texar|recur|utils|bos def _recur_strip(s): if is_str(s): if bos_token == '': return ' '.join(s.strip().split()) else: return ' '.join(s.strip().split()).replace(bos_token + ' ', '') else: s_ = [_recur_strip(si) for si in s] return _maybe_list_to_array(s_, s) 
NetworkBlock|models|wide|init|resnet def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0): super(NetworkBlock, self).__init__() self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate) 
texar|Vocab|vocabulary|map|ids|data|py|tokens|to def map_tokens_to_ids_py(self, tokens): """Maps text tokens into ids.  The input :attr:`tokens` and returned ids are both python arrays or list.  Args: tokens: A numpy array or (possibly nested) list of text tokens.  Returns: A numpy array of token ids of the same shape as :attr:`tokens`. """ return dict_lookup(self.token_to_id_map_py, tokens, self.unk_token_id) 
batches|sensor|create|correction|utils def create_batches(array, batch_size, pad=False): """Split dataset into batches of equal size.""" ix = np.arange(batch_size, array.shape[0], batch_size) batches = np.split(array, ix, 0) if len(batches[-1]) != batch_size: b = np.zeros(batches[0].shape) b[:batches[-1].shape[0]] = batches[-1] batches[-1] = b return batches 
utils|priorities|PrioritizedReplay|update @tf.function @tf.Module.with_name_scope def update_priorities(self, indices, priorities): """Updates the priorities of the items with the given indices.  Args: indices: <int64>[batch_size] tensor with the indices of the items to update. If duplicate indices are provided, the priority that will be set among possible ones is not specified. priorities: <float32>[batch_size] tensor with the new priorities. """ self._priorities.batch_scatter_update(tf.IndexedSlices(priorities, indices) ) 
plato|reinforcement|QPolicy|component|agent|action|policy|dialogue|learning|decode|q def decode_action(self, action_enc, system=True): """ Decode the action, given the role. Note that does not have to match the agent's role, as the agent may be decoding another agent's action (e.g. a system decoding the previous user act).  :param action_enc: action encoding to be decoded :param system: whether the role whose action we are decoding is a 'system' :return: the decoded action """ if system: if action_enc < len(self.dstc2_acts_sys): return [DialogueAct(self.dstc2_acts_sys[action_enc], [])] if action_enc < len(self.dstc2_acts_sys) + len(self. system_requestable_slots): return [DialogueAct('request', [DialogueActItem(self. system_requestable_slots[action_enc - len(self. dstc2_acts_sys)], Operator.EQ, '')])] if action_enc < len(self.dstc2_acts_sys) + len(self. system_requestable_slots) + len(self.requestable_slots): index = action_enc - len(self.dstc2_acts_sys) - len(self. system_requestable_slots) return [DialogueAct('inform', [DialogueActItem(self. requestable_slots[index], Operator.EQ, '')])] else: if action_enc < len(self.dstc2_acts_usr): return [DialogueAct(self.dstc2_acts_usr[action_enc], [])] if action_enc < len(self.dstc2_acts_usr) + len(self.requestable_slots): return [DialogueAct('request', [DialogueActItem(self. requestable_slots[action_enc - len(self.dstc2_acts_usr)], Operator.EQ, '')])] if action_enc < len(self.dstc2_acts_usr) + 2 * len(self. requestable_slots): return [DialogueAct('inform', [DialogueActItem(self. requestable_slots[action_enc - len(self.dstc2_acts_usr) - len(self.requestable_slots)], Operator.EQ, '')])] 
LorentzVector|Px|utilFunctions def Px(self): return self.x 
distribution|agents|init|model|network|Bernoulli|slac def __init__(self, base_depth, name=None): super(Bernoulli, self).__init__(name=name) self.dense1 = tf.keras.layers.Dense(base_depth, activation=tf.nn.leaky_relu ) self.dense2 = tf.keras.layers.Dense(base_depth, activation=tf.nn.leaky_relu ) self.output_layer = tf.keras.layers.Dense(1) 
GCNModelAE|linear|model|gae|build def _build(self): self.hidden = GraphConvolutionSparse(input_dim=self.input_dim, output_dim=FLAGS.hidden, adj=self.adj, features_nonzero=self. features_nonzero, act=tf.nn.relu, dropout=self.dropout, logging= self.logging)(self.inputs) self.z_mean = GraphConvolution(input_dim=FLAGS.hidden, output_dim=FLAGS .dimension, adj=self.adj, act=lambda x: x, dropout=self.dropout, logging=self.logging)(self.hidden) self.reconstructions = InnerProductDecoder(act=lambda x: x, logging= self.logging)(self.z_mean) 
permute|RegressionDataset|regression|data|misc|loader def permute(self, seed): perm = np.random.RandomState(seed=seed).permutation(self._data_x.shape[0]) self._data_x = self._data_x[perm] self._data_y = self._data_y[perm] 
deltas|generate|ClassifierLoader|EndToEndClassification|EnvClassification|loader def generate_deltas(self, X): """ Generates deltas for each of the mel-spectrograms.  Args: X (np.array): the mel-spectrograms.  Returns: (np.array): the mel-spectrograms with their deltas (added in the channel dimension). """ new_dim = np.zeros(np.shape(X)) X = np.concatenate((X, new_dim), axis=3) del new_dim for i in range(len(X)): X[(i), :, :, (1)] = generate_delta_031(X[(i), :, :, (0)]) return X 
seld|output|dcase2|19|file|metrics|master|description|format|evaluation|to def description_file_to_output_format(_desc_file_dict, _unique_classes, _hop_length_sec): """ Reads description file in csv format. Outputs, the dcase format results in dictionary, and additionally writes it to the _output_file  :param _unique_classes: unique classes dictionary, maps class name to class index :param _desc_file_dict: full path of the description file :param _hop_length_sec: hop length in seconds  :return: _output_dict: dcase output in dicitionary format """ _output_dict = {} for _ind, _tmp_start_sec in enumerate(_desc_file_dict['start']): _tmp_class = _unique_classes[_desc_file_dict['class'][_ind]] _tmp_azi = _desc_file_dict['azi'][_ind] _tmp_ele = _desc_file_dict['ele'][_ind] _tmp_end_sec = _desc_file_dict['end'][_ind] _start_frame = int(_tmp_start_sec / _hop_length_sec) _end_frame = int(_tmp_end_sec / _hop_length_sec) for _frame_ind in range(_start_frame, _end_frame + 1): if _frame_ind not in _output_dict: _output_dict[_frame_ind] = [] _output_dict[_frame_ind].append([_tmp_class, _tmp_azi, _tmp_ele]) return _output_dict 
unmeasure|BlurAddNoise|measure|amb|np def unmeasure_np(self, hparams, x_measured_val, theta_val): if hparams.unmeasure_type == 'wiener': x_unmeasured_val = amb_measure_utils.wiener_deconv(hparams, x_measured_val) else: raise NotImplementedError return x_unmeasured_val 
GPSig|init|kernels|SequentialMatern52 def __init__(self, input_dim, **kwargs): Sequential.__init__(self, input_dim, **kwargs) self._base_kern = self._Matern52 
utils|LoopLogger|update def update(self, i, values=None): self.n = i if self.n % self.step_size == 0 or self.n == self.max_value: if self.max_value is None: msg = 'On item ' + str(self.n) else: msg = '{:}/{:} = {:.1f}%'.format(self.n, self.max_value, 100.0 * self.n / self.max_value) if self.print_time: time_elapsed = time.time() - self.start_time time_per_step = time_elapsed / self.n msg += ', ELAPSED: {:.1f}s'.format(time_elapsed) msg += ', ETA: {:.1f}s'.format((self.max_value - self.n) * time_per_step) if values is not None: for k, v in values: msg += ' - ' + str(k) + ': ' + ('{:.4f}'.format(v) if isinstance(v, float) else str(v)) print(msg) 
python|grpc|OpsTest|starting|twice|ops|test|seed|rl|master def test_starting_twice(self): address = self.get_unix_address() server = ops.Server([address])  @tf.function(input_signature=[tf.TensorSpec([], tf.int32)]) def foo(x): return x + 1 server.bind(foo) server.start() with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, 'Server is already started'): server.start() 
functions|load|model|file def load_model_file(model_file): model_stuff = data.load_checkpoint(model_file) opt = model_stuff['opt'] state_dict = model_stuff['state_dict'] return opt, state_dict 
LocalActivationUnit|deepctr|core|layers|call def call(self, inputs, training=None, **kwargs): query, keys = inputs keys_len = keys.get_shape()[1] queries = K.repeat_elements(query, keys_len, 1) att_input = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1) att_out = self.dnn(att_input, training=training) attention_score = self.dense([att_out, self.kernel, self.bias]) return attention_score 
tradeoff|mnist|plot def plot_tradeoff(eps, mu, title_name, save): l1 = plot(np.arange(0, 1.01, 0.01), norm.cdf(norm.ppf(1 - np.arange(0, 1.01, 0.01)) - mu), color='r', linewidth=2, label=str(mu) + '-GDP by CLT') x_array1 = np.arange(0, (1 - np.exp(-eps)) / (np.exp(eps) - np.exp(-eps )), 0.01) x_array2 = np.arange((1 - np.exp(-eps)) / (np.exp(eps) - np.exp(-eps)), 1.01, 0.01) y_array1 = -np.exp(eps) * x_array1 + 1 y_array2 = -np.exp(-eps) * (x_array2 - 1) l2 = plot(np.concatenate((x_array1 + 0.003, x_array2 + 0.003)), np. concatenate((y_array1, y_array2)), color='royalblue', linewidth=2, linestyle='--', label='(' + str(eps) + ',1e-5)-DP by MA') xlabel('Type I error', fontsize=15) ylabel('Type II error', fontsize=15) xlim(0, 1) ylim(0, 1) title(title_name, fontsize=16) legend(loc='upper right', fontsize=12) gca().set_aspect('equal', adjustable='box') savefig(fname=save, format='pdf', bbox_inches='tight') show() return None 
AffineCouplingCondXY|init def __init__(self, x_shape, shift_and_log_scale_fn, layer_id=0, last_layer= False, validate_args=False, name='real_nvp'): super(AffineCouplingCondXY, self).__init__(forward_min_event_ndims=1, is_constant_jacobian=False, validate_args=validate_args, name=name) self.x_shape = x_shape self.i0, self.i1, self.ic = x_shape self._last_layer = last_layer self.id = layer_id self._shift_and_log_scale_fn = shift_and_log_scale_fn self.scale = tf.get_variable('rescaling_scale{}'.format(self.id), [], dtype=DTYPE, initializer=tf.constant_initializer(0.0001)) 
bias|conv|no|7x1|make|cnn|helpers def make_conv_7x1_no_bias(op_name, in_tensor, filters, strides=(1, 1, 1, 1), padding='VALID', weight_decay=0.0005, stddev=0.1): return make_conv_no_bias(op_name, in_tensor, 7, 1, filters, strides, padding, weight_decay, stddev) 
BertTokenizer|bert|tokenization|pytorch|init|pretrained def __init__(self, vocab_file, do_lower_case=True, max_len=None, never_split=('[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]')): if not os.path.isfile(vocab_file): raise ValueError( "Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`" .format(vocab_file)) self.vocab = load_vocab(vocab_file) self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()]) self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split) self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab) self.max_len = max_len if max_len is not None else int(1000000000000.0) 
extensions|multi|MultiModalApplication|switch|modal|and|application|connect|network|data|sampler def switch_sampler(for_training): with tf.name_scope('train' if for_training else 'validation'): sampler = self.get_sampler()[0][0 if for_training else -1] return sampler.pop_batch_op() 
bert|forward|OpenAIGPTDoubleHeadsModel|openai|pytorch|modeling|pretrained def forward(self, input_ids, mc_token_ids, lm_labels=None, mc_labels=None, token_type_ids=None, position_ids=None): hidden_states = self.transformer(input_ids, position_ids, token_type_ids) lm_logits = self.lm_head(hidden_states) mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids) losses = [] if lm_labels is not None: loss_fct = CrossEntropyLoss(ignore_index=-1) losses.append(loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))) if mc_labels is not None: loss_fct = CrossEntropyLoss() losses.append(loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))) if losses: return losses return lm_logits, mc_logits 
loadWeightsData|vgg def loadWeightsData(vgg16_npy_path=None): if vgg16_npy_path is None: path = inspect.getfile(Vgg16) path = os.path.abspath(os.path.join(path, os.pardir)) path = os.path.join(path, 'vgg16.npy') vgg16_npy_path = path print(vgg16_npy_path) return np.load(vgg16_npy_path, encoding='latin1').item() 
tangents|prod|get|stax|neural|normalising|full|outer def outer_prod_full(sqnorms1, sqnorms2): sqnorms1 = sqnorms1[:, (None), :, (None), :, (None)] sqnorms2 = sqnorms2[(None), :, (None), :, (None), :] return sqnorms1 * sqnorms2 
list|avod|add|field|BoxList|core|box def add_field(self, field, field_data): """Add field to box list.  This method can be used to add related box data such as weights/labels, etc.  Args: field: a string key to access the data via `get` field_data: a tensor containing the data to store in the BoxList """ self.data[field] = field_data 
test|tmp|inference|seed|master|facenet|affine def inference_affine_test(images): resh1 = tf.reshape(images, [-1, 27648]) affn1 = _affine(resh1, 27648, 1024) affn2 = _affine(affn1, 1024, 1024) affn3 = _affine(affn2, 1024, 1024) affn4 = _affine(affn3, 1024, 128) return affn4 
add|conv|convert|minigo|getMinigoWeightsV2 def add_conv(number, with_gamma=True): number = tensor_number(number) weight_names.append('conv2d{}/kernel:0'.format(number)) weight_names.append('conv2d{}/bias:0'.format(number)) if with_gamma: weight_names.append('batch_normalization{}/gamma:0'.format(number)) weight_names.append('batch_normalization{}/beta:0'.format(number)) weight_names.append('batch_normalization{}/moving_mean:0'.format(number)) weight_names.append('batch_normalization{}/moving_variance:0'.format( number)) 
EvalLowLevelRunner|predict|fn|thread|nmt|infeed|runner|low|level def infeed_thread_fn(sess, enqueue_ops): sess.run([enqueue_ops]) 
darc|initWrite|DataArchive def _initWrite(self): self.file.write(SIGNATURE) self.file.write(struct.pack('<I', VERSION)) self.file.write(struct.pack('<Q', 0)) self.file.write(struct.pack('<Q', 0)) 
feature|bytes|tfrecord|make|modelnet def _bytes_feature(value): """Returns a bytes_list from a string / byte.""" return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) 
models|create|ge|model|sdne def create_model(node_size, hidden_size=[256, 128], l1=1e-05, l2=0.0001): A = Input(shape=(node_size,)) L = Input(shape=(None,)) fc = A for i in range(len(hidden_size)): if i == len(hidden_size) - 1: fc = Dense(hidden_size[i], activation='relu', kernel_regularizer=l1_l2(l1, l2), name='1st')(fc) else: fc = Dense(hidden_size[i], activation='relu', kernel_regularizer=l1_l2(l1, l2))(fc) Y = fc for i in reversed(range(len(hidden_size) - 1)): fc = Dense(hidden_size[i], activation='relu', kernel_regularizer= l1_l2(l1, l2))(fc) A_ = Dense(node_size, 'relu', name='2nd')(fc) model = Model(inputs=[A, L], outputs=[A_, Y]) emb = Model(inputs=A, outputs=Y) return model, emb 
all|extensions|hved|application|u|subsets def all_subsets(l): return list(chain(*map(lambda x: combinations(l, x), range(1, len(l) + 1))) ) 
GAN|model|inference def inference(self, input_dir, result_dir): input_list = os.listdir(input_dir) if not os.path.exists(result_dir): os.mkdir(result_dir) print('Loading Model') self.build_model() print('Model Loaded') with tf.Session() as self.sess: init_op = tf.global_variables_initializer() self.sess.run(init_op) print('Loading Checkpoint') ckpt = tf.train.latest_checkpoint(self.ckpt_dir) self.saver.restore(self.sess, ckpt) self.step = tf.train.get_or_create_global_step() print('Checkpoint Loaded') for i, img_file in enumerate(input_list, 1): img = cv2.imread(os.path.join(input_dir, img_file), 1) print('Processing image', i, end='\r') img = np.expand_dims(img, axis=0) / 255 * 2 - 1 feed_dict = {self.RealA: img, self.isTrain: False} generated_B = self.FakeB.eval(feed_dict=feed_dict) generated_B = ((generated_B[0] + 1) / 2 * 255).astype(np.uint8) cv2.imwrite(os.path.join(result_dir, img_file), generated_B) print('Done.') 
81|attention|model|func|channel|free def channel_attention_free(cost_volume): x = GlobalAveragePooling3D()(cost_volume) x = Lambda(lambda y: K.expand_dims(K.expand_dims(K.expand_dims(y, 1), 1 ), 1))(x) x = Conv3D(170, 1, 1, 'same')(x) x = Activation('relu')(x) x = Conv3D(81, 1, 1, 'same')(x) x = Activation('sigmoid')(x) attention = Lambda(lambda y: K.reshape(y, (K.shape(y)[0], 1, 1, 1, 81)))(x) x = Lambda(lambda y: K.repeat_elements(y, 4, -1))(attention) return multiply([x, cost_volume]), attention 
level|nmt|runner|model|low|TrainLowLevelRunner|build def build_model(self, model_fn, params): """Build the TPU model and infeed enqueue ops.""" tf.logging.info('TrainLowLevelRunner: build_model method')  def tpu_train_step(loss): """Generate the TPU graph.""" del loss values = self.infeed_queue[0].generate_dequeue_op(tpu_device=0) unflattened_inputs = data_nest.pack_sequence_as(self. feature_structure, values) features = unflattened_inputs['features'] labels = unflattened_inputs['labels'] estimator_spec = model_fn(features, labels, tf.estimator.ModeKeys. TRAIN, params) loss, train_op = estimator_spec.loss, estimator_spec.train_op with tf.control_dependencies([train_op]): return tf.identity(loss)  def train_loop(): return tpu.repeat(self.iterations, tpu_train_step, [_INITIAL_LOSS]) with self.graph.as_default(): self.loss, = tpu.shard(train_loop, inputs=[], num_shards=self. hparams.num_shards, outputs_from_all_shards=False) global_initializer = tf.global_variables_initializer() local_initializer = tf.local_variables_initializer() graph_io.write_graph(self.graph.as_graph_def(add_shapes=True), self .hparams.out_dir, 'graph.pbtxt') self.saver = tf.train.Saver() self.sess.run(global_initializer) self.sess.run(local_initializer) checkpoint_path = tf.train.latest_checkpoint(self.hparams.out_dir) if checkpoint_path: self.saver.restore(self.sess, checkpoint_path) with self.graph.as_default(): for hook in self.hooks: hook.after_create_session(self.sess, None) 
YOLOv2|darkflow|PBLOAD|test|RETURNPREDICT def test_RETURNPREDICT_PBLOAD_YOLOv2(): options = {'pbLoad': pbPath, 'metaLoad': metaPath, 'threshold': 0.4} tfnet = TFNet(options) imgcv = cv2.imread(testImg['path']) loadedPredictions = tfnet.return_predict(imgcv) assert compareObjectData(testImg['expected-objects']['yolo'], loadedPredictions, testImg['width'], testImg['height'], threshCompareThreshold, posCompareThreshold ), 'Generated object predictions from return_predict() were not within margin of error compared to expected values.' 
cpplint|CheckMakePairUsesDeduction def CheckMakePairUsesDeduction(filename, clean_lines, linenum, error): """Check that make_pair's template arguments are deduced.  G++ 4.6 in C++11 mode fails badly if make_pair's template arguments are specified explicitly, and such use isn't intended in any case.  Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found. """ line = clean_lines.elided[linenum] match = _RE_PATTERN_EXPLICIT_MAKEPAIR.search(line) if match: error(filename, linenum, 'build/explicit_make_pair', 4, 'For C++11-compatibility, omit template arguments from make_pair OR use pair directly OR if appropriate, construct a pair directly' ) 
bert|chinese|tokenization|char|master|is|BasicTokenizer def _is_chinese_char(self, cp): """Checks whether CP is the codepoint of a CJK character.""" if (cp >= 19968 and cp <= 40959 or cp >= 13312 and cp <= 19903 or cp >= 131072 and cp <= 173791 or cp >= 173824 and cp <= 177983 or cp >= 177984 and cp <= 178207 or cp >= 178208 and cp <= 183983 or cp >= 63744 and cp <= 64255 or cp >= 194560 and cp <= 195103): return True return False 
image|target|utils|pseudorandom def pseudorandom_target_image(orig_index, total_indices): rng = np.random.RandomState(orig_index) target_img_index = orig_index while target_img_index == orig_index: target_img_index = rng.randint(0, total_indices) return target_img_index 
checkpoint|utils|save def save_checkpoint(sess, model_vars, name='model', epoch=None): model_saver = tf.train.Saver(var_list=model_vars, max_to_keep=10) model_save_path = os.path.join(FLAGS.train_dir, 'chks', name) model_saver.save(sess, model_save_path, global_step=epoch) if epoch is not None: logging.info('Model `%s` saved for epoch %d', name, epoch) else: logging.info('Final model `%s` saved', name) 
PruneMobileNetForCifar|prune|cifar|init|for|mobilenet def __init__(self, weights_dict, **prune_args): super(PruneMobileNetForCifar, self).__init__(weights_dict, **prune_args) 
ptb|PTBEnasController|enas|init|controller def __init__(self, rhn_depth=5, lstm_size=32, lstm_num_layers=2, lstm_keep_prob=1.0, tanh_constant=None, temperature=None, num_funcs=2, lr_init=0.001, lr_dec_start=0, lr_dec_every=100, lr_dec_rate=0.9, l2_reg=0, entropy_weight=None, clip_mode=None, grad_bound=None, bl_dec= 0.999, optim_algo='adam', sync_replicas=False, num_aggregate=None, num_replicas=None, name='controller'): print('-' * 80) print('Building PTBEnasController') self.rhn_depth = rhn_depth self.lstm_size = lstm_size self.lstm_num_layers = lstm_num_layers self.lstm_keep_prob = lstm_keep_prob self.tanh_constant = tanh_constant self.temperature = temperature self.num_funcs = num_funcs self.lr_init = lr_init self.lr_dec_start = lr_dec_start self.lr_dec_every = lr_dec_every self.lr_dec_rate = lr_dec_rate self.l2_reg = l2_reg self.entropy_weight = entropy_weight self.clip_mode = clip_mode self.grad_bound = grad_bound self.bl_dec = bl_dec self.optim_algo = optim_algo self.sync_replicas = sync_replicas self.num_aggregate = num_aggregate self.num_replicas = num_replicas self.name = name self._create_params() self._build_sampler() 
dark|darkflow|recollect|Layer|layer def recollect(self, w): self.w = w 
WordChatbot|chatbot|file|vocab|word @property def vocab_file(self): return self.vocab_filename 
def|CapturingContext|control|xlnet|flow|estimator|tpu|master|context|to def to_control_flow_context_def(self, context_def, export_scope=None): super(_CapturingContext, self).to_control_flow_context_def(context_def, export_scope) 
instances|vectors|datacode|LabelRepresentation|to|ner def instances_to_vectors(self, instances): ys = [] for instance in instances: ys.append(instance.label_emb) ys = np.asarray(ys) return ys 
compute|output|SequencePoolingLayer|sequence|deepctr|layers|shape def compute_output_shape(self, input_shape): if self.supports_masking: return None, 1, input_shape[-1] else: return None, 1, input_shape[0][-1] 
bert|WordpieceTokenizer|tokenization|tokenize|master def tokenize(self, text): """Tokenizes a piece of text into its word pieces.  This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary.  For example: input = "unaffable" output = ["un", "##aff", "##able"]  Args: text: A single token or whitespace separated tokens. This should have already been passed through `BasicTokenizer.  Returns: A list of wordpiece tokens. """ text = convert_to_unicode(text) output_tokens = [] for token in whitespace_tokenize(text): chars = list(token) if len(chars) > self.max_input_chars_per_word: output_tokens.append(self.unk_token) continue is_bad = False start = 0 sub_tokens = [] while start < len(chars): end = len(chars) cur_substr = None while start < end: substr = ''.join(chars[start:end]) if start > 0: substr = '##' + substr if substr in self.vocab: cur_substr = substr break end -= 1 if cur_substr is None: is_bad = True break sub_tokens.append(cur_substr) start = end if is_bad: output_tokens.append(self.unk_token) else: output_tokens.extend(sub_tokens) return output_tokens 
texar|space|SpaceTest|test|agent|agents|utils def test_space(self): """Tests descrete space. """ s = Space(shape=(), low=0, high=10, dtype=np.int32) self._test_space(s, (), 0, 10, np.dtype(np.int32)) self.assertTrue(s.contains(5)) self.assertFalse(s.contains(5.0)) self.assertFalse(s.contains(15)) s = Space(low=0, high=10, dtype=np.int32) self._test_space(s, (), 0, 10, np.dtype(np.int32)) 
topkProtect|conf|get|photo def get_conf_photo(score_matrix, label): score_matrix = softmax(score_matrix) photos_label_scores = score_matrix[:, (label)] keep_ids_rank = np.argsort(photos_label_scores)[::-1] return keep_ids_rank 
query|init|methods|CoreSetSampling def __init__(self, model, input_shape, num_labels, gpu): super().__init__(model, input_shape, num_labels, gpu) 
tangents|stax|pool|neural|5or6d|nngp|average def _average_pool_nngp_5or6d(mat, window_shape, strides, padding, normalize_edges): """Get covariances of average pooling outputs given inputs covariances `mat`.  Args: mat: a 5D or 6D `np.ndarray` containing sample-(sample-)pixel-pixel covariances. Has shape `[batch_size_1, (batch_size_2,) height, height, width, width]`. window_shape: tuple of two positive integers, the pooling spatial shape (e.g. `(3, 3)`). strides: tuple of two positive integers, the pooling strides, e.g. `(1, 1)`. padding: a `Padding` enum, e.g. `Padding.CIRCULAR`. normalize_edges: `True` to normalize output by the effective receptive field, `False` to normalize by the window size. Only has effect at the edges when `SAME` padding is used. Set to `True` to retain correspondence to `ostax.AvgPool`.  Returns: a 5D or 6D `np.ndarray` containing sample-(sample-)pixel-pixel covariances of the average pooling outputs. Has shape `[batch_size_1, (batch_size_2,) new_height, new_height, new_width, new_width]`. """ if not _is_array(mat): return mat if padding == Padding.CIRCULAR: pixel_axes = tuple(range(mat.ndim)[-4:]) mat = _same_pad_for_filter_shape(mat, _double_tuple(window_shape), _double_tuple(strides), pixel_axes, 'wrap') padding = Padding.VALID window_shape = (1,) * (mat.ndim - 4) + _double_tuple(window_shape) strides = (1,) * (mat.ndim - 4) + _double_tuple(strides) nngp_out = lax.reduce_window(mat, 0.0, lax.add, window_shape, strides, padding.name) if padding == Padding.SAME and normalize_edges: one = np.ones(mat.shape, mat.dtype) window_sizes = lax.reduce_window(one, 0.0, lax.add, window_shape, strides, padding.name) nngp_out /= window_sizes else: nngp_out /= np.prod(window_shape) return nngp_out 
strip|texar|test|UtilsTest|utils|bos def test_strip_bos(self): """Tests :func:`texar.utils.strip_bos` """ str_ = '<BOS> i am' self.assertEqual(utils.strip_bos(str_, '<BOS>'), 'i am') self.assertEqual(utils.strip_bos(str_, ''), '<BOS> i am') self.assertEqual(utils.strip_bos([str_], '<BOS>'), ['i am']) str_ = str_.split() self.assertEqual(utils.strip_bos(str_, '<BOS>', is_token_list=True), [ 'i', 'am']) self.assertEqual(utils.strip_bos([str_], '<BOS>', is_token_list=True), [['i', 'am']]) 
texar|observe|agent|agents|ActorCriticAgent|ac def _observe(self, reward, terminal, train_policy, feed_dict): self._train_actor(observ=self._observ, action=self._action, feed_dict= feed_dict) self._critic._observe(reward, terminal, train_policy, feed_dict) 
tangents|predict|fn|neural|make|expm1 def expm1_fn(evals, dt): return np.expm1(-np.maximum(evals, 0.0) * dt / normalization) 
get|tf|conditioned|vars|goal|hbaselines|globals|util def get_globals_vars(name=None): """Return the global variables.  Parameters ---------- name : str the scope  Returns ------- list of tf.Variable global variables """ return tf.compat.v1.get_collection(tf.compat.v1.GraphKeys. GLOBAL_VARIABLES, scope=name) 
bert|csqa|repr|SwagExample|run def __repr__(self): l = ['swag_id: {}'.format(self.swag_id), 'context_sentence: {}'.format( self.context_sentence), 'start_ending: {}'.format(self.start_ending ), 'ending_0: {}'.format(self.endings[0]), 'ending_1: {}'.format( self.endings[1]), 'ending_2: {}'.format(self.endings[2]), 'ending_3: {}'.format(self.endings[3]), 'ending_4: {}'.format(self. endings[4])] if self.label is not None: l.append('label: {}'.format(self.label)) return ', '.join(l) 
download|source|SRGANs|master|main|inception|Regularization|GANs|Spectral def main(args): outfile = args.outfile download_tf_params() model = Inception() set_tf_params(model) print('Saving ', outfile, '...') serializers.save_hdf5(outfile, model) 
scannet|augment|evaluate|fn1|withoverlap def augment_fn1(batch_input): augment_xyz = batch_input[:, :, 0:3] augment_xyz = data_util.rotate_perturbation_point_cloud(augment_xyz) augment_xyz = data_util.random_scale_point_cloud(augment_xyz) augment_xyz = data_util.shift_point_cloud(augment_xyz) augment_xyz = data_util.jitter_point_cloud(augment_xyz) batch_input[:, :, 0:3] = augment_xyz return batch_input 
official|test|logs|memory|logger|BenchmarkFileLoggerTest|utils|collect|info def test_collect_memory_info(self): run_info = {'machine_config': {}} logger._collect_memory_info(run_info) self.assertIsNotNone(run_info['machine_config']['memory_total']) self.assertIsNotNone(run_info['machine_config']['memory_available']) 
TestHACEnvironments|envs|validity|test|check def test_check_validity(self): bad_model_name = 'bad' model_name = 'good.xml' bad_initial_state_space = [(-1, 1), (2, 1)] initial_state_space = [(-1, 1), (1, 2)] bad_max_actions = -100 max_actions = 100 bad_timesteps_per_action = -100 timesteps_per_action = 100 self.assertRaises(AssertionError, check_validity, model_name= bad_model_name, initial_state_space=initial_state_space, max_actions=max_actions, timesteps_per_action=timesteps_per_action) self.assertRaises(AssertionError, check_validity, model_name=model_name, initial_state_space=bad_initial_state_space, max_actions= max_actions, timesteps_per_action=timesteps_per_action) self.assertRaises(AssertionError, check_validity, model_name=model_name, initial_state_space=initial_state_space, max_actions= bad_max_actions, timesteps_per_action=timesteps_per_action) self.assertRaises(AssertionError, check_validity, model_name=model_name, initial_state_space=initial_state_space, max_actions=max_actions, timesteps_per_action=bad_timesteps_per_action) 
thumt|batch|common|utils|tile def tile_batch(tensor, batch_size): shape = infer_shape(tensor) tile_dims = [1] * (tensor.shape.ndims + 1) tile_dims[1] = batch_size tensor = tf.tile(tf.expand_dims(tensor, axis=1), tile_dims) shape[0] = shape[0] * batch_size return tf.reshape(tensor, shape) 
sparse|gen|lth|plot|force|gnmt|iterative def plot_gnmt_sparse_iterative_lth_force(): common.lth_plot(network=common.GNMT, is_iterative=True, prune_method= common.UNSTRUCTURED, min_max_y=(-6, 1.5), min_max_x=(None, 0.02815), comparison_points=[(0.2, 26.86 - 26.77), (0.15, 26.52 - 26.77), ( 0.1, 26.19 - 26.77)], nbins=8, comparison_label=common.ZHU_GUPTA, to_ignore=['reinit', 'lr_lottery', 'finetune', 'lottery'], force_single=True) 
WeightContainer|bias|container|weight|classification|combine|controller def _combine_weight_bias(self): """ Combine weight and bias to sample the weight. :return: 2D Tensor of shape 'in_channels x out_channels' """ weight = tf.reshape(self._weight, (self._in_channels - 1, self. _out_channels)) bias = tf.expand_dims(self._bias, 0) return tf.concat([weight, bias], 0) 
json|save|utils|to|dict def save_dict_to_json(d, json_path): """Saves dict of floats in json file Args: d: (dict) of float-castable values (np.float, int, float, etc.) json_path: (string) path to json file """ with open(json_path, 'w') as f: d = {k: float(v) for k, v in d.items()} json.dump(d, f, indent=4) 
texar|ReplayMemoryBase|memories|core|replay|last def last(self): """Returns the latest element in the memeory. """ raise NotImplementedError 
process|applications|input|camrest|nlu|CamRestNLU|restaurants|cambridge def process_input(self, utterance, dialogue_state=None): """ Query the Ludwig model with the given utterance to obtain predictions for IOB tags and intent. Then parse the results and package them into a list of dialogue Acts.  :param utterance: a string, the utterance to be recognised :param dialogue_state: the current dialogue state, if available :return: a list of dialogue acts containing the recognised intents """ utterance = utterance.rstrip().lower() utterance = utterance.translate(self.punctuation_remover) result = self.model.predict(pd.DataFrame(data={'transcript': [utterance ]}), return_type=dict) dacts = [] last_sys_act = dialogue_state.last_sys_acts[0 ] if dialogue_state and dialogue_state.last_sys_acts else None utterance_parts = utterance.split(' ') iob_tags = [tag for tag in result['iob']['predictions'][0]] for intent in result['intent']['predictions'][0]: intent_parts = intent.split('_') intent = intent_parts[0] if intent == 'request' and len(intent_parts) > 1: dacts.append(DialogueAct(intent, [DialogueActItem(intent_parts[ 1], Operator.EQ, '')])) elif intent == 'dontcare' and len(intent_parts) > 1: if intent_parts[1] == 'this': if dialogue_state and last_sys_act and last_sys_act.params: dacts.append(DialogueAct('inform', [DialogueActItem( last_sys_act.params[0].slot, Operator.EQ, 'dontcare')]) ) else: dacts.append(DialogueAct('inform', [DialogueActItem( intent_parts[1], Operator.EQ, 'dontcare')])) elif intent not in ['inform', 'confirm', 'offer']: dacts.append(DialogueAct(intent, [])) if not dacts: intent = '' slot = '' value = '' for t in range(len(utterance_parts)): if t >= len(iob_tags): print( 'Warning! camrest_nlu cannot handle such a long sequence. Returning partial result.' ) break if iob_tags[t][0] == 'B': if value: if slot == 'name': dacts.append(DialogueAct('offer', [DialogueActItem( slot, Operator.EQ, value)])) else: dacts.append(DialogueAct('inform', [DialogueActItem (slot, Operator.EQ, value)])) else: tag_parts = iob_tags[t].split('-') intent = tag_parts[1] slot = tag_parts[2] value = utterance_parts[t] elif iob_tags[t][0] == 'I': if not value or not slot or not intent: tag_parts = iob_tags[t].split('-') intent = tag_parts[1] slot = tag_parts[2] value = utterance_parts[t] value += ' ' + utterance_parts[t] elif iob_tags[t] == 'O' and value: if slot == 'name': dacts.append(DialogueAct('offer', [DialogueActItem(slot, Operator.EQ, value)])) else: dacts.append(DialogueAct('inform', [DialogueActItem( slot, Operator.EQ, value)])) intent = '' slot = '' value = '' if value and intent: dacts.append(DialogueAct(intent, [DialogueActItem(slot, Operator.EQ, value)])) if not dacts: print( 'WARNING! camrest_nlu did not understand slots or values for utterance: {0}\n' .format(utterance)) return dacts 
Encoder|call|transformer def call(self, x, training): seq_len = tf.shape(x)[1] mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) x *= tf.math.sqrt(tf.cast(self.d_model_size, tf.float32)) x += self.pos_encoding[:, :seq_len, :] x = self.dropout(x, training=training) for i in range(self.num_layers): x = getattr(self, 'layer%i' % i)(x, training, mask) return self.layernorm(x) 
texar|eval|mode|is|py|utils def is_eval_mode_py(mode, default=False): """Returns a python boolean indicating whether the mode is EVAL.  Args: mode: A string taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`. Can be `None`. default (bool): The return value when :attr:`mode` is `None`. Default is `False`.  Returns: A python boolean. """ if mode is None: return default if mode not in context.valid_modes(): raise ValueError('Unknown mode: {}'.format(mode)) return mode == tf.estimator.ModeKeys.EVAL 
training|index|UniformIndexGenerator|vkge|call def __call__(self, n_samples, indices): if isinstance(indices, list): indices = np.array(indices) rand_ints = self.random_state.random_integers(0, indices.size - 1, n_samples) return indices[rand_ints] 
utils|test|fn|TPUEncodeTest|strategy|dataset def dataset_fn(unused_ctx):  def gen(): yield 0 yield 1 dataset = tf.data.Dataset.from_generator(gen, tf.int64) return dataset.map(lambda _: utils.tpu_encode(self.data)) 
retrofit|header|has def has_header(path): with open(path) as f: first_line = f.readline() first_line = first_line.split(' ') if len(first_line) == 2: return 1 return 0 
data|glue|download|main def main(arguments): parser = argparse.ArgumentParser() parser.add_argument('--data_dir', help='directory to save data to', type=str, default='data') parser.add_argument('--tasks', help= 'tasks to download data for as a comma separated string', type=str, default='all') parser.add_argument('--path_to_mrpc', help= 'path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt' , type=str, default='') args = parser.parse_args(arguments) if not os.path.isdir(args.data_dir): os.mkdir(args.data_dir) tasks = get_tasks(args.tasks) for task in tasks: if task == 'MRPC': import subprocess if not os.path.exists('data/MRPC'): subprocess.run('mkdir data/MRPC', shell=True) subprocess.run( 'wget -P data/MRPC/ https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt' , shell=True) subprocess.run( 'wget -P data/MRPC/ https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt' , shell=True) format_mrpc(args.data_dir, args.path_to_mrpc) subprocess.run('rm data/MRPC/msr_paraphrase_train.txt', shell=True) subprocess.run('rm data/MRPC/msr_paraphrase_test.txt', shell=True) elif task == 'diagnostic': download_diagnostic(args.data_dir) else: download_and_extract(task, args.data_dir) 
label|datacode|LabelRepresentation|idx|name|to|ner def label_name_to_label_idx(self, label): return self.label_name_to_label_idx_map[label] 
calculate|utils|projections|2d def calculate_2d_projections(coordinates_3d, intrinsics): """ Input: coordinates: [3, N] intrinsics: [3, 3] Return projected_coordinates: [N, 2] """ projected_coordinates = intrinsics @ coordinates_3d projected_coordinates = projected_coordinates[:2, : ] / projected_coordinates[(2), :] projected_coordinates = projected_coordinates.transpose() projected_coordinates = np.array(projected_coordinates, dtype=np.int32) return projected_coordinates 
fisher|ops|classification|NormalMeanVarianceNegativeLogProbLoss|mean|loss|functions|factor @property def _fisher_mean_factor(self): return 1.0 / self._scale 
datacode|init|Evaluation|ner def __init__(self, separator=' '): self.separator = separator 
StructuredFIFOQueue|utils|enqueue def enqueue(self, vals, name=None): tf.nest.assert_same_structure(vals, self._specs) return super(StructuredFIFOQueue, self).enqueue(tf.nest.flatten(vals), name=name) 
python|grpc|OpsTest|of|out|ops|test|scope|variable|seed|rl|master def test_variable_out_of_scope(self): address = self.get_unix_address() server = ops.Server([address])  def bind(): a = tf.Variable(1)  @tf.function(input_signature=[tf.TensorSpec([], tf.int32)]) def foo(x): return x + a server.bind(foo) bind() server.start() client = ops.Client(address) self.assertAllEqual(43, client.foo(42)) server.shutdown() 
tests|utils|test|layer def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None, input_data=None, expected_output=None, expected_output_dtype=None, fixed_batch_size=False, supports_masking=False): if input_data is None: if not input_shape: raise AssertionError() if not input_dtype: input_dtype = K.floatx() input_data_shape = list(input_shape) for i, e in enumerate(input_data_shape): if e is None: input_data_shape[i] = np.random.randint(1, 4) input_mask = [] if all(isinstance(e, tuple) for e in input_data_shape): input_data = [] for e in input_data_shape: input_data.append((10 * np.random.random(e)).astype( input_dtype)) if supports_masking: a = np.full(e[:2], False) a[:, :e[1] // 2] = True input_mask.append(a) else: input_data = 10 * np.random.random(input_data_shape) input_data = input_data.astype(input_dtype) if supports_masking: a = np.full(input_data_shape[:2], False) a[:, :input_data_shape[1] // 2] = True print(a) print(a.shape) input_mask.append(a) else: if input_shape is None: input_shape = input_data.shape if input_dtype is None: input_dtype = input_data.dtype if expected_output_dtype is None: expected_output_dtype = input_dtype layer = layer_cls(**kwargs) weights = layer.get_weights() layer.set_weights(weights) try: expected_output_shape = layer.compute_output_shape(input_shape) except Exception: expected_output_shape = layer._compute_output_shape(input_shape) if isinstance(input_shape, list): if fixed_batch_size: x = [Input(batch_shape=e, dtype=input_dtype) for e in input_shape] if supports_masking: mask = [Input(batch_shape=e[0:2], dtype=bool) for e in input_shape] else: x = [Input(shape=e[1:], dtype=input_dtype) for e in input_shape] if supports_masking: mask = [Input(shape=(e[1],), dtype=bool) for e in input_shape] elif fixed_batch_size: x = Input(batch_shape=input_shape, dtype=input_dtype) if supports_masking: mask = Input(batch_shape=input_shape[0:2], dtype=bool) else: x = Input(shape=input_shape[1:], dtype=input_dtype) if supports_masking: mask = Input(shape=(input_shape[1],), dtype=bool) if supports_masking: y = layer(Masking()(x), mask=mask) else: y = layer(x) if not K.dtype(y) == expected_output_dtype: raise AssertionError() if supports_masking: model = Model([x, mask], y) actual_output = model.predict([input_data, input_mask[0]]) else: model = Model(x, y) actual_output = model.predict(input_data) actual_output_shape = actual_output.shape for expected_dim, actual_dim in zip(expected_output_shape, actual_output_shape): if expected_dim is not None: if not expected_dim == actual_dim: raise AssertionError('expected_shape', expected_output_shape, 'actual_shape', actual_output_shape) if expected_output is not None: assert_allclose(actual_output, expected_output, rtol=0.001) model_config = model.get_config() recovered_model = model.__class__.from_config(model_config) if model.weights: weights = model.get_weights() recovered_model.set_weights(weights) _output = recovered_model.predict(input_data) assert_allclose(_output, actual_output, rtol=0.001) if has_arg(layer.call, 'training'): model.compile('rmsprop', 'mse') model.train_on_batch(input_data, actual_output) layer_config = layer.get_config() layer_config['batch_input_shape'] = input_shape layer = layer.__class__.from_config(layer_config) return actual_output 
codecs|buckets|craystack|from|cumulative|cdf def cdf(s): ret = np.take_along_axis(c_buckets, s[..., np.newaxis], axis=-1) return ret[..., 0] 
step|slac|choose|utils|gif def _choose_step(step): if step is None: return tf.compat.v1.train.get_or_create_global_step() if not isinstance(step, tf.Tensor): return tf.convert_to_tensor(step, tf.int64) return step 
ax|plot|fmt|flop|common def fmt_ax(ax, ax2=None): ax.set_xlabel('Compression Ratio') ax.set_ylabel('Speedup') ax.set_xscale('log') if min_max_y: ax.set_ylim((min_max_y[0], min_max_y[1])) if ax2: ax2.set_ylim((0.95, 1.3)) ax.set_xlim((ax.get_xlim()[1], ax.get_xlim()[0])) if dont_plot_x: densities = [all_densities[i] for i in range(len(all_densities)) if i not in dont_plot_x] ax.xaxis.set_major_locator(get_major_locator(all_densities, nbins=nbins)) ax.xaxis.set_minor_locator(matplotlib.ticker.NullLocator()) ax.xaxis.set_major_formatter(get_density_formatter()) ax.yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(nbins=nybins, steps=[1, 2, 2.5, 5, 10])) ax.grid(True) ax.set_ylim(0.1, ax.get_ylim()[1]) ax.yaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter( '{x:.2f}$\\times$')) fig.set_tight_layout(True) format_axes(ax) 
conv|operation|apply|nasnet|nets|NasNetABaseCell|utils def _apply_conv_operation(self, net, operation, stride, is_from_original_input, current_step): """Applies the predicted conv operation to net.""" if stride > 1 and not is_from_original_input: stride = 1 input_filters = get_channel_dim(net.shape) filter_size = self._filter_size if 'separable' in operation: net = _stacked_separable_conv(net, stride, operation, filter_size) elif operation in ['none']: if stride > 1 or input_filters != filter_size: net = tf.nn.relu(net) net = slim.conv2d(net, filter_size, 1, stride=stride, scope='1x1') net = slim.batch_norm(net, scope='bn_1') elif 'pool' in operation: net = _pooling(net, stride, operation) if input_filters != filter_size: net = slim.conv2d(net, filter_size, 1, stride=1, scope='1x1') net = slim.batch_norm(net, scope='bn_1') else: raise ValueError('Unimplemented operation', operation) if operation != 'none': net = self._apply_drop_path(net, current_step=current_step) return net 
args|from|dc|svae|policy|make|utils def make_policy_from_args(env, controller_class, max_episode_steps): env_name = env.spec.id if env_name.startswith(('Yumi', 'Franka', 'Sawyer')): from gym_bullet_extensions.control.waypts_policy import WaypointsPosPolicy, WaypointsEEPolicy, WaypointsMinJerkPolicy, WaypointsVelPolicy elif env_name.startswith('Daisy'): from gym_daisy_custom.control.gaits import DaisyGait27DPolicy, DaisyGait11DPolicy, DaisyTripod27DPolicy else: print('Please import controller class for your env', env_name) assert False policy_kwargs = {'controller_class': eval(controller_class), 'controller_dim': eval(controller_class).DIM, 't_max': max_episode_steps, 'robot': env.robot} if hasattr(env, 'get_init_pos'): policy_kwargs['get_init_pos_fxn'] = env.get_init_pos policy = StructuredPolicy(**policy_kwargs) return policy 
official|test|secs|logs|print|n|LoggingMetricHookTest|hook|metric|utils|validate|every def _validate_print_every_n_secs(self, sess, at_end): t = tf.constant(42.0, name='foo') train_op = tf.constant(3) hook = metric_hook.LoggingMetricHook(tensors=[t.name], every_n_secs=1.0, at_end=at_end, metric_logger=self._logger) hook.begin() mon_sess = monitored_session._HookedSession(sess, [hook]) sess.run(tf.global_variables_initializer()) mon_sess.run(train_op) self.assertRegexpMatches(str(self._logger.logged_metric), t.name) self._logger.logged_metric = [] mon_sess.run(train_op) self.assertEqual(str(self._logger.logged_metric).find(t.name), -1) time.sleep(1.0) self._logger.logged_metric = [] mon_sess.run(train_op) self.assertRegexpMatches(str(self._logger.logged_metric), t.name) self._logger.logged_metric = [] hook.end(sess) if at_end: self.assertRegexpMatches(str(self._logger.logged_metric), t.name) else: self.assertEqual(str(self._logger.logged_metric).find(t.name), -1) 
src|pose|decode|NMS def NMS(param, heatmaps, upsampFactor=1.0, bool_refine_center=True, bool_gaussian_filt=False): """ NonMaximaSuppression: find peaks (local maxima) in a set of grayscale images :param heatmaps: set of grayscale images on which to find local maxima (3d np.array, with dimensions image_height x image_width x num_heatmaps) :param upsampFactor: Size ratio between CPM heatmap output and the input image size. Eg: upsampFactor=16 if original image was 480x640 and heatmaps are 30x40xN :param bool_refine_center: Flag indicating whether: - False: Simply return the low-res peak found upscaled by upsampFactor (subject to grid-snap) - True: (Recommended, very accurate) Upsample a small patch around each low-res peak and fine-tune the location of the peak at the resolution of the original input image :param bool_gaussian_filt: Flag indicating whether to apply a 1d-GaussianFilter (smoothing) to each upsampled patch before fine-tuning the location of each peak. :return: a NUM_JOINTS x 5 np.array where each row represents a joint type (0=nose, 1=neck...) and the columns indicate the {x,y} position, the score (probability), a unique id (counter) and a flag this point used for assignment """ joint_list_per_joint_type = [] cnt_total_joints = 0 win_size = 2 for joint in range(NUM_JOINTS): map_orig = heatmaps[:, :, (joint)] peak_coords = find_peaks_v2(param, map_orig) peaks = np.zeros((len(peak_coords), 5)) for i, peak in enumerate(peak_coords): if bool_refine_center: x_min, y_min = np.maximum(0, peak - win_size) x_max, y_max = np.minimum(np.array(map_orig.T.shape) - 1, peak + win_size) patch = map_orig[y_min:y_max + 1, x_min:x_max + 1] map_upsamp = cv2.resize(patch, None, fx=upsampFactor, fy= upsampFactor, interpolation=cv2.INTER_CUBIC) map_upsamp = gaussian_filter(map_upsamp, sigma=3 ) if bool_gaussian_filt else map_upsamp location_of_max = np.unravel_index(map_upsamp.argmax(), map_upsamp.shape) location_of_patch_center = compute_resized_coords(peak[::-1 ] - [y_min, x_min], upsampFactor) refined_center = location_of_max - location_of_patch_center peak_score = map_upsamp[location_of_max] else: refined_center = [0, 0] peak_score = map_orig[tuple(peak[::-1])] peaks[(i), :] = tuple([int(math.floor(x)) for x in compute_resized_coords(peak_coords[i], upsampFactor) + refined_center[::-1]]) + (peak_score, cnt_total_joints, 0) cnt_total_joints += 1 joint_list_per_joint_type.append(peaks) return joint_list_per_joint_type 
image|utils|processing|ImageData def image_processing(self, filename): x = tf.read_file(filename) x_decode = tf.image.decode_jpeg(x, channels=self.channels) img = tf.image.resize_images(x_decode, [self.load_size, self.load_size]) img = tf.cast(img, tf.float32) / 127.5 - 1 if self.augment_flag: augment_size = self.load_size + (30 if self.load_size == 256 else 15) p = random.random() if p > 0.5: img = augmentation(img, augment_size) return img 
texar|train|mode|is|utils def is_train_mode(mode): """Returns a bool Tensor indicating whether the global mode is TRAIN. If :attr:`mode` is `None`, the mode is determined by :func:`texar.global_mode`. """ if mode is None: return context.global_mode_train() else: return tf.equal(mode, tf.estimator.ModeKeys.TRAIN) 
Model|init|model def __init__(self, mode): """ResNet constructor.  Args: mode: One of 'train' and 'eval'. """ self.mode = mode self._build_model() 
avod|tf|offsets|4c|encoder|core|box|to def tf_box_4c_to_offsets(boxes_4c, box_4c_gt): """Calculates box_4c offsets to regress to ground truth  Args: boxes_4c: boxes_4c to calculate offset for (N, 10) box_4c_gt: box_4c ground truth to regress to (10,)  Returns: box_4c offsets (N, 10) """ return box_4c_gt - boxes_4c 
envs|get|state|hac|Pendulum|hbaselines def get_state(self): """See parent class.""" return np.concatenate([np.cos(self.sim.data.qpos), np.sin(self.sim.data .qpos), self.sim.data.qvel]) 
evals|texar|lowercase|bleu def _lowercase(str_list): return [str_.lower() for str_ in str_list] 
gcn|ones|inits|master|text def ones(shape, name=None): """All ones.""" initial = tf.ones(shape, dtype=tf.float32) return tf.Variable(initial, name=name) 
template|borealisflows|real|layers|default|nvp def real_nvp_default_template(x_shape, is_training, hidden_layers, shift_only=False, activation=tf.nn.relu, name=None, *args, **kwargs): template_name = 'real_nvp_default_template' with tf.name_scope(name, template_name):  def _fn(x): """Fully connected MLP parameterized via `real_nvp_template`.""" i0, i1, ic = x_shape ic = int(ic / 2) output_units = i0 * i1 * ic num_output = (1 if shift_only else 2) * output_units x_flat = tf.reshape(x, (-1, i0 * i1 * ic)) for i, units in enumerate(hidden_layers): x_flat = tf.layers.dense(inputs=x_flat, units=units) x_flat = tf.cond(tf.equal(is_training, tf.constant(True)), lambda : batch_norm(x_flat, True, name='bn_nn_{}'. format(i)), lambda : batch_norm(x_flat, False, name= 'bn_nn_{}'.format(i))) x_flat = activation(x_flat) x_flat = tf.layers.dense(*args, inputs=x_flat, units=num_output, kernel_initializer=tf.zeros_initializer(), bias_initializer =tf.zeros_initializer(), activation=None, **kwargs) x = tf.reshape(x_flat, (-1, i0, i1, int(num_output / output_units * ic))) if shift_only: return x, None shift, log_scale = tf.split(x, 2, axis=-1) return shift, log_scale return tf.make_template(template_name, _fn) 
single|model|cell def single_cell(num_units, cell_type, keep_prob=1.0): """ Cell: build a recurrent cell num_units: number of hidden cell units cell_type: LSTM, GRU, LN_LSTM (layer_normalize) """ if cell_type == 'LSTM': cell = tf.nn.rnn_cell.BasicLSTMCell(num_units) elif cell_type == 'GRU': cell = tf.nn.rnn_cell.GRUCell(num_units) elif cell_type == 'LN_LSTM': cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units, layer_norm=True ) else: raise ValueError('Unknown cell type %s' % cell_type) if keep_prob < 1: cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob) return cell 
tangents|kernels|stax|neural|relu|transform|ab def _transform_kernels_ab_relu(kernels, a, b, do_backprop, do_stabilize): """Compute new kernels after an `ABRelu` layer.  See https://arxiv.org/pdf/1711.09090.pdf for the leaky ReLU derivation. """ var1, nngp, var2, ntk, marginal = (kernels.var1, kernels.nngp, kernels. var2, kernels.ntk, kernels.marginal) if do_stabilize: factor = np.max([np.max(np.abs(nngp)), 1e-12]) nngp /= factor var1 /= factor if var2 is not None: var2 /= factor prod11, prod12, prod22 = _get_normalising_prod(var1, var2, marginal) nngp, ntk = _get_ab_relu_kernel(nngp, prod12, a, b, do_backprop, ntk=ntk) if do_stabilize: nngp *= factor if marginal in (M.OVER_ALL, M.OVER_PIXELS): var1 *= (a ** 2 + b ** 2) / 2 if var2 is not None: var2 *= (a ** 2 + b ** 2) / 2 elif marginal in (M.OVER_POINTS, M.NO): var1, _ = _get_ab_relu_kernel(var1, prod11, a, b, do_backprop) if var2 is not None: var2, _ = _get_ab_relu_kernel(var2, prod22, a, b, do_backprop) else: raise NotImplementedError( 'Only implemented for `OVER_ALL`, `OVER_PIXELS`, `OVER_POINTS` and `NO`; supplied {}' .format(marginal)) if do_stabilize: var1 *= factor if var2 is not None: var2 *= factor return kernels._replace(var1=var1, nngp=nngp, var2=var2, ntk=ntk, is_gaussian=a == b, marginal=marginal) 
DTAN|get|theta|layer|DTANLayer def get_theta(self, X): theta = self.locnet.call(X) return theta 
models|loss|SampleAndAggregate|graphsage def _loss(self): for aggregator in self.aggregators: for var in aggregator.vars.values(): self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var) self.loss += self.link_pred_layer.loss(self.outputs1, self.outputs2, self.neg_outputs) tf.summary.scalar('loss', self.loss) 
set|ema|BinaryFullyConnectedLayer|decay|rate|layers def set_ema_decay_rate(self, session, new_rate): """Op to set decay  Arguments: session {tf session} -- Session new_rate {np float} -- New decay rates """ session.run(self.set_edr_op, {self.edr_ph: new_rate}) 
GAN|build|replicate def build(self): EPS = 1e-11 with tf.variable_scope('Placeholders') as scope: self.RealA = tf.placeholder(name='A', shape=[None, 256, 256, 3], dtype=tf.float32) self.RealB = tf.placeholder(name='B', shape=[None, 256, 256, 3], dtype=tf.float32) self.step = tf.train.get_or_create_global_step() with tf.variable_scope('Generator') as scope: self.FakeB = self.tiramisu(self.RealA) with tf.name_scope('Real_Discriminator'): with tf.variable_scope('Discriminator') as scope: self.predict_real = self.discriminator(self.RealA, self.RealB) with tf.name_scope('Fake_Discriminator'): with tf.variable_scope('Discriminator', reuse=True) as scope: self.predict_fake = self.discriminator(self.RealA, self.FakeB) with tf.name_scope('Real_VGG'): with tf.variable_scope('VGG') as scope: self.RealB_VGG = self.build_vgg(self.RealB) with tf.name_scope('Fake_VGG'): with tf.variable_scope('VGG', reuse=True) as scope: self.FakeB_VGG = self.build_vgg(self.FakeB) with tf.name_scope('DiscriminatorLoss'): self.D_loss = tf.reduce_mean(-(tf.log(self.predict_real + EPS) + tf .log(1 - self.predict_fake + EPS))) with tf.name_scope('GeneratorLoss'): self.gan_loss = tf.reduce_mean(-tf.log(self.predict_fake + EPS)) self.l1_loss = tf.reduce_mean(tf.abs(self.RealB - self.FakeB)) self.vgg_loss = 1e-05 * tf.losses.mean_squared_error(self.RealB_VGG, self.FakeB_VGG) self.G_loss = (self.gan_wt * self.gan_loss + self.l1_wt * self. l1_loss + self.vgg_wt * self.vgg_loss) with tf.name_scope('Summary'): dloss_sum = tf.summary.scalar('Discriminator Loss', self.D_loss) gloss_sum = tf.summary.scalar('Generator Loss', self.G_loss) gan_loss_sum = tf.summary.scalar('GAN Loss', self.gan_loss) l1_loss_sum = tf.summary.scalar('L1 Loss', self.l1_loss) vgg_loss_sum = tf.summary.scalar('VGG Loss', self.gan_loss) output_im = tf.summary.image('Output', self.FakeB, max_outputs=1) target_im = tf.summary.image('Target', self.RealB, max_outputs=1) input_im = tf.summary.image('Input', self.RealA, max_outputs=1) self.image_summary = tf.summary.merge([output_im, target_im, input_im]) self.g_summary = tf.summary.merge([gan_loss_sum, l1_loss_sum, vgg_loss_sum, gloss_sum]) self.d_summary = dloss_sum with tf.name_scope('Variables'): self.G_vars = [var for var in tf.trainable_variables() if var.name. startswith('Generator')] self.D_vars = [var for var in tf.trainable_variables() if var.name. startswith('Discriminator')] with tf.name_scope('Save'): self.saver = tf.train.Saver(max_to_keep=10) with tf.name_scope('Optimizer'): with tf.name_scope('Discriminator_Train'): discrim_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5) self.discrim_grads_and_vars = discrim_optim.compute_gradients(self .D_loss, var_list=self.D_vars) self.discrim_train = discrim_optim.apply_gradients(self. discrim_grads_and_vars, global_step=self.step) with tf.name_scope('Generator_Train'): gen_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5) self.gen_grads_and_vars = gen_optim.compute_gradients(self. G_loss, var_list=self.G_vars) self.gen_train = gen_optim.apply_gradients(self. gen_grads_and_vars, global_step=self.step) 
init|utils|dataLoaderUSR|data def __init__(self, SCALE=4): dataset_name = 'USR-248' self.SCALE = SCALE self.lr_res_, self.low_res_folder_ = self.get_lr_info() train_dir = '/mnt/data2/ImageSR/USR-248/train/' val_dir = '/mnt/data2/ImageSR/USR-248/val/' self.num_train, self.train_lr_paths, self.train_hr_paths = (self. get_lr_hr_paths(train_dir)) print('Loaded {0} pairs of image-paths for training'.format(self.num_train) ) self.num_val, self.val_lr_paths, self.val_hr_paths = self.get_lr_hr_paths( val_dir) 
texar|multi|MultiAlignedData|aligned|make|data|dataset def _make_dataset(self): datasets = [] for _, hparams_i in enumerate(self._hparams.datasets): dtype = hparams_i.data_type if _is_text_data(dtype) or _is_scalar_data(dtype): dataset = tf.data.TextLineDataset(hparams_i.files, compression_type=hparams_i.compression_type) datasets.append(dataset) elif _is_tfrecord_data(dtype): dataset = tf.data.TFRecordDataset(filenames=hparams_i.files) num_shards = hparams_i.num_shards shard_id = hparams_i.shard_id if num_shards is not None and shard_id is not None: dataset = dataset.shard(num_shards, shard_id) datasets.append(dataset) else: raise ValueError('Unknown data type: %s' % hparams_i.data_type) return tf.data.Dataset.zip(tuple(datasets)) 
bert|embedding|get|output|BertModel|master|modeling def get_embedding_output(self): """Gets output of the embedding lookup (i.e., input to the transformer).  Returns: float Tensor of shape [batch_size, seq_length, hidden_size] corresponding to the output of the embedding layer, after summing the word embeddings with the positional embeddings and the token type embeddings, then performing layer normalization. This is the input to the transformer. """ return self.embedding_output 
texar|test|AverageRecorderTest|ground|truth|recorder|recoder|utils|cal|single|average def _cal_ground_truth(n): """Calculates ((n-4)^2 + ... + n^5) / (n-4 + ... + n) """ lb = max(n - 4, 0) _sum = 0 _w = 0 for i in range(lb, n + 1): _sum += i * i _w += i if _w == 0: return 0 return _sum / _w 
official|test|logs|BaseBenchmarkLoggerTest|logger|setUp|utils def setUp(self): super(BaseBenchmarkLoggerTest, self).setUp() self._actual_log = tf.logging.info self.logged_message = None  def mock_log(*args, **kwargs): self.logged_message = args self._actual_log(*args, **kwargs) tf.logging.info = mock_log 
center|tmp|mnist|master|facenet|loss|data|type def data_type(): """Return the type of the activations, weights, and placeholder variables.""" if FLAGS.use_fp16: return tf.float16 else: return tf.float32 
WordChatbot|generate|chatbot|data|word def generate_data(self, data_dir, tmp_dir, task_id=-1): self.data_dir = data_dir self.mode = {problem.DatasetSplit.TRAIN: 'train', problem.DatasetSplit. EVAL: 'dev', problem.DatasetSplit.TEST: 'test'} filepath_fns = {problem.DatasetSplit.TRAIN: self.training_filepaths, problem.DatasetSplit.EVAL: self.dev_filepaths, problem.DatasetSplit .TEST: self.test_filepaths} split_paths = [(split['split'], filepath_fns[split['split']](data_dir, split['shards'], shuffled=self.already_shuffled)) for split in self .dataset_splits] all_paths = [] for _, paths in split_paths: all_paths.extend(paths) if self.is_generate_per_split: for split, paths in split_paths: self.preprocess_data(self.mode[split]) generator_utils.generate_files(self.generate_encoded_samples( data_dir, tmp_dir, split), paths) else: self.preprocess_data(self.mode[problem.DatasetSplit.TRAIN]) generator_utils.generate_files(self.generate_encoded_samples( data_dir, tmp_dir, problem.DatasetSplit.TRAIN), all_paths) generator_utils.shuffle_dataset(all_paths, extra_fn=self._pack_fn()) 
stdev|linear|ops|set|tflib|weights def set_weights_stdev(weights_stdev): global _weights_stdev _weights_stdev = weights_stdev 
src|layers|call|GraphConvolution def _call(self, inputs): x = inputs x = tf.nn.dropout(x, 1 - self.dropout) x = tf.matmul(x, self.vars['weights']) x = tf.sparse_tensor_dense_matmul(self.adj, x) outputs = self.act(x) return outputs 
src|dropout|cnn|model def dropout(incoming, is_training, keep_prob=0.5): return tf.contrib.layers.dropout(incoming, keep_prob=keep_prob, is_training=is_training) 
sampling|get|fn|inference|utils|thumt def _get_inference_fn(model_fns, features):  def inference_fn(inputs, state): local_features = {'source': features['source'], 'source_length': features['source_length'], 'target': tf.pad(inputs[:, 1:], [[0, 0], [0, 1]]), 'target_length': tf.fill([tf.shape(inputs)[0]], tf.shape(inputs)[1])} outputs = [] next_state = [] for model_fn, model_state in zip(model_fns, state): if model_state: output, new_state = model_fn(local_features, model_state) outputs.append(output) next_state.append(new_state) else: output = model_fn(local_features) outputs.append(output) next_state.append({}) log_prob = tf.add_n(outputs) / float(len(outputs)) return log_prob, next_state return inference_fn 
CRPM|inference|Cs|CNN|Net def inference_Cs_CNN(self): self.label = tf.placeholder(tf.int32, shape=[None], name='image_label') self.up_h_convs = OrderedDict() dw_h_convs = self.down_conv() x = dw_h_convs[self.layers - 1] flat_real = tf.reshape(x[0], [tf.shape(self.input_real)[0], -1]) flat_imag = tf.reshape(x[1], [tf.shape(self.input_real)[0], -1]) fc_real, fc_imag = complex_cross_fc(flat_real, flat_imag, 'fc', [self. features_root * 4, self.num_label], False) logits_real = tf.expand_dims(fc_real, -1) logits_imag = tf.expand_dims(fc_imag, -1) self.real = logits_real self.imag = logits_imag self.mold = tf.sqrt(tf.add(tf.square(logits_real), tf.square(logits_imag))) self.phase = tf.atan(tf.div(logits_imag, tf.add(logits_real, tf. constant(1e-08)))) self.logits = tf.concat((logits_real, logits_imag, self.mold, self. phase), -1) with tf.variable_scope('mold_phase'): self.weight = tf.get_variable(name='weight_model', shape=[4, 1], initializer=tf.contrib.layers.xavier_initializer(), dtype=tf. float32) self.logits = tf.reshape(tf.matmul(tf.reshape(self.logits, [-1, 4]), self.weight), [tf.shape(self.input_real)[0], self.num_label]) self.out = tf.argmax(self.logits, 1) 
W1|calc|gloss|w1|model def calc_gloss(self, x, y, ux, vy, config): return torch.mean(vy) 
point|maze|distance|env|utils def point_distance(p1, p2): """Return the distance between two points.  Parameters ---------- p1 : (float, float) (x,y) values of point 1 p2 : (float, float) (x,y) values of point 2  Returns ------- float the distance between the two points """ x1, y1 = p1 x2, y2 = p2 return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5 
initialize|HybridKmeans|centroids|implementation|utils def initialize_centroids(self, points): """returns k centroids from the initial points""" centroids = points.copy() np.random.shuffle(centroids) self.cluster_centers_ = centroids[:self.k_] while np.allclose(self.cluster_centers_[0], self.cluster_centers_[1]): np.random.shuffle(centroids) self.cluster_centers_ = centroids[:self.k_] 
CheckBracesSpacing|cpplint def CheckBracesSpacing(filename, clean_lines, linenum, error): """Checks for horizontal spacing near commas.  Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found. """ line = clean_lines.elided[linenum] match = Match('^(.*[^ ({>]){', line) if match: endline, endlinenum, endpos = CloseExpression(clean_lines, linenum, len(match.group(1))) trailing_text = '' if endpos > -1: trailing_text = endline[endpos:] for offset in xrange(endlinenum + 1, min(endlinenum + 3, clean_lines.NumLines() - 1)): trailing_text += clean_lines.elided[offset] if not Match('^[\\s}]*[{.;,)<>\\]:]', trailing_text): error(filename, linenum, 'whitespace/braces', 5, 'Missing space before {') if Search('}else', line): error(filename, linenum, 'whitespace/braces', 5, 'Missing space before else') if Search(':\\s*;\\s*$', line): error(filename, linenum, 'whitespace/semicolon', 5, 'Semicolon defining empty statement. Use {} instead.') elif Search('^\\s*;\\s*$', line): error(filename, linenum, 'whitespace/semicolon', 5, 'Line contains only semicolon. If this should be an empty statement, use {} instead.' ) elif Search('\\s+;\\s*$', line) and not Search('\\bfor\\b', line): error(filename, linenum, 'whitespace/semicolon', 5, 'Extra space before last semicolon. If this should be an empty statement, use {} instead.' ) 
nmt|BaseModel|model|init def __init__(self, hparams, mode, features, scope=None, extra_args=None): """Create the model.  Args: hparams: Hyperparameter configurations. mode: TRAIN | EVAL | INFER features: a dict of input features. scope: scope of the model. extra_args: model_helper.ExtraArgs, for passing customizable functions.  """ self._set_params_initializer(hparams, mode, features, scope, extra_args) res = self.build_graph(hparams, scope=scope) self._set_train_or_infer(res, hparams) 
test|testGetChannelIndex|nasnet|NasnetUtilsTest|nets|utils def testGetChannelIndex(self): data_formats = ['NHWC', 'NCHW'] for data_format in data_formats: index = nasnet_utils.get_channel_index(data_format) correct_index = 3 if data_format == 'NHWC' else 1 self.assertEqual(index, correct_index) 
incremental|graphsage|evaluate|train|supervised def incremental_evaluate(sess, model, minibatch_iter, size, test=False): t_test = time.time() finished = False val_losses = [] val_preds = [] labels = [] iter_num = 0 finished = False while not finished: feed_dict_val, batch_labels, finished, _ = (minibatch_iter. incremental_node_val_feed_dict(size, iter_num, test=test)) node_outs_val = sess.run([model.preds, model.loss], feed_dict= feed_dict_val) val_preds.append(node_outs_val[0]) labels.append(batch_labels) val_losses.append(node_outs_val[1]) iter_num += 1 val_preds = np.vstack(val_preds) labels = np.vstack(labels) f1_scores = calc_f1(labels, val_preds) return np.mean(val_losses), f1_scores[0], f1_scores[1], time.time( ) - t_test 
train|Trainer|trainer|bnn def train(self): log.infov('Training Starts!') output_save_step = 1000 buffer_save_step = 100 self.session.run(self.global_step.assign(0)) test_set = {'X': self.dataset.x_test, 'y': self.dataset.y_test} n_updates = 0 for ep in xrange(self.config.n_epoches): x_train, y_train = shuffle(self.dataset.x_train, self.dataset.y_train) max_batches = self.config.n_train // self.config.batch_size for bi in xrange(max_batches + 1): start = bi * self.config.batch_size end = min((bi + 1) * self.config.batch_size, self.config.n_train) batch_chunk = {'X': x_train[start:end], 'y': y_train[start:end]} step, summary, kl_loss, rmse, ll, step_time = self.run_single_step( batch_chunk) self.summary_writer.add_summary(summary, global_step=step) if n_updates % 50 == 0: self.log_step_message(step, rmse, ll, step_time) n_updates += 1 test_rmse, test_ll = self.session.run(self.model.get_error_and_ll(self. model.X, self.model.y, location=self.dataset.mean_y_train, scale= self.dataset.std_y_train), feed_dict=self.model.get_feed_dict(test_set) ) write_time = time.strftime('%m-%d-%H:%M:%S') with open(self.config.savepath + self.config.dataset + '_test_ll_%s.txt' % self.filepath, 'a') as f: f.write(repr(self.config.trial) + ',' + write_time + ',' + repr( test_ll) + '\n') with open(self.config.savepath + self.config.dataset + '_test_error_%s.txt' % self.filepath, 'a') as f: f.write(repr(self.config.trial) + ',' + write_time + ',' + repr( test_rmse) + '\n') if self.config.save: self.saver.save(self.session, os.path.join(self.train_dir, 'model'), global_step=step) 
content|loss|get|train def get_content_loss(output, content): masked_ = (1 - content) * tf.square(output - content) return tf.reduce_mean(masked_) 
convert|parseargs|model|old def parseargs(): parser = argparse.ArgumentParser(description='Convert old models') parser.add_argument('--input', type=str, required=True, help= 'Path of old model') parser.add_argument('--output', type=str, required=True, help= 'Path of output checkpoint') return parser.parse_args() 
tmp|master|network|facenet|affine def affine(inpOp, nIn, nOut, name, weight_decay=0.0): with tf.variable_scope(name): l2_regularizer = lambda t: l2_loss(t, weight=weight_decay) weights = tf.get_variable('weights', [nIn, nOut], initializer=tf. truncated_normal_initializer(stddev=0.1), regularizer= l2_regularizer, dtype=inpOp.dtype) biases = tf.get_variable('biases', [nOut], initializer=tf. constant_initializer(), dtype=inpOp.dtype) affine1 = tf.nn.relu_layer(inpOp, weights, biases) return affine1 
house|get|Graph|neighbors|utils def get_neighbors(self, source): return sorted(self.get_connections(source).keys()) 
construct|img|save|delta|patch def save_img(img, adv_img, fname): means = np.expand_dims(np.expand_dims(_CHANNEL_MEANS, 0), 0) img = (img + means).astype(np.uint8) adv_img = (adv_img + means).astype(np.uint8) pil_img = Image.fromarray(adv_img, 'RGB') pil_img.save(tf.gfile.GFile(os.path.join(os.getcwd(), 'data', 'bamboo_forest_patch', fname + '.jpg'), 'w')) 
kaffe|layers|LayerAdapter|parameters @property def parameters(self): name = NodeDispatch.get_handler_name(self.kind) name = '_'.join((name, 'param')) try: return getattr(self.layer, name) except AttributeError: raise NodeDispatchError( 'Caffe parameters not found for layer kind: %s' % self.kind) 
download|attributionpriors|feature|int64|mnist def _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=[value])) 
level|nmt|loop|runner|model|train|low|TrainLowLevelRunner|build def train_loop(): return tpu.repeat(self.iterations, tpu_train_step, [_INITIAL_LOSS]) 
fisher|FullFactor|ops|var|scope|classification|factors @property def _var_scope(self): return 'ff_full/' + self._orig_params_grads_name 
config|session|translator def session_config(params): optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions. L1, do_function_inlining=False) graph_options = tf.GraphOptions(optimizer_options=optimizer_options) config = tf.ConfigProto(allow_soft_placement=True, graph_options= graph_options) if params.device_list: device_str = ','.join([str(i) for i in params.device_list]) config.gpu_options.visible_device_list = device_str return config 
json|utils|to def to_json(output_path, *layers): with open(output_path, 'w') as layer_f: lines = '' for w, b, bn in layers: layer_idx = w.name.split('/')[0].split('h')[1] B = b.eval() if 'lin/' in w.name: W = w.eval() depth = W.shape[1] else: W = np.rollaxis(w.eval(), 2, 0) depth = W.shape[0] biases = {'sy': 1, 'sx': 1, 'depth': depth, 'w': [('%.2f' % elem) for elem in list(B)]} if bn != None: gamma = bn.gamma.eval() beta = bn.beta.eval() gamma = {'sy': 1, 'sx': 1, 'depth': depth, 'w': [('%.2f' % elem) for elem in list(gamma)]} beta = {'sy': 1, 'sx': 1, 'depth': depth, 'w': [('%.2f' % elem) for elem in list(beta)]} else: gamma = {'sy': 1, 'sx': 1, 'depth': 0, 'w': []} beta = {'sy': 1, 'sx': 1, 'depth': 0, 'w': []} if 'lin/' in w.name: fs = [] for w in W.T: fs.append({'sy': 1, 'sx': 1, 'depth': W.shape[0], 'w': [('%.2f' % elem) for elem in list(w)]}) lines += ( """ var layer_%s = { "layer_type": "fc", "sy": 1, "sx": 1, "out_sx": 1, "out_sy": 1, "stride": 1, "pad": 0, "out_depth": %s, "in_depth": %s, "biases": %s, "gamma": %s, "beta": %s, "filters": %s };""" % (layer_idx.split('_')[0], W.shape[1], W.shape[0], biases, gamma, beta, fs)) else: fs = [] for w_ in W: fs.append({'sy': 5, 'sx': 5, 'depth': W.shape[3], 'w': [('%.2f' % elem) for elem in list(w_.flatten())]}) lines += ( """ var layer_%s = { "layer_type": "deconv", "sy": 5, "sx": 5, "out_sx": %s, "out_sy": %s, "stride": 2, "pad": 1, "out_depth": %s, "in_depth": %s, "biases": %s, "gamma": %s, "beta": %s, "filters": %s };""" % (layer_idx, 2 ** (int(layer_idx) + 2), 2 ** (int( layer_idx) + 2), W.shape[0], W.shape[3], biases, gamma, beta, fs)) layer_f.write(' '.join(lines.replace("'", '').split())) 
infer|decoder|fn|dec|loop|decoding def _dec_loop_fn(i, prev_id, dec_state, dec_outputs, dec_out_index): dec_in = tf.nn.embedding_lookup(embedding_matrix, prev_id) dec_out, dec_state = dec_cell(dec_in, dec_state) dec_outputs = dec_outputs.write(i, dec_out) dec_logits = dec_proj(dec_out) dec_index = tf.argmax(dec_logits, axis=1, output_type=tf.int32) dec_out_index = dec_out_index.write(i, dec_index) return i + 1, dec_index, dec_state, dec_outputs, dec_out_index 
ConversationalGenericAgent|plato|agent|generic|conversational|dialogue|continue def continue_dialogue(self, args=None): """ Perform one dialogue turn  :param args: input to this agent :return: output of this agent """ utterance = None if self.INTERACTION_MODE == 'text' and not self.USE_GUI: utterance = input('USER > ') self.prev_m_out = ConversationalFrame(utterance) elif self.INTERACTION_MODE == 'speech' and not self.USE_GUI: with speech_rec.Microphone() as source: print('(listening...)') audio = self.asr.listen(source, phrase_time_limit=3) try: utterance = self.asr.recognize_google(audio) print('Google ASR: ' + utterance) self.prev_m_out = ConversationalFrame(utterance) except speech_rec.UnknownValueError: print('Google ASR did not understand you') except speech_rec.RequestError as e: print('Google ASR request error: {0}'.format(e)) elif args and 'input' in args: self.prev_m_out = ConversationalFrame(args['input']) for m in self.ConversationalModules: if isinstance(m, list): idx = 0 prev_m_out = deepcopy(self.prev_m_out) self.prev_m_out.content = {} for sm in m: sm.generic_receive_input(prev_m_out) sm_out = sm.generic_generate_output(prev_m_out) if not isinstance(sm_out, ConversationalFrame): sm_out = ConversationalFrame(sm_out) self.prev_m_out.content['sm' + str(idx)] = sm_out.content idx += 1 else: m.generic_receive_input(self.prev_m_out) self.prev_m_out = m.generic_generate_output(self.prev_m_out) if not isinstance(self.prev_m_out, ConversationalFrame): self.prev_m_out = ConversationalFrame(self.prev_m_out) if isinstance(self.prev_m_out.content, str): print(f'(DEBUG) {self.agent_role}> {str(self.prev_m_out.content)}') self.dialogue_turn += 1 return {'input_utterance': utterance, 'output_raw': self.prev_m_out. content, 'output_dacts': '', 'goal': self.agent_goal} 
detections|unmold|model|MaskRCNN def unmold_detections(self, detections, mrcnn_mask, mrcnn_coord, image_shape, window): """Reformats the detections of one image from the format of the neural network output to a format suitable for use in the rest of the application.  detections: [N, (y1, x1, y2, x2, class_id, score)] mrcnn_mask: [N, height, width, num_classes] mrcnn_coord: [N, height, width, num_classes, 3] image_shape: [height, width, depth] Original size of the image before resizing window: [y1, x1, y2, x2] Box in the image where the real image is excluding the padding.  Returns: boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels class_ids: [N] Integer class IDs for each bounding box scores: [N] Float probability scores of the class_id masks: [height, width, num_instances] Instance masks coords: [height, width, num_instances] """ zero_ix = np.where(detections[:, (4)] == 0)[0] N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0] boxes = detections[:N, :4] class_ids = detections[:N, (4)].astype(np.int32) scores = detections[:N, (5)] masks = mrcnn_mask[(np.arange(N)), :, :, (class_ids)] coords = mrcnn_coord[(np.arange(N)), :, :, (class_ids), :] exclude_ix = np.where((boxes[:, (2)] - boxes[:, (0)]) * (boxes[:, (2)] - boxes[:, (0)]) <= 0)[0] if exclude_ix.shape[0] > 0: boxes = np.delete(boxes, exclude_ix, axis=0) class_ids = np.delete(class_ids, exclude_ix, axis=0) scores = np.delete(scores, exclude_ix, axis=0) masks = np.delete(masks, exclude_ix, axis=0) coords = np.delete(coords, exclude_ix, axis=0) N = class_ids.shape[0] h_scale = image_shape[0] / (window[2] - window[0]) w_scale = image_shape[1] / (window[3] - window[1]) scale = min(h_scale, w_scale) shift = window[:2] scales = np.array([scale, scale, scale, scale]) shifts = np.array([shift[0], shift[1], shift[0], shift[1]]) boxes = np.multiply(boxes - shifts, scales).astype(np.int32) full_masks = [] full_coords = [] for i in range(N): full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape) full_masks.append(full_mask) full_coord = utils.unmold_coord(coords[i], boxes[i], image_shape) full_coords.append(full_coord) full_masks = np.stack(full_masks, axis=-1) if full_masks else np.empty( (0,) + masks.shape[1:3]) full_coords = np.stack(full_coords, axis=-2) if full_coords else np.empty( (0,) + coords.shape[1:4]) return boxes, class_ids, scores, full_masks, full_coords 
PlotAirf|y def y(theta, P, T, B, E, R, C): xx = x(theta, B) return T / 2 * np.abs(np.sin(theta)) ** B / np.sin(theta) * (1 - xx ** P ) + C * np.sin(xx ** E * np.pi) + R * np.sin(2 * np.pi * xx) 
env|utils|maze|construct def construct_maze(maze_id='Maze'): """Construct the structure of the maze.  Parameters ---------- maze_id : str type of environment. This dictates the structure the maze will adopt.  Returns ------- list of Any the structure of the maze """ if maze_id == 'Maze': structure = [[1, 1, 1, 1, 1], [1, 'r', 0, 0, 1], [1, 1, 1, 0, 1], [ 1, 0, 0, 0, 1], [1, 1, 1, 1, 1]] elif maze_id == 'Push': structure = [[1, 1, 1, 1, 1], [1, 0, 'r', 1, 1], [1, 0, Move.XY, 0, 1], [1, 1, 0, 1, 1], [1, 1, 1, 1, 1]] elif maze_id == 'Fall': structure = [[1, 1, 1, 1], [1, 'r', 0, 1], [1, 0, Move.YZ, 1], [1, -1, -1, 1], [1, 0, 0, 1], [1, 1, 1, 1]] elif maze_id == 'Block': structure = [[1, 1, 1, 1, 1], [1, 'r', 0, 0, 1], [1, 0, 0, 0, 1], [ 1, 0, 0, 0, 1], [1, 1, 1, 1, 1]] elif maze_id == 'BlockMaze': structure = [[1, 1, 1, 1], [1, 'r', 0, 1], [1, 1, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1]] else: raise NotImplementedError( 'The provided MazeId %s is not recognized' % maze_id) return structure 
texar|decoders|name|data|VarUttTextDataDecoder|tensor|text @text_tensor_name.setter def text_tensor_name(self, name): self._text_tensor_name = name 
utils|say|espeakng|ESpeakNG def say(self, txt, sync=False): txte = txt.encode('utf8') args = [] if self._audio_dev: args.extend(['-d', self._audio_dev]) args.append(txte) self._espeak_exe(args, sync=sync) 
get|test|inputs|and|monte|model|carlo def _get_inputs_and_model(width=1, n_classes=2, use_conv=True): key = random.PRNGKey(1) key, split = random.split(key) x1 = random.normal(key, (8, 4, 3, 2)) x2 = random.normal(split, (4, 4, 3, 2)) if not use_conv: x1 = np.reshape(x1, (x1.shape[0], -1)) x2 = np.reshape(x2, (x2.shape[0], -1)) init_fn, apply_fn, kernel_fn = stax.serial(stax.Conv(width, (3, 3)) if use_conv else stax.Dense(width), stax.Relu(), stax.Flatten(), stax. Dense(n_classes, 2.0, 0.5)) return x1, x2, init_fn, apply_fn, kernel_fn, key 
label|bbox|obj|indoor3d|to|util def bbox_label_to_obj(input_filename, out_filename_prefix, easy_view=False): """ Visualization of bounding boxes.  Args: input_filename: each line is x1 y1 z1 x2 y2 z2 label out_filename_prefix: OBJ filename prefix, visualize object by g_label2color easy_view: if True, only visualize furniture and floor Returns: output a list of OBJ file and MTL files with the same prefix """ bbox_label = np.loadtxt(input_filename) bbox = bbox_label[:, 0:6] label = bbox_label[:, (-1)].astype(int) v_cnt = 0 ins_cnt = 0 for i in range(bbox.shape[0]): if easy_view and label[i] not in g_easy_view_labels: continue obj_filename = out_filename_prefix + '_' + g_classes[label[i] ] + '_' + str(ins_cnt) + '.obj' mtl_filename = out_filename_prefix + '_' + g_classes[label[i] ] + '_' + str(ins_cnt) + '.mtl' fout_obj = open(obj_filename, 'w') fout_mtl = open(mtl_filename, 'w') fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename)) length = bbox[(i), 3:6] - bbox[(i), 0:3] a = length[0] b = length[1] c = length[2] x = bbox[i, 0] y = bbox[i, 1] z = bbox[i, 2] color = np.array(g_label2color[label[i]], dtype=float) / 255.0 material = 'material%d' % ins_cnt fout_obj.write('usemtl %s\n' % material) fout_obj.write('v %f %f %f\n' % (x, y, z + c)) fout_obj.write('v %f %f %f\n' % (x, y + b, z + c)) fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c)) fout_obj.write('v %f %f %f\n' % (x + a, y, z + c)) fout_obj.write('v %f %f %f\n' % (x, y, z)) fout_obj.write('v %f %f %f\n' % (x, y + b, z)) fout_obj.write('v %f %f %f\n' % (x + a, y + b, z)) fout_obj.write('v %f %f %f\n' % (x + a, y, z)) fout_obj.write('g default\n') v_cnt = 0 fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt)) fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt)) fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt)) fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt)) fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt)) fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt)) fout_obj.write('\n') fout_mtl.write('newmtl %s\n' % material) fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2])) fout_mtl.write('\n') fout_obj.close() fout_mtl.close() v_cnt += 8 ins_cnt += 1 
fisher|compute|ops|grads|classification|tensors|FullFB|blocks|to def tensors_to_compute_grads(self): return self._params 
deep|LudwigPolicy|ludwig|plato|del|component|agent|policy|dialogue|learning def __del__(self): """ Close the Ludwig model.  :return: """ if self.model: self.model.close() 
BertModelTest|bert|output|test|master|BertModelTester|check|modeling def check_output(self, result): self.parent.assertAllEqual(result['embedding_output'].shape, [self. batch_size, self.seq_length, self.hidden_size]) self.parent.assertAllEqual(result['sequence_output'].shape, [self. batch_size, self.seq_length, self.hidden_size]) self.parent.assertAllEqual(result['pooled_output'].shape, [self. batch_size, self.hidden_size]) 
xlnet|ImdbProcessor|create|examples|master|run|classifier def _create_examples(self, data_dir): examples = [] for label in ['neg', 'pos']: cur_dir = os.path.join(data_dir, label) for filename in tf.gfile.ListDirectory(cur_dir): if not filename.endswith('txt'): continue path = os.path.join(cur_dir, filename) with tf.gfile.Open(path) as f: text = f.read().strip().replace('<br />', ' ') examples.append(InputExample(guid='unused_id', text_a=text, text_b=None, label=label)) return examples 
post1|bbans|pixelvae|fn|elem|pvae|layer|param|two def post1_elem_param_fn(z2, mu1_post, sig1_post):  def g(z1_idxs, params=None, idx=None): _, _, mu1_prior, sig1_prior = np.moveaxis(params, -1, 0) z1 = mu1_prior + sig1_prior * bb_ans.std_gaussian_centres( prior_precision)[z1_idxs] mu1_prior, sig1_prior = gen_net1(z1, z2) return np.stack((mu1_post, sig1_post, mu1_prior, sig1_prior), axis=-1) return g 
convert|open|vocab def _open(filename, mode='r', encoding='utf-8'): if sys.version_info.major == 2: return open(filename, mode=mode) elif sys.version_info.major == 3: return open(filename, mode=mode, encoding=encoding) else: raise RuntimeError('Unknown Python version for running!') 
gcn|master|utils|loadWord2Vec|text def loadWord2Vec(filename): """Read Word Vectors""" vocab = [] embd = [] word_vector_map = {} file = open(filename, 'r') for line in file.readlines(): row = line.strip().split(' ') if len(row) > 2: vocab.append(row[0]) vector = row[1:] length = len(vector) for i in range(length): vector[i] = float(vector[i]) embd.append(vector) word_vector_map[row[0]] = vector print('Loaded Word Vectors!') file.close() return vocab, embd, word_vector_map 
init|ResidualGraphConvolutionalNetwork|model def __init__(self, train_batch_size, val_batch_size, num_layers=2, hidden_units=2048, init_weights=1e-05, layer_decay=0.4): self.train_batch_size = train_batch_size self.val_batch_size = val_batch_size self.num_layers = num_layers self.hidden_units = hidden_units self.init_weights = init_weights self.layer_decay = layer_decay 
texar|get|test|fn|activation|GetActivationFnTest|core|layers def test_get_activation_fn(self): """Tests. """ fn = layers.get_activation_fn() self.assertEqual(fn, tf.identity) fn = layers.get_activation_fn('relu') self.assertEqual(fn, tf.nn.relu) inputs = tf.random_uniform([64, 100], -5, 20, dtype=tf.int32) fn = layers.get_activation_fn('leaky_relu') fn_output = fn(inputs) ref_output = tf.nn.leaky_relu(inputs) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) fn_output_, ref_output_ = sess.run([fn_output, ref_output]) np.testing.assert_array_equal(fn_output_, ref_output_) fn = layers.get_activation_fn('leaky_relu', kwargs={'alpha': 0.1}) fn_output = fn(inputs) ref_output = tf.nn.leaky_relu(inputs, alpha=0.1) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) fn_output_, ref_output_ = sess.run([fn_output, ref_output]) np.testing.assert_array_equal(fn_output_, ref_output_) 
setattr|utils|AttributeDict def __setattr__(self, key, value): self.__setitem__(key, value) 
nmt|utils|get|iterator def get_iterator(src_dataset, tgt_dataset, src_vocab_table, tgt_vocab_table, batch_size, global_batch_size, sos, eos, random_seed, num_buckets, src_max_len=None, tgt_max_len=None, num_parallel_calls=4, output_buffer_size=None, skip_count=None, num_shards=1, shard_index=0, reshuffle_each_iteration=True, use_char_encode=False, filter_oversized_sequences=False): """Function that returns input dataset.""" mlperf_log.gnmt_print(key=mlperf_log.PREPROC_NUM_TRAIN_EXAMPLES, value= 4068191) if not output_buffer_size: output_buffer_size = global_batch_size * 100 if use_char_encode: src_eos_id = vocab_utils.EOS_CHAR_ID else: src_eos_id = tf.cast(src_vocab_table.lookup(tf.constant(eos)), tf.int32 ) tgt_sos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(sos)), tf.int32) tgt_eos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(eos)), tf.int32) src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset)) mlperf_log.gnmt_print(key=mlperf_log.INPUT_SHARD, value=num_shards) src_tgt_dataset = src_tgt_dataset.shard(num_shards, shard_index) if skip_count is not None: src_tgt_dataset = src_tgt_dataset.skip(skip_count) src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt: (tf.string_split ([src]).values, tf.string_split([tgt]).values), num_parallel_calls= num_parallel_calls).prefetch(output_buffer_size) src_tgt_dataset = src_tgt_dataset.filter(lambda src, tgt: tf. logical_and(tf.size(src) > 0, tf.size(tgt) > 0)) if filter_oversized_sequences: src_tgt_dataset = src_tgt_dataset.filter(lambda src, tgt: tf. logical_and(tf.size(src) < src_max_len, tf.size(tgt) < tgt_max_len) ) if src_max_len: src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt: (src[: src_max_len], tgt), num_parallel_calls=num_parallel_calls ).prefetch(output_buffer_size) if tgt_max_len: src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt: (src, tgt[: tgt_max_len]), num_parallel_calls=num_parallel_calls).prefetch( output_buffer_size) mlperf_log.gnmt_print(key=mlperf_log.PREPROC_TOKENIZE_TRAINING) if use_char_encode: src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt: (tf.reshape( vocab_utils.tokens_to_bytes(src), [-1]), tf.cast( tgt_vocab_table.lookup(tgt), tf.int32)), num_parallel_calls= num_parallel_calls) else: src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt: (tf.cast( src_vocab_table.lookup(src), tf.int32), tf.cast(tgt_vocab_table .lookup(tgt), tf.int32)), num_parallel_calls=num_parallel_calls) src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size) src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt: (src, tf.concat( ([tgt_sos_id], tgt), 0), tf.concat((tgt, [tgt_eos_id]), 0)), num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size) if use_char_encode: src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt_in, tgt_out: (src, tgt_in, tgt_out, tf.to_int32(tf.size(src) / vocab_utils. DEFAULT_CHAR_MAXLEN), tf.size(tgt_in)), num_parallel_calls= num_parallel_calls) else: src_tgt_dataset = src_tgt_dataset.map(lambda src, tgt_in, tgt_out: (src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in)), num_parallel_calls=num_parallel_calls) src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size) src_tgt_dataset = src_tgt_dataset.cache() src_tgt_dataset = src_tgt_dataset.shuffle(output_buffer_size, random_seed, reshuffle_each_iteration).repeat()  def batching_func(x): return x.padded_batch(batch_size, padded_shapes=(tf.TensorShape([ src_max_len]), tf.TensorShape([tgt_max_len]), tf.TensorShape([ tgt_max_len]), tf.TensorShape([]), tf.TensorShape([])), padding_values=(src_eos_id, tgt_eos_id, tgt_eos_id, 0, 0), drop_remainder=True) if num_buckets > 1:  def key_func(unused_1, unused_2, unused_3, src_len, tgt_len): """Calculate bucket_width by maximum source sequence length.""" if src_max_len: bucket_width = (src_max_len + num_buckets - 1) // num_buckets else: bucket_width = 10 bucket_id = tf.maximum(src_len // bucket_width, tgt_len // bucket_width) return tf.to_int64(tf.minimum(num_buckets, bucket_id))  def reduce_func(unused_key, windowed_data): return batching_func(windowed_data) batched_dataset = src_tgt_dataset.apply(tf.contrib.data. group_by_window(key_func=key_func, reduce_func=reduce_func, window_size=global_batch_size)) else: batched_dataset = batching_func(src_tgt_dataset) batched_dataset = batched_dataset.map(lambda src, tgt_in, tgt_out, source_size, tgt_in_size: {'source': src, 'target_input': tgt_in, 'target_output': tgt_out, 'source_sequence_length': source_size, 'target_sequence_length': tgt_in_size}) return batched_dataset 
output|Output|model|save|latest def save_model_latest(self, session, epoch, g_step): self.tf_saver_latest.save(session, '{}-{}-latest'.format(self. model_file_base, epoch), global_step=g_step) 
parallel|shard|features|utils|thumt def shard_features(features, device_list): num_datashards = len(device_list) sharded_features = {} with tf.device('/cpu:0'): for k, v in six.iteritems(features): v = tf.convert_to_tensor(v) if not v.shape.as_list(): v = tf.expand_dims(v, axis=-1) v = tf.tile(v, [num_datashards]) batch_size = tf.shape(v)[0] size_splits = [] for i in range(num_datashards): size_splits.append(tf.cond(tf.greater(tf.mod(batch_size, num_datashards), i), lambda : batch_size // num_datashards + 1, lambda : batch_size // num_datashards)) sharded_features[k] = tf.split(v, size_splits, 0) datashard_to_features = [] for d in range(num_datashards): feat = {k: v[d] for k, v in six.iteritems(sharded_features)} datashard_to_features.append(feat) return datashard_to_features 
lrp|dot|product|utils|thumt def dot_product(inputs, params): output = tf.identity(inputs[0]) for i in range(1, len(inputs)): output *= inputs[i] weight_ratios = wr.weight_ratio_dot_product(inputs, output) return {'output': output, 'weight_ratios': weight_ratios} 
actor|problem|action|ndh|select|NDHProblem def select_actor_action(self, env_output, agent_output): oracle_next_action = env_output.observation[constants.ORACLE_NEXT_ACTION] oracle_next_action_indices = tf.where(tf.equal(env_output.observation[ constants.CONN_IDS], oracle_next_action)) oracle_next_action_idx = tf.reduce_min(oracle_next_action_indices) assert self._mode, 'mode must be set.' if self._mode == 'train': if self._loss_type == common.CE_LOSS: action_idx = oracle_next_action_idx elif self._loss_type == common.AC_LOSS: action_idx = tfp.distributions.Categorical(logits=agent_output. policy_logits).sample() else: raise ValueError('Unsupported loss type {}'.format(self._loss_type) ) else: action_idx = tf.argmax(agent_output.policy_logits, axis=-1) action_val = env_output.observation[constants.CONN_IDS][action_idx] return common.ActorAction(chosen_action_idx=int(action_idx.numpy()), oracle_next_action_idx=int(oracle_next_action_idx.numpy())), int( action_val.numpy()) 
xlnet|configure|tpu|master|model|utils def configure_tpu(FLAGS): if FLAGS.use_tpu: tpu_cluster = tf.contrib.cluster_resolver.TPUClusterResolver(FLAGS. tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project) master = tpu_cluster.get_master() else: tpu_cluster = None master = FLAGS.master session_config = tf.ConfigProto(allow_soft_placement=True) if FLAGS.use_tpu: strategy = None tf.logging.info('Use TPU without distribute strategy.') elif FLAGS.num_core_per_host == 1: strategy = None tf.logging.info('Single device mode.') else: strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS. num_core_per_host) tf.logging.info('Use MirroredStrategy with %d devices.', strategy. num_replicas_in_sync) per_host_input = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2 run_config = tf.contrib.tpu.RunConfig(master=master, model_dir=FLAGS. model_dir, session_config=session_config, tpu_config=tf.contrib.tpu .TPUConfig(iterations_per_loop=FLAGS.iterations, num_shards=FLAGS. num_hosts * FLAGS.num_core_per_host, per_host_input_for_training= per_host_input), keep_checkpoint_max=FLAGS.max_save, save_checkpoints_secs=None, save_checkpoints_steps=FLAGS.save_steps, train_distribute=strategy) return run_config 
concat|ops|commons|conv|cond def conv_cond_concat(x, y): """Concatenate conditioning vector on feature map axis.""" x_shapes = x.get_shape() y_shapes = y.get_shape() return tf.concat([x, y * tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3) 
utils|align def align(class_ids, masks, coords, depth, intrinsics, synset_names, image_path, save_path=None, if_norm=False, with_scale=True, verbose=False): num_instances = len(class_ids) error_messages = '' elapses = [] if num_instances == 0: return np.zeros((0, 4, 4)), np.ones((0, 3)), error_messages, elapses RTs = np.zeros((num_instances, 4, 4)) bbox_scales = np.ones((num_instances, 3)) for i in range(num_instances): class_id = class_ids[i] mask = masks[:, :, (i)] coord = coords[:, :, (i), :] abs_coord_pts = np.abs(coord[mask == 1] - 0.5) bbox_scales[(i), :] = 2 * np.amax(abs_coord_pts, axis=0) pts, idxs = backproject(depth, intrinsics, mask) coord_pts = coord[(idxs[0]), (idxs[1]), :] - 0.5 if if_norm: scale = np.linalg.norm(bbox_scales[(i), :]) bbox_scales[(i), :] /= scale coord_pts /= scale try: start = time.time() scales, rotation, translation, outtransform = ( estimateSimilarityTransform(coord_pts, pts, False)) aligned_RT = np.zeros((4, 4), dtype=np.float32) if with_scale: aligned_RT[:3, :3] = np.diag(scales ) / 1000 @ rotation.transpose() else: aligned_RT[:3, :3] = rotation.transpose() aligned_RT[:3, (3)] = translation / 1000 aligned_RT[3, 3] = 1 if save_path is not None: coord_pts_rotated = aligned_RT[:3, :3] @ coord_pts.transpose( ) + aligned_RT[:3, 3:] coord_pts_rotated = coord_pts_rotated.transpose() np.savetxt(save_path + '_{}_{}_depth_pts.txt'.format(i, class_name), pts) np.savetxt(save_path + '_{}_{}_coord_pts.txt'.format(i, class_name), coord_pts) np.savetxt(save_path + '_{}_{}_coord_pts_aligned.txt'. format(i, class_name), coord_pts_rotated) if verbose: print('Mask ID: ', i) print('Scale: ', scales / 1000) print('Rotation: ', rotation.transpose()) print('Translation: ', translation / 1000) elapsed = time.time() - start print('elapsed: ', elapsed) elapses.append(elapsed) except Exception as e: message = ( '[ Error ] aligning instance {} in {} fails. Message: {}.'. format(synset_names[class_id], image_path, str(e))) print(message) error_messages += message + '\n' aligned_RT = np.identity(4, dtype=np.float32) z_180_RT = np.zeros((4, 4), dtype=np.float32) z_180_RT[:3, :3] = np.diag([-1, -1, 1]) z_180_RT[3, 3] = 1 RTs[(i), :, :] = z_180_RT @ aligned_RT return RTs, bbox_scales, error_messages, elapses 
CppLintState|cpplint|init def __init__(self): self.verbose_level = 1 self.error_count = 0 self.filters = _DEFAULT_FILTERS[:] self._filters_backup = self.filters[:] self.counting = 'total' self.errors_by_category = {} self.output_format = 'emacs' 
test|GPT2ModelTest|gpt2|modeling|run|tester def run_tester(self, tester): config_and_inputs = tester.prepare_config_and_inputs() output_result = tester.create_gpt2_model(*config_and_inputs) tester.check_gpt2_model_output(output_result) output_result = tester.create_gpt2_lm_head(*config_and_inputs) tester.check_gpt2_lm_head_output(output_result) tester.check_gpt2_lm_head_loss_output(output_result) output_result = tester.create_gpt2_double_heads(*config_and_inputs) tester.check_gpt2_double_heads_output(output_result) tester.check_gpt2_double_heads_loss_output(output_result) 
deepMOT|sot|SiamRPN|init|master|utils def SiamRPN_init(im, target_pos, target_sz, net, gt_id, train_bool=False, outputmode='torch', add_noise=False): """ init a track and save its reference image :param im: current frame, numpy array, [h, w, c] :param target_pos: target center position at previous time step t-1, numpy array, [c_x, c_y] :param target_sz: target bounding box size at previous time step t-1, numpy array, [w, h] :param net: SOT network, torch model :param gt_id: identity for the track to be initialized, int or String :param train_bool: train mode or not, bool :param outputmode: 'torch', String :param add_noise: add noise flag, for training data augmentation, bool :return: state dict saving track information, dict """ state = dict() p = TrackerConfig() p.update(net.cfg) state['im_h'] = im.shape[0] state['im_w'] = im.shape[1] if p.adaptive: if target_sz[0] * target_sz[1] / float(state['im_h'] * state['im_w'] ) < 0.004: p.instance_size = 287 else: p.instance_size = 271 p.score_size = (p.instance_size - p.exemplar_size) / p.total_stride + 1 p.anchor = generate_anchor(p.total_stride, p.scales, p.ratios, int(p. score_size)) avg_chans = np.mean(im, axis=(0, 1)) wc_z = target_sz[0] + p.context_amount * sum(target_sz) hc_z = target_sz[1] + p.context_amount * sum(target_sz) s_z = round(np.sqrt(wc_z * hc_z)) if outputmode == 'torch': z_crop = get_subwindow_tracking(im, target_pos, p.exemplar_size, s_z, avg_chans, noisy_bool=add_noise) z = z_crop.unsqueeze(0).cuda() else: z = get_subwindow_tracking(im, target_pos, p.exemplar_size, s_z, avg_chans, out_mode=outputmode, noisy_bool=add_noise) if p.windowing == 'cosine': window = np.outer(np.hanning(p.score_size), np.hanning(p.score_size)) elif p.windowing == 'uniform': window = np.ones((p.score_size, p.score_size)) window = np.tile(window.flatten(), p.anchor_num) assert isinstance(gt_id, str) and train_bool == True or isinstance(gt_id, int) and train_bool == False state['gt_id'] = gt_id if train_bool: state['temple'] = z else: state['temple'] = net.temple(z) state['p'] = p state['avg_chans'] = avg_chans state['window'] = window state['target_pos'] = target_pos state['target_sz'] = target_sz return state 
avod|mini|MiniBatchUtils|init|batch|core|utils def __init__(self, dataset): self._dataset = dataset self._mini_batch_sampler = (balanced_positive_negative_sampler. BalancedPositiveNegativeSampler()) self.kitti_utils_config = dataset.config.kitti_utils_config self._area_extents = self.kitti_utils_config.area_extents self._anchor_strides = np.reshape(self.kitti_utils_config. anchor_strides, (-1, 2)) self.config = self.kitti_utils_config.mini_batch_config self._density_threshold = self.config.density_threshold rpn_config = self.config.rpn_config rpn_iou_type = rpn_config.WhichOneof('iou_type') if rpn_iou_type == 'iou_2d_thresholds': self.rpn_iou_type = '2d' self.rpn_iou_thresholds = rpn_config.iou_2d_thresholds elif rpn_iou_type == 'iou_3d_thresholds': self.rpn_iou_type = '3d' self.rpn_iou_thresholds = rpn_config.iou_3d_thresholds self.rpn_neg_iou_range = [self.rpn_iou_thresholds.neg_iou_lo, self. rpn_iou_thresholds.neg_iou_hi] self.rpn_pos_iou_range = [self.rpn_iou_thresholds.pos_iou_lo, self. rpn_iou_thresholds.pos_iou_hi] self.rpn_mini_batch_size = rpn_config.mini_batch_size avod_config = self.config.avod_config self.avod_iou_type = '2d' self.avod_iou_thresholds = avod_config.iou_2d_thresholds self.avod_neg_iou_range = [self.avod_iou_thresholds.neg_iou_lo, self. avod_iou_thresholds.neg_iou_hi] self.avod_pos_iou_range = [self.avod_iou_thresholds.pos_iou_lo, self. avod_iou_thresholds.pos_iou_hi] self.avod_mini_batch_size = avod_config.mini_batch_size self.mini_batch_dir = avod.root_dir( ) + '/data/mini_batches/' + 'iou_{}/'.format(self.rpn_iou_type ) + dataset.name + '/' + dataset.cluster_split + '/' + dataset.bev_source self.col_length = 9 self.col_anchor_indices = 0 self.col_ious = 1 self.col_offsets_lo = 2 self.col_offsets_hi = 8 self.col_class_idx = 8 
gym|protocols|as|egocentric|scrolling|pycolab|participate def participate_as_egocentric(entity, the_plot, scrolling_group=''): """Register `entity` as egocentric with respect to the scrolling group.  Once registered, any entity that wishes to check or issue a scrolling order will need to make certain that scrolling the world "around" this entity will not wind up making the entity execute an impossible move. (See `permit`, `is_possible` and `order`.)  There is no harm in registering more than once, as long as `scrolling_group` remains the same.  Args: entity: the pycolab game entity we wish to register as egocentric. the_plot: the pycolab game's `Plot` object. scrolling_group: a string identifier for the scrolling group with respect to which we are marking `entity` as egocentric.  Raises: TypeError: `entity` is not a pycolab entity. Error: `entity` is known to belong to a scrolling group distinct from `scrolling_group`. """ _check_scrolling_group(entity, the_plot, scrolling_group) egocentrists = the_plot.setdefault('scrolling_{}_egocentrists'.format( scrolling_group), set()) egocentrists.add(entity) 
seld|output|dcase2|19|write|file|metrics|master|format|evaluation def write_output_format_file(_output_format_file, _output_format_dict): """ Writes DCASE output format csv file, given output format dictionary  :param _output_format_file: :param _output_format_dict: :return: """ _fid = open(_output_format_file, 'w') for _frame_ind in _output_format_dict.keys(): for _value in _output_format_dict[_frame_ind]: _fid.write('{},{},{},{}\n'.format(int(_frame_ind), int(_value[0 ]), int(_value[1]), int(_value[2]))) _fid.close() 
forward|det|AffineCouplingSdnGain|and|log|jacobian def _forward_and_log_det_jacobian(self, x, yy, nlf0=None, nlf1=None, iso= None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) beta1, beta2 = sdn_iso_model_params_3(iso) scale = tf.sqrt(beta1 * yy + beta2) y = x if scale is not None: y *= scale if scale is None: log_abs_det_J = tf.constant(0.0, dtype=x.dtype, name='fldj') else: log_abs_det_J = tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) return y, log_abs_det_J 
TPUEncodedUInt8Spec|utils|init def __init__(self, encoded_shape, original_shape): self._value_specs = tf.TensorSpec(encoded_shape, tf.uint32), self.original_shape = original_shape 
prepare|ptb|reader|data def prepare_data(data_path): """Preprocess PTB data. """ train_path = os.path.join(data_path, 'ptb.train.txt') if not tf.gfile.Exists(train_path): url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz' tx.data.maybe_download(url, data_path, extract=True) data_path = os.path.join(data_path, 'simple-examples', 'data') train_path = os.path.join(data_path, 'ptb.train.txt') valid_path = os.path.join(data_path, 'ptb.valid.txt') test_path = os.path.join(data_path, 'ptb.test.txt') word_to_id = tx.data.make_vocab(train_path, newline_token='<EOS>', return_type='dict') assert len(word_to_id) == 10000 train_text = tx.data.read_words(train_path, newline_token='<EOS>') train_text_id = [word_to_id[w] for w in train_text if w in word_to_id] valid_text = tx.data.read_words(valid_path, newline_token='<EOS>') valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id] test_text = tx.data.read_words(test_path, newline_token='<EOS>') test_text_id = [word_to_id[w] for w in test_text if w in word_to_id] data = {'train_text': train_text, 'valid_text': valid_text, 'test_text': test_text, 'train_text_id': train_text_id, 'valid_text_id': valid_text_id, 'test_text_id': test_text_id, 'vocab': word_to_id, 'vocab_size': len(word_to_id)} return data 
sn|celeba|sgan|256|t|sample def sample(path): n = 9 figure = np.zeros((img_dim * n, img_dim * n, 3)) for i in range(n): for j in range(n): z_sample = np.random.randn(1, z_dim) x_sample = g_model.predict(z_sample) digit = x_sample[0] figure[i * img_dim:(i + 1) * img_dim, j * img_dim:(j + 1) * img_dim ] = digit figure = (figure + 1) / 2 * 255 figure = np.round(figure, 0).astype(int) imageio.imwrite(path, figure) 
ImageNetDogAndCatDataset|and|cat|dog|transform|imagenet def transform(self, image): c, h, w = image.shape if c == 1: image = np.concatenate([image, image, image], axis=0) short_side = h if h < w else w if self.augmentation: crop_size = int(short_side * self.crop_ratio) top = random.randint(0, h - crop_size - 1) left = random.randint(0, w - crop_size - 1) if random.randint(0, 1): image = image[:, :, ::-1] else: crop_size = short_side top = (h - crop_size) // 2 left = (w - crop_size) // 2 bottom = top + crop_size right = left + crop_size image = image[:, top:bottom, left:right] _, h, w = image.shape image = scipy.misc.imresize(image.transpose(1, 2, 0), [self.size, self. size], self.resize_method).transpose(2, 0, 1) image = image / 128.0 - 1.0 image += np.random.uniform(size=image.shape, low=0.0, high=1.0 / 128) return image 
rnn|texar|get|test|cell|GetRNNCellTest|core|layers def test_get_rnn_cell(self): """Tests :func:`texar.core.layers.get_rnn_cell`. """ emb_dim = 4 num_units = 64 hparams = {'type': rnn.LSTMCell(num_units)} cell = layers.get_rnn_cell(hparams) self.assertTrue(isinstance(cell, rnn.LSTMCell)) hparams = {'type': rnn.LSTMCell, 'kwargs': {'num_units': 10}} cell = layers.get_rnn_cell(hparams) self.assertTrue(isinstance(cell, rnn.LSTMCell)) keep_prob_x = tf.placeholder(name='keep_prob', shape=[], dtype=tf.float32) hparams = {'type': 'tensorflow.contrib.rnn.GRUCell', 'kwargs': { 'num_units': num_units}, 'num_layers': 2, 'dropout': { 'input_keep_prob': 0.8, 'state_keep_prob': keep_prob_x, 'variational_recurrent': True, 'input_size': [emb_dim, num_units]}, 'residual': True, 'highway': True} hparams_ = HParams(hparams, layers.default_rnn_cell_hparams()) cell = layers.get_rnn_cell(hparams_) batch_size = 16 inputs = tf.zeros([batch_size, emb_dim], dtype=tf.float32) output, state = cell(inputs, cell.zero_state(batch_size, dtype=tf.float32)) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) feed_dict = {keep_prob_x: 1.0, context.global_mode(): tf.estimator. ModeKeys.TRAIN} output_, state_ = sess.run([output, state], feed_dict=feed_dict) self.assertEqual(output_.shape[0], batch_size) if isinstance(state_, (list, tuple)): self.assertEqual(state_[0].shape[0], batch_size) self.assertEqual(state_[0].shape[1], hparams_.kwargs.num_units) else: self.assertEqual(state_.shape[0], batch_size) self.assertEqual(state_.shape[1], hparams_.kwargs.num_units) 
texar|networks|FeedForwardNetworkBase|names|modules|network|base|layer @property def layer_names(self): """A list of uniquified layer names. """ return self._layer_names 
m|phate|pdist|kernel|square def square_pdist(X): return distance.squareform(distance.pdist(X)) 
python|grpc|OpsTest|ops|test|seed|rl|master|nests @parameterized.parameters(([], False), ([1], True)) def test_nests(self, dim, batched): address = self.get_unix_address() server = ops.Server([address]) signature = tf.TensorSpec(dim, tf.int32, name='arg1'), Some(tf. TensorSpec(dim, tf.int32, name='arg2'), [tf.TensorSpec(dim, tf. int32, name='arg3'), tf.TensorSpec(dim, tf.int32, name='arg4')])  @tf.function(input_signature=signature) def foo(*args): return tf.nest.map_structure(lambda t: t + 1, args) server.bind(foo, batched=batched) server.start() client = ops.Client(address) inputs = 1, Some(2, [3, 4]) expected_outputs = 2, Some(3, [4, 5]) outputs = client.foo(inputs) outputs = tf.nest.map_structure(lambda t: t.numpy(), outputs) tf.nest.assert_same_structure(expected_outputs, outputs) self.assertAllEqual(tf.nest.flatten(expected_outputs), tf.nest.flatten( outputs)) server.shutdown() 
common|prune|channel def prune_channel(weight, cut_channel, cut_type='input'): """ weight: of shape[h, w, input_channel, output_channel] cut_channle: list of numbers to be cut """ if cut_type == 'input': weight = np.delete(weight, cut_channel, axis=2) elif cut_type == 'output': weight = np.delete(weight, cut_channel, axis=3) elif cut_type == 'bias' or cut_type == 'beta' or cut_type == 'gamma' or cut_type == 'moving_mean' or cut_type == 'moving_variance' or cut_type == 'flatten': weight = np.delete(weight, cut_channel, axis=None) else: raise ValueError('unknown cut type') return weight 
Processor|esc5|Dataset|EndToEndClassification|processor|ESC5 def ESC50Processor(esc_50_path, destination_folder): """ Wrapper function for convenient processing. First subfolders are made in the destination folder for the processed dataset (1) and the logmel and raw waveform features (i.e. the segmented/augmented features). One can use the internal functions if more flexibility is required.  Args: esc_50_path (str): path to the esc50 data. destination_folder (str): path to a destination folder. """ if not (os.path.isdir(esc_50_path) and os.path.isdir(destination_folder)): raise ValueError( 'please provide valid paths to a source and a destination folder') processed_esc50_path = os.path.join(destination_folder, 'processed_esc50') os.mkdir(processed_esc50_path) features_path = os.path.join(destination_folder, 'features') os.mkdir(features_path) features_raw_path = os.path.join(features_path, 'raw') os.mkdir(features_raw_path) features_logmel_path = os.path.join(features_path, 'spect') os.mkdir(features_logmel_path) _process_esc50(esc_50_path, processed_esc50_path) _dump_features_processed_esc50_combined(processed_esc50_path, features_logmel_path, features_raw_path, augmentations=4, frames= 101, seed=41, batch_size=50) print('done') 
gan|op|deconv2d def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, sn_op=None, name='deconv2d'): with tf.variable_scope(name): w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_. get_shape()[-1]], initializer=tf.random_normal_initializer( stddev=stddev)) if sn_op is not None: w = tf.transpose(w, [0, 1, 3, 2]) w = spectral_normed_weight(w, update_collection=sn_op) w = tf.transpose(w, [0, 1, 3, 2]) try: deconv = tf.nn.conv2d_transpose(input_, w, output_shape= output_shape, strides=[1, d_h, d_w, 1]) except AttributeError: deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1]) biases = tf.get_variable('biases', [output_shape[-1]], initializer= tf.constant_initializer(0.0)) deconv = tf.reshape(tf.nn.bias_add(deconv, biases), output_shape) return deconv 
activation|layers|deepctr|layer def activation_layer(activation): if activation == 'dice' or activation == 'Dice': act_layer = Dice() elif isinstance(activation, str ) or sys.version_info.major == 2 and isinstance(activation, (str, unicode)): act_layer = tf.keras.layers.Activation(activation) elif issubclass(activation, Layer): act_layer = activation() else: raise ValueError( 'Invalid activation,found %s.You should use a str or a Activation Layer Class.' % activation) return act_layer 
list|avod|get|all|BoxList|core|fields|box def get_all_fields(self): """Returns all fields.""" return self.data.keys() 
src|master|facenet|flip def flip(image, random_flip): if random_flip and np.random.choice([True, False]): image = np.fliplr(image) return image 
size|state|QAAttGRUCell|deepctr|contrib|utils @property def state_size(self): return self._num_units 
feature|world|extract def world_feature_extract(wav_list, spk_list, feat_param_list, args): """EXTRACT WORLD FEATURE VECTOR""" for i, wav_name in enumerate(wav_list): bin_basename = os.path.basename(wav_name).replace('wav', 'bin') spk = os.path.dirname(wav_name).split('/')[-1][-3:] bin_name = os.path.join(args.bindir, 'noVAD', spk, bin_basename) vad_bin_name = os.path.join(args.bindir, 'VAD', spk, bin_basename) if os.path.exists(bin_name): if args.overwrite: logging.info('overwrite %s (%d/%d)' % (wav_name, i + 1, len (wav_list))) else: logging.info('skip %s (%d/%d)' % (wav_name, i + 1, len( wav_list))) continue else: logging.info('now processing %s (%d/%d)' % (wav_name, i + 1, len(wav_list))) feat_param = feat_param_list[spk_list.index(spk)] fs, x = wavfile.read(wav_name) x = np.array(x, dtype=np.float64) x = low_cut_filter(x, fs, cutoff=feat_param['highpass_cutoff']) if not fs == feat_param['fs']: logging.error('sampling frequency is not matched.') sys.exit(1) f0, time_axis = pw.harvest(x, feat_param['fs'], f0_floor=feat_param ['f0min'], f0_ceil=feat_param['f0max'], frame_period=feat_param ['shift_ms']) sp = pw.cheaptrick(x, f0, time_axis, feat_param['fs'], fft_size= feat_param['fftl']) ap = pw.d4c(x, f0, time_axis, feat_param['fs'], fft_size=feat_param ['fftl']) mcc = pysptk.sp2mc(sp, feat_param['mcep_dim'], feat_param['mcep_alpha'] ) en_sp, sp = energy_norm(sp) sp = np.log10(sp) en_mcc = mcc[:, (0)] f0 = np.expand_dims(f0, axis=-1) en_mcc = np.expand_dims(en_mcc, axis=-1) world_feats = np.concatenate([sp, mcc[:, 1:], ap, f0, en_sp, en_mcc ], axis=1) labels = spk_list.index(spk) * np.ones([sp.shape[0], 1], np.float32) feats = np.concatenate([world_feats, labels], axis=1).astype(np.float32 ) vad_idx = np.where(f0.copy().reshape([-1]) > 10)[0] vad_feats = feats[vad_idx[0]:vad_idx[-1] + 1] with open(bin_name, 'wb') as fp: fp.write(feats.tostring()) with open(vad_bin_name, 'wb') as fp: fp.write(vad_feats.tostring()) 
lottery|from|hooks|flags def hooks_from_flags(params): from_ = params['lottery_prune_at'] or None method = params['lottery_pruning_method'] or None to_ = params['lottery_reset_to'] or None gs_to_ = params['lottery_reset_global_step_to'] or None results_dir = params['lottery_results_dir'] or None model_dir = params.get('model_dir') or 'execution_data'.join(params[ 'lottery_results_dir'].rsplit('results', 1)) force_reprune = params['lottery_force_reprune'] if params['lottery_checkpoint_iters']: checkpoint_iters = set(map(int, params['lottery_checkpoint_iters']. split(','))) else: checkpoint_iters = None return get_hooks(results_dir=results_dir, prune_from_checkpoint=from_, prune_to_checkpoint=to_, reset_global_step_to=gs_to_, prune_method= method, checkpoint_iters=checkpoint_iters, model_dir=model_dir, force_reprune=force_reprune) 
read|predictions|evaluator def read_predictions(filename: str) ->Dict[str, List[str]]: predictions = {} with open(filename, 'rt', encoding='UTF-8', errors='replace') as f: reader = csv.reader(f) try: for row in reader: try: question_id = row[0] prediction_raw = row[1] except IndexError as e: logging.error( 'Error reading value from CSV file %s on line %d: %s', filename, reader.line_num, e) sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED) if question_id in predictions: logging.error('Key %s repeated in file %s on line %d', question_id, filename, reader.line_num) sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED) if question_id == '': logging.error('Key is empty in file %s on line %d', filename, reader.line_num) sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED) prediction = prediction_raw.split(';') for p in prediction: if p == '': logging.error( 'Key %s has empty labels for prediction in file %s on line %d' , question_id, filename, reader.line_num) sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED) predictions[question_id] = prediction except csv.Error as e: logging.error('file %s, line %d: %s', filename, reader.line_num, e) sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED) return predictions 
copy|sidd|utils|stats def copy_stats(hps): fns = ['pat_stats.npy', 'nll_bpd_gauss.npy', 'nll_bpd_sdn.npy'] for fn in fns: src = os.path.join('experiments', hps.problem, fn) dst = os.path.join(hps.logdir, fn) copyfile(src, dst) 
dense|official|fn|grad|vgg|loop|model|filter|run def _dense_grad_filter(gvs): """Only apply gradient updates to the final layer.  This function is used for fine tuning.  Args: gvs: list of tuples with gradients and variable info Returns: filtered gradients so that only the dense layer remains """ return [(g, v) for g, v in gvs if 'dense' in v.name] 
l1|mrcnn|coord|loss|model|graph def mrcnn_coord_l1_loss_graph(target_masks, target_coord, target_class_ids, pred_coord): """Mask L1 loss for the coordinates head.  target_masks: [batch, num_rois, height, width]. A float32 tensor of values 0 or 1. Uses zero padding to fill array. target_coord: [batch, num_rois, height, width]. Might be for x, y or z channel. A float32 tensor of values in the range of [0, 1]. Uses zero padding to fill array. target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded. pred_coord: [batch, proposals, height, width, num_classes] float32 tensor with values from 0 to 1. """ target_class_ids = K.reshape(target_class_ids, (-1,)) mask_shape = tf.shape(target_masks) target_masks = K.reshape(target_masks, (-1, mask_shape[2], mask_shape[3])) target_coord = K.reshape(target_coord, (-1, mask_shape[2], mask_shape[3])) pred_shape = tf.shape(pred_coord) pred_coord = K.reshape(pred_coord, (-1, pred_shape[2], pred_shape[3], pred_shape[4])) pred_coord = tf.transpose(pred_coord, [0, 3, 1, 2]) positive_ix = tf.where(target_class_ids > 0)[:, (0)] positive_class_ids = tf.cast(tf.gather(target_class_ids, positive_ix), tf.int64) indices = tf.stack([positive_ix, positive_class_ids], axis=1) y_true = tf.gather(target_coord, positive_ix) mask = tf.gather(target_masks, positive_ix) mask = tf.cast(mask, dtype=tf.bool) y_true_in_mask = tf.boolean_mask(y_true, mask) y_pred = tf.gather_nd(pred_coord, indices) y_pred_in_mask = tf.boolean_mask(y_pred, mask) coord_loss = K.abs(y_true_in_mask - y_pred_in_mask) mean_loss = K.mean(coord_loss) loss = K.switch(tf.size(y_true) > 0, mean_loss, tf.constant(0.0)) loss = K.reshape(loss, [1, 1]) return loss 
triangles|RenderTest|test|testRendersTwoCubesInBatch|rasterize def testRendersTwoCubesInBatch(self): """Renders a simple cube in two viewpoints to test the python wrapper.""" vertex_rgb = self.cube_vertex_positions * 0.5 + 0.5 vertex_rgba = tf.concat([vertex_rgb, tf.ones([8, 1])], axis=1) center = self.tf_float([[0.0, 0.0, 0.0]]) world_up = self.tf_float([[0.0, 1.0, 0.0]]) look_at_1 = camera_utils.look_at(self.tf_float([[2.0, 3.0, 6.0]]), center, world_up) look_at_2 = camera_utils.look_at(self.tf_float([[-3.0, 1.0, 6.0]]), center, world_up) projection_1 = tf.matmul(self.perspective, look_at_1) projection_2 = tf.matmul(self.perspective, look_at_2) projection = tf.concat([projection_1, projection_2], axis=0) background_value = [0.0, 0.0, 0.0, 0.0] rendered = rasterize_triangles.rasterize(tf.stack([self. cube_vertex_positions, self.cube_vertex_positions]), tf.stack([ vertex_rgba, vertex_rgba]), self.cube_triangles, projection, self. image_width, self.image_height, background_value) with self.test_session() as sess: images = sess.run(rendered, feed_dict={}) for i in (0, 1): image = images[(i), :, :, :] baseline_image_name = 'Unlit_Cube_{}.png'.format(i) baseline_image_path = os.path.join(self.test_data_directory, baseline_image_name) test_utils.expect_image_file_and_render_are_near(self, sess, baseline_image_path, image) 
init|chunkparser|ChunkParser def __init__(self, chunkdatasrc, shuffle_size=1, sample=1, buffer_size=1, batch_size=256, workers=None): """ Read data and yield batches of raw tensors.  'chunkdatasrc' is an object yeilding chunkdata 'shuffle_size' is the size of the shuffle buffer. 'sample' is the rate to down-sample. 'workers' is the number of child workers to use.  The data is represented in a number of formats through this dataflow pipeline. In order, they are:  chunk: The name of a file containing chunkdata  chunkdata: type Bytes. Either mutiple records of v1 format, or multiple records of v2 format.  v1: The original text format describing a move. 19 lines long. VERY slow to decode. Typically around 2500 bytes long. Used only for backward compatability.  v2: Packed binary representation of v1. Fixed length, no record seperator. The most compact format. Data in the shuffle buffer is held in this format as it allows the largest possible shuffle buffer. Very fast to decode. Preferred format to use on disk. 2176 bytes long.  raw: A byte string holding raw tensors contenated together. This is used to pass data from the workers to the parent. Exists because TensorFlow doesn't have a fast way to unpack bit vectors. 7950 bytes long. """ self.prob_reflection_table = [([remap_vertex(vertex, sym) for vertex in range(361)] + [361]) for sym in range(8)] self.full_reflection_table = [np.array([(remap_vertex(vertex, sym) + p * 361) for p in range(16) for vertex in range(361)]) for sym in range(8)] self.prob_reflection_table = [np.array(x, dtype=np.int64) for x in self .prob_reflection_table] self.full_reflection_table = [np.array(x, dtype=np.int64) for x in self .full_reflection_table] self.flat_planes = [b'\x01' * 361 + b'\x00' * 361, b'\x00' * 361 + b'\x01' * 361] self.sample = sample self.batch_size = batch_size self.shuffle_size = shuffle_size if workers is None: workers = max(1, mp.cpu_count() - 2) print('Using {} worker processes.'.format(workers)) self.readers = [] for _ in range(workers): read, write = mp.Pipe(duplex=False) mp.Process(target=self.task, args=(chunkdatasrc, write), daemon=True ).start() self.readers.append(read) write.close() self.init_structs() 
create|max|embeddings|pooling|glove def max_pooling(old, new): return np.maximum(old, new) 
darc|append|DataArchive def append(self, data, chunks=None, name=None): """Adds a data element to the archive, divided in chunks of the given size. If a dimension in 'chunks' is -1, maximum size will be used.  If a 'name' string is given, makes the current object accessible by darc_object[name] in addition to accessing by index. """ shape = np.array(data.shape) if not chunks: chunk_shape = shape else: chunk_shape = np.array([(chunks[i] if chunks[i] != -1 else shape[i] ) for i in range(len(shape))]) dimensions = len(shape) chunk_counts = (shape + chunk_shape - 1) // chunk_shape buffers = [] buffers.append(struct.pack('<I', len(shape))) buffers.append(struct.pack('<{}I'.format(dimensions), *shape)) buffers.append(struct.pack('<{}I'.format(dimensions), *chunk_shape)) buffers.append(struct.pack('<I', _dtype_to_int[data.dtype.type])) chunk_indices = (range(chunk_counts[d]) for d in range(dimensions)) for it in itertools.product(*chunk_indices): index = np.array(it, dtype=np.int) window_begin = index * chunk_shape window_end = window_begin + chunk_shape if np.all(window_end <= shape): window_slice = tuple(slice(window_begin[d], window_end[d]) for d in range(dimensions)) view = data[window_slice] buffers.append(view.tobytes()) else: window_end = np.minimum(window_end, shape) window_slice = tuple(slice(window_begin[d], window_end[d]) for d in range(dimensions)) temp_data = np.zeros(shape=chunk_shape, dtype=data.dtype) temp_data_slice = tuple(slice(0, window_end[d] - window_begin[d ]) for d in range(dimensions)) temp_data[temp_data_slice] = data[window_slice] buffers.append(temp_data.tobytes()) self.file.seek(0, 2) data_offset = self.file.tell() self.file.write(b''.join(buffers)) self.directory.append(data_offset) if name is not None: self.name_to_index[name] = len(self.directory) - 1 
gan|factorVAE|from|inference|FactorVAE def inference_from(self, img): latents = [] for i in range(int(math.ceil(float(img.shape[0]) / self.batch_size))): sub_latents = self.sess.run(self.de_input_test_tf, feed_dict={self. real_image_pl: img[i * self.batch_size:(i + 1) * self.batch_size]}) latents.append(sub_latents) return np.vstack(latents) 
forward|deepMOT|models|extras|SST|master|DAN def forward_extras(self, x, extras, sources): for k, v in enumerate(extras): x = v(x) if k % 6 == 3: sources.append(x) return x 
bert|forward|BertOutput|pytorch|modeling|pretrained def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states 
get|int|hot|dstn|one|masked|att def get_masked_one_hot(x_input_one_hot): data_mask = tf.cast(tf.greater(x_input_one_hot, 0), tf.float32) data_mask = tf.expand_dims(data_mask, axis=2) data_mask = tf.tile(data_mask, (1, 1, k)) data_embed_one_hot = tf.nn.embedding_lookup(emb_mat, x_input_one_hot) data_embed_one_hot_masked = tf.multiply(data_embed_one_hot, data_mask) return data_embed_one_hot_masked 
ensemble|elpips|permuteColor|apply def permuteColor(X, perms): shape = tf.shape(X) N, H, W, C = shape[0], shape[1], shape[2], shape[3] X = tf.transpose(X, [0, 3, 1, 2]) X = tf.reshape(X, [N * C, H, W]) X = tf.gather(X, perms) X = tf.reshape(X, [N, C, H, W]) X = tf.transpose(X, [0, 2, 3, 1]) return X 
tangents|parallel|neural|batch|utils def _parallel(kernel_fn, device_count=-1): """Returns a function that computes a kernel in batches in parallel.  When batching in parallel, the data is split over a set number of devices. The number of devices must be less than or equal to the number of physical devices. Moreover, the dataset size needs to divide the device count.  Given two datasets x1 and x2, parallel splits the kernel calculation over devices such that each device computes a batch of rows of shape [|x1| / device_count, |x2|].  Args: kernel_fn: A function that computes a kernel between two datasets, `kernel_fn(x1, x2)` or the compositional kernel for an input kernel `kernel_fn(kernel_in)`. Here x1 and x2 are `np.ndarray`s of floats of shape [n1] + input_shape and [n2] + input_shape; `kernel_in` is a Kernel object. The kernel function should return a PyTree. device_count: Integer specifying the number of devices over which to split the data. If device_count = 0, the computation is parallelized over all available devices.  Returns: A new function with the same signature as kernel_fn that computes the kernel by batching over the dataset in parallel over a specified number of cores. """ kernel_fn = _jit_or_pmap_broadcast(kernel_fn, device_count) if device_count == -1: device_count = xla_bridge.device_count()  def parallel_fn_x1(x1, x2=None, *args, **kwargs): x2_is_none = x2 is None if x2_is_none: x2 = x1 n1 = x1.shape[0] assert x1.shape[1:] == x2.shape[1:] input_shape = x1.shape[1:] _device_count = device_count n1_per_device, ragged = divmod(n1, device_count) if n1_per_device and ragged: raise ValueError( 'Dataset size ({}) must divide number of physical devices ({}).' .format(n1, device_count)) elif not n1_per_device: _device_count = ragged n1_per_device = 1 x1 = np.reshape(x1, (_device_count, n1_per_device) + input_shape) kernel = kernel_fn(x1, x2, *args, **kwargs) return _flatten_kernel(kernel, x2_is_none, True)  def parallel_fn_kernel(kernel, *args, **kwargs): n1 = kernel.var1.shape[0] _device_count = device_count n1_per_device, ragged = divmod(n1, device_count) if n1_per_device and ragged: raise ValueError( 'Dataset size ({}) must divide number of physical devices ({}).' .format(n1, device_count)) elif not n1_per_device: _device_count = ragged n1_per_device = 1 kernel_dict = kernel._asdict() var2 = kernel_dict['var2'] var2_is_none = var2 is None if var2 is None: var2 = kernel_dict['var1'] kernel_dict['var2'] = np.broadcast_to(var2, (_device_count,) + var2 .shape) for k, v in kernel_dict.items(): if k in ('nngp', 'ntk', 'var1'): kernel_dict[k] = np.reshape(v, (_device_count, n1_per_device) + v.shape[1:]) if k in ('shape1',): kernel_dict[k] = (n1_per_device,) + v[1:] kernel = kernel_fn(Kernel(**kernel_dict), *args, **kwargs) if var2_is_none: kernel = kernel._replace(var2=None) return _flatten_kernel(kernel, var2_is_none, True)  def parallel_fn(x1_or_kernel, x2=None, *args, **kwargs): if isinstance(x1_or_kernel, np.ndarray): return parallel_fn_x1(x1_or_kernel, x2, *args, **kwargs) elif isinstance(x1_or_kernel, Kernel): assert not x2 return parallel_fn_kernel(x1_or_kernel, *args, **kwargs) raise NotImplementedError() parallel_fn.is_parallel = True parallel_fn.device_count = device_count return parallel_fn 
init|nodes|deepzono|DeepzonoNonlinearity def __init__(self, input_names, output_name, output_shape): """ Arguments --------- input_names : iterable iterable with the name of the vector you want to apply the non-linearity to output_name : str name of this node's output output_shape : iterable iterable of ints with the shape of the output of this node """ add_input_output_information(self, input_names, output_name, output_shape) 
training|losses|loss|vkge|logistic def logistic_loss(scores, targets): """ Logistic loss as used in [1]  [1] http://jmlr.org/proceedings/papers/v48/trouillon16.pdf  :param scores: (N,) Tensor containing scores of examples. :param targets: (N,) Tensor containing {0, 1} targets of examples. :return: Loss value. """ logistic_losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=scores, labels=targets) loss = tf.reduce_sum(logistic_losses) return loss 
plato|reinforcement|QPolicy|component|agent|policy|dialogue|save|learning|q def save(self, path=None): """ Save the Q learning dialogue policy model  :param path: the path to save the model to :return: nothing """ if not self.is_training: return if not path: path = 'models/policies/q_policy.pkl' print('No dialogue policy file name provided. Using default: {0}'. format(path)) if not os.path.exists(os.path.dirname(path)): os.makedirs(os.path.dirname(path), exist_ok=True) obj = {'Q': self.Q, 'a': self.alpha, 'e': self.epsilon, 'g': self.gamma} with open(path, 'wb') as file: pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL) 
v1|MobilenetV1Test|test|testLogitsNotSqueezed|nets|mobilenet def testLogitsNotSqueezed(self): num_classes = 25 images = tf.random_uniform([1, 224, 224, 3]) logits, _ = mobilenet_v1.mobilenet_v1(images, num_classes=num_classes, spatial_squeeze=False) with self.test_session() as sess: tf.global_variables_initializer().run() logits_out = sess.run(logits) self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes]) 
decoder|initialize|InterpolationDecoder|interpolation def initialize(self, name=None): init = AttentionRNNDecoder.initialize(self, name) batch_size = tf.shape(init[0])[0] initial_decoded_ids = tf.ones((batch_size, 60), dtype=tf.int32) initial_rnn_state = init[2] initial_state = [initial_decoded_ids, initial_rnn_state] init[2] = initial_state return init 
scope|arg|model|inception|v4 def inception_arg_scope(weight_decay=4e-05, use_batch_norm=True, batch_norm_decay=0.9997, batch_norm_epsilon=0.001, activation_fn=tf.nn. relu, batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS, batch_norm_scale=False): """Defines the default arg scope for inception models.  Args: weight_decay: The weight decay to use for regularizing the model. use_batch_norm: "If `True`, batch_norm is applied after each convolution. batch_norm_decay: Decay for batch norm moving average. batch_norm_epsilon: Small float added to variance to avoid dividing by zero in batch norm. activation_fn: Activation function for conv2d. batch_norm_updates_collections: Collection for the update ops for batch norm. batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the activations in the batch normalization layer.  Returns: An `arg_scope` to use for the inception models. """ batch_norm_params = {'decay': batch_norm_decay, 'epsilon': batch_norm_epsilon, 'updates_collections': batch_norm_updates_collections, 'fused': None, 'scale': batch_norm_scale} if use_batch_norm: normalizer_fn = slim.batch_norm normalizer_params = batch_norm_params else: normalizer_fn = None normalizer_params = {} with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay)): with slim.arg_scope([slim.conv2d], weights_initializer=slim. variance_scaling_initializer(), activation_fn=activation_fn, normalizer_fn=normalizer_fn, normalizer_params=normalizer_params ) as sc: return sc 
reconstruct|torch|BinaryVAE|vae def reconstruct(self, x, epoch): x = x.view(-1, 784).float() z_mu, z_std = self.encode(x) z = self.reparameterize(z_mu, z_std) x_probs = self.decode(z) dist = Bernoulli(x_probs) x_recon = dist.sample() x_with_recon = torch.cat((x, x_recon)) save_image(x_with_recon.view(64, 1, 28, 28), 'results/epoch_{}_recon.png'.format(epoch)) 
numpy|visualize|parse def parse_numpy(string): string = string.replace('[', ' ').replace(']', ' ').replace(',', ' ') string = re.sub(' +', ' ', string) result = numpy.fromstring(string, sep=' ') return result 
main|train|network|GPU def train(num_iterations, train_batch_size): maxValidRate = 0 location = 0 for i in range(num_iterations + 1): print('Optimization Iteration: ' + str(i)) for x in range(int(SIZE / train_batch_size) + 1): train_batch = training_data[x * train_batch_size:(x + 1) * train_batch_size] train_batch_label = training_label[x * train_batch_size:(x + 1) * train_batch_size] feed_dict_train = {img_entry: train_batch, img_label: train_batch_label, prob: 0.5} session.run(optimizer, feed_dict=feed_dict_train) if i % 15 == 0: acc = session.run(accuracy, feed_dict={img_entry: validation_data, img_label: validation_label, prob: 1.0}) print('Model Performance, Validation accuracy: ' + str(acc * 100)) test_x, test_y = test_data, test_label feed_dict_validate = {img_entry: test_x, img_label: test_y, prob: 1.0} class_pred = np.zeros(shape=test_x.shape[0], dtype=np.int) class_pred[:test_x.shape[0]] = session.run(model[ 'predict_class_number'], feed_dict=feed_dict_validate) class_true = np.argmax(test_y, axis=1) report = classification_report(class_true, class_pred, target_names=report_label, digits=5) correct = (class_true == class_pred).sum() accuracy_test = float(correct) / test_x.shape[0] if accuracy_test > maxValidRate: maxValidRate = accuracy_test location = i print('Maximum Test accuracy: \t' + str(maxValidRate * 100) + '% at epoch' + str(location)) print('Overall Accuracy at Test: \t' + str(accuracy_test * 100) + '%') print('Confusion matrix') con_mat = confusion_matrix(class_true, class_pred) print(con_mat) print(report) 
src|get|paf def get_paf(keypoints, ori_height, ori_width, paf_height, paf_width, paf_channels, paf_width_thre): """ function that create paf based keypoints :param keypoints: ndarray with shape [person_num, joints_num, 3], each joint contains three attribute, [x, y, v] :param ori_height: ori_img height :param ori_width:  ori_img width :param paf_height: paf_height :param paf_width:  paf_width :param paf_channels: how many paf_channels will return. the number of paf_channels is 2 * connect_num, which connect_num is edges num of points. :param paf_width_thre: the threshold that controls the area about paf connection. :return: A ndarray with shape [paf_height, paf_width, paf_channels]. """ factorx = paf_width / ori_width factory = paf_height / ori_height pt1 = [0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 13, 13, 13] pt2 = [1, 2, 4, 5, 7, 8, 10, 11, 13, 0, 3, 6, 9] pafs = np.zeros((paf_channels, paf_height, paf_width), dtype=np.float32) for i in range(len(pt1)): count = np.zeros((paf_height, paf_width)) for j in range(keypoints.shape[0]): val = keypoints[j] if val[pt1[i], 0] == 0 and val[pt1[i], 1] == 0 or val[pt2[i], 0 ] == 0 and val[pt2[i], 1] == 0 or val[pt1[i], 2] == 3 or val[ pt2[i], 2] == 3: continue center_x = val[pt1[i], 0] * factorx center_y = val[pt1[i], 1] * factory centerA = np.asarray([center_x, center_y]) center_x = val[pt2[i], 0] * factorx center_y = val[pt2[i], 1] * factory centerB = np.asarray([center_x, center_y]) paf_map, mask = create_paf_map(centerA, centerB, paf_width, paf_height, paf_width_thre) pafs[2 * i:2 * i + 2, :, :] += paf_map count[mask == True] += 1 mask = count == 0 count[mask == True] = 1 pafs[2 * i:2 * i + 2, :, :] = np.divide(pafs[2 * i:2 * i + 2, :, :], count[(np.newaxis), :, :]) pafs = np.transpose(pafs, (1, 2, 0)) return pafs 
BiLSTM|sequence|deepctr|layers|call def call(self, inputs, mask=None, **kwargs): input_fw = inputs input_bw = inputs for i in range(self.layers): output_fw = self.fw_lstm[i](input_fw) output_bw = self.bw_lstm[i](input_bw) output_bw = Lambda(lambda x: K.reverse(x, 1), mask=lambda inputs, mask: mask)(output_bw) if i >= self.layers - self.res_layers: output_fw += input_fw output_bw += input_bw input_fw = output_fw input_bw = output_bw output_fw = input_fw output_bw = input_bw if self.merge_mode == 'fw': output = output_fw elif self.merge_mode == 'bw': output = output_bw elif self.merge_mode == 'concat': output = K.concatenate([output_fw, output_bw]) elif self.merge_mode == 'sum': output = output_fw + output_bw elif self.merge_mode == 'ave': output = (output_fw + output_bw) / 2 elif self.merge_mode == 'mul': output = output_fw * output_bw elif self.merge_mode is None: output = [output_fw, output_bw] return output 
output|test|lm|GPT2ModelTest|gpt2|head|loss|check|GPT2ModelTester|modeling def check_gpt2_lm_head_loss_output(self, result): self.parent.assertListEqual(list(result['loss'].size()), []) 
l2|utils|normalize def l2_normalize(x, dim, epsilon=1e-12, name=None): """Stable l2 normalization """ with tf.name_scope(name, 'l2_normalize', [x]) as name: x = tf.convert_to_tensor(x, name='x') x /= epsilon + tf.reduce_max(tf.abs(x), dim, keepdims=True) square_sum = tf.reduce_sum(tf.square(x), dim, keepdims=True) x_inv_norm = tf.rsqrt(np.sqrt(epsilon) + square_sum) return tf.multiply(x, x_inv_norm, name) 
feature|tfrecord|float|make|shapenet def _float_feature(value): """Returns a float_list from a float / double.""" return tf.train.Feature(float_list=tf.train.FloatList(value=[value])) 
utils|show def show(img, name='output.png'): """ Show MNSIT digits in the console. """ np.save('img', img) fig = np.around((img + 0.5) * 255) fig = fig.astype(np.uint8).squeeze() pic = Image.fromarray(fig) pic.save(name) remap = '  .*#' + '#' * 100 img = (img.flatten() + 0.5) * 3 return if len(img) != 784: return print('START') for i in range(28): print(''.join([remap[int(round(x))] for x in img[i * 28:i * 28 + 28]])) 
parallel|callback|replicate|replication|new|patch @functools.wraps(old_replicate) def new_replicate(module, device_ids): modules = old_replicate(module, device_ids) execute_replication_callbacks(modules) return modules 
def|get|mode|amb|dir def get_mode_dir(hparams): if hparams.train_mode in ['ambient', 'baseline']: mode_dir = '{}/'.format(hparams.train_mode) elif hparams.train_mode == 'unmeasure': mode_dir = '{}_{}/'.format(hparams.train_mode, hparams.unmeasure_type) else: raise NotImplementedError return mode_dir 
autogen|code|doc|spaces|count|leading def count_leading_spaces(s): ws = re.search('\\S', s) if ws: return ws.start() else: return 0 
func|PlotHeuristic def func(x, eps, c): return x + np.random.normal(c, eps, x.size) 
EndToEndClassification|MSTmodel|init|MSTtrainer def __init__(self, model, dataset, save_folder, restore_path=None, seed=42): """ Initializes the trainer.  Args: model (class): initialized model (MSTmodel). dataset (class): loaded (ESC50) dataset (MSTLoader). save_folder (str): save folder where the results, the model, and the predictions will be saved. restore_path (str or None): random initialization if None, else provide path to trained model. seed (int): seed for initializing the pseudo-rng for reproducibility purposes. """ self._rng = np.random.RandomState(seed=seed) self.model = model self.dataset = dataset self.restore_path = restore_path results_filename = 'MSTresults' + '_' + str(dataset.validation_fold) + str( dataset.test_fold) + '.pkl' predictions_filename = 'MSTpredictions' + '_' + str(dataset.validation_fold ) + str(dataset.test_fold) + '.pkl' if not os.path.isdir(save_folder): raise ValueError('please provide a valid save folder') self.save_results = os.path.join(save_folder, results_filename) self.save_predictions = os.path.join(save_folder, predictions_filename) model_name = self.model.model_name + '_' + str(dataset.validation_fold ) + str(dataset.test_fold) self.save_model_path = os.path.join(save_folder, model_name) os.mkdir(self.save_model_path) self.no_epochs = None self.batch_size = None self.lr = None self.overfit_window = None self.input_batch_shape = None self.label_batch_shape = None self.eval_input_shape = None self.eval_label_shape = None self.test_input_shape = None self.test_label_shape = None self.input_placeholder = None self.label_placeholder = None self.evaluation_input_placeholder = None self.evaluation_label_placeholder = None self.test_input_placeholder = None self.test_label_placeholder = None self.train_prediction_op = None self.eval_prediction_op = None self.test_prediction_op = None self.train_loss_op = None self.eval_loss_op = None self.test_loss_op = None self.grad_op = None self.train_op = None self.sess = None 
test|ops|nets|v2|mobilenet|find def find_ops(optype): """Find ops of a given type in graphdef or a graph.  Args: optype: operation type (e.g. Conv2D) Returns: List of operations. """ gd = tf.get_default_graph() return [var for var in gd.get_operations() if var.type == optype] 
generator|seld|DataGenerator|get|classes|cls|dcase2|19|master|nb|data def get_nb_classes(self): return self._nb_classes 
max|BasicModel|extract|metric|model def _extract_max_metric(self, met): index = met[0].index(max(met[0])) logging.info('[EVAL RESULT] Based on best MAP on validation set') logging.info('Achieve best result on iteration {0}'.format(index)) logging.info('\t\tMAP\tP@20\tNDCG@20') logging.info('[Valid]\t{0}\t{1}\t{2}'.format(met[0][index], met[1][ index], met[2][index])) logging.info('[Test]\t\t{0}\t{1}\t{2}'.format(met[3][index], met[4][ index], met[5][index])) return index, [met[3][index], met[4][index], met[5][index]] 
contributed|clustering|load|master|facenet|model def load_model(model_dir, meta_file, ckpt_file): model_dir_exp = os.path.expanduser(model_dir) saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file)) saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file)) 
devtools|cleverhans|tests|next|empty|docscrape|Reader|seek|non|line def seek_next_non_empty_line(self): for l in self[self._l:]: if l.strip(): break else: self._l += 1 
unroll|agents|cell def _unroll_cell(inputs, done, start_state, zero_state, recurrent_cell): """Applies a recurrent cell on inputs, taking care of managing state.  Args: inputs: A tensor of shape [time, batch_size, <remaining dims>]. These are the inputs passed to the recurrent cell. done: <bool>[time, batch_size]. start_state: Recurrent cell state at the beginning of the input sequence. Opaque tf.nest structure of tensors with batch front dimension. zero_state: Blank recurrent cell state. The current recurrent state will be replaced by this blank state whenever 'done' is true. Same shape as 'start_state'. recurrent_cell: Function that will be applied at each time-step. Takes (input_t: [batch_size, <remaining dims>], current_state) as input, and returns (output_t: [<cell output dims>], new_state).  Returns: A pair: - The time-stacked outputs of the recurrent cell. Shape [time, <cell output dims>]. - The last state output by the recurrent cell. """ stacked_outputs = [] state = start_state inputs_list = tf.unstack(inputs) done_list = tf.unstack(done) assert len(inputs_list) == len(done_list ), "Inputs and done tensors don't have same time dim {} vs {}".format( len(inputs_list), len(done_list)) for input_t, done_t in zip(inputs_list, done_list): state = tf.nest.map_structure(lambda x, y, done_t=done_t: tf.where( tf.reshape(done_t, [done_t.shape[0]] + [1] * (x.shape.rank - 1) ), x, y), zero_state, state) output_t, state = recurrent_cell(input_t, state) stacked_outputs.append(output_t) return tf.stack(stacked_outputs), state 
fn|las|speller|model|embedding def embedding_fn(ids): if hparams.embedding_size != 0: target_embedding = tf.get_variable('target_embedding', [hparams. target_vocab_size, hparams.embedding_size], dtype=tf.float32, initializer=tf_contrib.layers.xavier_initializer()) return tf.nn.embedding_lookup(target_embedding, ids) elif binary_outputs: if binf_embedding is None or mode == tf.estimator.ModeKeys.TRAIN: return tf.cast(ids, tf.float32) else: return tf.nn.embedding_lookup(tf.transpose(binf_embedding), ids) else: return tf.one_hot(ids, hparams.target_vocab_size) 
darkflow|forward|ops|convolution|net|convolutional def forward(self): pad = [[self.lay.pad, self.lay.pad]] * 2 temp = tf.pad(self.inp.out, [[0, 0]] + pad + [[0, 0]]) temp = tf.nn.conv2d(temp, self.lay.w['kernel'], padding='VALID', name= self.scope, strides=[1] + [self.lay.stride] * 2 + [1]) if self.lay.batch_norm: temp = self.batchnorm(self.lay, temp) self.out = tf.nn.bias_add(temp, self.lay.w['biases']) 
train|main def train(fold, train_patient_indexes, val_patient_indexes): log_dir = 'fold_' + str(fold) + '/' if not os.path.isdir(log_dir): os.mkdir(log_dir) num_slices_train = len(train_patient_indexes) * 189 num_slices_val = len(val_patient_indexes) * 189 K.clear_session() model = create_xception_unet_n(input_shape=input_shape, pretrained_weights_file=pretrained_weights_file) model.compile(optimizer=Adam(lr=0.001), loss=get_loss, metrics=[dice]) checkpoint = ModelCheckpoint(log_dir + 'ep={epoch:03d}-loss={loss:.3f}-val_loss={val_loss:.3f}.h5', verbose=1, monitor='val_loss', save_weights_only=True, save_best_only=True, period=1) reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_delta =0.001, patience=3, verbose=1) early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1) csv_logger = CSVLogger(log_dir + 'record.csv') tensorboard = TensorBoard(log_dir=log_dir) model.fit_generator(create_train_date_generator(patient_indexes= train_patient_indexes, h5_file_path=data_file_path, batch_size= batch_size), steps_per_epoch=max(1, num_slices_train // batch_size), validation_data=create_val_date_generator(patient_indexes= val_patient_indexes, h5_file_path=data_file_path, batch_size=9), validation_steps=max(1, num_slices_val // 9), epochs=100, initial_epoch=0, callbacks=[checkpoint, reduce_lr, early_stopping, tensorboard, csv_logger]) model.save_weights(log_dir + 'trained_final_weights.h5') predicts = [] labels = [] f = create_val_date_generator(patient_indexes=val_patient_indexes, h5_file_path=data_file_path) for _ in range(num_slices_val): img, label = f.__next__() predicts.append(model.predict(img)) labels.append(label) predicts = np.array(predicts) labels = np.array(labels) score_record = get_score_from_all_slices(labels=labels, predicts=predicts) df = pd.DataFrame(score_record) df.to_csv(os.path.join(log_dir, 'score_record.csv'), index=False) mean_score = {} for key in score_record.keys(): print('In fold ', fold, ', average', key, ' value is: \t ', np.mean (score_record[key])) mean_score[key] = np.mean(score_record[key]) K.clear_session() return mean_score 
bert|run|csqa|main def main(): parser = argparse.ArgumentParser() parser.add_argument('--data_dir', default=None, type=str, required=True, help= 'The input data dir. Should contain the .csv files (or other data files) for the task.' ) parser.add_argument('--bert_model', default=None, type=str, required= True, help= 'Bert pre-trained model selected in the list: bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese.' ) parser.add_argument('--output_dir', default=None, type=str, required= True, help= 'The output directory where the model checkpoints will be written.') parser.add_argument('--save_model_name', default='model', type=str, required=True, help= 'The output model name where the model checkpoints will be written.') parser.add_argument('--max_seq_length', default=128, type=int, help= """The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.""" ) parser.add_argument('--do_train', action='store_true', help= 'Whether to run training.') parser.add_argument('--wsc', action='store_true', help= 'Whether to run training with wsc.') parser.add_argument('--swag_transfer', action='store_true', help= 'Whether to run training with swag.') parser.add_argument('--inhouse', action='store_true', help= 'Whether to run eval on the inhouse train/dev set.') parser.add_argument('--do_eval', action='store_true', help= 'Whether to run eval on the dev set.') parser.add_argument('--do_test', action='store_true', help= 'Whether to run test on the test set.') parser.add_argument('--do_lower_case', action='store_true', help= 'Set this flag if you are using an uncased model.') parser.add_argument('--train_batch_size', default=32, type=int, help= 'Total batch size for training.') parser.add_argument('--eval_batch_size', default=8, type=int, help= 'Total batch size for eval.') parser.add_argument('--epoch_suffix', default=0, type=int, help= 'Epoch suffix number.') parser.add_argument('--learning_rate', default=0.0001, type=float, help ='The initial learning rate for Adam.') parser.add_argument('--mlp_hidden_dim', default=64, type=int, help= 'mlp_hidden_dim.') parser.add_argument('--mlp_dropout', default=0.1, type=float, help= 'hidden drop out') parser.add_argument('--weight_decay', default=0.01, type=float, help= 'Weight decay for optimization') parser.add_argument('--num_train_epochs', default=3.0, type=float, help ='Total number of training epochs to perform.') parser.add_argument('--warmup_proportion', default=0.1, type=float, help= 'Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10%% of training.' ) parser.add_argument('--no_cuda', action='store_true', help= 'Whether not to use CUDA when available') parser.add_argument('--local_rank', type=int, default=-1, help= 'local_rank for distributed training on gpus') parser.add_argument('--seed', type=int, default=42, help= 'random seed for initialization') parser.add_argument('--patience', type=int, default=5, help= 'early stop epoch nums on dev') parser.add_argument('--gradient_accumulation_steps', type=int, default= 1, help= 'Number of updates steps to accumulate before performing a backward/update pass.' ) parser.add_argument('--fp16', action='store_true', help= 'Whether to use 16-bit float precision instead of 32-bit') parser.add_argument('--loss_scale', type=float, default=0, help= """Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value. """ ) args = parser.parse_args() print('torch.cuda.is_available()', torch.cuda.is_available()) if args.local_rank == -1 or args.no_cuda: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') n_gpu = torch.cuda.device_count() else: torch.cuda.set_device(args.local_rank) device = torch.device('cuda', args.local_rank) n_gpu = 1 torch.distributed.init_process_group(backend='nccl') logger.info( 'device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}' .format(device, n_gpu, bool(args.local_rank != -1), args.fp16)) if args.gradient_accumulation_steps < 1: raise ValueError( 'Invalid gradient_accumulation_steps parameter: {}, should be >= 1' .format(args.gradient_accumulation_steps)) args.train_batch_size = (args.train_batch_size // args. gradient_accumulation_steps) random.seed(args.seed) np.random.seed(args.seed) torch.manual_seed(args.seed) if n_gpu > 0: torch.cuda.manual_seed_all(args.seed) if not args.do_train and not args.do_eval and not args.do_test: raise ValueError( 'At least one of `do_train` or `do_eval` must be True.') if os.path.exists(args.output_dir) and os.listdir(args.output_dir): print('WARNING: Output directory ({}) already exists and is not empty.' .format(args.output_dir)) if not os.path.exists(args.output_dir): os.makedirs(args.output_dir) tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) train_examples = None num_train_optimization_steps = None if args.do_train: ori_train_examples = read_csqa_examples(os.path.join(args.data_dir, 'train_rand_split.jsonl')) ori_dev_examples = read_csqa_examples(os.path.join(args.data_dir, 'dev_rand_split.jsonl')) ori_test_examples = read_csqa_examples(os.path.join(args.data_dir, 'train2_rand_split.jsonl')) if args.inhouse: train_examples = ori_train_examples[0:850] test_examples = ori_train_examples[8500:] dev_examples = ori_dev_examples[:] else: train_examples = ori_train_examples[:] dev_examples = ori_dev_examples[:] num_train_optimization_steps = int(len(train_examples) / args. train_batch_size / args.gradient_accumulation_steps ) * args.num_train_epochs if args.local_rank != -1: num_train_optimization_steps = (num_train_optimization_steps // torch.distributed.get_world_size()) model = BertForMultipleChoice.from_pretrained(args.bert_model, cache_dir=os.path.join(PYTORCH_PRETRAINED_BERT_CACHE, 'distributed_{}'.format(args.local_rank)), num_choices=5, mlp_hidden_dim=args.mlp_hidden_dim, mlp_dropout=args.mlp_dropout) if args.fp16: model.half() model.to(device) if args.local_rank != -1: try: from apex.parallel import DistributedDataParallel as DDP except ImportError: raise ImportError( 'Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.' ) model = DDP(model) elif n_gpu > 1: model = torch.nn.DataParallel(model) param_optimizer = list(model.named_parameters()) param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]] no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay}, {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}] if args.fp16: try: from apex.optimizers import FP16_Optimizer from apex.optimizers import FusedAdam except ImportError: raise ImportError( 'Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.' ) optimizer = FusedAdam(optimizer_grouped_parameters, lr=args. learning_rate, bias_correction=False, max_grad_norm=1.0) if args.loss_scale == 0: optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True) else: optimizer = FP16_Optimizer(optimizer, static_loss_scale=args. loss_scale) else: optimizer = BertAdam(optimizer_grouped_parameters, lr=args. learning_rate, warmup=args.warmup_proportion, t_total= num_train_optimization_steps) global_step = 0 if args.do_train: train_features = convert_examples_to_features(train_examples, tokenizer, args.max_seq_length, True) dev_features = convert_examples_to_features(dev_examples, tokenizer, args.max_seq_length, True) train_dataloader = get_train_dataloader(train_features, args) dev_dataloader = get_eval_dataloader(dev_features, args) if args.inhouse: test_features = convert_examples_to_features(test_examples, tokenizer, args.max_seq_length, True) test_dataloader = get_eval_dataloader(test_features, args) logger.info('***** Running training *****') logger.info('  Num examples = %d', len(train_examples)) logger.info('  Batch size = %d', args.train_batch_size) logger.info('  Num steps = %d', num_train_optimization_steps) logger.info('') logger.info('  Num train features = %d', len(train_features)) logger.info('  Num dev features = %d', len(dev_features)) best_dev_accuracy = 0 best_dev_epoch = 0 no_up = 0 epoch_tqdm = trange(int(args.num_train_epochs), desc='Epoch') for epoch in epoch_tqdm: model.train() tr_loss = 0 nb_tr_examples, nb_tr_steps = 0, 0 for step, batch in enumerate(tqdm(train_dataloader, desc= 'Iteration')): batch = tuple(t.to(device) for t in batch) input_ids, input_mask, segment_ids, label_ids = batch loss = model(input_ids, segment_ids, input_mask, label_ids) if n_gpu > 1: loss = loss.mean() if args.fp16 and args.loss_scale != 1.0: loss = loss * args.loss_scale if args.gradient_accumulation_steps > 1: loss = loss / args.gradient_accumulation_steps tr_loss += loss.item() nb_tr_examples += input_ids.size(0) nb_tr_steps += 1 if args.fp16: optimizer.backward(loss) else: loss.backward() if (step + 1) % args.gradient_accumulation_steps == 0: if args.fp16: lr_this_step = args.learning_rate * warmup_linear( global_step / num_train_optimization_steps, args.warmup_proportion) for param_group in optimizer.param_groups: param_group['lr'] = lr_this_step optimizer.step() optimizer.zero_grad() global_step += 1 dev_loss, dev_accuracy = evaluate(model, device, dev_dataloader, desc='Evaluate Dev') if args.inhouse: test_loss, test_accuracy = evaluate(model, device, test_dataloader, desc='Evaluate Test') if dev_accuracy > best_dev_accuracy: best_dev_accuracy = dev_accuracy best_dev_epoch = epoch + 1 no_up = 0 model_to_save = model.module if hasattr(model, 'module' ) else model output_model_file = os.path.join(args.output_dir, args. save_model_name + '.bin.%d' % epoch) torch.save(model_to_save.state_dict(), output_model_file) output_config_file = os.path.join(args.output_dir, args. save_model_name + '.config') with open(output_config_file, 'w') as fpp: fpp.write(model_to_save.config.to_json_string()) else: no_up += 1 tqdm.write('\t ***** Eval results (Epoch %s) *****' % str(epoch + 1)) tqdm.write('\t dev_accuracy = %s' % str(dev_accuracy)) tqdm.write('') if args.inhouse: tqdm.write('\t test_accuracy = %s' % str(test_accuracy)) tqdm.write('') tqdm.write('\t best_dev_accuracy = %s' % str(best_dev_accuracy)) tqdm.write('\t best_dev_epoch = %s' % str(best_dev_epoch)) tqdm.write('\t no_up = %s' % str(no_up)) tqdm.write('') if no_up >= args.patience: epoch_tqdm.close() break model.to(device) if args.do_eval and (args.local_rank == -1 or torch.distributed. get_rank() == 0): output_model_file = os.path.join(args.output_dir, args. save_model_name + '.bin.%d' % args.epoch_suffix) output_config_file = os.path.join(args.output_dir, args. save_model_name + '.config') config = BertConfig(output_config_file) model = BertForMultipleChoice(config, num_choices=5, mlp_hidden_dim =args.mlp_hidden_dim, mlp_dropout=args.mlp_dropout) model.load_state_dict(torch.load(output_model_file)) model.to(device) if args.wsc: eval_examples = read_csqa_examples('../datasets/wsc.jsonl') elif args.swag_transfer: eval_examples = read_csqa_examples( '../datasets/swagaf/data/val.jsonl') else: eval_examples = read_csqa_examples(os.path.join(args.data_dir, 'dev_rand_split.jsonl')) eval_features = convert_examples_to_features(eval_examples, tokenizer, args.max_seq_length, True) if args.inhouse: eval_examples_test = read_csqa_examples(os.path.join(args. data_dir, 'train_rand_split.jsonl'))[8500:] eval_features_test = convert_examples_to_features( eval_examples_test, tokenizer, args.max_seq_length, True) logger.info('***** Running evaluation *****') logger.info('  Num examples = %d', len(eval_examples)) logger.info('  Batch size = %d', args.eval_batch_size) all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long) all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long) all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long) all_label = torch.tensor([f.label for f in eval_features], dtype= torch.long) eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label) eval_sampler = SequentialSampler(eval_data) eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size) model.eval() eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 test_outputs = [] for input_ids, input_mask, segment_ids, label_ids in tqdm( eval_dataloader, desc='evaluating'): input_ids = input_ids.to(device) input_mask = input_mask.to(device) segment_ids = segment_ids.to(device) label_ids = label_ids.to(device) with torch.no_grad(): tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids) logits = model(input_ids, segment_ids, input_mask) logits = logits.detach().cpu().numpy() outputs = np.argmax(logits, axis=1) test_outputs += list(outputs) label_ids = label_ids.to('cpu').numpy() tmp_eval_accuracy = accuracy(logits, label_ids) eval_loss += tmp_eval_loss.mean().item() eval_accuracy += tmp_eval_accuracy nb_eval_examples += input_ids.size(0) nb_eval_steps += 1 eval_loss = eval_loss / nb_eval_steps eval_accuracy = eval_accuracy / nb_eval_examples if args.wsc: result = {'eval_accuracy': eval_accuracy} logger.info('***** Eval results *****') for key in sorted(result.keys()): logger.info('  %s = %s', key, str(result[key])) test_output_file = os.path.join(args.output_dir, args. save_model_name + '_wsc_prediction.csv') with open(test_output_file, 'w') as fout: with open(os.path.join('../datasets/wsc.jsonl'), 'r', encoding='utf-8') as fin: examples = [] for i, line in enumerate(fin.readlines()): csqa_json = json.loads(line) label_pred = chr(ord('A') + test_outputs[i]) if label_pred in ['C', 'E']: label_pred = 'A' if label_pred in ['D']: label_pred = 'B' fout.write(csqa_json['id'] + ',' + str(label_pred) + '\n') elif args.swag_transfer: result = {'eval_accuracy': eval_accuracy} logger.info('***** Eval results *****') for key in sorted(result.keys()): logger.info('  %s = %s', key, str(result[key])) test_output_file = os.path.join(args.output_dir, args. save_model_name + '_swag_val.csv') with open(test_output_file, 'w') as fout: with open(os.path.join('../datasets/swagaf/data/val.jsonl'), 'r', encoding='utf-8') as fin: examples = [] for i, line in enumerate(fin.readlines()): csqa_json = json.loads(line) label_pred = chr(ord('A') + test_outputs[i]) if label_pred == 'E': label_pred = 'A' fout.write(csqa_json['id'] + ',' + str(label_pred) + '\n') elif args.inhouse: dev_result = {'dev_eval_accuracy': eval_accuracy} logger.info('***** Running evaluation *****') logger.info('  Num examples = %d', len(eval_examples)) logger.info('  Batch size = %d', args.eval_batch_size) all_input_ids = torch.tensor(select_field(eval_features_test, 'input_ids'), dtype=torch.long) all_input_mask = torch.tensor(select_field(eval_features_test, 'input_mask'), dtype=torch.long) all_segment_ids = torch.tensor(select_field(eval_features_test, 'segment_ids'), dtype=torch.long) all_label = torch.tensor([f.label for f in eval_features_test], dtype=torch.long) eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label) eval_sampler = SequentialSampler(eval_data) eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size) model.eval() eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 test_outputs = [] for input_ids, input_mask, segment_ids, label_ids in eval_dataloader: input_ids = input_ids.to(device) input_mask = input_mask.to(device) segment_ids = segment_ids.to(device) label_ids = label_ids.to(device) with torch.no_grad(): tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids) logits = model(input_ids, segment_ids, input_mask) logits = logits.detach().cpu().numpy() outputs = np.argmax(logits, axis=1) test_outputs += list(outputs) label_ids = label_ids.to('cpu').numpy() tmp_eval_accuracy = accuracy(logits, label_ids) eval_loss += tmp_eval_loss.mean().item() eval_accuracy += tmp_eval_accuracy nb_eval_examples += input_ids.size(0) nb_eval_steps += 1 eval_loss = eval_loss / nb_eval_steps eval_accuracy = eval_accuracy / nb_eval_examples test_result = {'test_eval_accuracy': eval_accuracy} with open(output_eval_file, 'w') as writer: logger.info('***** Eval results *****') for key in sorted(result.keys()): logger.info('  %s = %s', key, str(result[key])) writer.write('%s = %s\n' % (key, str(result[key]))) else: result = {'eval_accuracy': eval_accuracy} test_output_file = os.path.join(args.output_dir, args. save_model_name + '_dev_output.csv') with open(test_output_file, 'w') as fout: with open(os.path.join(args.data_dir, 'dev_rand_split.jsonl'), 'r', encoding='utf-8') as fin: examples = [] for i, line in enumerate(fin.readlines()): csqa_json = json.loads(line) label_pred = chr(ord('A') + test_outputs[i]) fout.write(csqa_json['id'] + ',' + str(label_pred) + '\n') output_eval_file = os.path.join(args.output_dir, args. save_model_name + '_res_on_dev.txt') with open(output_eval_file, 'w') as writer: logger.info('***** Eval results *****') for key in sorted(result.keys()): logger.info('  %s = %s', key, str(result[key])) writer.write('%s = %s\n' % (key, str(result[key]))) if args.do_test and (args.local_rank == -1 or torch.distributed. get_rank() == 0): output_model_file = os.path.join(args.output_dir, args. save_model_name + '.bin.%d' % args.epoch_suffix) output_config_file = os.path.join(args.output_dir, args. save_model_name + '.config') config = BertConfig(output_config_file) model = BertForMultipleChoice(config, num_choices=5, mlp_hidden_dim =args.mlp_hidden_dim, mlp_dropout=args.mlp_dropout) model.load_state_dict(torch.load(output_model_file)) model.to(device) eval_examples = read_csqa_examples(os.path.join(args.data_dir, 'test_rand_split_no_answers.jsonl'), have_answer=False) eval_features = convert_examples_to_features(eval_examples, tokenizer, args.max_seq_length, True) logger.info('***** Running evaluation *****') logger.info('  Num examples = %d', len(eval_examples)) logger.info('  Batch size = %d', args.eval_batch_size) all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long) all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long) all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long) all_label = torch.tensor([f.label for f in eval_features], dtype= torch.long) eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label) eval_sampler = SequentialSampler(eval_data) eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size) model.eval() test_outputs = [] for input_ids, input_mask, segment_ids, label_ids in eval_dataloader: input_ids = input_ids.to(device) input_mask = input_mask.to(device) segment_ids = segment_ids.to(device) with torch.no_grad(): logits = model(input_ids, segment_ids, input_mask) logits = logits.detach().cpu().numpy() outputs = np.argmax(logits, axis=1) test_outputs += list(outputs) test_output_file = os.path.join(args.output_dir, args. save_model_name + '_test_output.csv') with open(test_output_file, 'w') as fout: with open(os.path.join(args.data_dir, 'test_rand_split_no_answers.jsonl'), 'r', encoding='utf-8' ) as fin: examples = [] for i, line in enumerate(fin.readlines()): csqa_json = json.loads(line) label_pred = chr(ord('A') + test_outputs[i]) fout.write(csqa_json['id'] + ',' + str(label_pred) + '\n') 
prepare|utils|data def prepare_data(args, config, train_path): """Downloads the PTB or COCO dataset """ if not os.path.exists(config.log_dir): os.mkdir(config.log_dir) ptb_url = 'https://jxhe.github.io/download/ptb_data.tgz' coco_url = 'https://VegB.github.io/downloads/coco_data.tgz' data_path = args.data_path if not tf.gfile.Exists(train_path): url = ptb_url if args.dataset == 'ptb' else coco_url tx.data.maybe_download(url, data_path, extract=True) os.remove('%s_data.tgz' % args.dataset) 
calc|SRGANs|evaluations|ref|stats|master|main|Regularization|GANs|Spectral def main(): parser = argparse.ArgumentParser() parser.add_argument('--gpu', '-g', type=int, default=0) parser.add_argument('--dataset', type=str, default='imagenet') parser.add_argument('--stat_dir_path', type=str, default='') parser.add_argument('--n_classes', type=int, default=1000) parser.add_argument('--tf', action='store_true', default=False) args = parser.parse_args() chainer.cuda.get_device_from_id(args.gpu).use() if args.dataset == 'imagenet': get_samples = get_imagenet_samples else: raise NotImplementedError if not os.path.exists(args.stat_dir_path): os.makedirs(args.stat_dir_path) if args.tf: import source.inception.inception_score_tf from source.inception.inception_score_tf import get_mean_and_cov as get_mean_cov else: from evaluation import get_mean_cov model = load_inception_model(args.inception_model_path) for c in range(args.n_classes): print('label:{}'.format(c)) all_ref_samples = get_samples(c) if args.tf: mean, cov = get_mean_cov(all_ref_samples) else: with chainer.using_config('train', False), chainer.using_config( 'enable_backprop', False): mean, cov = get_mean_cov(model, all_ref_samples) np.savez(os.path.join(args.stat_dir_path, '{}.npz'.format(int(c))), mean=mean, cov=cov) 
block|scannet|scene|parse|evaluate|withoverlap def parse_block_scene(datapath, scene_names): f = open(os.path.join(datapath, 'log_block.txt'), 'r') blocklist = f.read().splitlines() block2scene = [] ordered_testlist = [] for line in blocklist: str = line.split(', ') if str[0] in scene_names: block2scene.append(tuple(str)) tfrecord_name = os.path.join(datapath, '%s.tfrecord' % str[0]) if not tfrecord_name in ordered_testlist: ordered_testlist.append(tfrecord_name) return block2scene, ordered_testlist 
get|input|training|data|dataset|thumt def get_training_input(filenames, params): """ Get input for training stage  :param filenames: A list contains [source_filenames, target_filenames] :param params: Hyper-parameters  :returns: A dictionary of pair <Key, Tensor> """ with tf.device('/cpu:0'): src_dataset = tf.data.TextLineDataset(filenames[0]) tgt_dataset = tf.data.TextLineDataset(filenames[1]) dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset)) if distribute.is_distributed_training_mode(): dataset = dataset.shard(distribute.size(), distribute.rank()) dataset = dataset.shuffle(params.buffer_size) dataset = dataset.repeat() dataset = dataset.map(lambda src, tgt: (tf.string_split([src]). values, tf.string_split([tgt]).values), num_parallel_calls= params.num_threads) dataset = dataset.map(lambda src, tgt: (tf.concat([src, [tf. constant(params.eos)]], axis=0), tf.concat([tgt, [tf.constant( params.eos)]], axis=0)), num_parallel_calls=params.num_threads) dataset = dataset.map(lambda src, tgt: {'source': src, 'target': tgt, 'source_length': tf.shape(src), 'target_length': tf.shape( tgt)}, num_parallel_calls=params.num_threads) iterator = dataset.make_one_shot_iterator() features = iterator.get_next() src_table = tf.contrib.lookup.index_table_from_tensor(tf.constant( params.vocabulary['source']), default_value=params.mapping[ 'source'][params.unk]) tgt_table = tf.contrib.lookup.index_table_from_tensor(tf.constant( params.vocabulary['target']), default_value=params.mapping[ 'target'][params.unk]) features['source'] = src_table.lookup(features['source']) features['target'] = tgt_table.lookup(features['target']) features = batch_examples(features, params.batch_size, params. max_length, params.mantissa_bits, shard_multiplier=len(params. device_list), length_multiplier=params.length_multiplier, constant=params.constant_batch_size, num_threads=params.num_threads ) features['source'] = tf.to_int32(features['source']) features['target'] = tf.to_int32(features['target']) features['source_length'] = tf.to_int32(features['source_length']) features['target_length'] = tf.to_int32(features['target_length']) features['source_length'] = tf.squeeze(features['source_length'], 1) features['target_length'] = tf.squeeze(features['target_length'], 1) return features 
GPSig|TimeSeriesFeatureScaler|preprocessing|init def __init__(self, mode='standard', max_percentile=99, min_percentile=1, num_features=None): self.mode = mode self.max_percentile = max_percentile self.min_percentile = min_percentile self.num_features = num_features 
AffineCouplingGainEx1|inverse def _inverse(self, y, yy, nlf0=None, nlf1=None, iso=None, cam=None): scale = gain_model_params_ex1(iso) x = y if scale is not None: x /= scale if self._last_layer: return tf.layers.flatten(x) return x 
test|vtrace|log|from|and|logits|LogProbsFromLogitsAndActionsTest|actions|probs def test_log_probs_from_logits_and_actions(self): """Tests log_probs_from_logits_and_actions.""" batch_size = 2 seq_len = 7 num_actions = 3 policy_logits = _shaped_arange(seq_len, batch_size, num_actions) + 10 actions = np.random.randint(0, num_actions - 1, size=(seq_len, batch_size), dtype=np.int32) action_log_probs_tensor = vtrace.log_probs_from_logits_and_actions( policy_logits, actions) action_index_mask = actions[..., None] == np.arange(num_actions)  def index_with_mask(array, mask): return array[mask].reshape(*array.shape[:-1]) ground_truth_v = index_with_mask(np.log(_softmax(policy_logits)), action_index_mask) self.assertAllClose(ground_truth_v, action_log_probs_tensor) 
SRDRM|generator|res|mult|2x|model|nets|build def res_mult_2x(layer_input): l1 = Conv2D(64, kernel_size=4, strides=1, padding='same')(layer_input) l1 = Activation('relu')(l1) r = residual_block(l1, self.gf) for _ in range(self.n_residual_blocks - 1): r = residual_block(r, self.gf) l2 = Conv2D(64, kernel_size=4, strides=1, padding='same')(r) l2 = BatchNormalization(momentum=0.8)(l2) l2 = Add()([l2, l1]) layer_2x = deconv2d(l2) return layer_2x 
texar|networks|FeedForwardNetworkBase|modules|init|network|base def __init__(self, hparams=None): ModuleBase.__init__(self, hparams) self._layers = [] self._layer_names = [] self._layers_by_name = {} self._layer_outputs = [] self._layer_outputs_by_name = {} 
plot|click|interactive|on|latent|make|plots def on_click(click): global last_sample if click.xdata != None and click.ydata != None and click.inaxes == ax[0]: z1 = click.xdata z2 = click.ydata dream = decoder.predict(np.array([[z1, z2]])) pred, entropy, bald = model.get_results(dream) print('Predicted Class: {}, prob: {}'.format(pred.argmax(axis=1), pred.max(axis=1))) print('Predictive Entropy: {}'.format(entropy[0])) print('MI Score:         {}'.format(bald[0])) proj.set_data(dream.squeeze()) print(z1, z2) plt.draw() last_sample = dream 
restart|wolf|plato|reinforcement|WoLFPHCPolicy|component|agent|phc|policy|dialogue|learning def restart(self, args): """ Re-initialize relevant parameters / variables at the beginning of each dialogue.  :return: nothing """ if self.agent_role == 'user' and self.warmup_simulator: if 'goal' in args: self.warmup_simulator.initialize(args) else: print( 'WARNING! No goal provided for WoLF PHC policy user simulator @ restart' ) self.warmup_simulator.initialize({}) 
Output|end|validation|output def validation_end(self, session, epoch, global_step, test_loss, lr, top1_accuracy): self.log_metrics(session=session, epoch=epoch, loss=test_loss, lr=lr, top1_accuracy=top1_accuracy, is_test=True) self.tb_writer.flush() if top1_accuracy >= self.best_top1_accuracy: self.best_top1_accuracy = top1_accuracy self.save_model_best_top1(session, epoch, global_step) if test_loss < self.best_loss: self.best_loss = test_loss self.save_model_best_loss(session, epoch, global_step) 
utilities|pickle|Utilities|load|EndToEndClassification def load_pickle(load_path): """ Loads a .pickle file as dictionary.  Args: load_path (str): path to .pkl file.  Returns: (dict): dictionary. """ with open(load_path, 'rb') as f: pickle_to_load = pickle.load(f) return pickle_to_load 
finetuning|get|lm|corpus|BERTDataset|line|run def get_corpus_line(self, item): """ Get one sample from corpus consisting of a pair of two subsequent lines from the same doc. :param item: int, index of sample. :return: (str, str), two subsequent sentences from corpus """ t1 = '' t2 = '' assert item < self.corpus_lines if self.on_memory: sample = self.sample_to_doc[item] t1 = self.all_docs[sample['doc_id']][sample['line']] t2 = self.all_docs[sample['doc_id']][sample['line'] + 1] self.current_doc = sample['doc_id'] return t1, t2 else: if self.line_buffer is None: while t1 == '': t1 = next(self.file).strip() t2 = next(self.file).strip() else: t1 = self.line_buffer t2 = next(self.file).strip() while t2 == '' or t1 == '': t1 = next(self.file).strip() t2 = next(self.file).strip() self.current_doc = self.current_doc + 1 self.line_buffer = t2 assert t1 != '' assert t2 != '' return t1, t2 
hyperparams|texar|get|HParams def get(self, name, default=None): """Returns the hyperparameter value for the given name. If name is not available then returns :attr:`default`.  Args: name (str): the name of hyperparameter. default: the value to be returned in case name does not exist. """ try: return self.__getattr__(name) except AttributeError: return default 
evaluate|embeddings|wiki|node2vec def evaluate_embeddings(embeddings): X, Y = read_node_label('../data/wiki/wiki_labels.txt') tr_frac = 0.8 print('Training classifier using {:.2f}% nodes...'.format(tr_frac * 100)) clf = Classifier(embeddings=embeddings, clf=LogisticRegression()) clf.split_train_evaluate(X, Y, tr_frac) 
channel|utils|last|to @add_arg_scope def channel_to_last(inputs, data_format='NHWC', scope=None): """Move the channel axis to the last dimension. Allows to provide a consistent NHWC output format whatever the input data format.  Args: inputs: Input Tensor; data_format: NHWC or NCHW. """ with tf.name_scope(scope, 'channel_to_last', [inputs]): if data_format == 'NHWC': net = inputs elif data_format == 'NCHW': net = tf.transpose(inputs, perm=(0, 2, 3, 1)) return net 
moments|distributions|accountant|mu|calculate def mu0(y): return pdf_gauss(y, sigma=sigma, mean=0.0) 
td|not|Environment|wait|doom|or|env def _wait(self): time.sleep(self.sleep_time * self.action_repetition) 
HierarchicalRNNEncoder|texar|modules|hierarchical|init|encoders def __init__(self, encoder_major=None, encoder_minor=None, hparams=None): EncoderBase.__init__(self, hparams) encoder_major_hparams = utils.get_instance_kwargs(None, self._hparams. encoder_major_hparams) encoder_minor_hparams = utils.get_instance_kwargs(None, self._hparams. encoder_minor_hparams) if encoder_major is not None: self._encoder_major = encoder_major else: with tf.variable_scope(self.variable_scope.name): with tf.variable_scope('encoder_major'): self._encoder_major = utils.check_or_get_instance(self. _hparams.encoder_major_type, encoder_major_hparams, [ 'texar.modules.encoders', 'texar.custom']) if encoder_minor is not None: self._encoder_minor = encoder_minor elif self._hparams.config_share: with tf.variable_scope(self.variable_scope.name): with tf.variable_scope('encoder_minor'): self._encoder_minor = utils.check_or_get_instance(self. _hparams.encoder_major_type, encoder_major_hparams, [ 'texar.modules.encoders', 'texar.custom']) else: with tf.variable_scope(self.variable_scope.name): with tf.variable_scope('encoder_minor'): self._encoder_minor = utils.check_or_get_instance(self. _hparams.encoder_minor_type, encoder_minor_hparams, [ 'texar.modules.encoders', 'texar.custom']) 
plato|calculated|component|agent|next|action|CalculatedPolicy|policy|dialogue def next_action(self, state): """ Consult the dialogue policy and produce the agent's response  :param state: the current dialogue state :return: a list of actions, representing the agent's response """ sys_acts = [] state_enc = self.encode_state(state) if state not in self.policy: state_enc = '' if state.user_acts: for sa in state.user_acts: state_enc = sa.intent if sa.intent == 'offer': state_enc += '_name' elif sa.params: state_enc += '_' + sa.params[0].slot state_enc += ';' state_enc = state_enc[:-1] if state_enc in self.policy: sys_actions = list(self.policy[state_enc]['dacts'].keys()) probs = [self.policy[state_enc]['dacts'][i] for i in sys_actions] sys_act_slots = deepcopy(random.choices(sys_actions, weights=probs)[0] ).split(';') for sys_act_slot in sys_act_slots: if not sys_act_slot: continue sys_act = DialogueAct('UNK') sys_act_slot_parts = sys_act_slot.split('_') sys_act.intent = sys_act_slot_parts[0] if len(sys_act_slot_parts) > 1: sys_act.params = [DialogueActItem(sys_act_slot_parts[1], Operator.EQ, '')] if sys_act.intent == 'offer': sys_act.params = [] elif sys_act.intent == 'canthelp.exception': sys_act.intent = 'canthelp' sys_acts.append(sys_act) else: print( f'Warning! {self.agent_role} Calculated dialogue policy: state not found, selecting random action.' ) sys_act = DialogueAct('UNK') if self.agent_role == 'system': sys_act.intent = random.choice(['welcomemsg', 'inform', 'request']) elif self.agent_role == 'user': sys_act.intent = random.choice(['hello', 'inform', 'request']) else: sys_act.intent = random.choice(['bye', 'inform', 'request']) sys_acts.append(sys_act) return sys_acts 
in|only|avod|label|flip|kitti|datasets|3d|aug def flip_label_in_3d_only(obj_label): """Flips only the 3D position of an object label. The 2D bounding box is not flipped to save time since it is not used.  Args: obj_label: ObjectLabel  Returns: A flipped object """ flipped_label = copy.deepcopy(obj_label) if obj_label.ry >= 0: flipped_label.ry = np.pi - obj_label.ry else: flipped_label.ry = -np.pi - obj_label.ry flipped_t = -flipped_label.t[0], flipped_label.t[1], flipped_label.t[2] flipped_label.t = flipped_t return flipped_label 
deepMOT|models|sst|master|DAN|build def build_sst(phase, size=900, use_gpu=config['cuda']): """ create the SSJ Tracker Object :return: ssj tracker object """ if phase != 'test' and phase != 'train': print('Error: Phase not recognized') return if size != 900: print('Error: Sorry only SST{} is supported currently!'.format(size)) return base = config['base_net'] extras = config['extra_net'] final = config['final_net'] return SST(phase, *selector(vgg(base[str(size)], 3), add_extras(extras[ str(size)], 1024)), add_final(final[str(size)]), use_gpu) 
HybridKmeans|centroids|implementation|move|utils def move_centroids(self, points): """returns the new centroids assigned from the points closest to them""" centers = np.zeros(self.cluster_centers_.shape) for k in range(self.cluster_centers_.shape[0]): points_in_cluster = points[self.closest_centers_ == k] azi_mean = scipy.stats.circmean(points_in_cluster[:, (0)], high=np. pi, low=-np.pi) ele_mean = np.mean(points_in_cluster[:, (1)]) centers[k] = [azi_mean, ele_mean] self.cluster_centers_ = centers 
iter|LFattNet|threadsafe|next|train def __next__(self): with self.lock: return self.it.__next__() 
models|cae|load|Autoencoder def load(self, sess, file_path, verbose=True): """Load a model NOTE: when loading a model the method tf.global_variables_initializer() must not be called otherwise the variables are set to random values @param sess (tf.Session) the current session @param path to the model folder, note that the path should end with '/model.ckpt' even though this object does not exists in the path @param verbose (bool) if True print information on terminal """ if verbose: print('Loading network from: ' + str(file_path)) save_path = self.tf_saver.restore(sess, file_path) if verbose: print('Done!') 
mn|models|SRGANs|master|train|load|Regularization|GANs|Spectral def load_models(config): gen_conf = config.models['generator'] gen = yaml_utils.load_model(gen_conf['fn'], gen_conf['name'], gen_conf[ 'args']) dis_conf = config.models['discriminator'] dis = yaml_utils.load_model(dis_conf['fn'], dis_conf['name'], dis_conf[ 'args']) return gen, dis 
get|models|inference|func|transformer|Transformer|thumt def get_inference_func(self):  def encoding_fn(features, params=None): if params is None: params = copy.copy(self.parameters) else: params = copy.copy(params) with tf.variable_scope(self._scope): encoder_output = encoding_graph(features, 'infer', params) batch = tf.shape(encoder_output)[0] state = {'encoder': encoder_output, 'decoder': {('layer_%d' % i ): {'key': tf.zeros([batch, 0, params. attention_key_channels or params.hidden_size]), 'value': tf .zeros([batch, 0, params.attention_value_channels or params .hidden_size])} for i in range(params.num_decoder_layers)}} return state  def decoding_fn(features, state, params=None): if params is None: params = copy.copy(self.parameters) else: params = copy.copy(params) with tf.variable_scope(self._scope): log_prob, new_state = decoding_graph(features, state, 'infer', params) return log_prob, new_state return encoding_fn, decoding_fn 
learner|with|fn|address|step|train|run def step_fn(actor_output): """Per-replica StepFn.""" actor_output = tf.nest.pack_sequence_as(specs, actor_output) (initial_agent_state, env_output, actor_agent_output, actor_action, loss_type, info) = actor_output with tf.GradientTape() as tape: loss = loss_fns.compute_loss(study_loss_types=study_loss_types, current_batch_loss_type=loss_type, agent=agent, agent_state= initial_agent_state, env_output=env_output, actor_agent_output= actor_agent_output, actor_action=actor_action, num_steps=iterations ) grads = tape.gradient(loss, agent.trainable_variables) grad_norms = {} for var, grad in zip(agent.trainable_variables, grads): if grad is not None: grad_norms[var.name] = tf.norm(grad) optimizer.apply_gradients(zip(grads, agent.trainable_variables)) return info, grad_norms 
test|lm|GPT2ModelTest|gpt2|create|head|GPT2ModelTester|modeling def create_gpt2_lm_head(self, config, input_ids, token_type_ids, position_ids, mc_labels, lm_labels, mc_token_ids): model = GPT2LMHeadModel(config) model.eval() loss = model(input_ids, position_ids, token_type_ids, lm_labels) lm_logits, presents = model(input_ids, position_ids, token_type_ids) outputs = {'loss': loss, 'lm_logits': lm_logits, 'presents': presents} return outputs 
out|set|files|embeddings|OpenKE|Config|config def set_out_files(self, path): self.out_path = path 
commons|fn|model|uncond|arch def model_fn_uncond(hparams, z, x, generator, discriminator, mdevice): theta_ph = mdevice.get_theta_ph(hparams) theta_gen_ph = mdevice.get_theta_ph(hparams) x_gen = generator(hparams, z, 'gen', train=True, reuse=False) x_sample = generator(hparams, z, 'gen', train=False, reuse=True) x_lossy, x_gen_lossy = get_lossy(hparams, mdevice, x, theta_ph, x_gen, theta_gen_ph) _, d_logit = discriminator(hparams, x_lossy, 'discrim', train=True, reuse=False) _, d_gen_logit = discriminator(hparams, x_gen_lossy, 'discrim', train= True, reuse=True) d_loss, g_loss = get_loss(hparams, d_logit, d_gen_logit, x_lossy, x_gen_lossy, discriminator, None, None, None) d_update_op, g_update_op, iter_ph = utils.get_train_ops(hparams, d_loss, g_loss) return (x_lossy, x_sample, theta_ph, theta_gen_ph, d_loss, g_loss, d_update_op, g_update_op, iter_ph) 
mnist|setup|labels|extract def extract_labels(filename, num_images): with gzip.open(filename) as bytestream: bytestream.read(8) buf = bytestream.read(1 * num_images) labels = np.frombuffer(buf, dtype=np.uint8) return (np.arange(10) == labels[:, (None)]).astype(np.float32) 
kernels|Sequential|GPSig|diag|sequentializer @params_as_tensors def _sequentializer_diag(self, base_kern, X, override_full=False): """ Input :X:         (num_streams1, num_features * stream_length) array of flattened streams :base_kern: function handle to a kernel function that takes two arrays as input, say, z1 of size (num_samples1, num_features) and z2 of size (num_samples2, num_features), and computes k(z1,z2) of size (num_samples1, num_samples2) Output :K:         (num_streams1) array of (full-rank) sequentialized kernel entries corresponding to the diagonal entries. """ if self.low_rank: raise NotImplementedError( 'Diagonal mode not implemented for low-rank mode. Use feature-map output with sequentializer instead.' ) num_streams = tf.shape(X)[0] X = tf.reshape(X, [num_streams, -1, self.num_features]) stream_length = tf.shape(X)[1] if self.lengthscales is not None: X /= self.lengthscales[(None), (None), :] if self.offsets is not None: X -= self.offsets[(None), (None), :] num_features = self.num_features if self.num_lags > 0: num_features *= self.num_lags + 1 X = helpers.add_lags_to_streams(X, self.lags) M = base_kern(X) M = tf.reshape(M, [num_streams, stream_length, stream_length]) K = sequential_algs.sequentialize_kern_diag(M, self.variances, self. num_levels, self.normalize_levels, difference=self.difference) return K 
det|AffineCouplingSdnGain|log|and|jacobian|inverse def _inverse_and_log_det_jacobian(self, y, yy, nlf0=None, nlf1=None, iso= None, cam=None): beta1, beta2 = sdn_iso_model_params_3(iso) scale = tf.sqrt(beta1 * yy + beta2) x = y if scale is not None: x /= scale if scale is None: log_abs_det_J_inv = tf.constant(0.0, dtype=y.dtype, name='ildj') else: log_abs_det_J_inv = -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) if self._last_layer: return tf.layers.flatten(x), log_abs_det_J_inv return x, log_abs_det_J_inv 
gen|models|256|SRGANs|init|master|ResNetGenerator|Regularization|GANs|Spectral|resnet def __init__(self, ch=64, dim_z=128, bottom_width=4, activation=F.relu, n_classes=0, distribution='normal'): super(ResNetGenerator, self).__init__() initializer = chainer.initializers.GlorotUniform() self.bottom_width = bottom_width self.activation = activation self.distribution = distribution self.dim_z = dim_z self.n_classes = n_classes with self.init_scope(): self.l1 = L.Linear(dim_z, bottom_width ** 2 * ch * 16, initialW= initializer) self.block2 = Block(ch * 16, ch * 16, activation=activation, upsample=True, n_classes=n_classes) self.block3 = Block(ch * 16, ch * 8, activation=activation, upsample=True, n_classes=n_classes) self.block4 = Block(ch * 8, ch * 8, activation=activation, upsample =True, n_classes=n_classes) self.block5 = Block(ch * 8, ch * 4, activation=activation, upsample =True, n_classes=n_classes) self.block6 = Block(ch * 4, ch * 2, activation=activation, upsample =True, n_classes=n_classes) self.block7 = Block(ch * 2, ch, activation=activation, upsample= True, n_classes=n_classes) self.b8 = L.BatchNormalization(ch) self.l8 = L.Convolution2D(ch, 3, ksize=3, stride=1, pad=1, initialW =initializer) 
test|InceptionV2Test|inception|nets|testBuildAndCheckAllEndPointsUptoMixed5c|v2 def testBuildAndCheckAllEndPointsUptoMixed5c(self): batch_size = 5 height, width = 224, 224 inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = inception.inception_v2_base(inputs, final_endpoint= 'Mixed_5c') endpoints_shapes = {'Mixed_3b': [batch_size, 28, 28, 256], 'Mixed_3c': [batch_size, 28, 28, 320], 'Mixed_4a': [batch_size, 14, 14, 576], 'Mixed_4b': [batch_size, 14, 14, 576], 'Mixed_4c': [batch_size, 14, 14, 576], 'Mixed_4d': [batch_size, 14, 14, 576], 'Mixed_4e': [ batch_size, 14, 14, 576], 'Mixed_5a': [batch_size, 7, 7, 1024], 'Mixed_5b': [batch_size, 7, 7, 1024], 'Mixed_5c': [batch_size, 7, 7, 1024], 'Conv2d_1a_7x7': [batch_size, 112, 112, 64], 'MaxPool_2a_3x3': [batch_size, 56, 56, 64], 'Conv2d_2b_1x1': [ batch_size, 56, 56, 64], 'Conv2d_2c_3x3': [batch_size, 56, 56, 192], 'MaxPool_3a_3x3': [batch_size, 28, 28, 192]} self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys()) for endpoint_name in endpoints_shapes: expected_shape = endpoints_shapes[endpoint_name] self.assertTrue(endpoint_name in end_points) self.assertListEqual(end_points[endpoint_name].get_shape().as_list( ), expected_shape) 
gym|rooms|init|examples|PlayerSprite|pycolab|classics|four def __init__(self, corner, position, character): """Inform superclass that we can't walk through walls.""" super(PlayerSprite, self).__init__(corner, position, character, impassable='#') 
official|flags|log|define|benchmark|check|dir|utils @flags.multi_flags_validator(['benchmark_logger_type', 'benchmark_log_dir'], message= '--benchmark_logger_type=BenchmarkFileLogger will require --benchmark_log_dir being set' ) def _check_benchmark_log_dir(flags_dict): benchmark_logger_type = flags_dict['benchmark_logger_type'] if benchmark_logger_type == 'BenchmarkFileLogger': return flags_dict['benchmark_log_dir'] return True 
pad|texar|Vocab|vocabulary|token|data|id @property def pad_token_id(self): """The `int` index of the special token indicating padding token. """ return self.token_to_id_map_py[self._pad_token] 
sp|reshape|func|global def reshape_func(d, subcarrier_spacing): d = d[(...), ::subcarrier_spacing] d = np.transpose(d, [0, 1, 4, 3, 2]) d = d.reshape(d.shape[:-2] + (-1,)) return d 
regression|init|misc|layers|FeedForward def __init__(self, n_in, n_out, w_name, params=None): super(FeedForward, self).__init__(n_in, n_out, w_name, params) 
tangents|get|stax|attr|element|maximal|set|req|neural|covariances def _get_maximal_element(covs_req, comparison_op): for f in kernel_fns: if hasattr(f, _COVARIANCES_REQ): marginal = getattr(f, _COVARIANCES_REQ)['marginal'] cross = getattr(f, _COVARIANCES_REQ)['cross'] if comparison_op(marginal, covs_req['marginal']): covs_req['marginal'] = marginal if comparison_op(cross, covs_req['cross']): covs_req['cross'] = cross return covs_req 
evaluate|epoch|Evaluator def epoch(self, opt, model, data_loader, split, keyset=None): average_loss, nums = self.initialize_losses() data_loader.reset_offsets(splits=split, shuffle=False) model.eval() start = time.time() bar = utils.set_progress_bar(data_loader.total_size[split]) reset = False with torch.no_grad(): while not reset: start = data_loader.offset_summary(split) outputs = self.batch(opt, nums, average_loss, self. batch_variables, eval_mode=True) end = data_loader.offset_summary(split) reset = outputs['reset'] if not reset: bar.update(end - start) else: print(end) if cfg.toy and self.counter(nums) > 100: break if opt.eval.es != 'full' and self.counter(nums) > opt.eval.es: break nums = outputs['nums'] torch.cuda.synchronize() print('{} evaluation completed in: {} s'.format(split.capitalize(), time.time() - start)) average_loss = self.compute_final_scores(average_loss, nums) return average_loss 
core|avod|test|get|trainer|input|FakeBatchNormClassifier def get_input(self): """Creates an easy training set.""" np.random.seed(0) inputs = np.zeros((16, 4)) labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32) for i in range(16): j = int(2 * labels[i] + np.random.randint(0, 2)) inputs[i, j] = 1 random_seed.set_random_seed(0) tf_inputs = constant_op.constant(inputs, dtype=dtypes.float32) tf_labels = constant_op.constant(labels, dtype=dtypes.float32) return tf_inputs, tf_labels 
get|official|test|logs|benchmark|logger|utils|default|BenchmarkLoggerTest def test_get_default_benchmark_logger(self): with flagsaver.flagsaver(benchmark_logger_type='foo'): self.assertIsInstance(logger.get_benchmark_logger(), logger. BaseBenchmarkLogger) 
process|single|data|utils|text def process_single_text(raw_text, max_seq_length, encoder, BOS_token, EOS_token, PAD_token): """Processes a single piece of text. Performs BPE encoding, converting to indexes, truncation, and padding, etc. """ tokens = encoder.encode(raw_text) max_len = max_seq_length if BOS_token is not None and len(BOS_token) > 0: max_len -= 1 if EOS_token is not None and len(EOS_token) > 0: max_len -= 1 tokens = tokens[:max_len] if BOS_token is not None and len(BOS_token) > 0: tokens = [encoder.encoder[BOS_token]] + tokens if EOS_token is not None and len(EOS_token) > 0: tokens = tokens + [encoder.encoder[EOS_token]] token_length = len(tokens) PAD_token_id = encoder.encoder[PAD_token] while len(tokens) < max_seq_length: tokens.append(PAD_token_id) assert len(tokens) == max_seq_length return tokens, token_length 
val|best|acc|callbacks|on|begin|model|train|keras def on_train_begin(self, logs={}): self.best_model = keras.models.clone_model(self.model) self.best_val_acc = 0.0 return 
back|box8c|8c|avod|test|Box8cEncoderTest|and|encoder|box3d|core|box|to def test_box3d_to_box8c_and_back(self): boxes_3d = np.asarray([[-0.59, 1.9, 25.01, 3.2, 1.61, 1.66, 0.0], [- 0.69, 1.69, 25.01, 3.2, 1.66, 1.61, -1.59]], dtype=np.float32) box_8co_1 = box_8c_encoder.np_box_3d_to_box_8co(boxes_3d[0]) box_8co_2 = box_8c_encoder.np_box_3d_to_box_8co(boxes_3d[1]) boxes_8c = np.stack((box_8co_1, box_8co_2), axis=0) boxes_c8_tensor = tf.convert_to_tensor(boxes_8c, dtype=tf.float32) boxes_3d_tensor = box_8c_encoder.box_8c_to_box_3d(boxes_c8_tensor) sess = tf.Session() with sess.as_default(): boxes_3d_out = boxes_3d_tensor.eval() np.testing.assert_almost_equal(boxes_3d[0], boxes_3d_out[0], decimal=2) np.testing.assert_almost_equal(boxes_3d[1], boxes_3d_out[1], decimal=2) 
build|vocab|main def main(args): vocab = {} limit = args.limit count = 0 words, counts = count_words(args.corpus) ctrl_symbols = control_symbols(args.control) for sym in ctrl_symbols: vocab[sym] = len(vocab) for word, freq in zip(words, counts): if limit and len(vocab) >= limit: break if word in vocab: print('Warning: found duplicate token %s, ignored' % word) continue vocab[word] = len(vocab) count += freq save_vocab(args.output, vocab) print('Total words: %d' % sum(counts)) print('Unique words: %d' % len(words)) print('Vocabulary coverage: %4.2f%%' % (100.0 * count / sum(counts))) 
conf|date|test|main def main(): total_days = 16 data_folder = '/root/share/upload_wifi_data/' day_conf = parse_test_days(data_folder, total_days) to_json = json.dumps(day_conf) save_json_filename = 'day_conf.json' with open(save_json_filename, 'w') as f: f.write(to_json) print('json file was saved as ' + save_json_filename) 
minibatch|avod|test|subsample|more|true|than|no|when|MinibatchSamplerTest|elements|core|samples|num|sampler|shape def test_subsample_when_more_true_elements_than_num_samples_no_shape(self): np_indicator = [True, False, True, False, True, True, False] indicator = tf.placeholder(tf.bool) feed_dict = {indicator: np_indicator} samples = minibatch_sampler.MinibatchSampler.subsample_indicator(indicator, 3) with self.test_session() as sess: samples_out = sess.run(samples, feed_dict=feed_dict) self.assertTrue(np.sum(samples_out), 3) self.assertAllEqual(samples_out, np.logical_and(samples_out, np_indicator)) 
DataLogParser|log|from|parse|save|data def save_data(self, train_model): print('\nbegin to save data to file...') for k, o in self.label.items(): if train_model: self.out_data_train[o].tofile(self.file_prefix + 'training_' + str(o) + '.dat') self.out_data_test[o].tofile(self.file_prefix + 'training_test_' + str(o) + '.dat') else: self.out_data_test[o].tofile(self.file_prefix + 'test_' + str(o ) + '.dat') print('data files were saved successfully!\n') 
FeedForwardPolicy|optimizer|conditioned|actor|setup|goal|policy|hbaselines def _setup_actor_optimizer(self, scope): """Create the actor loss, gradient, and optimizer.""" if self.verbose >= 2: print('setting up actor optimizer') scope_name = 'model/pi/' if scope is not None: scope_name = scope + '/' + scope_name self.actor_loss = -tf.reduce_mean(self.critic_with_actor_tf[0]) if self.verbose >= 2: actor_shapes = [var.get_shape().as_list() for var in get_trainable_vars(scope_name)] actor_nb_params = sum([reduce(lambda x, y: x * y, shape) for shape in actor_shapes]) print('  actor shapes: {}'.format(actor_shapes)) print('  actor params: {}'.format(actor_nb_params)) optimizer = tf.compat.v1.train.AdamOptimizer(self.actor_lr) self.actor_optimizer = optimizer.minimize(self.actor_loss, var_list= get_trainable_vars(scope_name)) 
GetSimilarityMapFromWeights|utils|SMILESX def GetSimilarityMapFromWeights(mol, weights, colorMap=None, scale=-1, size =(250, 250), sigma=None, coordScale=1.5, step=0.01, colors='k', contourLines=10, alpha=0.5, **kwargs): """ Generates the similarity map for a molecule given the atomic weights. Parameters: mol -- the molecule of interest colorMap -- the matplotlib color map scheme, default is custom PiWG color map scale -- the scaling: scale < 0 -> the absolute maximum weight is used as maximum scale scale = double -> this is the maximum scale size -- the size of the figure sigma -- the sigma for the Gaussians coordScale -- scaling factor for the coordinates step -- the step for calcAtomGaussian colors -- color of the contour lines contourLines -- if integer number N: N contour lines are drawn if list(numbers): contour lines at these numbers are drawn alpha -- the alpha blending value for the contour lines kwargs -- additional arguments for drawing """ if mol.GetNumAtoms() < 2: raise ValueError('too few atoms') fig = Draw.MolToMPL(mol, coordScale=coordScale, size=size, **kwargs) if sigma is None: if mol.GetNumBonds() > 0: bond = mol.GetBondWithIdx(0) idx1 = bond.GetBeginAtomIdx() idx2 = bond.GetEndAtomIdx() sigma = 0.3 * math.sqrt(sum([((mol._atomPs[idx1][i] - mol. _atomPs[idx2][i]) ** 2) for i in range(2)])) else: sigma = 0.3 * math.sqrt(sum([((mol._atomPs[0][i] - mol._atomPs[ 1][i]) ** 2) for i in range(2)])) sigma = round(sigma, 2) x, y, z = Draw.calcAtomGaussians(mol, sigma, weights=weights, step=step) if scale <= 0.0: maxScale = max(math.fabs(np.min(z)), math.fabs(np.max(z))) minScale = min(math.fabs(np.min(z)), math.fabs(np.max(z))) else: maxScale = scale fig.axes[0].imshow(z, cmap=colorMap, interpolation='bilinear', origin= 'lower', extent=(0, 1, 0, 1), vmin=minScale, vmax=maxScale) if len([w for w in weights if w != 0.0]): contourset = fig.axes[0].contour(x, y, z, contourLines, colors= colors, alpha=alpha, **kwargs) for j, c in enumerate(contourset.collections): if contourset.levels[j] == 0.0: c.set_linewidth(0.0) elif contourset.levels[j] < 0: c.set_dashes([(0, (3.0, 3.0))]) fig.axes[0].set_axis_off() return fig 
models|init|aae|Autoencoder def __init__(self, batch_size, channels=1, conv_filters=8, style_size=32, content_size=10, ksize=(3, 3), start_iteration=0, dir_header='./', wdecay=0.0): """Init method @param sess (tf.Session) the current session @param conv_filters_* (int) the number of filters in the convolutional layers @param code_size (int) the number of units in the code layer @param gradient_clip (bool) applies gradient clipping on the gradient vector """ self.dir_header = dir_header self.start_iteration = start_iteration self.channels = channels weight_initializer = None weight_initializer_implicit = None bias_initializer_implicit = None if wdecay > 0.0: regularizer = tf.contrib.layers.l2_regularizer(wdecay) else: regularizer = None with tf.variable_scope('Input', reuse=False): self.x = tf.placeholder(tf.float32, [batch_size, 32, 32, self.channels] ) self.labels_placeholder = tf.placeholder(tf.int64, [batch_size]) with tf.variable_scope('Encoder', reuse=False): conv_1 = tf.layers.conv2d(inputs=self.x, filters=conv_filters, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_1') conv_1 = tf.layers.batch_normalization(conv_1, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_1') conv_1 = tf.nn.leaky_relu(conv_1, name='relu_1') conv_2 = tf.layers.conv2d(inputs=conv_1, filters=conv_filters * 2, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_2') conv_2 = tf.layers.batch_normalization(conv_2, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_2') conv_2 = tf.nn.leaky_relu(conv_2, name='relu_2') conv_3 = tf.layers.conv2d(inputs=conv_2, filters=conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_3') conv_3 = tf.layers.batch_normalization(conv_3, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_3') conv_3 = tf.nn.leaky_relu(conv_3, name='relu_3') conv_4 = tf.layers.conv2d(inputs=conv_3, filters=conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_4') conv_4 = tf.layers.batch_normalization(conv_4, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_4') conv_4 = tf.nn.leaky_relu(conv_4, name='relu_4') conv_5 = tf.layers.conv2d(inputs=conv_4, filters=style_size, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer_implicit, bias_initializer= bias_initializer_implicit, name='conv_5') self.code_style = tf.squeeze(conv_5) with tf.variable_scope('Decoder', reuse=False): self.code_content_deterministic = tf.one_hot(indices=self. labels_placeholder, depth=content_size) code = tf.concat([self.code_style, self.code_content_deterministic], axis=1) code_reshaped = tf.reshape(code, [batch_size, 1, 1, style_size + content_size]) deconv_1 = tf.layers.conv2d_transpose(code_reshaped, filters= conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_1') deconv_1 = tf.layers.batch_normalization(deconv_1, axis=-1, momentum=0.99, epsilon=0.001, name='norm_1') deconv_1 = tf.nn.leaky_relu(deconv_1, name='relu_1') deconv_2 = tf.layers.conv2d_transpose(deconv_1, filters= conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_2') deconv_2 = tf.layers.batch_normalization(deconv_2, axis=-1, momentum=0.99, epsilon=0.001, name='norm_2') deconv_2 = tf.nn.leaky_relu(deconv_2, name='relu_2') deconv_3 = tf.layers.conv2d_transpose(deconv_2, filters= conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_3') deconv_3 = tf.layers.batch_normalization(deconv_3, axis=-1, momentum=0.99, epsilon=0.001, name='norm_3') deconv_3 = tf.nn.leaky_relu(deconv_3, name='relu_3') deconv_4 = tf.layers.conv2d_transpose(deconv_3, filters= conv_filters * 2, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_4') deconv_4 = tf.layers.batch_normalization(deconv_4, axis=-1, momentum=0.99, epsilon=0.001, name='norm_4') deconv_4 = tf.nn.leaky_relu(deconv_4, name='relu_4') deconv_5 = tf.layers.conv2d_transpose(deconv_4, filters= conv_filters, kernel_size=ksize, strides=(2, 2), padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_5') deconv_5 = tf.layers.batch_normalization(deconv_5, axis=-1, momentum=0.99, epsilon=0.001, name='norm_5') deconv_5 = tf.nn.leaky_relu(deconv_5, name='relu_5') deconv_6 = tf.layers.conv2d_transpose(deconv_5, filters=self. channels, kernel_size=ksize, strides=(1, 1), padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_6') self.output = tf.nn.sigmoid(deconv_6, name='output') with tf.variable_scope('Discriminator', reuse=False): gaussian_input = tf.random_normal([batch_size, style_size], 0, 1, dtype=tf.float32) mixed_input = tf.concat([self.code_style, gaussian_input], axis=0) dense_1 = tf.layers.dense(inputs=mixed_input, units=512, activation =None, use_bias=True, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='dense_1') dense_1 = tf.nn.leaky_relu(dense_1, name='relu_1') self.logits_discriminator = tf.layers.dense(inputs=dense_1, units=1, activation=None, use_bias=True, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='out') self.output_discriminator = tf.nn.sigmoid(self.logits_discriminator) with tf.variable_scope('Training'): all_variables = tf.trainable_variables() discriminator_var = [var for var in all_variables if 'Discriminator' in var.name] encoder_var = [var for var in all_variables if 'Encoder' in var.name] decoder_var = [var for var in all_variables if 'Decoder' in var.name] encoder_decoder_var = encoder_var + decoder_var self.loss_reconstruction = tf.reduce_mean(tf.square(tf.subtract( self.x, self.output))) d_fake, d_real = tf.split(self.logits_discriminator, [batch_size, batch_size], 0) loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.ones([batch_size, 1]), logits=d_real)) loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.zeros([batch_size, 1]), logits=d_fake)) self.loss_discriminator = 0.5 * (loss_fake + loss_real) self.loss_generator = tf.reduce_mean(tf.nn. sigmoid_cross_entropy_with_logits(labels=tf.ones([batch_size, 1 ]), logits=d_fake)) self.learning_rate = tf.placeholder(tf.float32) self.train_op = tf.train.AdamOptimizer(learning_rate=self. learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(self .loss_reconstruction, var_list=encoder_decoder_var) self.discriminator_train_op = tf.train.AdamOptimizer(learning_rate= self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08 ).minimize(self.loss_discriminator, var_list=discriminator_var) self.generator_train_op = tf.train.AdamOptimizer(learning_rate=self .learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize( self.loss_generator, var_list=encoder_var) self.tf_saver = tf.train.Saver() self.train_iteration = start_iteration with tf.variable_scope('Summaries'): tf.summary.image('input_images', self.x, max_outputs=8, family= 'original') tf.summary.image('reconstruction_images', self.output, max_outputs= 8, family='reconstructed_left') tf.summary.scalar('loss_reconstruction', self.loss_reconstruction, family='losses_reconstruction') tf.summary.scalar('loss_discriminator', self.loss_discriminator, family='losses_discriminator') tf.summary.scalar('loss_generator', self.loss_generator, family= 'losses_generator') tf.summary.histogram('hist_style', self.code_style, family='code') tf.summary.histogram('hist_gaussian', gaussian_input, family='code') 
sparse|preprocessing|src|tuple|to def sparse_to_tuple(sparse_mx): if not sp.isspmatrix_coo(sparse_mx): sparse_mx = sparse_mx.tocoo() coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose() values = sparse_mx.data shape = sparse_mx.shape return coords, values, shape 
gpu|utils|optimize def optimize(optimizer, global_step, grads): with tf.control_dependencies(tf1.get_collection(tf1.GraphKeys.UPDATE_OPS)): with tf.device('/device:CPU:0'), tf.name_scope('apply_grads'): return optimizer.apply_gradients(grads, global_step) 
space|examples|function|main def main(unused_argv): print('Loading data.') x_train, y_train, x_test, y_test = datasets.get_dataset('mnist', FLAGS. train_size, FLAGS.test_size) init_fn, apply_fn, _ = stax.serial(stax.Dense(2048, 1.0, 0.05), stax. Erf(), stax.Dense(10, 1.0, 0.05)) key = random.PRNGKey(0) _, params = init_fn(key, (-1, 784)) opt_init, opt_apply, get_params = optimizers.sgd(FLAGS.learning_rate) state = opt_init(params) loss = lambda fx, y_hat: 0.5 * np.mean((fx - y_hat) ** 2) grad_loss = jit(grad(lambda params, x, y: loss(apply_fn(params, x), y))) ntk = nt.batch(nt.empirical_ntk_fn(apply_fn), batch_size=4, device_count=0) g_dd = ntk(x_train, None, params) g_td = ntk(x_test, x_train, params) predictor = nt.predict.gradient_descent_mse(g_dd, y_train, g_td) fx_train = apply_fn(params, x_train) fx_test = apply_fn(params, x_test) train_steps = int(FLAGS.train_time // FLAGS.learning_rate) print('Training for {} steps'.format(train_steps)) for i in range(train_steps): params = get_params(state) state = opt_apply(i, grad_loss(params, x_train, y_train), state) print('Computing analytic prediction.') fx_train, fx_test = predictor(FLAGS.train_time, fx_train, fx_test) util.print_summary('train', y_train, apply_fn(params, x_train), fx_train, loss) util.print_summary('test', y_test, apply_fn(params, x_test), fx_test, loss) 
help|darkflow|net|camera def camera(self): file = self.FLAGS.demo SaveVideo = self.FLAGS.saveVideo if file == 'camera': file = 0 else: assert os.path.isfile(file), 'file {} does not exist'.format(file) camera = cv2.VideoCapture(file) if file == 0: self.say('Press [ESC] to quit demo') assert camera.isOpened(), 'Cannot capture source' if file == 0: cv2.namedWindow('', 0) _, frame = camera.read() height, width, _ = frame.shape cv2.resizeWindow('', width, height) else: _, frame = camera.read() height, width, _ = frame.shape if SaveVideo: fourcc = cv2.VideoWriter_fourcc(*'XVID') if file == 0: fps = 1 / self._get_fps(frame) if fps < 1: fps = 1 else: fps = round(camera.get(cv2.CAP_PROP_FPS)) videoWriter = cv2.VideoWriter('video.avi', fourcc, fps, (width, height) ) buffer_inp = list() buffer_pre = list() elapsed = int() start = timer() self.say('Press [ESC] to quit demo') while camera.isOpened(): elapsed += 1 _, frame = camera.read() if frame is None: print('\nEnd of Video') break preprocessed = self.framework.preprocess(frame) buffer_inp.append(frame) buffer_pre.append(preprocessed) if elapsed % self.FLAGS.queue == 0: feed_dict = {self.inp: buffer_pre} net_out = self.sess.run(self.out, feed_dict) for img, single_out in zip(buffer_inp, net_out): postprocessed = self.framework.postprocess(single_out, img, False) if SaveVideo: videoWriter.write(postprocessed) if file == 0: cv2.imshow('', postprocessed) buffer_inp = list() buffer_pre = list() if elapsed % 5 == 0: sys.stdout.write('\r') sys.stdout.write('{0:3.3f} FPS'.format(elapsed / (timer() - start)) ) sys.stdout.flush() if file == 0: choice = cv2.waitKey(1) if choice == 27: break sys.stdout.write('\n') if SaveVideo: videoWriter.release() camera.release() if file == 0: cv2.destroyAllWindows() 
rnn|texar|output|AttentionRNNDecoder|modules|decoders|dtype @property def output_dtype(self): """Types of output of one step. """ dtype = nest.flatten(self._initial_state)[0].dtype return AttentionRNNDecoderOutput(logits=nest.map_structure(lambda _: dtype, self._rnn_output_size()), sample_id=self._helper. sample_ids_dtype, cell_output=nest.map_structure(lambda _: dtype, self._cell.output_size), attention_scores=nest.map_structure(lambda _: dtype, self._alignments_size()), attention_context=nest. map_structure(lambda _: dtype, self._cell.state_size.attention)) 
generator|pix2pix|blocks|nets|default def _default_generator_blocks(): """Returns the default generator block definitions.  Returns: A list of generator blocks. """ return [Block(64, 0.5), Block(128, 0.5), Block(256, 0.5), Block(512, 0), Block(512, 0), Block(512, 0), Block(512, 0)] 
gcn|master|Dense|layers|call|text def _call(self, inputs): x = inputs if self.sparse_inputs: x = sparse_dropout(x, 1 - self.dropout, self.num_features_nonzero) else: x = tf.nn.dropout(x, 1 - self.dropout) output = dot(x, self.vars['weights'], sparse=self.sparse_inputs) if self.bias: output += self.vars['bias'] return self.act(output) 
tflib|Deconv2D|ops|deconv2d def Deconv2D(name, input_dim, output_dim, filter_size, inputs, he_init=True, weightnorm=None, biases=True, gain=1.0, mask_type=None): """ inputs: tensor of shape (batch size, height, width, input_dim) returns: tensor of shape (batch size, 2*height, 2*width, output_dim) """ with tf.name_scope(name) as scope: if mask_type != None: raise Exception('Unsupported configuration')  def uniform(stdev, size): return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt(3), size=size).astype('float32') stride = 2 fan_in = input_dim * filter_size ** 2 / stride ** 2 fan_out = output_dim * filter_size ** 2 if he_init: filters_stdev = np.sqrt(4.0 / (fan_in + fan_out)) else: filters_stdev = np.sqrt(2.0 / (fan_in + fan_out)) if _weights_stdev is not None: filter_values = uniform(_weights_stdev, (filter_size, filter_size, output_dim, input_dim)) else: filter_values = uniform(filters_stdev, (filter_size, filter_size, output_dim, input_dim)) filter_values *= gain filters = lib.param(name + '.Filters', filter_values) if weightnorm == None: weightnorm = _default_weightnorm if weightnorm: norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0, 1, 3))) target_norms = lib.param(name + '.g', norm_values) with tf.name_scope('weightnorm') as scope: norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0, 1, 3])) filters = filters * tf.expand_dims(target_norms / norms, 1) inputs = tf.transpose(inputs, [0, 2, 3, 1], name='NCHW_to_NHWC') input_shape = tf.shape(inputs) try: output_shape = tf.pack([input_shape[0], 2 * input_shape[1], 2 * input_shape[2], output_dim]) except Exception as e: output_shape = tf.stack([input_shape[0], 2 * input_shape[1], 2 * input_shape[2], output_dim]) result = tf.nn.conv2d_transpose(value=inputs, filter=filters, output_shape=output_shape, strides=[1, 2, 2, 1], padding='SAME') if biases: _biases = lib.param(name + '.Biases', np.zeros(output_dim, dtype='float32')) result = tf.nn.bias_add(result, _biases) result = tf.transpose(result, [0, 3, 1, 2], name='NHWC_to_NCHW') return result 
tangents|predict|gp|gradient|neural|mse|descent def gradient_descent_mse_gp(kernel_fn, x_train, y_train, x_test, get, diag_reg=0.0, compute_cov=False): """Predicts the gaussian embedding induced by gradient descent with mse loss.  This is equivalent to an infinite ensemble of networks after marginalizing out the initialization.  Args: kernel_fn: A kernel function that computes NNGP and NTK. x_train: A `np.ndarray`, representing the training data. y_train: A `np.ndarray`, representing the labels of the training data. x_test: A `np.ndarray`, representing the test data. get: string, the mode of the Gaussian process, either "nngp" or "ntk", or a tuple. diag_reg: A float, representing the strength of the regularization. compute_cov: A boolean. If `True` computing both `mean` and `variance` and only `mean` otherwise.  Returns: A function that predicts the gaussian parameters at t: predict(t) -> Gaussian(mean, variance). If compute_cov is False, only returns the mean. """ if get is None: get = 'nngp', 'ntk' if isinstance(get, str): return lambda t: gradient_descent_mse_gp(kernel_fn, x_train, y_train, x_test, diag_reg=diag_reg, get=(get,), compute_cov= compute_cov)(t)[0] _, get = canonicalize_get(get) normalization = y_train.size op_fn = _make_inv_expm1_fn(normalization) eigenspace = {} kdd, ktd, ktt = _get_matrices(kernel_fn, x_train, x_test, get, compute_cov) gp_inference_mat = _gp_inference_mat_jit_cpu if _is_on_cpu(kdd ) else _gp_inference_mat_jit  @_jit_cpu(kdd) def predict(t=None): """`t=None` is equivalent to infinite time and calls `gp_inference`.""" if t is None: return gp_inference_mat(kdd, ktd, ktt, y_train, get, diag_reg) if not eigenspace: for g in get: k = kdd.nngp if g == 'nngp' else kdd.ntk k_dd_plus_reg = _add_diagonal_regularizer(k, diag_reg) eigenspace[g] = _eigh(k_dd_plus_reg) out = {} if 'nngp' in get: evals, evecs = eigenspace['nngp'] op_evals = -op_fn(evals, t) pred_mean = _mean_prediction_einsum(evecs, op_evals, ktd.nngp, y_train) if compute_cov: op_evals_x2 = -op_fn(evals, 2 * t) pred_cov = ktt - np.einsum('mj,ji,i,ki,lk->ml', ktd.nngp, evecs, op_evals_x2, evecs, ktd.nngp, optimize=True) out['nngp'] = Gaussian(pred_mean, pred_cov ) if compute_cov else pred_mean if 'ntk' in get: evals, evecs = eigenspace['ntk'] op_evals = -op_fn(evals, t) pred_mean = _mean_prediction_einsum(evecs, op_evals, ktd.ntk, y_train) if compute_cov: term_1 = np.einsum('mi,i,ki,lk->ml', evecs, op_evals, evecs, ktd.ntk, optimize=True) pred_cov = np.einsum('ji,jk,kl->il', term_1, kdd.nngp, term_1, optimize=True) term_2 = np.einsum('mj,ji,i,ki,lk->ml', ktd.ntk, evecs, op_evals, evecs, ktd.nngp, optimize=True) term_2 += np.transpose(term_2) pred_cov += -term_2 + ktt out['ntk'] = Gaussian(pred_mean, pred_cov ) if compute_cov else pred_mean returntype = named_tuple_factory('Gaussians', get) return returntype(*tuple(out[g] for g in get)) return predict 
init|WordSorting|task def __init__(self, alphabet: list): """ :param alphabet: Alphabet used. Last element is used as delimiter. """ a_len = len(alphabet) self.alphabet = alphabet self.delimiter = alphabet[a_len - 1] self.probability = generate_probability(a_len, 1 / a_len) 
preprocessing|fetch|atari|grayscale|AtariPreprocessing|observation def _fetch_grayscale_observation(self, output): """Returns the current observation in grayscale.  The returned observation is stored in 'output'.  Args: output: numpy array, screen buffer to hold the returned observation.  Returns: observation: numpy array, the current observation in grayscale. """ self.environment.ale.getScreenGrayscale(output) return output 
texar|models|ModelBase|init|model|base def __init__(self, hparams=None): self._hparams = HParams(hparams, self.default_hparams(), allow_new_hparam=True) 
load|data|tags def load_tags_data(data_dir, num_items): data = {} data['tags'] = load_file(data_dir + 'tag-item.dat') data['citations'] = load_file(data_dir + 'citations.dat') num_tags = len(data['tags']) print('Number of tags: ', num_tags) item_tag_matrix = np.zeros(shape=(num_items, num_tags)) for i in range(num_tags): item_ids = data['tags'][i] for j in item_ids: item_tag_matrix[j, i] = 1 item_tag_matrix_with_citations = item_tag_matrix.copy() for i in range(len(data['citations'])): item_ids = data['citations'][i] for j in item_ids: for k in range(num_tags): if item_tag_matrix[j][k] == 1: item_tag_matrix_with_citations[i][k] = item_tag_matrix[j][k ] return item_tag_matrix_with_citations 
parsing|ChunkParserTest|chunkparser|test def test_parsing(self): """ Test game position decoding pipeline.  We generate a V1 record, and feed it all the way through the parsing pipeline to final tensors, checking that what we get out is what we put in. """ batch_size = 256 planes, probs, winner = self.generate_fake_pos() items = [] for p in range(16): h = np.packbits([int(x) for x in planes[p][0:360]]).tobytes().hex() h += str(planes[p][360]) + '\n' items.append(h) items.append(str(int(planes[17][0])) + '\n') items.append(' '.join([str(x) for x in probs]) + '\n') items.append(str(int(winner[0])) + '\n') chunkdata = ''.join(items).encode('ascii') chunkdatasrc = ChunkDataSrc([chunkdata for _ in range(batch_size * 2)]) parser = ChunkParser(chunkdatasrc, shuffle_size=1, workers=1, batch_size=batch_size) batchgen = parser.parse() data = next(batchgen) batch = np.reshape(np.frombuffer(data[0], dtype=np.uint8), (batch_size, 18, 19 * 19)).tolist(), np.reshape(np.frombuffer(data[1], dtype=np. float32), (batch_size, 19 * 19 + 1)).tolist(), np.reshape(np. frombuffer(data[2], dtype=np.float32), (batch_size, 1)).tolist() for i in range(batch_size): data = batch[0][i], batch[1][i], batch[2][i] result = False for symmetry in range(8): sym_planes = [[plane[remap_vertex(vertex, symmetry)] for vertex in range(361)] for plane in planes] sym_probs = [probs[remap_vertex(vertex, symmetry)] for vertex in range(361)] + [probs[361]] if symmetry == 0: assert sym_planes == planes assert sym_probs == probs if data == (sym_planes, sym_probs, winner): result = True break assert result == True print('Test parse passes') for _ in batchgen: pass 
topologically|sorted|visit|Graph|kaffe|graph def visit(node): if node in temp_marked: raise KaffeError('Graph is not a DAG.') if node in perm_marked: return temp_marked.add(node) for child in node.children: visit(child) perm_marked.add(node) temp_marked.remove(node) sorted_nodes.insert(0, node) 
dense|official|fn|grad|loop|model|filter|run|resnet def _dense_grad_filter(gvs): """Only apply gradient updates to the final layer.  This function is used for fine tuning.  Args: gvs: list of tuples with gradients and variable info Returns: filtered gradients so that only the dense layer remains """ return [(g, v) for g, v in gvs if 'dense' in v.name] 
funcs|ctr|print|time def print_time(): now = datetime.datetime.now() time_str = now.strftime(time_style) print(time_str) 
eval|14|train|ruemonge2|one|epoch def eval_one_epoch(sess, ops, next_test_element, test_writer): """ ops: dict mapping from string to tf ops """ is_training = False cur_batch_input = np.zeros((BATCH_SIZE, NUM_POINT, INPUT_DIM)) cur_batch_label = np.zeros((BATCH_SIZE, NUM_POINT), dtype=np.int32) total_correct = 0 total_seen = 0 loss_sum = 0 batch_idx = 0 total_seen_class = [(0) for _ in range(NUM_CLASSES)] total_correct_class = [(0) for _ in range(NUM_CLASSES)] total_union_class = [(0) for _ in range(NUM_CLASSES)] class_names = list(classes.keys()) class_iou = {cat: (0.0) for cat in class_names} class_acc = {cat: (0.0) for cat in class_names} test_time = 0.0 while True: try: padded_all = sess.run(next_test_element) bsize = padded_all.shape[0] batch_input = np.zeros((bsize, NUM_POINT, INPUT_DIM)) batch_label = np.zeros((bsize, NUM_POINT), dtype=np.int32) for b in range(bsize): loc = np.where(padded_all[(b), :, (-1)] < 0) if len(loc[0]) == 0: num = padded_all.shape[1] else: num = loc[0][0] if num == 0: print(loc, padded_all[(b), 0:10, :]) print('problem of eval') exit() if num < NUM_POINT: sample_index = np.random.choice(num, NUM_POINT, replace =True) else: sample_index = np.random.choice(num, NUM_POINT, replace =False) batch_input[b, ...] = padded_all[(b), (sample_index), 0:-1] batch_label[(b), :] = padded_all[b, sample_index, -1] cur_batch_input[0:bsize, (...)] = batch_input cur_batch_label[0:bsize, :] = batch_label feed_dict = {ops['input_pl']: cur_batch_input, ops['label_pl']: cur_batch_label, ops['training_pl']: is_training} now = time.time() summary, global_step, loss_val, pred_val = sess.run([ops[ 'merged'], ops['global_step'], ops['loss'], ops['pred']], feed_dict=feed_dict) test_time += time.time() - now test_writer.add_summary(summary, global_step) pred_label = np.argmax(pred_val, 2) for b in range(bsize): correct = np.sum(pred_label[b] == batch_label[b]) total_correct += correct total_seen += pred_label.shape[1] for l in range(NUM_CLASSES): total_seen_class[l] += np.sum(batch_label[b] == l) total_correct_class[l] += np.sum((pred_label[b] == l) & (batch_label[b] == l)) total_union_class[l] += np.sum((pred_label[b] == l) | ( batch_label[b] == l)) loss_sum += loss_val batch_idx += 1 except tf.errors.OutOfRangeError: break for cat in class_names: l = classes[cat] class_iou[cat] = total_correct_class[l] / (float(total_union_class[ l]) + np.finfo(float).eps) class_acc[cat] = total_correct_class[l] / (float(total_seen_class[l ]) + np.finfo(float).eps) log_string('eval mean loss: %f' % (loss_sum / batch_idx)) log_string('eval overall accuracy: %f' % (total_correct / float( total_seen))) log_string('eval avg class acc: %f' % np.mean(list(class_acc.values()))) for i in range(len(classes)): cat = list(classes.keys())[list(classes.values()).index(i)] log_string('eval mIoU of %s:\t %f' % (cat, class_iou[cat])) log_string('eval mIoU of all classes: %f' % np.mean(list(class_iou. values()))) log_string('testing one batch require %.2f milliseconds' % (1000 * test_time / batch_idx)) return 
texar|test|adversarial|losses|binary|AdvLossesTest|adv def test_binary_adversarial_losses(self): """Tests :meth:`~texar.losses.adv_losses.binary_adversarial_losse`. """ batch_size = 16 data_dim = 64 real_data = tf.zeros([batch_size, data_dim], dtype=tf.float32) fake_data = tf.ones([batch_size, data_dim], dtype=tf.float32) const_logits = tf.zeros([batch_size], dtype=tf.float32) gen_loss, disc_loss = binary_adversarial_losses(real_data, fake_data, lambda x: const_logits) gen_loss_2, disc_loss_2 = binary_adversarial_losses(real_data, fake_data, lambda x: const_logits, mode='min_fake') with self.test_session() as sess: gen_loss_, disc_loss_ = sess.run([gen_loss, disc_loss]) gen_loss_2_, disc_loss_2_ = sess.run([gen_loss_2, disc_loss_2]) self.assertAlmostEqual(gen_loss_, -gen_loss_2_) self.assertAlmostEqual(disc_loss_, disc_loss_2_) 
size|tf|variable|codec|train|rvae def rvae_variable_size_codec(codec_from_shape, latent_from_image_shape, image_count, dimensions=4, dimension_bits=16): size_append, size_pop = codecs.repeat(codecs.Uniform(dimension_bits), dimensions)  def append(message, symbol): """append sizes and array in alternating order""" assert len(symbol.shape) == dimensions symbol_append, _ = codec_from_shape(symbol.shape) head_size = np.prod(latent_from_image_shape(symbol.shape)) + np.prod( symbol.shape) message = codecs.reshape_head(message, (head_size,)) message = symbol_append(message, symbol) message = codecs.reshape_head(message, (1,)) message = size_append(message, np.array(symbol.shape)) return message  def pop(message): message, size = size_pop(message) size = size[:, (0)] assert size.shape == (dimensions,) size = size.astype(np.int) head_size = np.prod(latent_from_image_shape(size)) + np.prod(size) _, symbol_pop = codec_from_shape(tuple(size)) message = codecs.reshape_head(message, (head_size,)) message, symbol = symbol_pop(message) message = codecs.reshape_head(message, (1,)) return message, symbol return codecs.serial([(append, pop)] * image_count) 
sequence|BiasEncoding|deepctr|layers|call def call(self, inputs, mask=None): """ :param concated_embeds_value: None * field_size * embedding_size :return: None*1 """ transformer_out = [] for i in range(self.sess_max_count): transformer_out.append(inputs[i] + self.item_bias_embedding + self. seq_bias_embedding + self.sess_bias_embedding[i]) return transformer_out 
nhwc|utils|to @add_arg_scope def to_nhwc(inputs, data_format='NHWC', scope=None): """Move the channel axis to the last dimension. Allows to provide a consistent NHWC output format whatever the input data format.  Args: inputs: Input Tensor; data_format: NHWC or NCHW. """ with tf.name_scope(scope, 'to_nhwc', [inputs]): if data_format == 'NHWC': net = inputs elif data_format == 'NCHW': net = tf.transpose(inputs, perm=(0, 2, 3, 1)) return net 
LFM|main def main(_): vkge.LFM(embedding_size=FLAGS.embedding_size, distribution=FLAGS. distribution, epsilon=FLAGS.epsilon, no_batches=FLAGS.no_batches, dataset=FLAGS.dataset, negsamples=FLAGS.negsamples, lr=FLAGS.lr, file_name=FLAGS.file_name, alt_prior=FLAGS.alt_prior, projection= FLAGS.projection, score_func=FLAGS.score_func, s_o=FLAGS.s_o) 
m|phate|distance|kernel|to def distance_to_kernel(D, bandwidth): A = np.exp(-1 * (D / bandwidth) ** 2) A = (A + A.T) / 2 return A 
conditioned|log|goal|training|hbaselines|TD3|algorithm def _log_training(self, file_path, start_time): """Log training statistics.  Parameters ---------- file_path : str the list of cumulative rewards from every episode in the evaluation phase start_time : float the time when training began. This is used to print the total training time. """ duration = time.time() - start_time combined_stats = {'rollout/return': np.mean(self.epoch_episode_rewards), 'rollout/return_history': np.mean(self.episode_rewards_history), 'rollout/episode_steps': np.mean(self.epoch_episode_steps), 'rollout/actions_mean': np.mean(self.epoch_actions), 'rollout/Q1_mean': np.mean(self.epoch_q1s), 'rollout/Q2_mean': np. mean(self.epoch_q2s), 'train/loss_actor': np.mean(self. epoch_actor_losses), 'train/loss_critic': np.mean(self. epoch_critic_losses), 'total/duration': duration, 'total/steps_per_second': self.total_steps / duration, 'total/episodes': self.episodes, 'rollout/episodes': self. epoch_episodes, 'rollout/actions_std': np.std(self.epoch_actions), 'total/epochs': self.epoch + 1, 'total/steps': self.total_steps} if file_path is not None: exists = os.path.exists(file_path) with open(file_path, 'a') as f: w = csv.DictWriter(f, fieldnames=combined_stats.keys()) if not exists: w.writeheader() w.writerow(combined_stats) print('-' * 57) for key in sorted(combined_stats.keys()): val = combined_stats[key] print('| {:<25} | {:<25} |'.format(key, val)) print('-' * 57) print('') 
Conv1D|tflib|ops|conv1d def Conv1D(name, input_dim, output_dim, filter_size, inputs, he_init=True, mask_type=None, stride=1, weightnorm=None, biases=True, gain=1.0): """ inputs: tensor of shape (batch size, num channels, width) mask_type: one of None, 'a', 'b'  returns: tensor of shape (batch size, num channels, width) """ with tf.name_scope(name) as scope: if mask_type is not None: mask_type, mask_n_channels = mask_type mask = np.ones((filter_size, input_dim, output_dim), dtype= 'float32') center = filter_size // 2 mask[center + 1:, :, :] = 0.0 for i in range(mask_n_channels): for j in range(mask_n_channels): if (mask_type == 'a' and i >= j or mask_type == 'b' and i > j): mask[(center), i::mask_n_channels, j::mask_n_channels ] = 0.0  def uniform(stdev, size): return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt(3), size=size).astype('float32') fan_in = input_dim * filter_size fan_out = output_dim * filter_size / stride if mask_type is not None: fan_in /= 2.0 fan_out /= 2.0 if he_init: filters_stdev = np.sqrt(4.0 / (fan_in + fan_out)) else: filters_stdev = np.sqrt(2.0 / (fan_in + fan_out)) filter_values = uniform(filters_stdev, (filter_size, input_dim, output_dim)) filter_values *= gain filters = lib.param(name + '.Filters', filter_values) if weightnorm == None: weightnorm = _default_weightnorm if weightnorm: norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0, 1)) ) target_norms = lib.param(name + '.g', norm_values) with tf.name_scope('weightnorm') as scope: norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0, 1])) filters = filters * (target_norms / norms) if mask_type is not None: with tf.name_scope('filter_mask'): filters = filters * mask result = tf.nn.conv1d(value=inputs, filters=filters, stride=stride, padding='SAME', data_format='NCHW') if biases: _biases = lib.param(name + '.Biases', np.zeros([output_dim], dtype='float32')) result = tf.expand_dims(result, 3) result = tf.nn.bias_add(result, _biases, data_format='NCHW') result = tf.squeeze(result) return result 
autogen|code|methods|class|doc|collect def collect_class_methods(cls, methods): if isinstance(methods, (list, tuple)): return [(getattr(cls, m) if isinstance(m, str) else m) for m in methods ] methods = [] for _, method in inspect.getmembers(cls, predicate=inspect.isroutine): if method.__name__[0] == '_' or method.__name__ in EXCLUDE: continue methods.append(method) return methods 
test|hparams|tf|parse|HParamsTestCase|utils def test_parse(self): hps = HParams(int_value=13, float_value=17.5, bool_value=True, str_value='test') self.assertEqual(hps.parse('int_value=10').int_value, 10) self.assertEqual(hps.parse('float_value=10').float_value, 10) self.assertEqual(hps.parse('float_value=10.3').float_value, 10.3) self.assertEqual(hps.parse('bool_value=true').bool_value, True) self.assertEqual(hps.parse('bool_value=True').bool_value, True) self.assertEqual(hps.parse('bool_value=false').bool_value, False) self.assertEqual(hps.parse('str_value=value').str_value, 'value') 
GlobalConfiguration|download|speech|corpus|init def __init__(self, config): if not isinstance(config, dict): raise ValueError('The argument must be a dictionary.') self.extensions = ExtensionList(config.get('extensions', ['wav'])) 
GetPreviousNonBlankLine|cpplint def GetPreviousNonBlankLine(clean_lines, linenum): """Return the most recent non-blank line and its line number.  Args: clean_lines: A CleansedLines instance containing the file contents. linenum: The number of the line to check.  Returns: A tuple with two elements.  The first element is the contents of the last non-blank line before the current line, or the empty string if this is the first non-blank line.  The second is the line number of that line, or -1 if this is the first non-blank line. """ prevlinenum = linenum - 1 while prevlinenum >= 0: prevline = clean_lines.elided[prevlinenum] if not IsBlankLine(prevline): return prevline, prevlinenum prevlinenum -= 1 return '', -1 
linear|Linear|ops|uniform|tflib def uniform(stdev, size): if _weights_stdev is not None: stdev = _weights_stdev return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt( 3), size=size).astype('float32') 
codecs|test|craystack|ub|gaussian def test_gaussian_ub(): bin_lb = np.array([-2.0, -3.0]) bin_ub = np.array([2.0, 3.0]) n_bins = 1000 coding_precision = 16 batch_size = 5 means = rng.randn(batch_size, 2) / 10 stdds = np.exp(rng.random((batch_size, 2)) / 2) data = np.array([rng.choice(n_bins, 2) for _ in range(batch_size)]) check_codec((batch_size, 2), cs.DiagGaussian_UnifBins(means, stdds, bin_lb, bin_ub, coding_precision, n_bins), data) 
seld|scores|compute|dcase2|19|sed|metrics|master|evaluation def compute_sed_scores(pred, gt, nb_frames_1s): """ Computes SED metrics for one second segments  :param pred: predicted matrix of dimension [nb_frames, nb_classes], with 1 when sound event is active else 0 :param gt:  reference matrix of dimension [nb_frames, nb_classes], with 1 when sound event is active else 0 :param nb_frames_1s: integer, number of frames in one second :return: """ f1o = f1_overall_1sec(pred, gt, nb_frames_1s) ero = er_overall_1sec(pred, gt, nb_frames_1s) scores = [ero, f1o] return scores 
imresample|face|detect|align|src|master|facenet def imresample(img, sz): im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) return im_data 
ludwig|plato|component|nlg|agent|save|LudwigNLG def save(self, path=None): """ Save the Ludwig model.  :param path: the path to save to :return: """ if not path: print('WARNING: Ludwig nlg model not saved (no path provided).') else: self.model.save(path) 
calculations|map|rank|GPSig|Nystrom|low def Nystrom_map(X, kern, num_components=None, inducing_samples=None, return_inducing_samples=False): """ Computes the Nystrom features with uniform sampling given a kernel and num_components See e.g. https://dl.acm.org/citation.cfm?id=2343678 ------------------------------------------------------------------- Input :X:             input data points with size (num_samples, num_dims) :kern:          function handle to a kernel function that takes two matrices as input e.g. X1 (num_samples1, num_dims) and X2 (num_samples2, num_dim), and computes the matrix k(X1, X2) matrix of size (num_samples1, num_samples2) :num_components:number of components to take, i.e. the rank of the low-rank kernel matrix Output :X_nys:         Nystrom features of size (num_samples, num_components) """ num_samples = tf.shape(X)[0] if inducing_samples is None: idx, idx_not, rev_map = draw_indices(num_samples, num_components, need_inv=True) X_sampled = tf.gather(X, idx, axis=-2) X_not_sampled = tf.gather(X, idx_not, axis=-2) W = kern(X_sampled, X_sampled) + tf.diag(settings.numerics. jitter_level * tf.random_uniform([num_components], dtype= settings.float_type)) K21 = kern(X_not_sampled, X_sampled) C = tf.concat((W, K21), axis=-2) S, U = tf.self_adjoint_eig(W) D = tf.sqrt(tf.maximum(tf.cast(settings.numerics.jitter_level, settings.float_type) ** 2, S)) X_nys = tf.matmul(C, U) / tf.expand_dims(D, axis=-2) X_nys = tf.gather(X_nys, rev_map, axis=-2) else: num_components = tf.shape(inducing_samples)[0] W = kern(inducing_samples, inducing_samples) + tf.diag(settings. numerics.jitter_level * tf.random_uniform([num_components], dtype=settings.float_type)) Kxy = kern(tf.reshape(X, [-1, tf.shape(X)[-1]]), inducing_samples) S, U = tf.self_adjoint_eig(W) D = tf.sqrt(tf.maximum(tf.cast(settings.numerics.jitter_level, settings.float_type) ** 2, S)) X_nys = tf.matmul(Kxy, U) / tf.expand_dims(D, axis=-2) X_nys = tf.reshape(X_nys, tf.concat((tf.shape(X)[:-1], [ num_components]), axis=0)) if return_inducing_samples: return X_nys, X_sampled else: return X_nys 
tmp|vgg|visualize|master|facenet|model|sqErrorLossStyle def sqErrorLossStyle(sess, modelGraph):  def intermediateCalc(x, y): N = x.shape[3] M = x.shape[1] * x.shape[2] A = tf.matmul(tf.transpose(tf.reshape(x, (M, N))), tf.reshape(x, (M, N))) G = tf.matmul(tf.transpose(tf.reshape(y, (M, N))), tf.reshape(y, (M, N))) return 1 / (4 * N ** 2 * M ** 2) * tf.reduce_sum(tf.pow(G - A, 2)) E = [intermediateCalc(sess.run(modelGraph[layerName]), modelGraph[ layerName]) for layerName, _ in styleLayers] W = [w for _, w in styleLayers] return sum([(W[layerNumber] * E[layerNumber]) for layerNumber in range( len(styleLayers))]) 
deepMOT|io|master|read|utils|gtV2|txt def read_txt_gtV2(textpath): """ read gt.txt to a dict :param textpath: text path, string :return: a dict with key = frameid and value is a list of lists [object id, x1, y1, x2, y2] in the frame, dict """ with open(textpath) as f: f_csv = csv.reader(f) frames = {} for line in f_csv: if len(line) == 1: line = line[0].split(' ') if len(line) < 7 or line[7 ] not in persons_class and 'MOT2015' not in textpath or int( float(line[6])) == 0: continue if not line[0] in frames: frames[line[0]] = [] bbox = xywh2xyxy(line[2:6]) frames[line[0]].append([line[1]] + bbox) ordered = reorder_frameID(frames) return ordered 
l2|tmp|loss|master|network|facenet def l2_loss(tensor, weight=1.0, scope=None): """Define a L2Loss, useful for regularize, i.e. weight decay. Args: tensor: tensor to regularize. weight: an optional weight to modulate the loss. scope: Optional scope for op_scope. Returns: the L2 loss op. """ with tf.name_scope(scope): weight = tf.convert_to_tensor(weight, dtype=tensor.dtype.base_dtype, name='loss_weight') loss = tf.multiply(weight, tf.nn.l2_loss(tensor), name='value') return loss 
evaluate|SoftNetwork|SoftNetworkModel def evaluate(self, session, x, y): pp = self.predict(session, x) fig, stats = lp.localizationPlot(pp, y, n_samples=20, dist_threshold= self.config['tolerence'], factor=1, bias=self.config['temporal_bias']) return fig, stats 
tail|fn|vgg16|head|inference|model|batch|scratch|norm def inference_batch_norm_vgg16_head_tail_scratch(inputs): inputs = snt.Conv2D(output_channels=64, kernel_shape=3, rate=1, padding ='SAME', name='conv1_1')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=64, kernel_shape=3, rate=1, padding ='SAME', name='conv1_2')(inputs) inputs = tf.nn.relu(inputs) inputs = tf.nn.max_pool(inputs, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') inputs = snt.Conv2D(output_channels=128, kernel_shape=3, rate=1, padding='SAME', name='conv2_1')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=128, kernel_shape=3, rate=1, padding='SAME', name='conv2_2')(inputs) inputs = tf.nn.relu(inputs) inputs = tf.nn.max_pool(inputs, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') inputs = snt.Conv2D(output_channels=256, kernel_shape=3, rate=1, padding='SAME', name='conv3_1')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=256, kernel_shape=3, rate=1, padding='SAME', name='conv3_2')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=256, kernel_shape=3, rate=1, padding='SAME', name='conv3_3')(inputs) inputs = tf.nn.relu(inputs) inputs = tf.nn.max_pool(inputs, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') inputs = snt.Conv2D(output_channels=512, kernel_shape=3, rate=1, padding='SAME', name='conv4_1')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=512, kernel_shape=3, rate=1, padding='SAME', name='conv4_2')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=512, kernel_shape=3, rate=1, padding='SAME', name='conv4_3')(inputs) inputs = tf.nn.relu(inputs) inputs = tf.nn.max_pool(inputs, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') inputs = snt.Conv2D(output_channels=512, kernel_shape=3, rate=1, padding='SAME', name='conv5_1')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=512, kernel_shape=3, rate=1, padding='SAME', name='conv5_2')(inputs) inputs = tf.nn.relu(inputs) inputs = snt.Conv2D(output_channels=512, kernel_shape=3, rate=1, padding='SAME', name='conv5_3')(inputs) inputs = tf.nn.relu(inputs) inputs = tf.nn.max_pool(inputs, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') inputs_h = snt.Conv2D(output_channels=1, kernel_shape=1, padding='SAME', name='conv6_1_h')(inputs) norm_heatmap_h, coords_h = dsnt.dsnt(inputs_h) inputs_t = snt.Conv2D(output_channels=1, kernel_shape=1, padding='SAME', name='conv6_1_t')(inputs) norm_heatmap_t, coords_t = dsnt.dsnt(inputs_t) return norm_heatmap_h, coords_h, norm_heatmap_t, coords_t 
mars|image|datasets|split|train|read|to def read_train_split_to_image(dataset_dir, image_shape=(128, 64)): """Read training images to memory. This consumes a lot of memory.  Parameters ---------- dataset_dir : str Path to the MARS dataset directory; ``bbox_train`` should be a subdirectory of this folder. image_shape : Tuple[int, int] A tuple (height, width) of the desired image size.  Returns ------- (np.ndarray, np.ndarray, np.ndarray, np.ndarray) Returns a tuple with the following entries:  * Tensor of images in BGR color space. * One dimensional array of unique IDs for the individuals in the images. * One dimensional array of camera indices. * One dimensional array of tracklet indices.  """ train_dir = os.path.join(dataset_dir, 'bbox_train') return read_train_test_directory_to_image(train_dir, image_shape) 
size|chatbots|name|vocab|CornellChatbotSeparateNames|cornell|targeted @property def targeted_name_vocab_size(self): return PROBLEM_HPARAMS['name_vocab_size'] 
devtools|checks|cleverhans|CleverHansTest|setUp def setUp(self): self.test_start = time.time() np.random.seed(1234) 
thumt|enable|training|utils|distribute|distributed def enable_distributed_training(): global _ENGINE try: import horovod.tensorflow as hvd _ENGINE = hvd hvd.init() except ImportError: sys.stderr.write( """Error: You must install horovod first in order to enable distributed training. """ ) exit() 
layers|BinaryConvolutionLayer|set|vars def set_vars(self, x, layer_id): """Set layer_id and variables  Arguments: x {tf tensor} -- Input to layer layer_id {int} -- Id of layer """ self.layer_id = layer_id in_dim = x.get_shape().as_list()[-1] with tf.variable_scope('layer' + str(self.layer_id)): wshape = [self.filter_size, self.filter_size, in_dim, self.out_dim] winit = self.init_func(wshape) weights = tf.get_variable('weights', wshape, self.dtype, initializer=tf.constant_initializer(winit)) ema_dH = tf.get_variable('ema_dH', wshape, self.dtype, trainable= False, initializer=tf.random_uniform_initializer(minval=-1e-06, maxval=1e-06)) self.ema_decay_rate = tf.get_variable('ema_decay_rate', [], self. dtype, trainable=False, initializer=tf.constant_initializer( self.init_edr)) self.rho = tf.get_variable('rho', [], self.dtype, trainable=False, initializer=tf.constant_initializer(self.init_rho)) self.vars = [weights] self.dH_ema = [ema_dH] 
UGATIT|dir|model @property def model_dir(self): n_res = str(self.n_res) + 'resblock' n_dis = str(self.n_dis) + 'dis' if self.smoothing: smoothing = '_smoothing' else: smoothing = '' if self.sn: sn = '_sn' else: sn = '' return '{}_{}_{}_{}_{}_{}_{}_{}_{}_{}{}{}'.format(self.model_name, self .dataset_name, self.gan_type, n_res, n_dis, self.n_critic, self. adv_weight, self.cycle_weight, self.identity_weight, self. cam_weight, sn, smoothing) 
layers|Hash|deepctr|utils|build def build(self, input_shape): super(Hash, self).build(input_shape) 
gen|lth|plot|resnet34|A|structured def plot_resnet34_structured_A_lth(): common.lth_plot(network=common.RESNET34, is_iterative=False, prune_method=common.STRUCTURED_A, min_max_y=(-0.03, 0.01), comparison_points=[(0.9246381154681216, 0.7303 - 0.7331)], comparison_label=common.RETHINKING) 
official|test|logs|logger|BenchmarkFileLoggerTest|setUp|utils def setUp(self): super(BenchmarkFileLoggerTest, self).setUp() self.original_environ = dict(os.environ) os.environ.clear() 
main def main(hparams): utils.setup_vals(hparams) utils.setup_dirs(hparams) basic_utils.print_hparams(hparams) basic_utils.save_hparams(hparams) mdevice = measure.get_mdevice(hparams) if hparams.dataset == 'mnist': gan_def = mnist_gan_def inf_def = mnist_inf_def real_val_iterator = mnist_utils.RealValIterator() elif hparams.dataset == 'celebA': gan_def = celebA_gan_def inf_def = None real_val_iterator = celebA_utils.RealValIterator() else: raise NotImplementedError if hparams.model_type == 'dcgan': generator = gan_def.generator_dcgan discriminator = gan_def.discriminator_dcgan elif hparams.model_type == 'wgangp': generator = gan_def.generator_wgangp discriminator = gan_def.discriminator_wgangp else: raise NotImplementedError if hparams.model_class == 'unconditional': z_ph, x_ph = utils.get_phs_uncond(hparams) phs = z_ph, x_ph if mdevice.output_type == 'vector': discriminator = gan_def.discriminator_fc (x_lossy, x_sample, theta_ph, theta_gen_ph, d_loss, g_loss, d_update_op, g_update_op, iter_ph) = (arch.model_fn_uncond( hparams, z_ph, x_ph, generator, discriminator, mdevice)) sess = utils.train(hparams, phs, d_update_op, g_update_op, d_loss, g_loss, x_sample, x_lossy, real_val_iterator, theta_ph, theta_gen_ph, mdevice, iter_ph, inf_def) elif hparams.model_class == 'conditional': z_ph, x_ph, y_ph = utils.get_phs_cond(hparams) phs = z_ph, x_ph, y_ph if mdevice.output_type == 'vector': discriminator = gan_def.discriminator_fc_cond (x_lossy, x_sample, theta_ph, theta_gen_ph, d_loss, g_loss, d_update_op, g_update_op, iter_ph) = (arch.model_fn_cond( hparams, z_ph, x_ph, y_ph, generator, discriminator, mdevice)) sess = utils.train(hparams, phs, d_update_op, g_update_op, d_loss, g_loss, x_sample, x_lossy, real_val_iterator, theta_ph, theta_gen_ph, mdevice, iter_ph, inf_def) else: raise NotImplementedError 
train|main def train(lang): if lang == 'en': EOE, input_feed, dynamic_window = en_dh.get_train_batch(batch_size) ss = en_ss bi_ss = ch_ss srl = en_srl bi_srl = ch_srl elif lang == 'ch': EOE, input_feed, dynamic_window = ch_dh.get_train_batch(batch_size) ss = ch_ss bi_ss = en_ss srl = ch_srl bi_srl = en_srl if EOE: return EOE sense_greedy_choice = sess.run(ss.sense_greedy, feed_dict={ss. context_indices: input_feed['context_indices'], ss.bi_info: tf. SparseTensorValue(input_feed['bi_loc'], input_feed['bi_indices'], input_feed['bi_shape']), ss.lengths: input_feed['bi_lengths'], ss. eval_mode: False, ss.major_weight: major_weight, ss.reg_weight: reg_weight}) bi_sense_greedy_choice = sess.run(bi_ss.sense_greedy, feed_dict={bi_ss. context_indices: input_feed['bi_context_indices'], bi_ss.bi_info: tf.SparseTensorValue(input_feed['bi_loc'], input_feed['bi_indices'], input_feed['bi_shape']), bi_ss.lengths: input_feed['bi_lengths'], bi_ss.eval_mode: True, bi_ss.major_weight: major_weight, bi_ss. reg_weight: reg_weight}) sense_selection_mask = np.random.rand(context_window * 2 + batch_size ) < 0.05 sense_selected = sense_selection_mask * np.random.randint(0, sense_dim, context_window * 2 + batch_size) + (1 - sense_selection_mask ) * sense_greedy_choice target_selected, bi_target_selected = data.get_train_sample_batch( batch_size, sense_selected, input_feed, bi_sense_greedy_choice, context_window, sense_dim) _ = sess.run([srl.train, srl.train_bilingual], feed_dict={srl. selected_sense_input_indices: input_feed[ 'selected_sense_input_indices'], srl.selected_sense_output_indices: input_feed['selected_sense_output_indices'], srl. selected_bi_sense_output_indices: input_feed[ 'bi_selected_sense_input_indices']}) reward = sess.run(srl.reward_sense_prob, feed_dict={srl. selected_sense_input_indices: input_feed[ 'selected_sense_input_indices'], srl.selected_sense_output_indices: input_feed['selected_sense_output_indices']}) _, cost, ent = sess.run([ss.update, ss.print_cost, ss.print_ent], feed_dict={ss.context_indices: input_feed['context_indices'], ss. reward_prob: reward, ss.target_sense_sampled_indices: target_selected, ss.bi_info: tf.SparseTensorValue(input_feed[ 'bi_loc'], input_feed['bi_indices'], input_feed['bi_shape']), ss. lengths: input_feed['bi_lengths'], ss.eval_mode: False, ss. major_weight: major_weight, ss.reg_weight: reg_weight}) bi_reward = sess.run(srl.bi_reward_sense_prob, feed_dict={srl. selected_sense_input_indices: input_feed[ 'selected_sense_input_indices'], srl. selected_bi_sense_output_indices: input_feed[ 'bi_selected_sense_input_indices']}) _ = sess.run(bi_ss.update, feed_dict={bi_ss.context_indices: input_feed ['bi_context_indices'], bi_ss.reward_prob: bi_reward, bi_ss.bi_info: tf.SparseTensorValue(input_feed['bi_loc'], input_feed['bi_indices'], input_feed['bi_shape']), bi_ss.target_sense_sampled_indices: bi_target_selected, bi_ss.lengths: input_feed['bi_lengths'], bi_ss. eval_mode: True, bi_ss.major_weight: major_weight, bi_ss.reg_weight: reg_weight}) return EOE 
python|grpc|add|method|ops|seed|rl|master|Client def _add_method(self, name, output_specs): """Adds a method to the client.""" flat_output_dtypes = [s.dtype for s in tf.nest.flatten(output_specs or [])]  def call(self, *inputs): """Makes a call to the server.""" flat_inputs = tf.nest.flatten(inputs) flat_outputs = gen_grpc_ops.grpc_client_call(fn_name=name, handle= self._handle, input_list=flat_inputs, Toutput_list= flat_output_dtypes) if output_specs is None: return None else: return tf.nest.pack_sequence_as(output_specs, flat_outputs) setattr(self, name, types.MethodType(call, self)) 
rnn|texar|cell|modules|UnidirectionalRNNEncoder|encoders @property def cell(self): """The RNN cell. """ return self._cell 
moments|bounded|mp|accountant|calculate|integral def integral_bounded_mp(fn, lb, ub): integral, _ = mp.quad(fn, [lb, ub], error=True) return integral 
CamRestDST|applications|camrest|save|dst|restaurants|cambridge def save(self, model_path=None): """ Saves the Ludwig model.  :param model_path: path to save the model to :return: """ if model_path: super(CamRestDST, self).save(model_path) 
RMSNorm|init|rmsnorm|torch def __init__(self, d, p=-1.0, eps=1e-08, bias=False): """ Root Mean Square Layer Normalization :param d: model size :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled) :param eps:  epsilon value, default 1e-8 :param bias: whether use bias term for RMSNorm, disabled by default because RMSNorm doesn't enforce re-centering invariance. """ super(RMSNorm, self).__init__() self.eps = eps self.d = d self.p = p self.bias = bias self.scale = nn.Parameter(torch.ones(d)) self.register_parameter('scale', self.scale) if self.bias: self.offset = nn.Parameter(torch.zeros(d)) self.register_parameter('offset', self.offset) 
vocab|utils|load|tokenization def load_vocab(vocab_file): """Loads a vocabulary file into a dictionary.""" vocab = collections.OrderedDict() index = 0 with tf.gfile.GFile(vocab_file, 'r') as reader: while True: token = tf.compat.as_text(reader.readline()) if not token: break token = token.strip() vocab[token] = index index += 1 return vocab 
get|lottery|prune|nmt|name|by|function|functions def get_prune_function_by_name(name): perc_re = '(?P<percent>\\d+(\\.\\d*)?)' regexp_map = {'prune_global(?:_(?:{}|(?P<fc>fc)|(?P<nofc>nofc)))*'. format(perc_re): prune_global_x, 'zprune_global(?:_(?:{}|(?P<fc>fc)|(?P<nofc>nofc)))*'.format( perc_re): zprune_global_x, 'prune_to_global_{}'.format(perc_re): prune_to_global_x, 'prune_all_to_global_{}'.format(perc_re): prune_all_to_global_x, 'zprune_all_to_global_{}'.format(perc_re): zprune_all_to_global_x} regexp_map = {re.compile(k): v for k, v in regexp_map.items()} for regexp, func in regexp_map.items(): match = regexp.match(name) if match is None: continue percentage = float(match.group('percent')) kwargs = {k: v for k, v in match.groupdict().items() if v is not None and k != 'percent'}  def prune(names, weights, masks): return func(names, weights, masks, percentage, **kwargs) return prune return globals()[name] 
fisher|FullyConnectedDiagonalFactor|ops|classification|factors|cov|shape @property def _cov_shape(self): return [self._squared_inputs.shape[1], self._outputs_grads[0].shape[1]] 
W1|get|w1|model|data def get_data(self, config): """override z with gz""" z = utils.to_var(next(self.z_generator)) gz = self.g(z) r = utils.to_var(next(self.r_generator)) if gz.size() != r.size(): z = utils.to_var(next(self.z_generator)) gz = self.g(z) r = utils.to_var(next(self.r_generator)) return r, gz 
metrics|update|fda def update_metrics(metrics, normal_out, adv_out, gt_real, class_offset): normal_out = normal_out.astype(np.float64) adv_out = adv_out.astype(np.float64) normal_top = np.argmax(normal_out, 1) - class_offset adv_top = np.argmax(adv_out, 1) - class_offset metrics['normal_prediction'].extend(normal_top) metrics['adv_prediction'].extend(adv_top) metrics['gt_real'].extend(gt_real) normal_ranking = np.argsort(normal_out, 1)[:, ::-1] - class_offset adv_ranking = np.argsort(adv_out, 1)[:, ::-1] - class_offset old_label_rank_now = [list(adv_ranking[i]).index(normal_top[i]) for i in range(normal_out.shape[0])] new_label_rank_old = [list(normal_ranking[i]).index(adv_top[i]) for i in range(normal_out.shape[0])] metrics['old_label_rank_new'].extend(old_label_rank_now) metrics['new_label_rank_old'].extend(new_label_rank_old) real_acc = np.mean(np.array(metrics['normal_prediction']) == np.array( metrics['gt_real'])) adv_acc = np.mean(np.array(metrics['adv_prediction']) == np.array( metrics['gt_real'])) fr = np.mean(np.array(metrics['adv_prediction']) != np.array(metrics[ 'normal_prediction'])) metrics['real_acc'].append(real_acc) metrics['adv_acc'].append(adv_acc) metrics['fr'].append(fr) return metrics 
get|download|and|extract|token|src|master|facenet|confirm def get_confirm_token(response): for key, value in response.cookies.items(): if key.startswith('download_warning'): return value return None 
fn|lstm|map|attention|model|for|inception def map_fn_for_attention(image, bbox, attention_h): """ attention mechanism for each image """ offset_height, offset_width, target_height, target_width = tf.cast(bbox [0] * height, tf.int32), tf.cast(bbox[1] * width, tf.int32), tf.cast( bbox[2] * height, tf.int32), tf.cast(bbox[3] * width, tf.int32) image = tf.compat.v1.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width) attention_x = tf.reshape(image, [-1, channel]) attention_x = tf.matmul(attention_x, W) att = tf.tanh(attention_x + tf.expand_dims(attention_h, axis=0)) att = tf.add(tf.matmul(att, W_att), b_att) alpha = tf.nn.softmax(att, axis=0) x = attention_x * alpha attention_feature = tf.reduce_sum(x, axis=0) alpha = tf.reshape(alpha, [target_height, target_width]) paddings = [[offset_height, height - target_height - offset_height], [ offset_width, width - offset_width - target_width]] alpha = tf.pad(alpha, paddings, 'CONSTANT') return attention_feature, alpha 
label|datacode|LabelRepresentation|hot|vector|name|one|to|ner def label_name_to_one_hot_vector(self, label): vector = np.zeros(self.get_num_labels()) vector[self.label_name_to_label_idx_map[label]] = 1 return vector 
lrp|heads|split|utils|thumt def split_heads(inputs, num_heads, name=None): with tf.name_scope(name, default_name='split_heads', values=[inputs]): x = inputs n = num_heads old_shape = x.get_shape().dims last = old_shape[-1] new_shape = old_shape[:-1] + [n] + [last // n if last else None] ret = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [n, -1]], 0)) ret.set_shape(new_shape) return tf.transpose(ret, [0, 3, 1, 2, 4]) 
forward|LMModel|gpt def forward(self, x, sequence_mask=None): h = self.transformer(x, sequence_mask) lm_logits = self.lm_head(h) if self.return_probs: lm_logits = F.softmax(lm_logits + self.pos_emb_mask, dim=-1) elif self.return_acts: lm_logits = lm_logits + self.pos_emb_mask return lm_logits 
decode|torch|BinaryVAE|vae def decode(self, z): """Take a z and output a probability (ie Bernoulli param) on each dim""" h3 = F.relu(self.fc3(z)) return self.sigmoid(self.fc4(h3)) 
evaluate|bert|csqa|extract def evaluate(model, device, eval_dataloader, desc='Evaluate'): model.eval() eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=desc): input_ids = input_ids.to(device) input_mask = input_mask.to(device) segment_ids = segment_ids.to(device) label_ids = label_ids.to(device) with torch.no_grad(): tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids ) logits = model(input_ids, segment_ids, input_mask) logits = logits.detach().cpu().numpy() label_ids = label_ids.to('cpu').numpy() tmp_eval_accuracy = accuracy(logits, label_ids) eval_loss += tmp_eval_loss.mean().item() eval_accuracy += tmp_eval_accuracy nb_eval_examples += input_ids.size(0) nb_eval_steps += 1 eval_loss = eval_loss / nb_eval_steps eval_accuracy = eval_accuracy / nb_eval_examples return eval_loss, eval_accuracy 
vscaleextexp|cases|generate|test def generate_test_cases(ukernel, elements_tile, isa): """Generates all tests cases for a Vector ScaleExtExp micro-kernel.  Args: ukernel: C name of the micro-kernel function. elements_tile: Number of batch elements processed per one iteration of the inner loop of the micro-kernel. isa: instruction set required to run the micro-kernel. Generated unit test will skip execution if the host processor doesn't support this ISA.  Returns: Code for the test case. """ _, test_name = ukernel.split('_', 1) _, datatype, _ = ukernel.split('_', 2) return xngen.preprocess(RADDEXTEXP_TEST_TEMPLATE, {'TEST_FUNCTION': ukernel, 'TEST_NAME': test_name.upper().replace('UKERNEL_', ''), 'DATATYPE': datatype, 'ELEMENTS_TILE': elements_tile, 'ISA_CHECK': xnncommon.generate_isa_check_macro(isa)}) 
topKProtect|h def h(F, true_label, k): S = softmax(F) idx_sort = np.argsort(S[:, (true_label)])[::-1] n, m = np.shape(S) F = F[(idx_sort), :] for i in range(0, n): F_r = F[i:, :] f_v = np.sum(F_r, axis=0) topk = np.argsort(f_v)[::-1][:k] if true_label not in topk: return i return n 
get|dim|nasnet|nets|channel|utils @tf.contrib.framework.add_arg_scope def get_channel_dim(shape, data_format=INVALID): assert data_format != INVALID assert len(shape) == 4 if data_format == 'NHWC': return int(shape[3]) elif data_format == 'NCHW': return int(shape[1]) else: raise ValueError('Not a valid data_format', data_format) 
darkflow|net|yolov2|train|tensor|expit def expit_tensor(x): return 1.0 / (1.0 + tf.exp(-x)) 
ops|with|w|connected|fully def fully_connected_with_w(x, use_bias=True, sn=False, reuse=False, scope= 'linear'): with tf.variable_scope(scope, reuse=reuse): x = flatten(x) bias = 0.0 shape = x.get_shape().as_list() channels = shape[-1] w = tf.get_variable('kernel', [channels, 1], tf.float32, initializer=weight_init, regularizer=weight_regularizer) if sn: w = spectral_norm(w) if use_bias: bias = tf.get_variable('bias', [1], initializer=tf. constant_initializer(0.0)) x = tf.matmul(x, w) + bias else: x = tf.matmul(x, w) if use_bias: weights = tf.gather(tf.transpose(tf.nn.bias_add(w, bias)), 0) else: weights = tf.gather(tf.transpose(w), 0) return x, weights 
models|LeNet|lenet|test def test(self, sess, input_features, input_labels): """Single step test of the autoencoder @param sess (tf.Session) the current session @param input_feature (np.array) matrix or array of feature @param lambda_e explicit mixing coefficient @param lambda_i implicit mixing coefficient @return (float) the losses: loss, loss_r, loss_c, acc_c, loss_e, loss_i """ loss = sess.run([self.loss, self.accuracy], feed_dict={self.x: input_features, self.labels_placeholder: input_labels}) return loss 
texar|moses|corpus|bleu|evals def corpus_bleu_moses(list_of_references, hypotheses, lowercase=False, return_all=False): """Calculates corpus-level BLEU score using the **MOSES multi-bleu.perl** script.  Args: list_of_references: A list of lists of references for each hypothesis. Each reference can be either a string, or a list of string tokens. List can also be numpy array. hypotheses: A list of hypothesis sentences. Each hyperthsis can be either a string, or a list of string tokens. List can also be numpy array. lowercase (bool): If `True`, pass the "-lc" flag to the multi-bleu script. return_all (bool): If `True`, returns BLEU and all n-gram precisions.  Returns: If :attr:`return_all` is `False` (default), returns a float32 BLEU score.  If :attr:`return_all` is `True`, returns a list of 5 float32 scores: `[BLEU, 1-gram precision, ..., 4-gram precision]`. """ list_of_references = compat_as_text(list_of_references) hypotheses = compat_as_text(hypotheses) if np.size(hypotheses) == 0: return np.float32(0.0) cur_dir = os.path.dirname(os.path.realpath(__file__)) multi_bleu_path = os.path.abspath(os.path.join(cur_dir, '..', '..', 'bin', 'utils', 'multi-bleu.perl')) result_path = tempfile.mkdtemp() hfile_path = os.path.join(result_path, 'hyp') hyps = [_maybe_list_to_str(h) for h in hypotheses] with open(hfile_path, 'w', encoding='utf-8') as hfile: text = '\n'.join(hyps) hfile.write(text) hfile.write('\n') max_nrefs = max([len(refs) for refs in list_of_references]) rfile_path = os.path.join(result_path, 'ref') for rid in range(max_nrefs): with open(rfile_path + '%d' % rid, 'w', encoding='utf-8') as rfile: for refs in list_of_references: if rid < len(refs): ref = _maybe_list_to_str(refs[rid]) rfile.write(ref + '\n') else: rfile.write('\n') multi_bleu_cmd = [multi_bleu_path] if lowercase: multi_bleu_cmd += ['-lc'] multi_bleu_cmd += [rfile_path] with open(hfile_path, 'r') as hyp_input: try: multi_bleu_ret = subprocess.check_output(multi_bleu_cmd, stdin= hyp_input, stderr=subprocess.STDOUT) multi_bleu_ret = multi_bleu_ret.decode('utf-8') bleu_score = _parse_multi_bleu_ret(multi_bleu_ret, return_all) except subprocess.CalledProcessError as error: if error.output is not None: tf.logging.warning( 'multi-bleu.perl returned non-zero exit code') tf.logging.warning(error.output) if return_all: bleu_score = [np.float32(0.0)] * 5 else: bleu_score = np.float32(0.0) shutil.rmtree(result_path) return np.float32(bleu_score) 
DeepWalker|init|layers def __init__(self, args, vocab_size, degrees): """ Initialization of the layer with proper matrices and biases. The input variables are also initialized here. """ self.args = args self.vocab_size = vocab_size self.degrees = degrees self.train_labels = tf.placeholder(tf.int64, shape=[None, self.args. window_size]) self.train_inputs = tf.placeholder(tf.int64, shape=[None]) self.embedding_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.args.dimensions], -0.1 / self.args.dimensions, 0.1 / self.args .dimensions)) self.nce_weights = tf.Variable(tf.truncated_normal([self.vocab_size, self.args.dimensions], stddev=1.0 / math.sqrt(self.args.dimensions))) self.nce_biases = tf.Variable(tf.random_uniform([self.vocab_size], -0.1 / self.args.dimensions, 0.1 / self.args.dimensions)) 
collection|variables|ops|LayerCollection|classification|layer|registered @property def registered_variables(self): """A tuple of all of the variables currently registered.""" tuple_of_tuples = (ensure_sequence(key) for key, block in six.iteritems (self.fisher_blocks)) flat_tuple = tuple(item for tuple_ in tuple_of_tuples for item in tuple_) return flat_tuple 
worker|init|fn|run def init_fn(ses): logger.info('Initializing all parameters.') logger.info('Uninizialied Variables after init_fn: %s', ses.run( report_uninitialized_variables)) 
update|all|result|Result|utils def update_all(self, docid_list, score_list): self.set_docid_list(docid_list) self.set_score_list(score_list) self.update_ranking() 
output|test|same|shape|and|cyclegan|input|nets|kernel4|CycleganTest def input_and_output_same_shape_kernel4(self): self._input_and_output_same_shape_helper(4) 
observation|unpackbits def unpackbits(frame):  def _(frame): bit_patterns = [2 ** 7, 2 ** 6, 2 ** 5, 2 ** 4, 2 ** 3, 2 ** 2, 2 ** 1, 2 ** 0, 2 ** 15, 2 ** 14, 2 ** 13, 2 ** 12, 2 ** 11, 2 ** 10, 2 ** 9, 2 ** 8] frame = tf.bitwise.bitwise_and(frame[..., tf.newaxis], bit_patterns) frame = tf.cast(tf.cast(frame, tf.bool), tf.float32) * 255 frame = tf.reshape(frame, frame.shape[:-2] + (frame.shape[-2] * frame.shape[-1],)) return frame if tf.test.is_gpu_available(): return tf.xla.experimental.compile(_, [frame])[0] return _(frame) 
infer|PCGNModel|model|PCGN def infer(self, sess, batch): feed_dict = {self.encoder_inputs: batch[0], self.encoder_length: batch[ 1], self.user_desc: batch[6], self.desc_length: batch[7], self. user_feat: batch[8]} predict = sess.run(self.infer_outputs, feed_dict=feed_dict) return predict 
one|epoch|train|shapenet def train_one_epoch(sess, ops, next_train_element, train_writer): """ ops: dict mapping from string to tf ops """ log_string(str(datetime.now())) cur_batch_xyz = np.zeros((BATCH_SIZE, NUM_POINT, 3)) cur_batch_label = np.zeros((BATCH_SIZE, NUM_POINT), dtype=np.int32) total_correct = 0 total_seen = 0 loss_sum = 0 batch_idx = 0 train_time = 0.0 while True: try: padded_all = sess.run(next_train_element) bsize = padded_all.shape[0] batch_xyz = np.zeros((bsize, NUM_POINT, INPUT_DIM)) batch_label = np.zeros((bsize, NUM_POINT), dtype=np.int32) for b in range(bsize): loc = np.where(padded_all[(b), :, (-1)] < 0) if len(loc[0]) == 0: num = padded_all.shape[1] else: num = loc[0][0] if num == 0: print(loc, padded_all[(b), 0:10, :]) print('problem of train') exit() if num < NUM_POINT: sample_index = np.random.choice(num, NUM_POINT, replace =True) else: sample_index = np.random.choice(num, NUM_POINT, replace =False) batch_xyz[b, ...] = padded_all[(b), (sample_index), 0:-1] batch_label[(b), :] = padded_all[b, sample_index, -1] batch_xyz, batch_label = augment_fn(batch_xyz, batch_label) cur_batch_xyz[0:bsize, (...)] = batch_xyz cur_batch_label[0:bsize, :] = batch_label feed_dict = {ops['xyz_pl']: cur_batch_xyz, ops['label_pl']: cur_batch_label, ops['training_pl']: True} now = time.time() summary, global_step, _, loss_val, pred_val = sess.run([ops[ 'merged'], ops['global_step'], ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict) train_time += time.time() - now train_writer.add_summary(summary, global_step) pred_val = np.argmax(pred_val, 2) correct = np.sum(pred_val[0:bsize, :] == batch_label[0:bsize, :]) total_correct += correct total_seen += bsize * NUM_POINT loss_sum += loss_val if (batch_idx + 1) % 10 == 0: log_string(' ---- batch: %03d ----' % (batch_idx + 1)) log_string('mean loss: %f' % (loss_sum / 10)) log_string('accuracy: %f' % (total_correct / float(total_seen)) ) total_correct = 0 total_seen = 0 loss_sum = 0 batch_idx += 1 except tf.errors.OutOfRangeError: break log_string('training one batch require %.2f milliseconds' % (1000 * train_time / batch_idx)) return 
embeddings|normalize|dimensionwise|length def length_normalize_dimensionwise(matrix): xp = get_array_module(matrix) norms = xp.sqrt(xp.sum(matrix ** 2, axis=0)) norms[norms == 0] = 1 matrix /= norms 
xlnet|checkpoint|from|init|master|model|utils def init_from_checkpoint(FLAGS, global_vars=False): tvars = tf.global_variables() if global_vars else tf.trainable_variables() initialized_variable_names = {} scaffold_fn = None if FLAGS.init_checkpoint is not None: if FLAGS.init_checkpoint.endswith('latest'): ckpt_dir = os.path.dirname(FLAGS.init_checkpoint) init_checkpoint = tf.train.latest_checkpoint(ckpt_dir) else: init_checkpoint = FLAGS.init_checkpoint tf.logging.info('Initialize from the ckpt {}'.format(init_checkpoint)) assignment_map, initialized_variable_names = ( get_assignment_map_from_checkpoint(tvars, init_checkpoint)) if FLAGS.use_tpu:  def tpu_scaffold(): tf.train.init_from_checkpoint(init_checkpoint, assignment_map) return tf.train.Scaffold() scaffold_fn = tpu_scaffold else: tf.train.init_from_checkpoint(init_checkpoint, assignment_map) tf.logging.info('**** Global Variables ****') for var in tvars: init_string = '' if var.name in initialized_variable_names: init_string = ', *INIT_FROM_CKPT*' tf.logging.info('  name = %s, shape = %s%s', var.name, var. shape, init_string) return scaffold_fn 
tangents|stax|fn|neural|preprocess|kernel def _preprocess_kernel_fn(init_fn, kernel_fn):  def new_kernel_fn(x1_or_kernel, x2=None, get=None, marginalization='auto'): """Returns the `Kernel` resulting from applying `ker_fun` to given inputs.  Args: x1_or_kernel: either a `np.ndarray` with shape `[batch_size_1] + input_shape`, or a `Kernel`. x2: an optional `np.ndarray` with shape `[batch_size_2] + input_shape`. `None` means `x2 == x1` or `x1_or_kernel is Kernel`. get: either `None`, a string, or a tuple of strings specifying which data should be returned by the kernel function. Can be "nngp", "ntk", "var1", "var2", "is_gaussian", "is_height_width", "marginal", "cross". marginalization: Either a string with value "auto" or "none" or a dict. If "auto" then stax attempts to automatically identify which dimensions are most appropriate to marginalize over. If "none" then no marginalization is performed. If a dict then the user can manually specify the marginalization for the self- and cross- correlations. Returns: If `get` is a string, returns the requested `np.ndarray`. If `get` is a tuple, returns an `AnalyticKernel` namedtuple containing only the requested information. If `get` is None then a Kernel object is returned containing all the data. """ if isinstance(x1_or_kernel, Kernel) or isinstance(x1_or_kernel, list ) and all(isinstance(k, Kernel) for k in x1_or_kernel): _check_marginalization(kernel_fn, x1_or_kernel) return _apply_kernel(init_fn, kernel_fn, x1_or_kernel) return outer_kernel_fn(x1_or_kernel, x2, get, marginalization)  @utils.get_namedtuple('AnalyticKernel') def outer_kernel_fn(x1, x2, get, marginalization): if not isinstance(x1, np.ndarray): raise TypeError( 'Inputs to a kernel propagation function should be a `Kernel`, a `list` of `Kernel`s, or a (tuple of) `np.ndarray`(s), got %s.' % type(x1)) if not (x2 is None or isinstance(x2, np.ndarray)): raise TypeError( '`x2` to a kernel propagation function should be `None` or a `np.ndarray`, got %s.' % type(x2)) include_ntk = get is None or 'ntk' in get covs_req = getattr(kernel_fn, _COVARIANCES_REQ, _DEFAULT_MARGINALIZATION) if marginalization != 'auto': if isinstance(marginalization, dict): for k in covs_req: if marginalization[k] > covs_req[k]: covs_req[k] = marginalization[k] elif marginalization == 'none' and x1.ndim > 2: covs_req = {'marginal': M.OVER_POINTS, 'cross': M.NO} else: raise NotImplementedError( 'marginalization should be set to one of "auto", "none", or a dictspecifying the marginalization levels of the variance and the covariance respectively. Found {}.' .format(marginalization)) kernel = _inputs_to_kernel(x1, x2, compute_ntk=include_ntk, **covs_req) return _apply_kernel(init_fn, kernel_fn, kernel) if hasattr(kernel_fn, _COVARIANCES_REQ): setattr(new_kernel_fn, _COVARIANCES_REQ, getattr(kernel_fn, _COVARIANCES_REQ)) return new_kernel_fn 
VerboseLevel|cpplint def _VerboseLevel(): """Returns the module's verbosity setting.""" return _cpplint_state.verbose_level 
label|labels|name|to|imagenet def label_to_name(label): global _lut return _lut[label] 
gym|rendering|pycolab|ObservationToFeatureArray|call def __call__(self, observation): """Derives an array from an `Observation`.  Returns a 3-D `float32` array whose 2-D submatrices, indexed by the major index, are the float-cast binary layers of the `Observation` corresponding to respective entries in the `layers` constructor argument.  Note: the returned array should be accessed in a *read-only* manner exclusively; furthermore, if this method is called again, the contents of the array returned in any prior call to this method are *undefined* (i.e. not guaranteed to be anything---could be blank, random garbage, whatever).  Args: observation: an `Observation` from which this method derives a numpy array.  Returns: a numpy array derived from `observation` as described.  Raises: RuntimeError: the `layers` constructor argument contains no entries that are present in the `layers` member of `observation`. """ if not any(l in observation.layers for l in self._layers): raise RuntimeError( 'The layers argument to this ObservationToFeatureArray, {}, has no entry that refers to an actual feature in the input observation. Actual features in the observation are {}.' .format(repr(self._layers), repr(''.join(sorted(observation. layers))))) if self._array is None or self._array.shape[1:] != observation.board.shape: rows, cols = observation.board.shape self._array = np.zeros((self._depth, rows, cols), dtype=np.float32) for index, character in enumerate(self._layers): try: np.copyto(self._array[index], observation.layers[character]) except KeyError: self._array[index] = 0.0 if self._permute is None: return self._array else: return np.transpose(self._array, self._permute) 
DataInjector|transformers|kaffe|load|using|pb def load_using_pb(self): data = get_caffe_resolver().NetParameter() data.MergeFromString(open(self.data_path, 'rb').read()) pair = lambda layer: (layer.name, self.normalize_pb_data(layer)) layers = data.layers or data.layer self.params = [pair(layer) for layer in layers if layer.blobs] self.did_use_pb = True 
colorize|from|file|fasterai|VideoColorizer|name|visualize def colorize_from_file_name(self, file_name: str, render_factor: int=None ) ->Path: source_path = self.source_folder / file_name return self._colorize_from_path(source_path, render_factor=render_factor) 
ImageEncoderTest|test|image|encoding|encoder def test_encoding(self): batch_size = 8 attention_space_size = 128 lstm_space_size = 256 num_panos = 5 image_feature_size = 64 num_hidden_layers = 2 encoder = image_encoder.ImageEncoder(attention_space_size, lstm_space_size, num_hidden_layers=num_hidden_layers) image_features = tf.random.normal([batch_size, num_panos, image_feature_size]) states = [(tf.random.normal([batch_size, lstm_space_size]), tf.random. normal([batch_size, lstm_space_size])), (tf.random.normal([ batch_size, lstm_space_size]), tf.random.normal([batch_size, lstm_space_size]))] output_hidden_state, next_state = encoder(image_features, states) self.assertEqual(output_hidden_state.shape, [batch_size, lstm_space_size]) self.assertEqual(len(states), len(next_state)) for i in range(len(states)): self.assertEqual(states[i][0].shape, next_state[i][0].shape) self.assertEqual(states[i][1].shape, next_state[i][1].shape) 
fisher|eigendecomp|get|ops|InverseProvidingFactor|classification|factors def get_eigendecomp(self): return self._eigendecomp 
tmp|dlib|align|init|master|facenet|AlignDlib def __init__(self, facePredictor): """ Instantiate an 'AlignDlib' object.  :param facePredictor: The path to dlib's :type facePredictor: str """ assert facePredictor is not None self.detector = dlib.get_frontal_face_detector() self.predictor = dlib.shape_predictor(facePredictor) 
art|gym|game|ascii|pycolab|to def ascii_art_to_game(art, what_lies_beneath, sprites=None, drapes=None, backdrop=things.Backdrop, update_schedule=None, z_order=None): """Construct a pycolab game from an ASCII art diagram.  This function helps to turn ASCII art diagrams like the following (which is a Sokoban-like puzzle):  [' @@@@@@ ', ' @  . @ ',        # '@' means "wall" '@@ab @@ ',        # 'P' means "player" '@  .c @ ',        # '.' means "box storage location" '@.  dP@ ',        # 'a'-'g' are all for separate boxes '@.@@@@@@',        # ' ' means "open, traversable space" '@ @ @@ @', '@ e  . @', '@@@@@@@@',]  into pycolab games. The basic idea is that you supply the diagram, along with hints about which characters correspond to `Sprite`s and `Drape`s and the classes that implement those `Sprite`s and `Drape`s. This function then returns an initialised `Engine` object, all ready for you to call the `its_showtime` method and start the game.  Several of this function's arguments require you to supply subclasses of the classes found in `things.py`. If your subclass constructors take the same number of arguments as their `things.py` superclasses, then they can be listed directly. Otherwise, you will need to pack the subclasses and their additional `args` and `kwargs` into a `Partial` object. So, for example, if you have a `Sprite` subclass with a constructor like this:  class MySprite(Sprite): def __init__(self, corner, position, character, mood, drink_quantity): ...  you could package `MySprite` and the "extra" arguments in any of the following ways (among others):  Partial(MySprite, 'drowsy', 'two pints') Partial(MySprite, 'yawning', drink_quantity='three pints') Partial(MySprite, mood='asleep', drink_quantity='four pints')  Args: art: An ASCII art diagram depicting a game board. This should be a list or tuple whose values are all strings containing the same number of ASCII characters. what_lies_beneath: a single-character ASCII string that will be substituted into the `art` diagram at all places where a character that keys `sprites` or `drapes` is found; *or*, this can also be an entire second ASCII art diagram whose values will be substituted into `art` at (only) those locations. In either case, the resulting diagram will be used to initialise the game's `Backdrop`. sprites: a dict mapping single-character ASCII strings to `Sprite` classes (not objects); or to `Partial` objects that hold the classes and "extra" `args`es and `kwargs`es to use during their construction. It's fine if a character used as a key doesn't appear in the `art` diagram: in this case, we assume that the corresponding `Sprite` will be located at `0, 0`. (If you intend your `Sprite` to be invisible, the `Sprite` will have to take care of that on its own after it is built.) (Optional; omit if your game has no sprites.) drapes: a dict mapping single-character ASCII strings to `Drape` classes (not objects); or to `Partial` objects that hold the classes and "extra" `args`es and `kwargs`es to use during their construction. It's fine if a character used as a key doesn't appear in the `art` diagram: in this case, we assume that the `Drape`'s curtain (i.e. its mask) is completely empty (i.e. False). (Optional; omit if your game has no drapes.) backdrop: a `Backdrop` class (not an object); or a `Partial` object that holds the class and "extra" `args` and `kwargs` to use during its construction. (Optional; if unset, `Backdrop` is used directly, which is fine for a game where the background scenery never changes and contains no game logic.) update_schedule: A list of single-character ASCII strings indicating the order in which the `Sprite`s and `Drape`s should be consulted by the `Engine` for updates; or, a list of lists that imposes an ordering as well, but that groups the entities in each list into separate update groups (refer to `Engine` documentation). (Optional; if unspecified, the ordering will be arbitrary---be mindful of this if your game uses advanced features like scrolling, where update order is pretty important.) z_order: A list of single-character ASCII strings indicating the depth ordering of the `Sprite`s and `Drape`s (from back to front). (Optional; if unspecified, the ordering will be the same as what's used for `update_schedule`).  Returns: An initialised `Engine` object as described.  Raises: TypeError: when `update_schedule` is neither a "flat" list of characters nor a list of lists of characters. ValueError: numerous causes, nearly always instances of the user not heeding the requirements stipulated in Args:. The exception messages should make most errors fairly easy to debug. """ if sprites is None: sprites = {} if drapes is None: drapes = {} sprites = {char: (sprite if isinstance(sprite, Partial) else Partial( sprite)) for char, sprite in six.iteritems(sprites)} drapes = {char: (drape if isinstance(drape, Partial) else Partial(drape )) for char, drape in six.iteritems(drapes)} if not isinstance(backdrop, Partial): backdrop = Partial(backdrop) non_backdrop_characters = set() non_backdrop_characters.update(sprites.keys()) non_backdrop_characters.update(drapes.keys()) if update_schedule is None: update_schedule = list(non_backdrop_characters) if isinstance(update_schedule, str): update_schedule = list(update_schedule) if all(isinstance(item, str) for item in update_schedule): update_schedule = [update_schedule] try: flat_update_schedule = list(itertools.chain.from_iterable( update_schedule)) except TypeError: raise TypeError( 'if any element in update_schedule is an iterable (like a list), all elements in update_schedule must be' ) if set(flat_update_schedule) != non_backdrop_characters: raise ValueError( 'if specified, update_schedule must list each sprite and drape exactly once.' ) if z_order is None: z_order = flat_update_schedule if set(z_order) != non_backdrop_characters: raise ValueError( 'if specified, z_order must list each sprite and drape exactly once.' ) if isinstance(what_lies_beneath, str) and len(what_lies_beneath) != 1: raise ValueError( 'what_lies_beneath may either be a single-character ASCII string or a list of ASCII-character strings' ) try: _ = [ord(character) for character in ''.join(what_lies_beneath)] _ = [ord(character) for character in non_backdrop_characters] _ = [ord(character) for character in z_order] _ = [ord(character) for character in flat_update_schedule] except TypeError: raise ValueError( 'keys of sprites, keys of drapes, what_lies_beneath (or its entries), values in z_order, and (possibly nested) values in update_schedule must all be single-character ASCII strings.' ) if non_backdrop_characters.intersection(''.join(what_lies_beneath)): raise ValueError( 'any character specified in what_lies_beneath must not be one of the characters used as keys in the sprites or drapes arguments.' ) art = ascii_art_to_uint8_nparray(art) if isinstance(what_lies_beneath, str): what_lies_beneath = np.full_like(art, ord(what_lies_beneath)) else: what_lies_beneath = ascii_art_to_uint8_nparray(what_lies_beneath) if art.shape != what_lies_beneath.shape: raise ValueError( 'if not a single ASCII character, what_lies_beneath must be ASCII art whose shape is the same as that of the ASCII art in art.' ) update_group_for = {} for i, update_group in enumerate(update_schedule): group_id = '{:05d}'.format(i) update_group_for.update({character: group_id for character in update_group}) game = engine.Engine(*art.shape) for character in flat_update_schedule: game.update_group(update_group_for[character]) mask = art == ord(character) if character in drapes: partial = drapes[character] game.add_prefilled_drape(character, mask, partial.pycolab_thing, *partial.args, **partial.kwargs) if character in sprites: row, col = np.where(mask) if len(row) > 1: raise ValueError( 'sprite character {} can appear in at most one place in art.' .format(character)) row, col = (int(row), int(col)) if len(row) > 0 else (0, 0) partial = sprites[character] game.add_sprite(character, (row, col), partial.pycolab_thing, * partial.args, **partial.kwargs) art[mask] = what_lies_beneath[mask] game.set_z_order(z_order) game.set_prefilled_backdrop(*backdrop.args, characters=''.join(chr(c) for c in np.unique(art)), prefill=art.view(np.uint8), backdrop_class= backdrop.pycolab_thing, **backdrop.kwargs) return game 
rows|official|test|io|file|medium|BaseTest|data|core|utils|tiny def test_tiny_rows_medium_core(self): self._test_sharding(**_TEST_CASES[3]) 
delta|construct|optimize|patch def optimize(imgs, dog_imgs, loc, mask, img_name, num_steps=300, step_size= 500, func_name='resnet_model/final_dense:0', input_name='input_tensor:0'): with tf.Session(graph=tf.Graph()) as sess: graph = tf.get_default_graph() new_image_node = build_graph(sess, num_steps, step_size, dog_imgs, loc, mask) input_tensor = graph.get_tensor_by_name(input_name) new_image = sess.run(new_image_node, feed_dict={input_tensor: imgs})[0] logits = tf.nn.softmax(graph.get_tensor_by_name(func_name)) img_logit = list(sess.run([logits], feed_dict={input_name: imgs})[0][0] ) new_img_logit = list(sess.run([logits], feed_dict={input_name: [ new_image]})[0][0]) fx_node = graph.get_tensor_by_name(func_name) fx = sess.run([fx_node], feed_dict={input_name: imgs})[0][0] fxd = sess.run([fx_node], feed_dict={input_name: [new_image]})[0][0] l2 = np.linalg.norm(fxd - fx) print('Logit layer L2 distance = {}'.format(l2)) fname = img_name[:-4] + '_' + str(int(l2)) save_img(imgs[0], new_image, fname) 
enqueue|queued|thread|trainer|QueuedTrainer|run def _run_enqueue_thread(self, session): try: for data in self._feed_generator: if self._coordinator.should_stop(): break try: feed_dict = {var: value for var, value in zip(self. _input_vars, data)} session.run(self._enqueue_op, feed_dict=feed_dict) except (tf.errors.CancelledError, tf.errors.AbortedError): break except Exception as e: print('EnqueueError:', e) self._stop_all_threads(session) 
MultiHeadAttention|init|transformer def __init__(self, d_model_size, num_heads): super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.d_model_size = d_model_size self.depth = d_model_size / self.num_heads self.Wq = tf.keras.layers.Dense(d_model_size) self.Wk = tf.keras.layers.Dense(d_model_size) self.Wv = tf.keras.layers.Dense(d_model_size) self.dense = tf.keras.layers.Dense(d_model_size) 
list|avod|ops|test|scale|BoxListOpsTest|core|box def test_scale(self): corners = tf.constant([[0, 0, 100, 200], [50, 120, 100, 140]], dtype=tf .float32) boxes = box_list.BoxList(corners) boxes.add_field('extra_data', tf.constant([[1], [2]])) y_scale = tf.constant(1.0 / 100) x_scale = tf.constant(1.0 / 200) scaled_boxes = box_list_ops.scale(boxes, y_scale, x_scale) exp_output = [[0, 0, 1, 1], [0.5, 0.6, 1.0, 0.7]] with self.test_session() as sess: scaled_corners_out = sess.run(scaled_boxes.get()) self.assertAllClose(scaled_corners_out, exp_output) extra_data_out = sess.run(scaled_boxes.get_field('extra_data')) self.assertAllEqual(extra_data_out, [[1], [2]]) 
mag|LorentzVector|utilFunctions def mag(self): return math.sqrt(self.x * self.x + self.y * self.y + self.z * self.z) 
xlnet|regression|loss|master|modeling def regression_loss(hidden, labels, initializer, scope, reuse=None, return_logits=False): with tf.variable_scope(scope, reuse=reuse): logits = tf.layers.dense(hidden, 1, kernel_initializer=initializer, name='logit') logits = tf.squeeze(logits, axis=-1) loss = tf.square(logits - labels) if return_logits: return loss, logits return loss 
masks|overlaps|utils|compute def compute_overlaps_masks(masks1, masks2): """Computes IoU overlaps between two sets of masks. masks1, masks2: [Height, Width, instances] """ masks1 = np.reshape(masks1 > 0.5, (-1, masks1.shape[-1])).astype(np.float32 ) masks2 = np.reshape(masks2 > 0.5, (-1, masks2.shape[-1])).astype(np.float32 ) area1 = np.sum(masks1, axis=0) area2 = np.sum(masks2, axis=0) intersections = np.dot(masks1.T, masks2) union = area1[:, (None)] + area2[(None), :] - intersections overlaps = intersections / union return overlaps 
deeppoly|nodes|DeeppolyResadd|init def __init__(self, input_names, output_name, output_shape, has_relu): """ Arguments --------- input_names : iterable iterable with the names of the two nodes you want to add output_name : str name of this node's output output_shape : iterable iterable of ints with the shape of the output of this node """ self.has_relu = has_relu add_input_output_information_deeppoly(self, input_names, output_name, output_shape) 
CIFAR1|providers|init|data|DataProvider def __init__(self, which_set='train', batch_size=100, max_num_batches=-1, shuffle_order=True, rng=None): """Create a new CIFAR-10 data provider object.  Args: which_set: One of 'train' or 'valid'. Determines which portion of the CIFAR-10 data this object should provide. batch_size (int): Number of data points to include in each batch. max_num_batches (int): Maximum number of batches to iterate over in an epoch. If `max_num_batches * batch_size > num_data` then only as many batches as the data can be split into will be used. If set to -1 all of the data will be used. shuffle_order (bool): Whether to randomly permute the order of the data before each epoch. rng (RandomState): A seeded random number generator. """ self.num_classes = 10 dataDir = './data/cifar10/' inputs = np.zeros((0, 3072), np.uint8) targets_list = [] for filename in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']: data_path = os.path.join(dataDir, '{0}'.format(filename)) assert os.path.isfile(data_path ), '!!!!!Data file does not exist at expected path: ' + data_path dict_tmp = unpickle(data_path) inputs = np.vstack((inputs, dict_tmp['data'])) targets_list = targets_list + dict_tmp['labels'] dbg = 3 targets = np.array(targets_list) targets_onehot = np.zeros((targets.shape[0], self.num_classes)) targets_onehot[np.arange(targets.shape[0]), targets] = 1 inputs = inputs.astype(np.float32) inputs = inputs / 255.0 * 2 - 1 inputs = np.transpose(np.reshape(inputs, (-1, 3, 32, 32)), (0, 2, 3, 1)) super(CIFAR10DataProvider, self).__init__(inputs, targets, batch_size, max_num_batches, shuffle_order, rng) 
models|tdnn|enc|dec def enc_dec_tdnn(time_window, n_features, n_latent, hidden=None, rnn_opts= dict(), activation_func=tf.keras.layers.ELU(alpha=1.0)): """ timedelay-NN (not recurrent) """ if not hidden: hidden = [time_window, time_window] enc = tf.keras.Sequential() enc.add(tf.keras.layers.Flatten()) enc.add(tf.keras.layers.GaussianNoise(0.5, input_shape=(time_window,))) enc.add(tf.keras.layers.Dense(hidden[0], **rnn_opts)) enc.add(tf.keras.layers.BatchNormalization()) enc.add(tf.keras.layers.Activation(activation_func)) enc.add(tf.keras.layers.Dense(hidden[1], **rnn_opts)) enc.add(tf.keras.layers.BatchNormalization()) enc.add(tf.keras.layers.Activation(activation_func)) enc.add(tf.keras.layers.Dense(n_latent, input_shape=(time_window,), ** rnn_opts)) enc.add(tf.keras.layers.BatchNormalization()) enc.add(tf.keras.layers.Reshape((n_latent,))) dec = tf.keras.Sequential() dec.add(tf.keras.layers.Flatten()) dec.add(tf.keras.layers.GaussianNoise(0.5, input_shape=(n_latent,))) dec.add(tf.keras.layers.Dense(hidden[1], input_shape=(n_latent,), ** rnn_opts)) dec.add(tf.keras.layers.BatchNormalization()) dec.add(tf.keras.layers.Activation(activation_func)) dec.add(tf.keras.layers.Dense(hidden[0], **rnn_opts)) dec.add(tf.keras.layers.BatchNormalization()) dec.add(tf.keras.layers.Activation(activation_func)) dec.add(tf.keras.layers.Dense(n_latent, **rnn_opts)) dec.add(tf.keras.layers.BatchNormalization()) dec.add(tf.keras.layers.Activation(activation_func)) dec.add(tf.keras.layers.Reshape((time_window, n_features))) return enc, dec 
position|from|translate|read|utils|text def read_text_from_position(filename, from_position=None): with open(filename) as f: if from_position is not None: f.seek(from_position) while True: line = f.readline() if not line: break yield line, f.tell() 
one|examples|datasets|hot def _one_hot(x, k, dtype=np.float32): """Create a one-hot encoding of x of size k.""" return np.array(x[:, (None)] == np.arange(k), dtype) 
init|LossScalingOptimizer|mixprec def __init__(self, optimizer, scale=None, name='LossScalingOptimizer', use_locking=False): super(LossScalingOptimizer, self).__init__(name=name, use_locking= use_locking) self._optimizer = optimizer self._scale = float(scale) if scale is not None else 1.0 
off|avod|cls|ang|builder|loss|build|builders def _build_cls_off_ang_loss(model, prediction_dict): """Builds classification, offset, and angle vector losses.  Args: model: network model prediction_dict: prediction dictionary  Returns: losses_output: losses dictionary """ mb_cls_logits = prediction_dict[model.PRED_MB_CLASSIFICATION_LOGITS] mb_cls_softmax = prediction_dict[model.PRED_MB_CLASSIFICATION_SOFTMAX] mb_offsets = prediction_dict[model.PRED_MB_OFFSETS] mb_angle_vectors = prediction_dict[model.PRED_MB_ANGLE_VECTORS] mb_cls_gt = prediction_dict[model.PRED_MB_CLASSIFICATIONS_GT] mb_offsets_gt = prediction_dict[model.PRED_MB_OFFSETS_GT] mb_orientations_gt = prediction_dict[model.PRED_MB_ORIENTATIONS_GT] with tf.variable_scope('avod_gt_angle_vectors'): mb_angle_vectors_gt = (orientation_encoder. tf_orientation_to_angle_vector(mb_orientations_gt)) with tf.variable_scope('avod_losses'): with tf.variable_scope('classification'): cls_loss = _get_cls_loss(model, mb_cls_logits, mb_cls_gt) with tf.variable_scope('regression'): final_reg_loss, offset_loss_norm, ang_loss_norm = ( _get_off_ang_loss(model, mb_offsets, mb_offsets_gt, mb_angle_vectors, mb_angle_vectors_gt, mb_cls_softmax, mb_cls_gt)) with tf.variable_scope('avod_loss'): avod_loss = cls_loss + final_reg_loss tf.summary.scalar('avod_loss', avod_loss) losses_output = dict() losses_output[KEY_CLASSIFICATION_LOSS] = cls_loss losses_output[KEY_REGRESSION_LOSS] = final_reg_loss losses_output[KEY_AVOD_LOSS] = avod_loss losses_output[KEY_OFFSET_LOSS_NORM] = offset_loss_norm losses_output[KEY_ANG_LOSS_NORM] = ang_loss_norm return losses_output 
localizationPlot def localizationPlot(p_pred, y, n_samples=10, start_sample=0, dist_threshold=5, color_patch=['w', 'cornflowerblue'], bg_color=None, factor=2, bias=0, decimals=3): fig = plt.figure(figsize=(20, 9)) count_statistics = np.zeros([3]) time_steps = y.shape[2] for ii in range(p_pred.shape[0]): if ii < n_samples: ax = plt.subplot(4, np.ceil(n_samples / 4), ii + 1) for jj in range(y.shape[1]): list_hits = np.where(y[(ii + start_sample), (jj), :].astype(np. int16) == 1)[0] // factor list_hits_pred = np.where(p_pred[(ii + start_sample), (jj), :])[0 ] + bias distance_matrix = np.abs(np.tile(list_hits[(np.newaxis), :], [ len(list_hits_pred), 1]).T - np.tile(list_hits_pred[(np. newaxis), :], [len(list_hits), 1])) while distance_matrix.size > 0 and np.min(distance_matrix ) <= dist_threshold: idx_min = np.unravel_index(distance_matrix.argmin(), distance_matrix.shape) if ii < n_samples: plt.scatter(list_hits[idx_min[0]] / 200 * factor, jj, s =80, facecolors='none', edgecolors=np.reshape([0.4, 0.4, 0.4], [1, 3])) plt.scatter(list_hits_pred[idx_min[1]] / 200 * factor, jj, c='cornflowerblue', marker='*') list_hits = np.delete(list_hits, idx_min[0]) list_hits_pred = np.delete(list_hits_pred, idx_min[1]) distance_matrix = np.delete(np.delete(distance_matrix, idx_min[0], axis=0), idx_min[1], axis=1) count_statistics[0] += 1 if ii < n_samples: plt.scatter(list_hits / 200 * factor, [jj] * len(list_hits), s=80, facecolors='none', edgecolors='crimson') plt.scatter(list_hits_pred / 200 * factor, [jj] * len( list_hits_pred), c='crimson', marker='*') count_statistics[1] += len(list_hits) count_statistics[2] += len(list_hits_pred) if ii < n_samples: r2 = patches.Rectangle((-0.3, jj - 0.5), 8, 1) collection = PatchCollection([r2], facecolor=color_patch[jj % 2], alpha=0.05) ax.add_collection(collection) if bg_color is not None and bg_color[ii + start_sample] == 1: ax.set_axis_bgcolor([0.9, 0.9, 1]) if ii < n_samples: plt.xlim([-0.3, (time_steps + 2) / 200]) plt.yticks(np.arange(0), '') plt.ylim([-0.5, p_pred.shape[1] - 0.5]) ax.spines['right'].set_visible(False) ax.spines['top'].set_visible(False) plt.xlabel('Sample ' + str(ii), fontweight='bold') stats = {} TP = count_statistics[0] FN = count_statistics[1] FP = count_statistics[2] precision = nonZeroDivision(TP, TP + FP) recall = nonZeroDivision(TP, TP + FN) f1 = 2 * nonZeroDivision(precision * recall, precision + recall) print(np.round(f1, decimals), np.round(precision, decimals), np.round( recall, decimals)) stats['f1'] = f1 stats['precision'] = precision stats['recall'] = recall plt.text((time_steps + 2) / 100 * 0.5, -0.3, str(int(np.round(100 * precision, 0))) + '%/' + str(int(np.round(100 * recall, 0))) + '%', fontweight='bold') return fig, stats 
bert|openai|tokenization|len|OpenAIGPTTokenizer|pytorch|pretrained def __len__(self): return len(self.encoder) + len(self.special_tokens) 
bert|get|list|modeling|shape def get_shape_list(tensor, expected_rank=None, name=None): """Returns a list of the shape of tensor, preferring static dimensions.  Args: tensor: A tf.Tensor object to find the shape of. expected_rank: (optional) int. The expected rank of `tensor`. If this is specified and the `tensor` has a different rank, and exception will be thrown. name: Optional name of the tensor for the error message.  Returns: A list of dimensions of the shape of tensor. All static dimensions will be returned as python integers, and dynamic dimensions will be returned as tf.Tensor scalars. """ if name is None: name = tensor.name if expected_rank is not None: assert_rank(tensor, expected_rank, name) shape = tensor.shape.as_list() non_static_indexes = [] for index, dim in enumerate(shape): if dim is None: non_static_indexes.append(index) if not non_static_indexes: return shape dyn_shape = tf.shape(tensor) for index in non_static_indexes: shape[index] = dyn_shape[index] return shape 
parseGameBody|analysis|resign def parseGameBody(filename, fh, tfh, verbose, resignthr): gs = GameStats(filename) movenum = 0 while 1: movenum += 1 for _ in range(16): line = tfh.readline() if not line: break to_move = int(tfh.readline()) policy_weights = tfh.readline() side_to_move_won = int(tfh.readline()) if not gs.winner: if side_to_move_won == 1: gs.winner = to_move else: gs.winner = 1 - to_move netwinrate, root_uctwinrate, child_uctwinrate, bestmovevisits = (fh .readline().split()) netwinrate = float(netwinrate) root_uctwinrate = float(root_uctwinrate) child_uctwinrate = float(child_uctwinrate) bestmovevisits = int(bestmovevisits) if side_to_move_won == 1: if verbose >= 3: print('+', to_move, movenum, netwinrate, child_uctwinrate, bestmovevisits) if not gs.resign_type and child_uctwinrate < resignthr: if verbose >= 1: print( 'Wrong resign -- %s rt=%0.3f wr=%0.3f winner=%s movenum=%d' % (filename, resignthr, child_uctwinrate, to_move_str(to_move), movenum)) if verbose >= 3: print('policy_weights', policy_weights) gs.resign_type = 'Wrong' gs.resign_movenum = movenum else: if verbose >= 2: print('-', to_move, movenum, netwinrate, child_uctwinrate, bestmovevisits) if not gs.resign_type and child_uctwinrate < resignthr: if verbose >= 2: print('Correct resign -- %s' % filename) gs.resign_type = 'Correct' gs.resign_movenum = movenum gs.total_moves = movenum return gs 
texar|mle|clas|with|entropy|sigmoid|losses|binary|cross def binary_sigmoid_cross_entropy_with_clas(clas_fn, pos_inputs=None, neg_inputs=None, average_across_batch=True, average_across_classes=True, sum_over_batch=False, sum_over_classes=False, return_pos_neg_losses= False, name=None): """Computes sigmoid cross entropy of binary classifier.  .. role:: python(code) :language: python  Args: clas_fn: A callable takes data (e.g., :attr:`pos_inputs` and :attr:`fake_inputs`) and returns the logits of being positive. The signature of `clas_fn` must be: :python:`logits (, ...) = clas_fn(inputs)`. The return value of `clas_fn` can be the logits, or a tuple where the logits are the first element. pos_inputs: The positive data fed into `clas_fn`. neg_inputs: The negative data fed into `clas_fn`. average_across_batch (bool): If set, average the loss across the batch dimension. Must not set `average_across_batch`' and `sum_over_batch` at the same time. average_across_classes (bool): If set, average the loss across the class dimension (if exists). Must not set `average_across_classes`' and `sum_over_classes` at the same time. Ignored if :attr:`logits` is a 1D Tensor. sum_over_batch (bool): If set, sum the loss across the batch dimension. Must not set `average_across_batch` and `sum_over_batch` at the same time. sum_over_classes (bool): If set, sum the loss across the class dimension. Must not set `average_across_classes` and `sum_over_classes` at the same time. Ignored if :attr:`logits` is a 2D Tensor. return_pos_neg_losses (bool): If set, additionally returns the losses on :attr:`pos_logits` and :attr:`neg_logits`, respectively. name (str, optional): A name for the operation.  Returns: By default, a Tensor containing the loss, of rank 0, 1, or 2 depending on the arguments :attr:`{average_across}/{sum_over}_{batch}/{classes}`. For example:  - If :attr:`sum_over_batch` and :attr:`average_across_classes`              are `True` (default), the return Tensor is of rank 0.  - If  arguments are `False`, the return Tensor is of shape             `[batch_size(, num_classes)]`.  If :attr:`return_pos_neg_losses`=`True`, returns a tuple `(loss, pos_loss, neg_loss)`, where `loss` is the loss above; `pos_loss` is the loss on `pos_logits` only; and `neg_loss` is the loss on `neg_logits` only. They have `loss = pos_loss + neg_loss`. """ pos_logits = None if pos_inputs is not None: pos_logits = clas_fn(pos_inputs) if isinstance(pos_logits, (list, tuple)): pos_logits = pos_logits[0] neg_logits = None if neg_inputs is not None: neg_logits = clas_fn(neg_inputs) if isinstance(neg_logits, (list, tuple)): neg_logits = neg_logits[0] return binary_sigmoid_cross_entropy(pos_logits=pos_logits, neg_logits= neg_logits, average_across_batch=average_across_batch, average_across_classes=average_across_classes, sum_over_batch= sum_over_batch, sum_over_classes=sum_over_classes, return_pos_neg_losses=return_pos_neg_losses, name=name) 
strip|texar|utils|token def strip_token(str_, token, is_token_list=False, compat=True): """Returns a copy of strings with leading and trailing tokens removed.  Note that besides :attr:`token`, all leading and trailing whitespace characters are also removed.  If :attr:`is_token_list` is False, then the function assumes tokens in :attr:`str_` are separated with whitespace character.  Args: str\\_: A `str`, or an `n`-D numpy array or (possibly nested) list of `str`. token (str): The token to strip, e.g., the '<PAD>' token defined in :class:`~texar.data.SpecialTokens`.PAD is_token_list (bool): Whether each sentence in :attr:`str_` is a list of tokens. If False, each sentence in :attr:`str_` is assumed to contain tokens separated with space character. compat (bool): Whether to convert tokens into `unicode` (Python 2) or `str` (Python 3).  Returns: The stripped strings of the same structure/shape as :attr:`str_`.  Example:  .. code-block:: python  str_ = '<PAD> a sentence <PAD> <PAD>  ' str_stripped = strip_token(str_, '<PAD>') # str_stripped == 'a sentence'  str_ = ['<PAD>', 'a', 'sentence', '<PAD>', '<PAD>', '', ''] str_stripped = strip_token(str_, '<PAD>', is_token_list=True) # str_stripped == 'a sentence' """  def _recur_strip(s): if is_str(s): if token == '': return ' '.join(s.strip().split()) else: return ' '.join(s.strip().split()).replace(' ' + token, '' ).replace(token + ' ', '') else: s_ = [_recur_strip(si) for si in s] return _maybe_list_to_array(s_, s) s = str_ if compat: s = compat_as_text(s) if is_token_list: s = str_join(s, compat=False) strp_str = _recur_strip(s) if is_token_list: strp_str = _recur_split(strp_str, str_) return strp_str 
download|attributionpriors|feature|bytes|mnist def _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) 
pg|cartpole|main def _main(_): env = gym.make('CartPole-v0') env = env.unwrapped env_config = tx.agents.get_gym_env_config(env) agent = PGAgent(env_config, policy_kwargs={'action_space': env_config. action_space}, hparams=config.pg_agent_hparams) sess = tf.Session() agent.sess = sess sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer()) feed_dict = {tx.global_mode(): tf.estimator.ModeKeys.TRAIN} for e in range(300): reward_sum = 0.0 observ = env.reset() agent.reset() while True: action = agent.get_action(observ, feed_dict=feed_dict) next_observ, reward, terminal, _ = env.step(action=action) if terminal: reward = 0.0 agent.observe(reward, terminal, feed_dict=feed_dict) observ = next_observ reward_sum += reward if terminal: break if (e + 1) % 10 == 0: print('episode {}: {}'.format(e + 1, reward_sum)) sess.close() 
texar|DataIterator|get|iterators|next|data def get_next(self): """Returns the next element of the activated dataset. """ return self._iterator.get_next() 
BertConfig|bert|init|modeling def __init__(self, vocab_size, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act='gelu', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=16, initializer_range=0.02): """Constructs BertConfig.  Args: vocab_size: Vocabulary size of `inputs_ids` in `BertModel`. hidden_size: Size of the encoder layers and the pooler layer. num_hidden_layers: Number of hidden layers in the Transformer encoder. num_attention_heads: Number of attention heads for each attention layer in the Transformer encoder. intermediate_size: The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder. hidden_act: The non-linear activation function (function or string) in the encoder and pooler. hidden_dropout_prob: The dropout probability for all fully connected layers in the embeddings, encoder, and pooler. attention_probs_dropout_prob: The dropout ratio for the attention probabilities. max_position_embeddings: The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). type_vocab_size: The vocabulary size of the `token_type_ids` passed into `BertModel`. initializer_range: The stdev of the truncated_normal_initializer for initializing all weight matrices. """ self.vocab_size = vocab_size self.hidden_size = hidden_size self.num_hidden_layers = num_hidden_layers self.num_attention_heads = num_attention_heads self.hidden_act = hidden_act self.intermediate_size = intermediate_size self.hidden_dropout_prob = hidden_dropout_prob self.attention_probs_dropout_prob = attention_probs_dropout_prob self.max_position_embeddings = max_position_embeddings self.type_vocab_size = type_vocab_size self.initializer_range = initializer_range 
fisher|ops|classification|FullFB|blocks|registered|num|minibatches @property def num_registered_minibatches(self): return len(self._batch_sizes) 
rnn|texar|test|infer|decoder|modules|decoders|HelpersTest|helpers def test_infer_helpers(self): """Tests inference helpers. """  def _test_fn(helper): _, next_inputs, _ = helper.next_inputs(time=1, outputs=tf.ones([ self._batch_size, self._vocab_size]), state=None, sample_ids=tf .ones([self._batch_size], dtype=tf.int32)) self.assertEqual(helper.sample_ids_shape, tf.TensorShape([])) self.assertEqual(next_inputs.get_shape(), tf.TensorShape([self. _batch_size, self._emb_dim])) output_layer = tf.layers.Dense(self._vocab_size) decoder = BasicRNNDecoder(vocab_size=self._vocab_size, output_layer =output_layer) outputs, final_state, sequence_lengths = decoder(helper=helper, max_decoding_length=self._max_seq_length) cell_dim = decoder.hparams.rnn_cell.kwargs.num_units with self.test_session() as sess: sess.run(tf.global_variables_initializer()) outputs_, final_state_, sequence_lengths_ = sess.run([outputs, final_state, sequence_lengths]) max_length = max(sequence_lengths_) self.assertEqual(outputs_.logits.shape, (self._batch_size, max_length, self._vocab_size)) self.assertEqual(outputs_.sample_id.shape, (self._batch_size, max_length)) self.assertEqual(final_state_[0].shape, (self._batch_size, cell_dim)) helper = GreedyEmbeddingHelper(self._embedding, self._start_tokens, self._end_token) _test_fn(helper) embedder = WordEmbedder(self._embedding) helper = GreedyEmbeddingHelper(embedder, self._start_tokens, self. _end_token) _test_fn(helper) word_embedder = WordEmbedder(self._embedding) pos_embedder = PositionEmbedder(position_size=self._max_seq_length)  def _emb_fn(ids, times): return word_embedder(ids) + pos_embedder(times) helper = GreedyEmbeddingHelper(_emb_fn, self._start_tokens, self._end_token ) _test_fn(helper) 
build|model def build_model(self): self.patches_lab = tf.placeholder(tf.float32, [F.batch_size, self. patch_shape[0], self.patch_shape[1], self.patch_shape[2], F.num_mod ], name='real_images_l') self.patches_unlab = tf.placeholder(tf.float32, [F.batch_size, self. patch_shape[0], self.patch_shape[1], self.patch_shape[2], F.num_mod ], name='real_images_unl') self.z_gen = tf.placeholder(tf.float32, [None, F.noise_dim], name='noise') self.labels = tf.placeholder(tf.uint8, [F.batch_size, self.patch_shape[ 0], self.patch_shape[1], self.patch_shape[2]], name='image_labels') self.phase = tf.placeholder(tf.bool) self.labels_1hot = tf.one_hot(self.labels, depth=F.num_classes) self.patches_fake = self.generator(self.z_gen, self.phase) self.D_logits_lab, self.D_probdist, _ = self.discriminator(self. patches_lab, reuse=False) self.D_logits_unlab, _, self.features_unlab = self.discriminator(self. patches_unlab, reuse=True) self.D_logits_fake, _, self.features_fake = self.discriminator(self. patches_fake, reuse=True) self.Val_output = tf.argmax(self.D_probdist, axis=-1) class_weights = tf.constant([[0.33, 1.5, 0.83, 1.33]]) weights = tf.reduce_sum(class_weights * self.labels_1hot, axis=-1) unweighted_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits= self.D_logits_lab, labels=self.labels_1hot) weighted_losses = unweighted_losses * weights self.d_loss_lab = tf.reduce_mean(weighted_losses) self.unl_lsexp = tf.reduce_logsumexp(self.D_logits_unlab, -1) self.fake_lsexp = tf.reduce_logsumexp(self.D_logits_fake, -1) self.true_loss = -F.tlw * tf.reduce_mean(self.unl_lsexp ) + F.tlw * tf.reduce_mean(tf.nn.softplus(self.unl_lsexp)) self.fake_loss = F.flw * tf.reduce_mean(tf.nn.softplus(self.fake_lsexp)) self.d_loss_unlab = self.true_loss + self.fake_loss self.d_loss = self.d_loss_lab + self.d_loss_unlab self.g_loss_fm = tf.reduce_mean(tf.abs(tf.reduce_mean(self. features_unlab, 0) - tf.reduce_mean(self.features_fake, 0))) if F.badGAN: self.mu, self.log_sigma = self.encoder(self.patches_fake, self.phase) self.vi_loss = gaussian_nll(self.mu, self.log_sigma, self.z_gen) self.g_loss = self.g_loss_fm + F.vi_weight * self.vi_loss else: self.g_loss = self.g_loss_fm t_vars = tf.trainable_variables() self.d_vars = [var for var in t_vars if 'd_' in var.name] self.g_vars = [var for var in t_vars if 'g_' in var.name] if F.badGAN: self.e_vars = [var for var in t_vars if 'e_' in var.name] self.saver = tf.train.Saver() 
det|log|and|jacobian|AffineCouplingSdnEx1|inverse def _inverse_and_log_det_jacobian(self, y, yy, nlf0=None, nlf1=None, iso= None, cam=None): scale = sdn_model_params_ex1(yy, iso) x = y if scale is not None: x /= scale if scale is None: log_abs_det_J_inv = tf.constant(0.0, dtype=y.dtype, name='ildj') else: log_abs_det_J_inv = -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) if self._last_layer: return tf.layers.flatten(x), log_abs_det_J_inv return x, log_abs_det_J_inv 
block|basic|resnet18|ResNet18 def basic_block(self, net, channels, strides, use_bias, is_training, initializer, regularizer, weights_dict, weight_decay): """ channels should be a list of shape [2,] """ origin_input = net with tf.variable_scope('m1') as nsc: net = tf.layers.conv2d(net, channels[0], [3, 3], strides=strides, padding='same', use_bias=use_bias, kernel_initializer= initializer(), kernel_regularizer=regularizer(weight_decay)) self.restore_weights(nsc, 'conv', weights_dict) net = tf.layers.batch_normalization(net, axis=-1, training= is_training, epsilon=1e-05, momentum=0.997) self.restore_weights(nsc, 'bn', weights_dict) net = tf.nn.relu(net) with tf.variable_scope('m2') as nsc: net = tf.layers.conv2d(net, channels[1], [3, 3], strides=1, padding ='same', use_bias=use_bias, kernel_initializer=initializer(), kernel_regularizer=regularizer(weight_decay)) self.restore_weights(nsc, 'conv', weights_dict) net = tf.layers.batch_normalization(net, axis=-1, training= is_training, epsilon=1e-05, momentum=0.997) self.restore_weights(nsc, 'bn', weights_dict) if strides > 1 or origin_input.get_shape().as_list()[3] != channels[1]: with tf.variable_scope('shortcut') as nsc: origin_input = tf.layers.conv2d(origin_input, channels[1], [1, 1], strides=strides, padding='same', use_bias=use_bias, kernel_initializer=initializer(), kernel_regularizer= regularizer(weight_decay)) self.restore_weights(nsc, 'conv', weights_dict) origin_input = tf.layers.batch_normalization(origin_input, axis =-1, training=is_training, epsilon=1e-05, momentum=0.997) self.restore_weights(nsc, 'bn', weights_dict) net += origin_input net = tf.nn.relu(net) return net 
image|utils|get def get_image(image_path, input_height, input_width, resize_height=64, resize_width=64, crop=True, grayscale=False): image = imread(image_path, grayscale) return transform(image, input_height, input_width, resize_height, resize_width, crop) 
texar|shard|utils|fn|random|data|dataset def _shard_fn(dataset): sharded_dataset = tf.data.Dataset.from_tensor_slices(boundaries).shuffle( num_shards, seed=seed).flat_map(lambda lb: dataset.skip(lb).take( shard_size)) return sharded_dataset 
transformer|deeppoly|nodes|DeeppolyTanhNodeFirst def transformer(self, nn, man, element, nlb, nub, use_area_heuristic): """ transformer for the first layer of a neural network, if that first layer is fully connected with tanh  Arguments --------- man : ElinaManagerPtr man to which element belongs element : ElinaAbstract0Ptr abstract element onto which the transformer gets applied  Return ------ output : ElinaAbstract0Ptr abstract element after the transformer """ ffn_handle_first_tanh_layer(man, element, *self.get_arguments()) return element 
capitals|utils|espeakng|ESpeakNG @capitals.setter def capitals(self, v): self._capitals = v 
loss|tfprocess|TFProcess|tower def tower_loss(self, x, y_, z_): y_conv, z_conv = self.construct_net(x) if self.model_dtype != tf.float32: y_conv = tf.cast(y_conv, tf.float32) z_conv = tf.cast(z_conv, tf.float32) cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv) policy_loss = tf.reduce_mean(cross_entropy) mse_loss = tf.reduce_mean(tf.squared_difference(z_, z_conv)) reg_variables = tf.get_collection(tf.GraphKeys.WEIGHTS) reg_term = self.l2_scale * tf.add_n([tf.cast(tf.nn.l2_loss(v), tf. float32) for v in reg_variables]) loss = 1.0 * policy_loss + 1.0 * mse_loss + reg_term return loss, policy_loss, mse_loss, reg_term, y_conv 
src|learning|adapter|dual def dual_learning(self, dual_loss_alpha): """ dual learning for training adapter :return: """ print('#####add mse back!########') back_size = self.input.shape[-1] back_output = self.adapter_nn(self.adapter_output, back_size, self.name + '_back', self.forward_method) back_loss = tf.reduce_mean((back_output - self.input) ** 2, 1) self.back_loss = dual_loss_alpha * tf.reduce_mean(back_loss, 0) 
multi|kld|plot def plot_kld_multi(data_path, plot_dict, clrs): n_folders = len(plot_dict['folders']) xs = [None] * n_folders kld_g = None kld_nlf = None kld_nf = [None] * n_folders kld_r = None fpath = None maxlen = 0 for i in range(n_folders): subdir = plot_dict['folders'][i]['folder'] fpath = os.path.join(data_path, subdir) df_sample = get_sample_data(fpath) if df_sample is None: continue xs[i], kld_g_, kld_nlf_, kld_nf[i], kld_r_ = get_sample_kld(df_sample) if maxlen < len(kld_r_): maxlen = len(kld_r_) xg = xs[i] kld_g = kld_g_ kld_nlf = kld_nlf_ kld_r = kld_r_ fig = plt.figure(figsize=plot_dict['figsize']) bnll = [] plt.plot(xg, kld_g, label='Gaussian', linestyle='-.', color=clrs[0]) plt.plot(xg, kld_nlf, label='Camera NLF', linestyle='-.', color=clrs[1]) bnll.append(np.min(kld_g)) bnll.append(np.min(kld_nlf)) for i in range(n_folders): if xs[i] is None: print('skipping %d, %s' % (i, plot_dict['folders'][i]['folder'])) continue plt.plot(xs[i], kld_nf[i], label=plot_dict['folders'][i]['legend'], color=clrs[i + 2], linestyle='-') bnll.append(np.min(kld_nf[i])) plt.plot(xg, kld_r, label='Real noise', linestyle='--', color=clrs[3]) improv = [] for t in range(len(bnll)): improv.append((np.absolute(bnll[-1]) - np.absolute(bnll[t])) / np. absolute(bnll[t])) np.savetxt(os.path.join(fpath, 'best_kld.txt'), bnll) np.savetxt(os.path.join(fpath, 'best_improv.txt'), improv) if plot_dict['xlims'] is not None: plt.xlim(plot_dict['xlims']) if plot_dict['ylims'] is not None: plt.ylim(plot_dict['ylims']) plt.xlabel('Epoch') plt.ylabel('Marginal $D_{KL}$') if plot_dict['title'] is not None: plt.title(plot_dict['title']) plt.legend(loc='best', bbox_to_anchor=None, prop={'size': 14}, ncol=1, fancybox=False, shadow=False) plt.tight_layout() fig.savefig(os.path.join(fpath, '..', 'kld_' + plot_dict['fig_fn'] + '.png')) fig.savefig(os.path.join(fpath, '..', 'kld_' + plot_dict['fig_fn'] + '.pdf')) 
official|flags|pool|and|vgg|set|thread|loop|override|envars|for|gpu|run def override_flags_and_set_envars_for_gpu_thread_pool(flags_obj): """Override flags and set env_vars for performance.  These settings exist to test the difference between using stock settings and manual tuning. It also shows some of the ENV_VARS that can be tweaked to squeeze a few extra examples per second.  These settings are defaulted to the current platform of interest, which changes over time.  On systems with small numbers of cpu cores, e.g. under 8 logical cores, setting up a gpu thread pool with `tf_gpu_thread_mode=gpu_private` may perform poorly.  Args: flags_obj: Current flags, which will be adjusted possibly overriding what has been set by the user on the command-line. """ cpu_count = multiprocessing.cpu_count() tf.logging.info('Logical CPU cores: %s', cpu_count) per_gpu_thread_count = 1 total_gpu_thread_count = per_gpu_thread_count * flags_obj.num_gpus os.environ['TF_GPU_THREAD_MODE'] = flags_obj.tf_gpu_thread_mode os.environ['TF_GPU_THREAD_COUNT'] = str(per_gpu_thread_count) tf.logging.info('TF_GPU_THREAD_COUNT: %s', os.environ[ 'TF_GPU_THREAD_COUNT']) tf.logging.info('TF_GPU_THREAD_MODE: %s', os.environ['TF_GPU_THREAD_MODE']) main_thread_count = cpu_count - total_gpu_thread_count flags_obj.inter_op_parallelism_threads = main_thread_count num_monitoring_threads = 2 * flags_obj.num_gpus flags_obj.datasets_num_private_threads = (cpu_count - total_gpu_thread_count - num_monitoring_threads) 
helpers|load|w2v|data def load_w2v(w2v_file, embedding_dim, is_skip=False): fp = open(w2v_file) if is_skip: fp.readline() w2v = [] word_dict = dict() w2v.append([0.0] * embedding_dim) cnt = 0 for line in fp: cnt += 1 line = line.split() if len(line) != embedding_dim + 1: print('a bad word embedding: {}'.format(line[0])) cnt -= 1 continue w2v.append([float(v) for v in line[1:]]) word_dict[line[0]] = cnt w2v = np.asarray(w2v, dtype=np.float32) w2v = np.row_stack((w2v, np.sum(w2v, axis=0) / cnt)) print(np.shape(w2v)) word_dict['UNK'] = cnt + 1 print(word_dict['UNK'], len(w2v)) return word_dict, w2v 
v1|test|ResnetCompleteNetworkTest|nets|testAtrousFullyConvolutionalUnknownHeightWidth|resnet def testAtrousFullyConvolutionalUnknownHeightWidth(self): batch = 2 height, width = 65, 65 global_pool = False output_stride = 8 inputs = create_test_input(batch, None, None, 3) with slim.arg_scope(resnet_utils.resnet_arg_scope()): output, _ = self._resnet_small(inputs, None, global_pool= global_pool, output_stride=output_stride) self.assertListEqual(output.get_shape().as_list(), [batch, None, None, 32]) images = create_test_input(batch, height, width, 3) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) output = sess.run(output, {inputs: images.eval()}) self.assertEqual(output.shape, (batch, 9, 9, 32)) 
models|build|SampleAndAggregate|graphsage def build(self): self._build() self._loss() self._accuracy() self.loss = self.loss / tf.cast(self.batch_size, tf.float32) grads_and_vars = self.optimizer.compute_gradients(self.loss) clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var) for grad, var in grads_and_vars] self.grad, _ = clipped_grads_and_vars[0] self.opt_op = self.optimizer.apply_gradients(clipped_grads_and_vars) 
tests|test|check|utils|version def test_check_version(): check_version('0.1.0') check_version(20191231) 
lanenet|next|DataSet|data|batch|processor def next_batch(self, batch_size): return tf.train.batch([self._img, self._label_instance, self. _label_existence], batch_size=batch_size, num_threads=CFG.TRAIN.CPU_NUM ) 
functions|plot|graphs|tf def tf_plot_graphs(A): n_plots = len(A) columns = 1 rows = n_plots f = plt.figure(1) f.set_size_inches(12, 8) idx = 1 for title, item in A.items(): plt.subplot(rows, columns, idx) if idx % 2 == 0: plt.plot(item.eval()) else: plt.plot(item, 'r') plt.title(title) idx += 1 plt.subplots_adjust(hspace=0.4) plt.show() 
add|args|regression|user|custom|parameters def __add_regression_args(parser): """ keywords defined for regression tasks  :param parser: :return: """ parser.add_argument('--loss_border', metavar='', help= 'Set the border size for the loss function to ignore', type=int, default=0) parser.add_argument('--error_map', metavar='', help= 'Set whether to output the regression error maps (the maps will be stored in $model_dir/error_maps; the error maps can be used for window sampling).' , type=str2boolean, default=False) from niftynet.application.regression_application import SUPPORTED_INPUT parser = add_input_name_args(parser, SUPPORTED_INPUT) return parser 
transformer|deeppoly|nodes|DeeppolyConv2dNodeFirst def transformer(self, nn, man, element, nlb, nub, use_area_heuristic): """ transformer for a convolutional layer, if that layer is the first of the network  Arguments --------- man : ElinaManagerPtr man to which element belongs element : ElinaAbstract0Ptr abstract element onto which the transformer gets applied  Return ------ output : ElinaAbstract0Ptr abstract element after the transformer """ conv_handle_first_layer(man, element, *self.get_arguments()) bounds = box_for_layer(man, element, nn.ffn_counter + nn.conv_counter) num_neurons = get_num_neurons_in_layer(man, element, nn.ffn_counter + nn.conv_counter) lbi = [] ubi = [] for i in range(num_neurons): inf = bounds[i].contents.inf sup = bounds[i].contents.sup lbi.append(inf.contents.val.dbl) ubi.append(sup.contents.val.dbl) nlb.append(lbi) nub.append(ubi) elina_interval_array_free(bounds, num_neurons) nn.ffn_counter += 1 return element 
models|GaussianMixture|sample def sample(self, n_samples): n_samples = int(n_samples) if self.is_train: cat_probs = self._cat(n_samples) agg_mu = tf.reduce_sum(tf.expand_dims(cat_probs, 2) * self._mu, axis=1) agg_var = tf.reduce_sum(tf.expand_dims(cat_probs, 2) * self._var, axis=1) raw = tf.random_normal([n_samples, self.dim]) ret = agg_mu + tf.sqrt(agg_var) * raw else: cat_samples = self._cat.sample(n_samples) samples_raw_indices = array_ops.reshape(math_ops.range(0, n_samples ), cat_samples.get_shape().as_list()) partitioned_samples_indices = data_flow_ops.dynamic_partition(data= samples_raw_indices, partitions=cat_samples, num_partitions= self.n_components) samples_class = [None for _ in range(self.n_components)] for c in range(self.n_components): n_class = array_ops.size(partitioned_samples_indices[c]) raw = tf.random_normal([n_class, self.dim]) samples_class_c = self._mu[c] + raw * tf.sqrt(self._var[c]) samples_class[c] = samples_class_c ret = data_flow_ops.dynamic_stitch(indices= partitioned_samples_indices, data=samples_class) ret.set_shape((int(n_samples), self.dim)) return ret 
get|SimpleQA|emb|src|load|data|word def get_word_emb(self, emb_path, voc_path): tf.logging.info('===>load pre-trained word emb and voc<===') self.word_embedding = np.load(emb_path) self.word_vocabulary = pkl.load(open(voc_path, 'rb')) 
len|RegressionDataset|regression|data|misc|loader def __len__(self): return self._data_x.shape[0] 
cpplint|RepositoryName|FileInfo def RepositoryName(self): """FullName after removing the local path to the repository.  If we have a real absolute path name here we can try to do something smart: detecting the root of the checkout and truncating /path/to/checkout from the name so that we get header guards that don't include things like "C:\\Documents and Settings\\..." or "/home/username/..." in them and thus people on different computers who have checked the source out to different locations won't see bogus errors. """ fullname = self.FullName() if os.path.exists(fullname): project_dir = os.path.dirname(fullname) if os.path.exists(os.path.join(project_dir, '.svn')): root_dir = project_dir one_up_dir = os.path.dirname(root_dir) while os.path.exists(os.path.join(one_up_dir, '.svn')): root_dir = os.path.dirname(root_dir) one_up_dir = os.path.dirname(one_up_dir) prefix = os.path.commonprefix([root_dir, project_dir]) return fullname[len(prefix) + 1:] root_dir = os.path.dirname(fullname) while root_dir != os.path.dirname(root_dir) and not os.path.exists(os .path.join(root_dir, '.git')) and not os.path.exists(os.path. join(root_dir, '.hg')) and not os.path.exists(os.path.join( root_dir, '.svn')): root_dir = os.path.dirname(root_dir) if os.path.exists(os.path.join(root_dir, '.git')) or os.path.exists(os .path.join(root_dir, '.hg')) or os.path.exists(os.path.join( root_dir, '.svn')): prefix = os.path.commonprefix([root_dir, project_dir]) return fullname[len(prefix) + 1:] return fullname 
CIN|compute|output|interaction|deepctr|layers|shape def compute_output_shape(self, input_shape): if self.split_half: featuremap_num = sum(self.layer_size[:-1]) // 2 + self.layer_size[-1] else: featuremap_num = sum(self.layer_size) return None, featuremap_num 
cleverhans|tf|attacks|jacobian|graph def jacobian_graph(predictions, x, nb_classes): """ Create the Jacobian graph to be ran later in a TF session :param predictions: the model's symbolic output (linear output, pre-softmax) :param x: the input placeholder :param nb_classes: the number of classes the model has :return: """ list_derivatives = [] for class_ind in xrange(nb_classes): derivatives, = tf.gradients(predictions[:, (class_ind)], x) list_derivatives.append(derivatives) return list_derivatives 
setupCamera|utils def setupCamera(v, cameraParams): chDistMat = geometry.Translate(x=0, y=cameraParams['Zshift'], z=0) chRotElMat = geometry.RotateX(a=-cameraParams['chCamEl']) chCamModelWorld = ch.dot(chRotElMat, chDistMat) flipZYRotation = np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 0, 1.0, 0.0], [ 0.0, -1.0, 0, 0.0], [0.0, 0.0, 0.0, 1.0]]) chMVMat = ch.dot(chCamModelWorld, flipZYRotation) chInvCam = ch.inv(chMVMat) modelRotation = chInvCam[0:3, 0:3] chRod = opendr.geometry.Rodrigues(rt=modelRotation).reshape(3) chTranslation = chInvCam[0:3, (3)] translation, rotation = chTranslation, chRod camera = ProjectPoints(v=v, rt=rotation, t=translation, f=1000 * cameraParams['chCamFocalLength'] * cameraParams['a'], c= cameraParams['c'], k=ch.zeros(5)) flipXRotation = np.array([[1.0, 0.0, 0.0, 0.0], [0.0, -1.0, 0.0, 0.0], [0.0, 0.0, -1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]) camera.openglMat = flipXRotation return camera, modelRotation, chMVMat 
triangles|RenderTest|test|testInternalRenderGradientComputation|rasterize def testInternalRenderGradientComputation(self): """Isolates and verifies the Jacobian matrix for the custom kernel.""" image_height = 21 image_width = 28 clip_coordinates = tf.placeholder(tf.float32, shape=[8, 4]) barycentric_coordinates, _, _ = (rasterize_triangles. rasterize_triangles_module.rasterize_triangles(clip_coordinates, self.cube_triangles, image_width, image_height)) with self.test_session(): ndc_init = np.array([[-0.43889722, -0.53184521, 0.85293502, 1.0], [ -0.37635487, 0.22206162, 0.90555805, 1.0], [-0.22849123, 0.76811147, 0.80993629, 1.0], [-0.2805393, -0.14092168, 0.71602166, 1.0], [0.18631913, -0.62634289, 0.88603103, 1.0], [ 0.16183566, 0.08129397, 0.93020856, 1.0], [0.44147962, 0.53497446, 0.85076219, 1.0], [0.53008741, -0.31276882, 0.77620775, 1.0]], dtype=np.float32) theoretical, numerical = tf.test.compute_gradient(clip_coordinates, (8, 4), barycentric_coordinates, (image_height, image_width, 3), x_init_value=ndc_init, delta=0.04) jacobians_match, message = test_utils.check_jacobians_are_nearly_equal( theoretical, numerical, 0.01, 0.01) self.assertTrue(jacobians_match, message) 
forward|det|log|and|jacobian|AffineCouplingCondYG def _forward_and_log_det_jacobian(self, x, yy, nlf0=None, nlf1=None, iso= None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) shift, log_scale = self._shift_and_log_scale_fn(yy, iso) log_scale = self.scale * tf.tanh(log_scale) y = x if shift is not None: y -= shift if log_scale is not None: y *= tf.exp(-log_scale) if log_scale is None: log_abs_det_J = tf.constant(0.0, dtype=x.dtype, name='fldj') else: log_abs_det_J = -tf.reduce_sum(log_scale, axis=[1, 2, 3]) return y, log_abs_det_J 
ImageReader|Segment|image|reader|init|deeplab|segment|resnet def __init__(self, data_dir, data_list, input_size, seed, random_scale, random_mirror, num_classes, adapt, coord): """Initialise an ImageReader.  Args: data_dir: path to the directory with images and masks. data_list: path to the file with lines of the form '/path/to/image /path/to/attn /path/to/sal /path/to/image-labels' input_size: a tuple with (height, width) values, to which all the images will be resized. seed: Random seed. random_scale: whether to randomly scale the images prior to random crop. random_mirror: whether to randomly mirror the images prior to random crop. num_classes: Total number of classes (including background) in the dataset. adapt: Whether to obtain adapted ground truth cues (True/ False). coord: TensorFlow queue coordinator. """ self.data_dir = data_dir self.data_list = data_list self.input_size = input_size self.seed = seed self.coord = coord self.image_list, self.attn_list, self.sal_list, self.catg_list = ( read_labeled_image_list(self.data_dir, self.data_list)) self.images = tf.convert_to_tensor(self.image_list, dtype=tf.string) self.attns = tf.convert_to_tensor(self.attn_list, dtype=tf.string) self.saliencys = tf.convert_to_tensor(self.sal_list, dtype=tf.string) self.catgs = tf.convert_to_tensor(self.catg_list, dtype=tf.string) self.queue = tf.train.slice_input_producer([self.images, self.attns, self.saliencys, self.catgs], shuffle=input_size is not None) self.image, self.label, self.catg = read_images_from_disk(self.queue, self.input_size, random_scale, random_mirror, self.seed, num_classes, adapt) 
step|BowSeq2seq|train|bow|seq2seq|enc def enc_train_step(self, sess, batch_dict): """Single step training on encoder""" max_len = batch_dict['enc_inputs'].shape[1] if self.bow_pred_method == 'seq2seq': feed_dict = {self.enc_inputs: batch_dict['enc_inputs'], self. enc_lens: batch_dict['enc_lens'], self.enc_targets: batch_dict[ 'enc_targets'], self.enc_seq2seq_inputs: batch_dict[ 'enc_seq2seq_inputs'], self.enc_seq2seq_targets: batch_dict[ 'enc_seq2seq_targets'], self.enc_seq2seq_lens: batch_dict[ 'enc_seq2seq_lens'], self.dec_bow: batch_dict['dec_bow'], self. max_len: max_len, self.drop_out: 0.3} else: feed_dict = {self.enc_inputs: batch_dict['enc_inputs'], self. enc_lens: batch_dict['enc_lens'], self.enc_targets: batch_dict[ 'enc_targets'], self.dec_bow: batch_dict['dec_bow'], self. max_len: max_len, self.drop_out: 0.3} enc_train_output = sess.run(self.enc_train_output, feed_dict=feed_dict) return enc_train_output 
europilot|filename|train|Writer @property def filename(self): return self._filename 
alignment|argparser|UCR def argparser(): parser = argparse.ArgumentParser(description='Process args') parser.add_argument('--tess_size', type=int, default=16, help= 'CPA velocity field partition') parser.add_argument('--smoothness_prior', default=True, help= 'smoothness prior flag', action='store_true') parser.add_argument('--no_smoothness_prior', dest='smoothness_prior', default=True, help='no smoothness prior flag', action='store_false') parser.add_argument('--lambda_smooth', type=float, default=1, help= 'lambda_smooth, larger values -> smoother warps') parser.add_argument('--lambda_var', type=float, default=0.1, help= 'lambda_var, larger values -> larger warps') parser.add_argument('--n_recurrences', type=int, default=1, help= 'number of recurrences of R-DTAN') parser.add_argument('--zero_boundary', type=bool, default=True, help= 'zero boundary constrain') parser.add_argument('--n_epochs', type=int, default=1000, help= 'number of epochs') args = parser.parse_args() return args 
bim|wrapper|ResNetModelWrapper|init|model|resnet def __init__(self, sess, model_dir, categories_file): """ Args: categories_file: file path that contains alphabetically sorted categories and their corresponding index. e.g. /a/abbey 0 """ self.model_name = 'ResNet' self.sess = sess image_shape = 224, 224, 3 super(ResNetModelWrapper, self).__init__(image_shape) tf.saved_model.loader.load(sess, ['serve'], model_dir) self.labels = [line.split(' ')[0] for line in tf.gfile.Open( categories_file).read().splitlines()] node_dict = {'input': 'input_tensor', 'block_layer1': 'resnet_model/block_layer1', 'block_layer2': 'resnet_model/block_layer2', 'block_layer3': 'resnet_model/block_layer3', 'block_layer4': 'resnet_model/block_layer4', 'logit': 'resnet_model/final_dense'} self.bottlenecks_tensors = {} for endpoint in node_dict: self.bottlenecks_tensors[endpoint] = sess.graph.get_operation_by_name( node_dict[endpoint]).outputs[0] self.bottleneck_names = self.bottlenecks_tensors.keys() self.ends = {} self.ends['input'] = self.bottlenecks_tensors['input'] self.ends['logit'] = self.bottlenecks_tensors['logit'] self.ends['prediction'] = self.ends['logit'] with sess.graph.as_default(): self.y_input = tf.placeholder(tf.int64, shape=[None]) self.pred = tf.expand_dims(self.ends['prediction'][0], 0) self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( labels=tf.one_hot(self.y_input, len(self.labels)), logits=self. pred)) self._make_gradient_tensors() 
Logger|translate|logger|init|summary def __init__(self, log_dir): """Creates a summary writer logging to log_dir.""" self.writer = tf.summary.FileWriter(log_dir) 
tangents|prediction|predict|neural|mean|einsum def _mean_prediction_einsum(evecs, op_evals, g_td, y_train): """Einsum powered version of _mean_prediction.""" fl, ufl = _make_flatten_uflatten(g_td, y_train) mean_pred = np.einsum('lj,ji,i,ki,k...->l...', g_td, evecs, op_evals, evecs, fl(y_train), optimize=True) return ufl(mean_pred) 
conll|NERInstance|reader|length def length(self): return self.sentence.length() 
FSM|conv2d|bn|relu def conv2d_bn_relu(input, filters, kernel_size, strides=(1, 1), padding= 'same', dilation_rate=(1, 1), kernel_initializer='he_normal', kernel_regularizer=l2(1e-05)): x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate, kernel_initializer= kernel_initializer, kernel_regularizer=kernel_regularizer)(input) x = BatchNormalization()(x) x = ReLU()(x) return x 
test|vgg|testFullyConvolutional|nets|VGG16Test def testFullyConvolutional(self): batch_size = 1 height, width = 256, 256 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False) self.assertEquals(logits.op.name, 'vgg_16/fc8/BiasAdd') self.assertListEqual(logits.get_shape().as_list(), [batch_size, 2, 2, num_classes]) 
gen|bbans|pixelvae|pixelcnn|pvae|net2|layer|two def gen_net2_pixelcnn(x_partial, theta_): logits_ = session.run(logits, feed_dict={theta: theta_, images_tf: x_partial, bn_is_training: False, bn_stats_iter: 0, total_iters: 99999} ) return softmax(logits_) 
match|FixedStrPattern|download|speech|corpus def match(self, target): """ Check the given string matches the pattern. """ return target == self.expected 
models|tests|test|PNN @pytest.mark.parametrize('use_inner, use_outter,sparse_feature_num', [(True, True, 1), (True, False, 2), (False, True, 3), (False, False, 1)]) def test_PNN(use_inner, use_outter, sparse_feature_num): model_name = 'PNN' sample_size = SAMPLE_SIZE x, y, feature_dim_dict = get_test_data(sample_size, sparse_feature_num, sparse_feature_num) model = PNN(feature_dim_dict, embedding_size=8, dnn_hidden_units=[32, 32], dnn_dropout=0.5, use_inner=use_inner, use_outter=use_outter) check_model(model, model_name, x, y) 
vggface|tmp|visualize|master|facenet|showarray def showarray(a): a = np.uint8(np.clip(a, 0, 1) * 255) plt.imshow(a) plt.show() 
DropMaskType2|commons|theta|measure|sample def sample_theta(self, hparams): raise NotImplementedError 
similarity|get|datasets|Verb143 def get_Verb143(): data = pd.read_csv(Verb143_path, sep='\t', header=None).values return Bunch(X=data[:, 0:2].astype('object'), y=data[:, (2)].astype(np. float) * 10.0) 
initialise|extensions|multi|MultiModalApplication|modal|application|sampler def initialise_sampler(self): if self.is_training: self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]() elif self.is_inference: self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]() 
net|test|get|stax def _get_net(W_std, b_std, filter_shape, is_conv, use_pooling, is_res, padding, phi, strides, width, is_ntk, proj_into_2d, layer_norm): fc = partial(stax.Dense, W_std=W_std, b_std=b_std) conv = partial(stax.Conv, filter_shape=filter_shape, strides=strides, padding=padding, W_std=W_std, b_std=b_std) affine = conv(width) if is_conv else fc(width) res_unit = stax.serial(stax.AvgPool((2, 3), None, 'SAME' if padding == 'SAME' else 'CIRCULAR') if use_pooling else stax.Identity(), phi, affine) if is_res: block = stax.serial(affine, stax.FanOut(2), stax.parallel(stax. Identity(), res_unit), stax.FanInSum(), stax.Identity() if layer_norm is None else stax.LayerNorm(axis=layer_norm)) else: block = stax.serial(affine, res_unit, stax.Identity() if layer_norm is None else stax.LayerNorm(axis=layer_norm)) if proj_into_2d == 'FLAT': proj_layer = stax.Flatten() elif proj_into_2d == 'POOL': proj_layer = stax.GlobalAvgPool() elif proj_into_2d.startswith('ATTN'): n_heads = int(np.sqrt(width)) n_chan_val = int(np.round(float(width) / n_heads)) fixed = proj_into_2d == 'ATTN_FIXED' proj_layer = stax.serial(stax.GlobalSelfAttention(width, n_chan_key =width, n_chan_val=n_chan_val, n_heads=n_heads, fixed=fixed, W_key_std=W_std, W_value_std=W_std, W_query_std=W_std, W_out_std=1.0, b_std=b_std), stax.Flatten()) else: raise ValueError(proj_into_2d) readout = stax.serial(proj_layer, fc(1 if is_ntk else width)) return stax.serial(block, readout) 
DailyDialogChatbot|dialog|create|chatbot|data|daily def create_data(self, train_mode): """ Params: :train_mode: Whether we are in train or dev mode. """ (trainSource, trainTarget, devSource, devTarget, testSource, testTarget ) = self.open_6_files() dialogs = open(os.path.join(self._raw_data, 'dialogues_text.txt'), errors='ignore') vocabulary = Counter() number_of_dialogs = 0 line_counter = 0 dataset_split_counter = 0 for dialog in dialogs: dataset_split_counter += 1 if number_of_dialogs % 1000 == 0: print('t2t_csaky_log: Parsed ' + str(number_of_dialogs) + ' dialogs.') utterances = dialog.split('__eou__')[:-1] if dataset_split_counter <= self.dataset_split['train']: source_file = trainSource target_file = trainTarget elif dataset_split_counter <= self.dataset_split['train' ] + self.dataset_split['val']: source_file = devSource target_file = devTarget else: source_file = testSource target_file = testTarget i = 0 for utterance in utterances: line_counter += 1 utterance = self.clean_line(utterance.lower()) i += 1 if dataset_split_counter <= self.dataset_split['train']: words = utterance.split() for word in words: if word in vocabulary: vocabulary[word] += 1 else: vocabulary[word] = 1 if i != len(utterances): source_file.write(utterance + '\n') if i != 1: target_file.write(utterance + '\n') number_of_dialogs += 1 if dataset_split_counter == 100: dataset_split_counter = 0 if (self.targeted_dataset_size != 0 and self.targeted_dataset_size < line_counter): break self.close_n_files([trainSource, trainTarget, devSource, devTarget, testSource, testTarget]) dialogs.close() self.save_vocab(vocabulary) 
print|log|and|setup|read def log_setup(args_in): """ Function to setup the logging hash table. """ log = dict() log['times'] = [] log['losses'] = [] log['cluster_quality'] = [] log['params'] = vars(args_in) return log 
imagenet|official|fn|resnet|input|ImageNetTFExampleInput def input_fn(self, params): """Input function which provides a single batch for train or eval.  Args: params: `dict` of parameters passed from the `TPUEstimator`. `params['batch_size']` is always provided and should be used as the effective batch size.  Returns: A `tf.data.Dataset` object. """ batch_size = params['batch_size'] if 'context' in params: current_host = params['context'].current_input_fn_deployment()[1] num_hosts = params['context'].num_hosts else: current_host = 0 num_hosts = 1 dataset = self.make_source_dataset(current_host, num_hosts) dataset = dataset.apply(tf.contrib.data.map_and_batch(self. dataset_parser, batch_size=batch_size, num_parallel_batches=self. num_parallel_calls, drop_remainder=True)) if self.transpose_input: dataset = dataset.map(lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels), num_parallel_calls=self.num_parallel_calls) dataset = dataset.map(functools.partial(self.set_shapes, batch_size)) dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE) return dataset 
codecs|test|mixture|craystack|logistic def test_logistic_mixture(): precision = 12 batch_size = 2 nr_mix = 10 shape = batch_size, nr_mix means, log_scales, logit_probs = rng.randn(*shape), rng.randn(*shape ), rng.randn(*shape) means = means + 100 log_scales = log_scales - 10 data = np.array([rng.choice(256) for _ in range(batch_size)]).astype( 'uint64') check_codec((shape[0],), cs.LogisticMixture_UnifBins(means, log_scales, logit_probs, precision, bin_prec=8, bin_lb=-1.0, bin_ub=1.0), data) 
utils|visualize def visualize(sess, dcgan, config, option): image_frame_dim = int(math.ceil(config.batch_size ** 0.5)) if option == 0: if config.z_uniform: z_sample = np.random.uniform(-1, 1, size=(config.batch_size, dcgan.z_dim)) else: z_sample = np.random.normal(0, config.z_std, size=(config. batch_size, dcgan.z_dim)) samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample}) save_images(samples, [image_frame_dim, image_frame_dim], ('./' + config.sample_dir + '/test_%s.png') % strftime('%Y%m%d%H%M%S', gmtime())) elif option == 5: totalImagesToSave = 50000 savedImages = 0 for idx in xrange(1500): if config.z_uniform: sample_z = np.random.uniform(-1, 1, size=(config.batch_size, dcgan.z_dim)) else: sample_z = np.random.normal(0, config.z_std, size=(config. batch_size, dcgan.z_dim)) samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: sample_z}) samples = inverse_transform(samples) samples_separated = np.split(samples, config.batch_size, axis=0) dir = './' + config.dataset + '_samples_single_images' for j in xrange(config.batch_size): filename_tmp = (dir + '/%s.png') % savedImages scipy.misc.imsave(filename_tmp, samples_separated[j][0]) savedImages = savedImages + 1 if savedImages >= totalImagesToSave: return elif option == 1: values = np.arange(0, 1, 1.0 / config.batch_size) for idx in xrange(100): print(' [*] %d' % idx) z_sample = np.zeros([config.batch_size, dcgan.z_dim]) for kdx, z in enumerate(z_sample): z[idx] = values[kdx] if config.dataset == 'mnist': y = np.random.choice(10, config.batch_size) y_one_hot = np.zeros((config.batch_size, 10)) y_one_hot[np.arange(config.batch_size), y] = 1 samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample, dcgan.y: y_one_hot}) else: samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample} ) save_images(samples, [image_frame_dim, image_frame_dim], './samples/test_arange_%s.png' % idx) elif option == 2: values = np.arange(0, 1, 1.0 / config.batch_size) for idx in [random.randint(0, 99) for _ in xrange(100)]: print(' [*] %d' % idx) z = np.random.uniform(-0.2, 0.2, size=dcgan.z_dim) z_sample = np.tile(z, (config.batch_size, 1)) for kdx, z in enumerate(z_sample): z[idx] = values[kdx] if config.dataset == 'mnist': y = np.random.choice(10, config.batch_size) y_one_hot = np.zeros((config.batch_size, 10)) y_one_hot[np.arange(config.batch_size), y] = 1 samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample, dcgan.y: y_one_hot}) else: samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample} ) try: make_gif(samples, './samples/test_gif_%s.gif' % idx) except: save_images(samples, [image_frame_dim, image_frame_dim], './samples/test_%s.png' % strftime('%Y%m%d%H%M%S', gmtime())) elif option == 3: values = np.arange(0, 1, 1.0 / config.batch_size) for idx in xrange(100): print(' [*] %d' % idx) z_sample = np.zeros([config.batch_size, dcgan.z_dim]) for kdx, z in enumerate(z_sample): z[idx] = values[kdx] samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample}) make_gif(samples, './samples/test_gif_%s.gif' % idx) elif option == 4: image_set = [] values = np.arange(0, 1, 1.0 / config.batch_size) for idx in xrange(100): print(' [*] %d' % idx) z_sample = np.zeros([config.batch_size, dcgan.z_dim]) for kdx, z in enumerate(z_sample): z[idx] = values[kdx] image_set.append(sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})) make_gif(image_set[-1], './samples/test_gif_%s.gif' % idx) new_image_set = [merge(np.array([images[idx] for images in image_set]), [10, 10]) for idx in range(64) + range(63, -1, -1)] make_gif(new_image_set, './samples/test_gif_merged.gif', duration=8) 
tensorflow|Network|kaffe|network|relu @layer def relu(self, input, name): return tf.nn.relu(input, name=name) 
array|log|from|parse|append|data def append_array(array_a, array_b, axis=0): if array_a.size == 0: array_a = array_a.astype(array_b.dtype) array_a = array_a.reshape((0,) + array_b.shape[1:]) array_a = np.concatenate([array_a, array_b], axis) return array_a 
createDarc|dataset|train|Dataset def createDarc(self): """Loads a dataset of images from disk and stores it in the Darc format. Darc is similar in idea to HDF5 but much simpler and should handle multi-process parallelization issues better.""" print("Reading dataset '{}' from disk.".format(self.getCacheKey())) archive = darc.DataArchive(self.getDarcPath(), 'w') judge_files = cached_listdir(os.path.join(self.full_path, 'judge')) judge_files = [os.path.join(self.full_path, 'judge', file) for file in judge_files if os.path.splitext(file)[1] == '.npy'] judge_files = sorted(judge_files) p0_files = cached_listdir(os.path.join(self.full_path, 'p0')) p0_files = [os.path.join(self.full_path, 'p0', file) for file in p0_files if os.path.splitext(file)[1] == '.png'] p0_files = sorted(p0_files) p1_files = cached_listdir(os.path.join(self.full_path, 'p1')) p1_files = [os.path.join(self.full_path, 'p1', file) for file in p1_files if os.path.splitext(file)[1] == '.png'] p1_files = sorted(p1_files) ref_files = cached_listdir(os.path.join(self.full_path, 'ref')) ref_files = [os.path.join(self.full_path, 'ref', file) for file in ref_files if os.path.splitext(file)[1] == '.png'] ref_files = sorted(ref_files)  def handle_task(judge_path, p0_path, p1_path, ref_path): judge = np.load(judge_path)[0] p0 = load_image_uint(p0_path) p1 = load_image_uint(p1_path) ref = load_image_uint(ref_path) assert len(p0.shape) >= 3 and p0.shape[2] >= 3 assert len(p1.shape) >= 3 and p1.shape[2] >= 3 assert len(ref.shape) >= 3 and ref.shape[2] >= 3 p0 = p0[:, :, 0:3] p1 = p1[:, :, 0:3] ref = ref[:, :, 0:3] if self.dataset_mode == '2afc' and p0.shape[0:2] != (self.load_size, self.load_size): p0 = skimage.transform.resize(p0, [self.load_size, self. load_size, 3], mode='reflect', anti_aliasing=False) p1 = skimage.transform.resize(p1, [self.load_size, self. load_size, 3], mode='reflect', anti_aliasing=False) ref = skimage.transform.resize(ref, [self.load_size, self. load_size, 3], mode='reflect', anti_aliasing=False) return judge, p0, p1, ref, judge_path, p0_path, p1_path, ref_path if not all(len(collection) == len(judge_files) for collection in ( p0_files, p1_files, ref_files)): raise Exception('Dataset files missing!') with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: postponed_image_sets = [] for i in range(len(judge_files)): judge_path, p0_path, p1_path, ref_path = judge_files[i], p0_files[i ], p1_files[i], ref_files[i] postponed_image_sets.append((judge_path, p0_path, p1_path, ref_path)) queued_image_sets = [] i = 0 while queued_image_sets or postponed_image_sets: while len(queued_image_sets) < 20 and postponed_image_sets: judge_path, p0_path, p1_path, ref_path = postponed_image_sets[0 ] queued_image_sets.append(executor.submit(handle_task, judge_path, p0_path, p1_path, ref_path)) postponed_image_sets.pop(0) judge, p0, p1, ref, judge_path, p0_path, p1_path, ref_path = ( queued_image_sets[0].result()) queued_image_sets.pop(0) print('{}/{}'.format(i, len(judge_files))) p_tensor = np.stack([p0, p1, ref]) archive.append(p_tensor, chunks=[1, -1, -1, -1], name='{}_p'. format(i)) judge_tensor = np.asarray(judge).reshape([1]) archive.append(judge_tensor, name='{}_judge'.format(i)) i += 1 archive.close() 
src|init|adapter def __init__(self, input, target, keep_prob, gather_index, name): """ :param input: a tensor :param target: a tensor :param keep_prob: dropout layer in adapter :param gather_index: the seen index of input :param name: relation or word """ self.input = input self.target = target self.keep_prob = keep_prob self.gather_index = gather_index self.name = name self.write_log = False self.target_size = self.target.shape[-1] 
test|tf|setup|alig|Test def setup_tf(self): tf.reset_default_graph() self.model_tf = ModelTf(self.w1, self.b1, self.w2, self.b2) y_one_hot = np.zeros((self.n_samples, self.n_classes), dtype=np.float32) y_one_hot[np.arange(0, self.n_samples), self.y] = 1 self.feed_dict_tf = {self.model_tf.X: self.x, self.model_tf.Y: y_one_hot} 
bayesian|kl|regression|controller|learning|BayesianLearning @property def kl(self): """ KL divergence of the variational posterior and prior. :return: tensor of shape [n_particles] """ kl = 0.0 for l in self._net.layers: if not hasattr(l, 'kl_exact'): kl = kl + tf.reduce_mean(l.kl_appro) else: kl = kl + l.kl_exact if hasattr(l, 'kl_gamma'): kl = kl + l.kl_gamma if hasattr(self._net.outsample, 'kl_exact'): kl = kl + self._net.outsample.kl_exact return kl 
dcgan|discriminator|test|DCGANTest|graph|nets def test_discriminator_graph(self): for i, batch_size in zip(xrange(1, 6), xrange(3, 8)): tf.reset_default_graph() img_w = 2 ** i image = tf.random_uniform([batch_size, img_w, img_w, 3], -1, 1) output, end_points = dcgan.discriminator(image, depth=32) self.assertAllEqual([batch_size, 1], output.get_shape().as_list()) expected_names = [('conv%i' % j) for j in xrange(1, i + 1)] + ['logits' ] self.assertSetEqual(set(expected_names), set(end_points.keys())) for j in range(1, i + 1): layer = end_points['conv%i' % j] self.assertEqual(32 * 2 ** (j - 1), layer.get_shape().as_list()[-1] ) 
greedy|translate|model|decoding|Seq2SeqModel|seq2seq def greedy_decoding(self, token_ids, align=False, beam_size=1): for model in self.models: model.dropout_off.run() data = [(ids + [[] for _ in self.decoders] if len(ids) == len(self. encoders) else ids) for ids in token_ids] batch = self.get_batch(data, decoding=True) encoder_inputs, targets, input_length = batch input_feed = {self.beam_size: beam_size} for model in self.models: input_feed[model.targets] = targets input_feed[model.feed_previous] = 1.0 input_feed[model.training] = False for i in range(len(model.encoders)): input_feed[model.encoder_inputs[i]] = encoder_inputs[i] input_feed[model.encoder_input_length[i]] = input_length[i] output_feed = {'outputs': self.beam_outputs} if align: output_feed['weights'] = self.attention_weights res = tf.get_default_session().run(output_feed, input_feed) return [res['outputs'][:, (0), :]], res.get('weights') 
functions|atomic|inputs|set def set_atomic_inputs(input_event, category, data_loader, text_encoder): XMB = torch.zeros(1, data_loader.max_event + 1).long().to(cfg.device) prefix, suffix = data.atomic_data.do_example(text_encoder, input_event, None, True, None) XMB[:, :len(prefix)] = torch.LongTensor(prefix) XMB[:, (-1)] = torch.LongTensor([text_encoder.encoder['<{}>'.format( category)]]) batch = {} batch['sequences'] = XMB batch['attention_mask'] = data.atomic_data.make_attention_mask(XMB) return batch 
nets|pix2pix|discriminator def pix2pix_discriminator(net, num_filters, padding=2, is_training=False): """Creates the Image2Image Translation Discriminator.  Args: net: A `Tensor` of size [batch_size, height, width, channels] representing the input. num_filters: A list of the filters in the discriminator. The length of the list determines the number of layers in the discriminator. padding: Amount of reflection padding applied before each convolution. is_training: Whether or not the model is training or testing.  Returns: A logits `Tensor` of size [batch_size, N, N, 1] where N is the number of 'patches' we're attempting to discriminate and a dictionary of model end points. """ del is_training end_points = {} num_layers = len(num_filters)  def padded(net, scope): if padding: with tf.variable_scope(scope): spatial_pad = tf.constant([[0, 0], [padding, padding], [ padding, padding], [0, 0]], dtype=tf.int32) return tf.pad(net, spatial_pad, 'REFLECT') else: return net with tf.contrib.framework.arg_scope([layers.conv2d], kernel_size=[4, 4], stride=2, padding='valid', activation_fn=tf.nn.leaky_relu): net = layers.conv2d(padded(net, 'conv0'), num_filters[0], normalizer_fn=None, scope='conv0') end_points['conv0'] = net for i in range(1, num_layers - 1): net = layers.conv2d(padded(net, 'conv%d' % i), num_filters[i], scope='conv%d' % i) end_points['conv%d' % i] = net net = layers.conv2d(padded(net, 'conv%d' % (num_layers - 1)), num_filters[-1], stride=1, scope='conv%d' % (num_layers - 1)) end_points['conv%d' % (num_layers - 1)] = net logits = layers.conv2d(padded(net, 'conv%d' % num_layers), 1, stride=1, activation_fn=None, normalizer_fn=None, scope= 'conv%d' % num_layers) end_points['logits'] = logits end_points['predictions'] = tf.sigmoid(logits) return logits, end_points 
test|testTrainEvalWithReuse|InceptionTest|inception|nets|v2|resnet def testTrainEvalWithReuse(self): train_batch_size = 5 eval_batch_size = 2 height, width = 150, 150 num_classes = 1000 with self.test_session() as sess: train_inputs = tf.random_uniform((train_batch_size, height, width, 3)) inception.inception_resnet_v2(train_inputs, num_classes) eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3)) logits, _ = inception.inception_resnet_v2(eval_inputs, num_classes, is_training=False, reuse=True) predictions = tf.argmax(logits, 1) sess.run(tf.global_variables_initializer()) output = sess.run(predictions) self.assertEquals(output.shape, (eval_batch_size,)) 
size|state|attention|model|PCGNWrapper|PCGN @property def state_size(self): """The `state_size` property of `PCGNWrapper`. Returns: An `PCGNWrapperState` tuple containing shapes used by this object. """ return PCGNWrapperState(cell_state=self._cell.state_size, time= tensor_shape.TensorShape([]), attention=self._attention_layer_size, user_feat_mem=self._user_feat_mem_units, alignments=self. _item_or_tuple(a.alignments_size for a in self. _attention_mechanisms), alignment_history=self._item_or_tuple(() for _ in self._attention_mechanisms)) 
args|eval|parse def parse_args(): parser = argparse.ArgumentParser(description='Run model evaluation.') parser.add_argument('--data', type=str, help='data in TFRecord format') parser.add_argument('--vocab', type=str, required=True, help= 'vocabulary table, listing vocabulary line by line') parser.add_argument('--norm', type=str, default=None, help= 'normalization params') parser.add_argument('--mapping', type=str, help= 'additional mapping when evaluation') parser.add_argument('--model_dir', type=str, required=True, help= 'path of saving model') parser.add_argument('--batch_size', type=int, default=8, help='batch size') parser.add_argument('--num_channels', type=int, default=39, help= 'number of input channels') parser.add_argument('--binf_map', type=str, default='misc/binf_map.csv', help='Path to CSV with phonemes to binary features map') return parser.parse_args() 
post1|bbans|pixelvae|codec|elem|pvae|layer|two def post1_elem_codec(params, idx): return cs.substack(codecs.DiagGaussian_GaussianBins(params[..., 0], params[..., 1], params[..., 2], params[..., 3], q_precision, prior_precision), lambda head: head[idx]) 
texar|networks|test|conv|modules|seq|length|Conv1DNetworkTest|unknown def test_unknown_seq_length(self): """Tests use of pooling layer when the seq_length dimension of inputs is `None`. """ network_1 = Conv1DNetwork() inputs_1 = tf.placeholder(tf.float32, [64, None, 300]) outputs_1 = network_1(inputs_1) self.assertEqual(outputs_1.shape, [64, 128]) hparams = {'num_conv_layers': 2, 'filters': 128, 'kernel_size': [[3, 4, 5], 4], 'pooling': 'AveragePooling', 'pool_size': [2, None], 'num_dense_layers': 1, 'dense_size': 10} network = Conv1DNetwork(hparams) self.assertEqual(len(network.layers), 1 + 1 + 1 + 1 + 1 + 1) self.assertTrue(isinstance(network.layer_by_name('pool_2'), tx.core. AverageReducePooling1D)) inputs = tf.placeholder(tf.float32, [64, None, 300]) outputs = network(inputs) self.assertEqual(outputs.shape, [64, 10]) hparams_2 = {'num_conv_layers': 1, 'filters': 128, 'kernel_size': 4, 'other_conv_kwargs': {'data_format': 'channels_first'}, 'pooling': 'MaxPooling', 'other_pool_kwargs': {'data_format': 'channels_first' }, 'num_dense_layers': 1, 'dense_size': 10} network_2 = Conv1DNetwork(hparams_2) inputs_2 = tf.placeholder(tf.float32, [64, 300, None]) outputs_2 = network_2(inputs_2) self.assertEqual(outputs_2.shape, [64, 10]) 
PlotFunc|f2 def f2(c, L): return r * (L + 1) 
collection|approximation|ops|LayerCollection|classification|layer|conv2d|default @property def default_conv2d_approximation(self): return self._default_convolution_2d_approximation 
tmp|n|split|lap|master|facenet|main|deepdream def lap_split_n(img, n): """Build Laplacian pyramid with n splits""" levels = [] for _ in range(n): img, hi = lap_split(img) levels.append(hi) levels.append(img) return levels[::-1] 
europilot|FlowController|train|init def __init__(self, inq): threading.Thread.__init__(self) self.daemon = True self._inq = inq self._acquired = False 
xlnet|pair|seq|master|truncate|utils|classifier def _truncate_seq_pair(tokens_a, tokens_b, max_length): """Truncates a sequence pair in place to the maximum length.""" while True: total_length = len(tokens_a) + len(tokens_b) if total_length <= max_length: break if len(tokens_a) > len(tokens_b): tokens_a.pop() else: tokens_b.pop() 
viz|pointclouds|show def show_pointclouds(points, colors, text=[], title='Default', png_path='', interactive=True, orientation='horizontal'): """ Show multiple point clouds specified as lists. First clouds at the bottom. :param points: list of pointclouds, item: numpy (N x 3) XYZ :param colors: list of corresponding colors, item: numpy (N x 3) RGB [0..255] :param title: window title :param text: text per point cloud :param png_path: where to save png image :param interactive: wether to display window or not, useful if you only want to take screenshot :return: nothing """ assert isinstance(points, type([])), 'Pointclouds argument must be a list' assert isinstance(colors, type([])), 'Colors argument must be a list' assert len(points) == len(colors ), 'Number of pointclouds (%d) is different then number of colors (%d)' % ( len(points), len(colors)) while len(text) < len(points): text.append('') num_pointclouds = len(points) point_size = 4 pointclouds = [VtkPointCloud(point_size) for _ in range(num_pointclouds)] renderers = [vtk.vtkRenderer() for _ in range(num_pointclouds)] height = 1.0 / max(num_pointclouds, 1) viewports = [(i * height, (i + 1) * height) for i in range(num_pointclouds) ] for i, pc in enumerate(points): pc = pc.squeeze() co = colors[i].squeeze() assert pc.shape[0] == co.shape[0 ], 'expected same number of points (%d) then colors (%d), cloud index = %d' % ( pc.shape[0], co.shape[0], i) assert pc.shape[1 ] == 3, 'expected points to be N x 3, got N x %d' % pc.shape[1] assert co.shape[1 ] == 3, 'expected colors to be N x 3, got N x %d' % co.shape[1] for j in range(pc.shape[0]): point = pc[(j), :] color = co[(j), :] pointclouds[i].add_point(point, color) renderers[i].AddActor(pointclouds[i].vtkActor) renderers[i].AddActor(vtk.vtkAxesActor()) renderers[i].SetBackground(1.0, 1.0, 1.0) if orientation == 'horizontal': renderers[i].SetViewport(viewports[i][0], 0.0, viewports[i][1], 1.0 ) elif orientation == 'vertical': renderers[i].SetViewport(0.0, viewports[i][0], 1.0, viewports[i][1] ) else: raise Exception('Not a valid orientation!') renderers[i].ResetCamera() renderers[0].AddActor(getActorCircle()) renderers[0].AddActor(getActorCircle(50, 49, color=(0, 1, 0))) text_actors = [vtk.vtkTextActor() for _ in text] for i, ta in enumerate(text_actors): if orientation == 'horizontal': ta.SetInput('                ' + text[i]) elif orientation == 'vertical': ta.SetInput(text[i] + '\n\n\n\n\n\n') else: raise Exception('Not a valid orientation!') txtprop = ta.GetTextProperty() txtprop.SetFontFamilyToArial() txtprop.SetFontSize(20) txtprop.SetColor(0, 0, 0) renderers[i].AddActor(ta) render_window = vtk.vtkRenderWindow() for renderer in renderers: render_window.AddRenderer(renderer) render_window_interactor = vtk.vtkRenderWindowInteractor() render_window_interactor.SetInteractorStyle(vtk. vtkInteractorStyleTrackballCamera()) render_window_interactor.SetRenderWindow(render_window) [center_x, center_y, center_z] = np.mean(points[0].squeeze(), axis=0) camera = vtk.vtkCamera() camera.SetViewUp(0, 0, 1) if orientation == 'horizontal': camera.SetPosition(3, -10, 2) camera.SetFocalPoint(3, 1.5, 1.5) elif orientation == 'vertical': camera.SetPosition(1.5, -6, 2) camera.SetFocalPoint(1.5, 1.5, 1.5) else: raise Exception('Not a valid orientation!') camera.SetClippingRange(0.002, 1000) for renderer in renderers: renderer.SetActiveCamera(camera) render_window.Render() render_window.SetWindowName(title) if orientation == 'horizontal': render_window.SetSize(1800, 300) elif orientation == 'vertical': render_window.SetSize(600, 1388) else: raise Exception('Not a valid orientation!') if interactive: render_window_interactor.Start() if png_path: w2if = vtk.vtkWindowToImageFilter() w2if.SetInput(render_window) w2if.Update() writer = vtk.vtkPNGWriter() writer.SetFileName(png_path) writer.SetInputConnection(w2if.GetOutputPort()) writer.Write() 
xlnet|estimator|tpu|init|master|TPUInfeedOutfeedSessionHook def __init__(self, ctx, enqueue_ops, dequeue_ops, tpu_compile_op, run_infeed_loop_on_coordinator=True, rendezvous=None, master=None, session_config=None): self._master_job = ctx.master_job self._enqueue_ops = enqueue_ops self._dequeue_ops = dequeue_ops self._rendezvous = rendezvous self._master = master self._session_config = session_config self._run_infeed_loop_on_coordinator = run_infeed_loop_on_coordinator self._initial_infeed_sleep_secs = (ctx.config.tpu_config. initial_infeed_sleep_secs) self._feed_error = None self._finished = False self._should_initialize_tpu = True self._tpu_compile_op = tpu_compile_op 
avod|optimizer|builder|create|rate|learning|builders def _create_learning_rate(learning_rate_config, global_summaries, global_step): """Create optimizer learning rate based on config.  Args: learning_rate_config: A LearningRate proto message. global_summaries: A set to attach learning rate summary to. global_step: A tensor that contains the global step.  Returns: A learning rate.  Raises: ValueError: when using an unsupported input data type. """ learning_rate = None learning_rate_type = learning_rate_config.WhichOneof('learning_rate') if learning_rate_type == 'constant_learning_rate': config = learning_rate_config.constant_learning_rate learning_rate = config.learning_rate elif learning_rate_type == 'exponential_decay_learning_rate': config = learning_rate_config.exponential_decay_learning_rate learning_rate = tf.train.exponential_decay(config. initial_learning_rate, global_step, config.decay_steps, config. decay_factor, staircase=config.staircase) if learning_rate is None: raise ValueError('Learning_rate %s not supported.' % learning_rate_type ) global_summaries.add(tf.summary.scalar('Learning_Rate', learning_rate)) return learning_rate 
linear|weight|ratio|utils|thumt def weight_ratio_linear(inputs, weights, output, bias=None, stab=0): """ inputs: [(..., dim_in_i)] weights: [(dim_in_i, dim_out)] bias: [(dim_out)] output: (..., dim_out) weight ratios: [(..., dim_in_i, dim_out)] """ assert len(inputs) == len(weights) output_shape = [] for i in range(len(inputs)): os = tf.concat([tf.shape(inputs[i]), tf.shape(weights[i])[-1:]], -1) output_shape.append(os) inputs = [tf.reshape(inp, [-1, inp.shape[-1].value]) for inp in inputs] output = tf.reshape(output, [-1, output.shape[-1].value]) weight_ratios = [] for i in range(len(inputs)): r = tf.expand_dims(inputs[i], -1) * tf.expand_dims(weights[i], -3) w = r / tf.expand_dims(stabilize(output, stab), -2) weight_ratios.append(w) weight_ratios = [tf.reshape(wr, os) for os, wr in zip(output_shape, weight_ratios)] return weight_ratios 
ml|data|balance def balance_data(pairs, classes, n_proportion): classes = np.array(classes) pairs = np.array(pairs) indices_true = np.where(classes == 1)[0] indices_false = np.where(classes == 0)[0] np.random.shuffle(indices_false) indices = indices_false[:n_proportion * indices_true.shape[0]] print('+/-:', len(indices_true), len(indices), len(indices_false)) pairs = np.concatenate((pairs[indices_true], pairs[indices]), axis=0) classes = np.concatenate((classes[indices_true], classes[indices]), axis=0) return pairs, classes 
build|vgg|Vgg16 def build(self, rgb): """ load variable from npy to build the VGG :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1] """ start_time = time.time() print('build model started') rgb_scaled = rgb * 255.0 red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled) assert red.get_shape().as_list()[1:] == [224, 224, 1] assert green.get_shape().as_list()[1:] == [224, 224, 1] assert blue.get_shape().as_list()[1:] == [224, 224, 1] bgr = tf.concat(axis=3, values=[blue - VGG_MEAN[0], green - VGG_MEAN[1], red - VGG_MEAN[2]]) assert bgr.get_shape().as_list()[1:] == [224, 224, 3] self.conv1_1 = self.conv_layer(bgr, 'conv1_1') self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2') self.pool1 = self.max_pool(self.conv1_2, 'pool1') self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1') self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2') self.pool2 = self.max_pool(self.conv2_2, 'pool2') self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1') self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2') self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3') self.pool3 = self.max_pool(self.conv3_3, 'pool3') self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1') self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2') self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3') self.pool4 = self.max_pool(self.conv4_3, 'pool4') self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1') self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2') self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3') self.pool5 = self.max_pool(self.conv5_3, 'pool5') self.fc6 = self.fc_layer(self.pool5, 'fc6') assert self.fc6.get_shape().as_list()[1:] == [4096] self.relu6 = tf.nn.relu(self.fc6) self.fc7 = self.fc_layer(self.relu6, 'fc7') self.relu7 = tf.nn.relu(self.fc7) self.fc8 = self.fc_layer(self.relu7, 'fc8') self.prob = tf.nn.softmax(self.fc8, name='prob') self.data_dict = None print('build model finished: %ds' % (time.time() - start_time)) 
cleverhans|tf|vatm|attacks def vatm(model, x, logits, eps, num_iterations=1, xi=1e-06, clip_min=None, clip_max=None, scope=None): """ Tensorflow implementation of the perturbation method used for virtual adversarial training: https://arxiv.org/abs/1507.00677 :param model: the model which returns the network unnormalized logits :param x: the input placeholder :param logits: the model's unnormalized output tensor (the input to the softmax layer) :param eps: the epsilon (input variation parameter) :param num_iterations: the number of iterations :param xi: the finite difference parameter :param clip_min: optional parameter that can be used to set a minimum value for components of the example returned :param clip_max: optional parameter that can be used to set a maximum value for components of the example returned :param seed: the seed for random generator :return: a tensor for the adversarial example """ with tf.name_scope(scope, 'virtual_adversarial_perturbation'): d = tf.random_normal(tf.shape(x)) for i in range(num_iterations): d = xi * utils_tf.l2_batch_normalize(d) logits_d = model.get_logits(x + d) kl = utils_tf.kl_with_logits(logits, logits_d) Hd = tf.gradients(kl, d)[0] d = tf.stop_gradient(Hd) d = eps * utils_tf.l2_batch_normalize(d) adv_x = x + d if clip_min is not None and clip_max is not None: adv_x = tf.clip_by_value(adv_x, clip_min, clip_max) return adv_x 
fisher|ops|dtype|classification|factors|ConvOutputKroneckerFactor @property def _dtype(self): return self._outputs_grads[0].dtype 
core|avod|evaluator|losses|save|Evaluator|proposal|results def save_proposal_losses_results(self, eval_rpn_losses, num_valid_samples, global_step, predictions_base_dir): """Helper function to save the RPN loss evaluation results. """ sum_rpn_obj_loss = eval_rpn_losses[KEY_SUM_RPN_OBJ_LOSS] sum_rpn_reg_loss = eval_rpn_losses[KEY_SUM_RPN_REG_LOSS] sum_rpn_total_loss = eval_rpn_losses[KEY_SUM_RPN_TOTAL_LOSS] sum_rpn_obj_accuracy = eval_rpn_losses[KEY_SUM_RPN_OBJ_ACC] avg_rpn_obj_loss = sum_rpn_obj_loss / num_valid_samples avg_rpn_reg_loss = sum_rpn_reg_loss / num_valid_samples avg_rpn_total_loss = sum_rpn_total_loss / num_valid_samples avg_rpn_obj_accuracy = sum_rpn_obj_accuracy / num_valid_samples print( 'Step {}: Average RPN Losses: objectness {:.3f}, regression {:.3f}, total {:.3f}' .format(global_step, avg_rpn_obj_loss, avg_rpn_reg_loss, avg_rpn_total_loss)) print('Step {}: Average Objectness Accuracy:{} '.format(global_step, avg_rpn_obj_accuracy)) avg_loss_file_path = predictions_base_dir + '/rpn_avg_losses.csv' with open(avg_loss_file_path, 'ba') as fp: np.savetxt(fp, np.reshape([global_step, avg_rpn_obj_loss, avg_rpn_reg_loss, avg_rpn_total_loss], (1, 4)), fmt= '%d, %.5f, %.5f, %.5f') avg_acc_file_path = predictions_base_dir + '/rpn_avg_obj_acc.csv' with open(avg_acc_file_path, 'ba') as fp: np.savetxt(fp, np.reshape([global_step, avg_rpn_obj_accuracy], (1, 2)), fmt='%d, %.5f') 
forward|nn|stochastic|StoSequential def forward(self, input, alpha): for module in self._modules.values(): input = module(input, alpha) return input 
td|alg|not|NetworkCreator|input|or|config|layers def _config_input(self, input_layer): if input_layer is None: assert self._connector is not None return self._connector else: return input_layer 
evaluators|ndcg|eval|NDCGEvaluator def eval(self, reco_items, k=50, on_train=False): """ Compute the Top-K nDCG for a particular user given the predicted scores to items. :param reco_items: :param k: :param on_train: :return: ndcg@k """ ndcg = [] for user_id, tops in reco_items.items(): train_set = self.train_user_items.get(user_id, set()) test_set = self.test_user_items.get(user_id, set()) ref_set = train_set if on_train else test_set top_n_items = 0 user_hits = [] for i in tops: if not on_train and i in train_set: continue pred = 1 if i in ref_set else 0 user_hits.append(pred) top_n_items += 1 if top_n_items == k: break user_hits = np.array(user_hits, dtype=np.float32) if len(ref_set) >= k: ideal_rels = np.ones(k) else: ideal_rels = np.pad(np.ones(len(ref_set)), (0, k - len(ref_set) ), 'constant') ndcg.append(self.dcg_at_k(user_hits, k) / self.dcg_at_k(ideal_rels, k)) return ndcg 
open|build|vocab def _open(filename, mode='r', encoding='utf-8'): if sys.version_info.major == 2: return open(filename, mode=mode) elif sys.version_info.major == 3: return open(filename, mode=mode, encoding=encoding) else: raise RuntimeError('Unknown Python version for running!') 
print|WaypointsMinJerkPolicy|waypts|policy def print(self): print('WaypointsMinJerkTorquePolicy\n', self.waypts) 
compute|output|sequence|deepctr|DynamicGRU|layers|shape def compute_output_shape(self, input_shape): rnn_input_shape = input_shape[0] if self.return_sequence: return rnn_input_shape else: return None, 1, rnn_input_shape[2] 
DifferentialEvolutionSolver|models|differential|evolution|mutate def _mutate(self, candidate): """ create a trial vector based on a mutation strategy """ trial = np.copy(self.population[candidate]) rng = self.random_number_generator fill_point = rng.randint(0, self.parameter_count) if self.strategy in ['currenttobest1exp', 'currenttobest1bin']: bprime = self.mutation_func(candidate, self._select_samples( candidate, 5)) else: bprime = self.mutation_func(self._select_samples(candidate, 5)) if self.strategy in self._binomial: crossovers = rng.rand(self.parameter_count) crossovers = crossovers < self.cross_over_probability crossovers[fill_point] = True trial = np.where(crossovers, bprime, trial) return trial elif self.strategy in self._exponential: i = 0 while i < self.parameter_count and rng.rand( ) < self.cross_over_probability: trial[fill_point] = bprime[fill_point] fill_point = (fill_point + 1) % self.parameter_count i += 1 return trial 
finetuning|lm|BERTDataset|init|run def __init__(self, corpus_path, tokenizer, seq_len, encoding='utf-8', corpus_lines=None, on_memory=True): self.vocab = tokenizer.vocab self.tokenizer = tokenizer self.seq_len = seq_len self.on_memory = on_memory self.corpus_lines = corpus_lines self.corpus_path = corpus_path self.encoding = encoding self.current_doc = 0 self.sample_counter = 0 self.line_buffer = None self.current_random_doc = 0 self.num_docs = 0 self.sample_to_doc = [] if on_memory: self.all_docs = [] doc = [] self.corpus_lines = 0 with open(corpus_path, 'r', encoding=encoding) as f: for line in tqdm(f, desc='Loading Dataset', total=corpus_lines): line = line.strip() if line == '': self.all_docs.append(doc) doc = [] self.sample_to_doc.pop() else: sample = {'doc_id': len(self.all_docs), 'line': len(doc)} self.sample_to_doc.append(sample) doc.append(line) self.corpus_lines = self.corpus_lines + 1 if self.all_docs[-1] != doc: self.all_docs.append(doc) self.sample_to_doc.pop() self.num_docs = len(self.all_docs) else: if self.corpus_lines is None: with open(corpus_path, 'r', encoding=encoding) as f: self.corpus_lines = 0 for line in tqdm(f, desc='Loading Dataset', total=corpus_lines ): if line.strip() == '': self.num_docs += 1 else: self.corpus_lines += 1 if line.strip() != '': self.num_docs += 1 self.file = open(corpus_path, 'r', encoding=encoding) self.random_file = open(corpus_path, 'r', encoding=encoding) 
neural|tangents|argspec|utils def _argspec(func): """Python 2 and 3 compatible argspec.""" if six.PY3: return inspect.getfullargspec(func) else: return inspect.getargspec(func) 
cli|command|parse def parse_command(): parser = argparse.ArgumentParser() _add_dataset_parser(parser) _add_model_parser(parser) _add_optimization_parser(parser) _add_loss_parser(parser) _add_misc_parser(parser) args = parser.parse_args() filter_args(args) return args 
image|sample|get|split def get_sample(data_r, data_i, label, points, image_size): data_lst, data_lst_raw, label_lst = [[], []], [[], []], [] for p in points: x_raw_r = data_r[p[0]:p[0] + image_size, p[1]:p[1] + image_size, :] x_raw_i = data_i[p[0]:p[0] + image_size, p[1]:p[1] + image_size, :] x_r = image_add_border(x_raw_r, [image_size + 6, image_size + 6]) x_i = image_add_border(x_raw_i, [image_size + 6, image_size + 6]) l = label[p[0]:p[0] + image_size, p[1]:p[1] + image_size] if p[2] == 0: data_lst[0].append(x_r) data_lst[1].append(x_i) data_lst_raw[0].append(x_raw_r) data_lst_raw[1].append(x_raw_i) label_lst.append(l) elif p[2] == 1: a, b = rot(np.copy(x_raw_r), np.copy(l), 1) data_lst_raw[0].append(a) data_lst[0].append(image_add_border(a, [image_size + 6, image_size + 6])) label_lst.append(b) a, b = rot(np.copy(x_raw_i), np.copy(l), 1) data_lst_raw[1].append(a) data_lst[1].append(image_add_border(a, [image_size + 6, image_size + 6])) else: a, b = flip(np.copy(x_raw_r), np.copy(l)) data_lst_raw[0].append(a) data_lst[0].append(image_add_border(a, [image_size + 6, image_size + 6])) label_lst.append(b) a, b = flip(np.copy(x_raw_i), np.copy(l)) data_lst_raw[1].append(a) data_lst[1].append(image_add_border(a, [image_size + 6, image_size + 6])) return data_lst, data_lst_raw, label_lst 
gan|out|dim|JointLatent|latent|reg @property def reg_out_dim(self): return self._reg_out_dim 
SetCountingStyle|cpplint def _SetCountingStyle(level): """Sets the module's counting options.""" _cpplint_state.SetCountingStyle(level) 
tangents|width|stax|flip|neural|height def _flip_height_width(kernels): """Flips the order of spatial axes in the covariance matrices.  Args: kernels: a `Kernel` object.  Returns: A `Kernel` object with `height` and `width` axes order flipped in all covariance matrices. For example, if `kernels.nngp` has shape `[batch_size_1, batch_size_2, height, height, width, width]`, then `_flip_height_width(kernels).nngp` has shape `[batch_size_1, batch_size_2, width, width, height, height]`. """ var1, nngp, var2, ntk, is_height_width, marginal, cross = (kernels.var1, kernels.nngp, kernels.var2, kernels.ntk, kernels.is_height_width, kernels.marginal, kernels.cross)  def flip_5or6d(mat): return np.moveaxis(mat, (-2, -1), (-4, -3)) if marginal == M.OVER_PIXELS: var1 = np.transpose(var1, (0, 2, 1)) var2 = np.transpose(var2, (0, 2, 1)) if var2 is not None else var2 elif marginal in [M.OVER_POINTS, M.NO]: var1 = flip_5or6d(var1) var2 = flip_5or6d(var2) if var2 is not None else var2 else: raise NotImplementedError( 'Only implemented for `OVER_PIXELS`, `OVER_POINTS` and `NO`; supplied {}' .format(marginal)) if cross == M.OVER_PIXELS: nngp = np.moveaxis(nngp, -1, -2) ntk = np.moveaxis(ntk, -1, -2) if _is_array(ntk) else ntk elif cross in [M.OVER_POINTS, M.NO]: nngp = flip_5or6d(nngp) ntk = flip_5or6d(ntk) if _is_array(ntk) else ntk return kernels._replace(var1=var1, nngp=nngp, var2=var2, ntk=ntk, is_height_width=not is_height_width) 
hparam|add|HParams def add_hparam(self, name, value): """Adds {name, value} pair to hyperparameters.  Args: name: Name of the hyperparameter. value: Value of the hyperparameter. Can be one of the following types: int, float, string, int list, float list, or string list.  Raises: ValueError: if one of the arguments is invalid. """ if getattr(self, name, None) is not None: raise ValueError('Hyperparameter name is reserved: %s' % name) if isinstance(value, (list, tuple)): if not value: raise ValueError( 'Multi-valued hyperparameters cannot be empty: %s' % name) self._hparam_types[name] = type(value[0]), True else: self._hparam_types[name] = type(value), False setattr(self, name, value) 
utils|get|test|layer def get_layer(model, name): for layer in model.layers: if layer.name == name: return layer 
predict|data2text|latent|LatentBowData2text|bow def predict(self, sess, batch_dict): """Single step prediction""" feed_dict = {self.enc_keys: batch_dict['enc_keys'], self.enc_vals: batch_dict['enc_vals'], self.enc_locs: batch_dict['enc_locs'], self .enc_lens: batch_dict['enc_lens'], self.dec_targets: batch_dict[ 'dec_targets'], self.drop_out: 0.0, self.gumbel_tau: 1e-05} output_dict = sess.run(self.infer_output, feed_dict=feed_dict) return output_dict 
compute|perplexity|PCGNModel|model|PCGN def compute_perplexity(self, sess, batch): feed_dict = {self.encoder_inputs: batch[0], self.encoder_length: batch[ 1], self.decoder_inputs: batch[2], self.decoder_length: batch[3], self.decoder_targets: batch[4], self.decoder_targets_masks: batch[5 ], self.user_desc: batch[6], self.desc_length: batch[7], self. user_feat: batch[8]} loss = sess.run(self.CE, feed_dict=feed_dict) perplexity = math.exp(float(loss)) return perplexity 
nets|v1|152|resnet def resnet_v1_152(inputs, num_classes=None, is_training=True, global_pool= True, output_stride=None, store_non_strided_activations=False, spatial_squeeze=True, reuse=None, scope='resnet_v1_152'): """ResNet-152 model of [1]. See resnet_v1() for arg and return description.""" blocks = [resnet_v1_block('block1', base_depth=64, num_units=3, stride= 2), resnet_v1_block('block2', base_depth=128, num_units=8, stride=2 ), resnet_v1_block('block3', base_depth=256, num_units=36, stride=2 ), resnet_v1_block('block4', base_depth=512, num_units=3, stride=1)] return resnet_v1(inputs, blocks, num_classes, is_training, global_pool= global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, store_non_strided_activations= store_non_strided_activations, reuse=reuse, scope=scope) 
modelnet|augment|train|fn def augment_fn(batch_xyz, batch_label, augment_ratio=0.5): bsize, num_point, _ = batch_xyz.shape idx = np.arange(bsize) np.random.shuffle(idx) batch_xyz = batch_xyz[(idx), :, :] batch_label = batch_label[idx] batch_xyz = data_util.shuffle_points(batch_xyz) augSize = np.int32(augment_ratio * bsize) augment_xyz = batch_xyz[0:augSize, :, :] augment_xyz = data_util.rotate_point_cloud(augment_xyz) augment_xyz = data_util.rotate_perturbation_point_cloud(augment_xyz) augment_xyz = data_util.random_scale_point_cloud(augment_xyz) augment_xyz = data_util.shift_point_cloud(augment_xyz) batch_xyz[0:augSize, :, :] = augment_xyz return batch_xyz, batch_label 
get|label|avod|LabelClusterUtils|file|path|cluster|core|utils def _get_cluster_file_path(self, dataset, cls, num_clusters): """ Returns a unique file path for a text file based on the dataset name, split, object class, and number of clusters. The file path will look like: avod/data/<dataset_name>/<data_split>/<class>_<n_clusters>   Args: dataset: Dataset object cls: str, Object class num_clusters: number of clusters for the class  Returns: str Unique file path to text file """ file_path = '{}/{}/{}/'.format(self.data_dir, dataset.name, dataset. cluster_split, dataset.data_split) file_path += '{}_{}.txt'.format(cls, num_clusters) return file_path 
get|vec|varlen|input|deepctr|embedding|dict def get_varlen_embedding_vec_dict(embedding_dict, sequence_input_dict, sequence_fg_list): varlen_embedding_vec_dict = {} for fg in sequence_fg_list: feat_name = fg.name if fg.hash_flag: lookup_idx = Hash(fg.dimension, mask_zero=True)(sequence_input_dict [feat_name]) else: lookup_idx = sequence_input_dict[feat_name] varlen_embedding_vec_dict[feat_name] = embedding_dict[feat_name]( lookup_idx) return varlen_embedding_vec_dict 
arch|train|bnn def train_arch(args, epoch, net, trainLoader, trainLoader_arch, optimizer, biOptimizer, ngpus): net.train() nProcessed = 0 nTrain = len(trainLoader.dataset) train_loss = 0 incorrect = 0 for batch_idx, (data, target) in enumerate(trainLoader): data, target = data.cuda().repeat(ngpus, 1, 1, 1), target.cuda( ).repeat(ngpus) data_search, target_search = None, None output, loss = biOptimizer.step(batch_idx, data, target, data_search, target_search, optimizer, epoch) nProcessed += len(data) train_loss += loss pred = output.data.max(1)[1] incorrect += pred.ne(target.data).cpu().sum() train_loss /= len(trainLoader) err = 100.0 * incorrect / nProcessed logging.info( 'Epoch: {}, training arch, average loss: {:.4f}, Error: {}/{} ({:.0f}%)' .format(epoch, train_loss, incorrect, nProcessed, err)) 
IsMacroDefinition|cpplint def IsMacroDefinition(clean_lines, linenum): if Search('^#define', clean_lines[linenum]): return True if linenum > 0 and Search('\\\\$', clean_lines[linenum - 1]): return True return False 
NoiseFlow|flow|borealisflows|model|noise|revnet2d def revnet2d(self, name, x_shape, flow_permutation): """Affine coupling""" print('sidd_cond = %s' % self.hps.sidd_cond) bijectors = [] with tf.variable_scope(name): if self.hps.append_sdn2: with tf.variable_scope('bijector_sdn2'): print('|-AffineCouplingFitSdnGain2') bijectors.append(AffineCouplingFitSdnGain2(name= 'ac_fitSdnGain2_%d' % self.depth, last_layer=False, x_shape=x_shape, shift_and_log_scale_fn=None)) if self.hps.append_sdn_first: with tf.variable_scope('bijector_sdn'): print('|-AffineCouplingFitSdnGain') bijectors.append(AffineCouplingSdnGain(name= 'ac_fitSdnGain_%d' % self.depth, last_layer=False, x_shape=x_shape, shift_and_log_scale_fn=None)) if self.hps.append_cY: with tf.variable_scope('bijector_cy'): print('|-AffineCouplingCondY') bijectors.append(AffineCouplingCondY(name='ac_cY_first', last_layer=False, x_shape=x_shape, shift_and_log_scale_fn=real_nvp_conv_template(x_shape= x_shape[:-1] + [x_shape[-1] * 2], is_training=self. _is_training, width=self.hps.width))) for i in range(self.depth): with tf.variable_scope('bijector{}'.format(i)): is_last_layer = False if flow_permutation == 0: print('|-tfb.Permute') bijectors.append(tfb.Permute(permutation=list(range( x_shape[-1]))[::-1])) elif flow_permutation == 1: print('|-Conv2d1x1') bijectors.append(Conv2d1x1(x_shape, layer_id=i, bias= False, decomp=self.hps.decomp, name='Conv2d_1x1_{}' .format(i))) else: print('|-No permutation specified. Not using any.') if self.hps.sidd_cond == 'condY': print('|-AffineCouplingCondY') bijectors.append(AffineCouplingCondY(name='ac_cY_%d' % i, last_layer=is_last_layer, x_shape=x_shape, shift_and_log_scale_fn=real_nvp_conv_template( x_shape=x_shape[:-1] + [x_shape[-1] * 2], is_training=self._is_training, width=self.hps.width))) elif self.hps.sidd_cond == 'condYG': print('|-AffineCouplingCondYG') bijectors.append(AffineCouplingCondYG(name='ac_cYG_%d' % i, last_layer=is_last_layer, x_shape=x_shape, shift_and_log_scale_fn=real_nvp_conv_template_iso( x_shape=x_shape[:-1] + [x_shape[-1] * 2], is_training=self._is_training, width=self.hps.width))) elif self.hps.sidd_cond == 'condXY': print('|-AffineCouplingCondXY') bijectors.append(AffineCouplingCondXY(name='ac_cXY_%d' % i, last_layer=is_last_layer, x_shape=x_shape, shift_and_log_scale_fn=real_nvp_conv_template( x_shape=x_shape, is_training=self._is_training, width=self.hps.width))) elif self.hps.sidd_cond == 'condXYG': print('|-AffineCouplingCondXYG') bijectors.append(AffineCouplingCondXYG(name= 'ac_cXYG_%d' % i, last_layer=is_last_layer, x_shape =x_shape, shift_and_log_scale_fn= real_nvp_conv_template_iso(x_shape=x_shape, is_training=self._is_training, width=self.hps.width))) elif self.hps.sidd_cond == 'condSDN': print('|-AffineCouplingSDN') bijectors.append(AffineCouplingCamSdn(name='ac_cSDN_%d' % i, last_layer=is_last_layer, x_shape=x_shape, shift_and_log_scale_fn=real_nvp_conv_template( x_shape=x_shape[:-1] + [x_shape[-1] * 2], is_training=self._is_training, width=self.hps.width))) elif self.hps.sidd_cond == 'fitSDN': print('|-AffineCouplingFitSDN') bijectors.append(AffineCouplingSdnGain(name= 'ac_fitSDN_%d' % i, last_layer=is_last_layer, x_shape=x_shape, shift_and_log_scale_fn=None)) else: print('|-AffineCoupling') bijectors.append(AffineCoupling(name='ac_unc_%d' % i, last_layer=is_last_layer, x_shape=x_shape, shift_and_log_scale_fn=real_nvp_conv_template( x_shape=x_shape, is_training=self._is_training, width=self.hps.width))) if self.hps.append_sdn: with tf.variable_scope('bijector{}'.format(self.depth)): print('|-AffineCouplingFitSDN') bijectors.append(AffineCouplingSdnGain(name='ac_fitSDN_%d' % self.depth, last_layer=False, x_shape=x_shape, shift_and_log_scale_fn=None)) return bijectors 
get|CIFAR1|cifar1|Dataset|example def get_example(self, i): image = np.asarray(self.dset[i][0] / 128.0 - 1.0, np.float32) image += np.random.uniform(size=image.shape, low=0.0, high=1.0 / 128) return image, self.dset[i][1] 
gen|linear|Linear|wganlib|mnist def Linear(name, input_dim, output_dim, inputs, biases=True, initialization =None, weightnorm=None, gain=1.0): """ initialization: None, `lecun`, 'glorot', `he`, 'glorot_he', `orthogonal`, `("uniform", range)` """ with tf.name_scope(name) as scope:  def uniform(stdev, size): if _weights_stdev is not None: stdev = _weights_stdev return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt(3), size=size).astype('float32') if initialization == 'lecun': weight_values = uniform(np.sqrt(1.0 / input_dim), (input_dim, output_dim)) elif initialization == 'glorot' or initialization == None: weight_values = uniform(np.sqrt(2.0 / (input_dim + output_dim)), (input_dim, output_dim)) elif initialization == 'he': weight_values = uniform(np.sqrt(2.0 / input_dim), (input_dim, output_dim)) elif initialization == 'glorot_he': weight_values = uniform(np.sqrt(4.0 / (input_dim + output_dim)), (input_dim, output_dim)) elif initialization == 'orthogonal' or initialization == None and input_dim == output_dim:  def sample(shape): if len(shape) < 2: raise RuntimeError( 'Only shapes of length 2 or more are supported.') flat_shape = shape[0], np.prod(shape[1:]) a = np.random.normal(0.0, 1.0, flat_shape) u, _, v = np.linalg.svd(a, full_matrices=False) q = u if u.shape == flat_shape else v q = q.reshape(shape) return q.astype('float32') weight_values = sample((input_dim, output_dim)) elif initialization[0] == 'uniform': weight_values = np.random.uniform(low=-initialization[1], high= initialization[1], size=(input_dim, output_dim)).astype( 'float32') else: raise Exception('Invalid initialization!') weight_values *= gain weight = lib.param(name + '.W', weight_values) if weightnorm == None: weightnorm = _default_weightnorm if weightnorm: norm_values = np.sqrt(np.sum(np.square(weight_values), axis=0)) target_norms = lib.param(name + '.g', norm_values) with tf.name_scope('weightnorm') as scope: norms = tf.sqrt(tf.reduce_sum(tf.square(weight), reduction_indices=[0])) weight = weight * (target_norms / norms) if inputs.get_shape().ndims == 2: result = tf.matmul(inputs, weight) else: reshaped_inputs = tf.reshape(inputs, [-1, input_dim]) result = tf.matmul(reshaped_inputs, weight) result = tf.reshape(result, tf.pack(tf.unpack(tf.shape(inputs)) [:-1] + [output_dim])) if biases: result = tf.nn.bias_add(result, lib.param(name + '.b', np.zeros ((output_dim,), dtype='float32'))) return result 
gan|UniformLatent|latent|sample|uniformly def uniformly_sample(self, num_sample, num_rep): data = np.stack([np.linspace(self.low, self.high, num_sample)] * self. _in_dim, axis=1) return np.repeat(data, num_rep, axis=0) 
utils|show def show(path, label_path= '/home/iecas7/Project/polsar_segmentation/Flevoland/test1.mat'): label_pred = np.load(path) row, col = label_pred.shape image = Image.new('RGB', (row, col)) image_local = Image.new('RGB', (row, col)) label = sio.loadmat(label_path)['clas1'] label = label - 1 for i in range(row): for j in range(col): if label_pred[i][j] == 0: image.putpixel([i, j], (0, 132, 40)) elif label_pred[i][j] == 1: image.putpixel([i, j], (41, 118, 167)) elif label_pred[i][j] == 2: image.putpixel([i, j], (149, 122, 101)) elif label_pred[i][j] == 3: image.putpixel([i, j], (244, 234, 41)) elif label_pred[i][j] == 4: image.putpixel([i, j], (255, 153, 102)) elif label_pred[i][j] == 5: image.putpixel([i, j], (185, 52, 40)) elif label_pred[i][j] == 6: image.putpixel([i, j], (102, 153, 51)) elif label_pred[i][j] == 7: image.putpixel([i, j], (115, 179, 207)) elif label_pred[i][j] == 8: image.putpixel([i, j], (255, 153, 255)) elif label_pred[i][j] == 9: image.putpixel([i, j], (174, 206, 114)) elif label_pred[i][j] == 10: image.putpixel([i, j], (134, 45, 109)) elif label_pred[i][j] == 11: image.putpixel([i, j], (214, 150, 60)) elif label_pred[i][j] == 12: image.putpixel([i, j], (255, 215, 179)) elif label_pred[i][j] == 13: image.putpixel([i, j], (0, 164, 255)) elif label_pred[i][j] == 14: image.putpixel([i, j], (204, 204, 204)) elif label_pred[i][j] == 15: image.putpixel([i, j], (255, 51, 51)) if label[i][j] < 16: color = image.getpixel((i, j)) image_local.putpixel([i, j], color) image.show() 
forward|deepMOT|features|models|SST|differentiable|master|DAN|stacker def forward_stacker_features_differentiable(self, xp, xn, fill_up_column= True, toNumpy=True): pre_rest_num = self.max_object - xp.shape[1] next_rest_num = self.max_object - xn.shape[1] pre_num = xp.shape[1] next_num = xn.shape[1] x = self.forward_stacker2(self.resize_dim(xp, pre_rest_num, dim=1), self.resize_dim(xn, next_rest_num, dim=1)) x = self.final_dp(x) x = self.forward_final(x, self.final_net).contiguous() x = x.squeeze(0) x = x.squeeze(0) x = self.resize_dim(x, 1, dim=0, constant=self.false_constant) x = self.resize_dim(x, 1, dim=1, constant=self.false_constant) x_f = F.softmax(x, dim=1) x_t = F.softmax(x, dim=0) return x_f, x_t, 0.5 * (x_f + x_t), pre_rest_num, next_rest_num 
sent|decode|find def decode(sent, idx, lang): if lang == 'en': ss = en_ss dh = en_dh else: ss = ch_ss dh = ch_dh test_ids = [] for word in sent: if word in dh.word2id: test_ids.append(dh.word2id[word]) else: test_ids.append(dh.word2id['_UNK']) left = [] right = [] if idx < context_window: left = [PAD_ID] * (context_window - idx) if idx + context_window + 1 > len(test_ids): right = (idx + context_window + 1 - len(test_ids)) * [PAD_ID] tar_indices = left + test_ids[max(0, idx - context_window):idx + context_window + 1] + right c_indices = [] for _ in range(context_window * 2 + batch_size): c_indices.append(tar_indices) sense_probability = sess.run(ss.sense_prob, feed_dict={ss. context_indices: c_indices, ss.eval_mode: True, ss.bi_info: tf. SparseTensorValue([[0, 0]], [0], [1, 1]), ss.lengths: [(0) for _ in range(context_window * 2 + batch_size)], ss.major_weight: 0.0}) print(sense_probability[0].tolist()) 
gen|plot|core|A|resnet56|for|structured|epoch def plot_resnet56_structured_A_epoch_for_epoch_core(): common.epoch_for_epoch_plot(network=common.RESNET56, is_iterative=False, prune_method=common.STRUCTURED_A, min_max_y=(-0.03, 0.01), to_ignore=['lr_lottery', 'reinit']) 
process|image|gcs|to|imagenet def _process_image(filename, coder): """Process a single image file.  Args: filename: string, path to an image file e.g., '/path/to/example.JPG'. coder: instance of ImageCoder to provide TensorFlow image coding utils. Returns: image_buffer: string, JPEG encoding of RGB image. height: integer, image height in pixels. width: integer, image width in pixels. """ with tf.gfile.FastGFile(filename, 'rb') as f: image_data = f.read() if _is_png(filename): tf.logging.info('Converting PNG to JPEG for %s' % filename) image_data = coder.png_to_jpeg(image_data) elif _is_cmyk(filename): tf.logging.info('Converting CMYK to RGB for %s' % filename) image_data = coder.cmyk_to_rgb(image_data) image = coder.decode_jpeg(image_data) assert len(image.shape) == 3 height = image.shape[0] width = image.shape[1] assert image.shape[2] == 3 return image_data, height, width 
bias|sn|deconv|tsgan|no|Attention|loss|build def build(self, input_shape): self.gamma = self.add_weight(name='gamma', shape=[1], initializer= 'zeros', trainable=True) 
generator|test|nonsquare|cyclegan|graph|nets|CycleganTest def test_generator_graph_nonsquare(self): self._test_generator_graph_helper([2, 80, 400, 3]) 
end|DataGeneratorPatch|on|data|epoch def on_epoch_end(self): self.indexes = np.random.permutation(self.nb_inst_total) 
cleverhans|attack|tf|attacks|CarliniWagnerL|single def attack_single(self, instance, target): """ Run the attack on a single instance and label. """ valid = np.ones(instance.shape) prev = np.copy([instance]) last_solution = [instance] const = self.initial_const equal_count = 0 while True: self.l2_attack.source_image = np.copy(prev) while const < self.largest_const: self.l2_attack.initial_const = const res = self.l2_attack.attack_batch(np.copy([instance]), np.array ([target]), mask=np.array([valid])) if res is not None: break const *= self.const_factor if res is None: _logger.debug('Attack succeeded with {} fixed values.'.format( equal_count)) return last_solution restarted = False gradientnorm, scores, nimg = res equal_count = np.sum(np.abs(instance - nimg[0]) < 0.0001) if np.sum(valid) == 0: return [instance] _logger.debug('Next iteration; {} fixed values.'.format(equal_count)) orig_shape = valid.shape valid = valid.flatten() totalchange = abs(nimg[0] - instance) * np.abs(gradientnorm[0]) totalchange = totalchange.flatten() num_changed = 0 for e in np.argsort(totalchange): if np.all(valid[e]): num_changed += 1 valid[e] = 0 if totalchange[e] > self.max_pixel_change: break abort = self.pixel_change_fraction * equal_count ** 0.5 if num_changed >= abort: break valid = np.reshape(valid, orig_shape) last_solution = prev = nimg 
2|conv|custom|layer def conv_2(inputs, filter_num, kernel_size=(3, 3), strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=kernel_regularizer ): conv_ = Conv2D(filter_num, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(inputs) conv_ = BatchNormalization()(conv_) conv_ = Activation('relu')(conv_) conv_ = Conv2D(filter_num, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(conv_) conv_ = BatchNormalization()(conv_) conv_ = Activation('relu')(conv_) return conv_ 
fisher|compute|ops|classification|factors|cov def _compute_cov(tensor, tensor_right=None, normalizer=None): """Compute the empirical second moment of the rows of a 2D Tensor. This function is meant to be applied to random matrices for which the true row mean is zero, so that the true second moment equals the true covariance. Args: tensor: A 2D Tensor. tensor_right: An optional 2D Tensor. If provided, this function computes the matrix product tensor^T * tensor_right instead of tensor^T * tensor. normalizer: optional scalar for the estimator (by default, the normalizer is the number of rows of tensor). Returns: A square 2D Tensor with as many rows/cols as the number of input columns. """ if normalizer is None: normalizer = array_ops.shape(tensor)[0] if tensor_right is None: cov = math_ops.matmul(tensor, tensor, transpose_a=True ) / math_ops.cast(normalizer, tensor.dtype) return (cov + array_ops.transpose(cov)) / math_ops.cast(2.0, cov.dtype) else: return math_ops.matmul(tensor, tensor_right, transpose_a=True ) / math_ops.cast(normalizer, tensor.dtype) 
VGG|get|size|input|data|InputData|dataset def get_dataset_size(self): return self.trainNum 
fisher|compute|ops|damping|pi|classification|adjusted|blocks def _compute_pi_adjusted_damping(left_cov, right_cov, damping): if PI_TYPE == PI_TRACENORM_NAME: pi = _compute_pi_tracenorm(left_cov, right_cov) return damping * pi, damping / pi elif PI_TYPE == PI_OFF_NAME: return damping, damping 
MachineLearningUQ|ML|predict def predict(mod_GP, mod_net, method_train, points, scaler_point, min_value, max_value, path): print(method_train) y_pred_GP = mod_GP.predict(points).reshape(-1) y_pred_GP = Utils.scale_inverse_data(y_pred_GP, scaler_point, min_value, max_value) y_pred_net = mod_net.predict(points).reshape(-1) y_pred_net = Utils.scale_inverse_data(y_pred_net, scaler_point, min_value, max_value) y_pred_mods = [y_pred_net, y_pred_GP] y_pred_mods = np.array(y_pred_mods).transpose() if method_train == 'Mean': y_predicted = (y_pred_GP + y_pred_net) / 2 elif method_train == 'Ensemble': ensemb = joblib.load(path + '/model_ens.sav') y_predicted = ensemb.predict(y_pred_mods) elif method_train == 'NET': y_predicted = y_pred_net elif method_train == 'GP': y_predicted = y_pred_GP else: raise ValueError() return y_predicted 
fisher|FullFactor|compute|ops|classification|factors|cov|new def _compute_new_cov(self, idx=0): with _maybe_colocate_with(self._params_grads_flat[idx], self. _colocate_cov_ops_with_inputs): return self._params_grads_flat[idx] * array_ops.transpose(self. _params_grads_flat[idx]) / math_ops.cast(self._batch_size, self ._params_grads_flat[idx].dtype) 
NoiseMatrixLayer|layers|init def __init__(self, **kwargs): kwargs['use_bias'] = False kwargs['activation'] = 'softmax' units = None super().__init__(units, **kwargs) 
WS353|get|datasets|set2|similarity def get_WS353_set2(): data = pd.read_csv(WS353_set2_path, sep='\t', header=0).values return Bunch(X=data[:, 0:2].astype('object'), y=data[:, (2)].astype(np. float)) 
test|setup|alig|th|Test def setup_th(self): torch.set_default_dtype(torch.double) self.model_th = ModelTh(self.w1, self.b1, self.w2, self.b2) self.feed_dict_th = {'x': torch.from_numpy(self.x), 'y': torch. from_numpy(self.y)} 
hyperparams|texar|keys|HParams def keys(self): """Returns the list of hyperparam names """ return self._hparams.keys() 
init|envs|TestPendulum|test def test_init(self): """Ensure that all variables are being initialized properly.""" self.assertEqual(self.env.name, 'pendulum.xml') self.assertEqual(self.env.observation_space.shape[0], 3) self.assertEqual(self.env.action_space.shape[0], 1) np.testing.assert_array_almost_equal((self.env.action_space.high - self .env.action_space.low) / 2, [2]) np.testing.assert_array_almost_equal((self.env.action_space.high + self .env.action_space.low) / 2, [0]) self.assertEqual(len(self.env.context_range), 2) np.testing.assert_array_almost_equal(self.env.end_goal_thresholds, [ 0.16580628, 0.6]) self.assertEqual(self.env.context_range, [(-0.2792526803190927, 0.2792526803190927), (-0.6, 0.6)]) self.assertEqual(self.env.max_actions, 1000) self.assertEqual(self.env.visualize, False) self.assertEqual(self.env.viewer, None) self.assertEqual(self.env.num_frames_skip, 1) np.testing.assert_array_almost_equal(self.env.context_space.low, [- 0.279253, -0.6]) np.testing.assert_array_almost_equal(self.env.context_space.high, [ 0.279253, 0.6]) 
det|log|jacobian|AffineCouplingGain|inverse def _inverse_log_det_jacobian(self, z, yy, nlf0=None, nlf1=None, iso=None, cam=None): scale = gain_model_params(iso) if scale is None: return tf.constant(0.0, dtype=z.dtype, name='ildj') return -tf.log(scale) 
evaluateModel|aligning def evaluateModel(OutTransform, SourceHom, TargetHom, PassThreshold): Diff = TargetHom - np.matmul(OutTransform, SourceHom) ResidualVec = np.linalg.norm(Diff[:3, :], axis=0) Residual = np.linalg.norm(ResidualVec) InlierIdx = np.where(ResidualVec < PassThreshold) nInliers = np.count_nonzero(InlierIdx) InlierRatio = nInliers / SourceHom.shape[1] return Residual, InlierRatio, InlierIdx[0] 
hparams|4k|chatbot|batch|transformer @registry.register_hparams def chatbot_transformer_batch_4k(): hparams = chatbot_cornell_base() hparams.batch_size = 4096 return hparams 
lrp|models|model|transformer|graph|thumt def model_graph(features, labels, mode, params): hidden_size = params.hidden_size src_seq = features['source'] tgt_seq = features['target'] src_len = features['source_length'] tgt_len = features['target_length'] src_mask = tf.sequence_mask(src_len, maxlen=tf.shape(features['source'] )[1], dtype=tf.float32) tgt_mask = tf.sequence_mask(tgt_len, maxlen=tf.shape(features['target'] )[1], dtype=tf.float32) src_embedding, tgt_embedding, weights = get_weights(params) bias = tf.get_variable('bias', [hidden_size]) inputs = tf.gather(src_embedding, src_seq) * hidden_size ** 0.5 targets = tf.gather(tgt_embedding, tgt_seq) * hidden_size ** 0.5 inputs = inputs * tf.expand_dims(src_mask, -1) targets = targets * tf.expand_dims(tgt_mask, -1) encoder_input = tf.nn.bias_add(inputs, bias) encoder_input = layers.attention.add_timing_signal(encoder_input) enc_attn_bias = layers.attention.attention_bias(src_mask, 'masking') dec_attn_bias = layers.attention.attention_bias(tf.shape(targets)[1], 'causal') decoder_input = tf.pad(targets, [[0, 0], [1, 0], [0, 0]])[:, :-1, :] decoder_input = layers.attention.add_timing_signal(decoder_input) if params.residual_dropout: keep_prob = 1.0 - params.residual_dropout encoder_input = tf.nn.dropout(encoder_input, keep_prob) decoder_input = tf.nn.dropout(decoder_input, keep_prob) encoder_out = transformer_encoder(encoder_input, enc_attn_bias, params) encoder_output = encoder_out['outputs'] w_x_enc = encoder_out['weight_ratios'] decoder_out = transformer_decoder(decoder_input, encoder_output, dec_attn_bias, enc_attn_bias, w_x_enc, params) decoder_output = decoder_out['outputs'] w_x_dec = decoder_out['weight_ratios'] weights_true = tf.gather(weights, labels) logits_elewise_true = decoder_output * weights_true logits_true = tf.reduce_sum(logits_elewise_true, -1) logits_stab = lrp.stabilize(tf.expand_dims(logits_true, -1), params.stab) wr_logit_decoder = logits_elewise_true / logits_stab w_x_true = w_x_dec * tf.expand_dims(wr_logit_decoder, 1) w_x_true = tf.reduce_sum(w_x_true, -1) if mode == 'infer': decoder_output = decoder_output[:, (-1), :] logits = tf.matmul(decoder_output, weights, False, True) return logits decoder_output = tf.reshape(decoder_output, [-1, hidden_size]) logits = tf.matmul(decoder_output, weights, False, True) ce = losses.smoothed_softmax_cross_entropy_with_logits(logits=logits, labels=labels, smoothing=params.label_smoothing, normalize=True) ce = tf.reshape(ce, tf.shape(tgt_seq)) loss = tf.reduce_sum(ce * tgt_mask) / tf.reduce_sum(tgt_mask) rlv_info = {} R_x_true = tf.transpose(w_x_true, [0, 2, 1]) rlv_info['result'] = normalize(R_x_true, True) return loss, rlv_info 
cleverhans|get|name|KerasModelWrapper|utils|softmax|keras def _get_softmax_name(self): """ Looks for the name of the softmax layer. :return: Softmax layer name """ for i, layer in enumerate(self.model.layers): cfg = layer.get_config() if 'activation' in cfg and cfg['activation'] == 'softmax': return layer.name raise Exception('No softmax layers found') 
refinement|box|utils def box_refinement(box, gt_box): """Compute refinement needed to transform box to gt_box. box and gt_box are [N, (y1, x1, y2, x2)]. (y2, x2) is assumed to be outside the box. """ box = box.astype(np.float32) gt_box = gt_box.astype(np.float32) height = box[:, (2)] - box[:, (0)] width = box[:, (3)] - box[:, (1)] center_y = box[:, (0)] + 0.5 * height center_x = box[:, (1)] + 0.5 * width gt_height = gt_box[:, (2)] - gt_box[:, (0)] gt_width = gt_box[:, (3)] - gt_box[:, (1)] gt_center_y = gt_box[:, (0)] + 0.5 * gt_height gt_center_x = gt_box[:, (1)] + 0.5 * gt_width dy = (gt_center_y - center_y) / height dx = (gt_center_x - center_x) / width dh = np.log(gt_height / height) dw = np.log(gt_width / width) return np.stack([dy, dx, dh, dw], axis=1) 
JSONDataBase|domain|plato|init|database def __init__(self, filename): """ Initializes the internal structures of the json parser Base  :param filename: path to the json database """ super(JSONDataBase, self).__init__(filename) 
ukernel|test|generate|split|vmulcaddc|name def split_ukernel_name(name): match = re.match('^xnn_(f16|f32)_vmulcaddc_ukernel_c(\\d+)__(.+)_(\\d+)x$', name) assert match is not None channel_tile = int(match.group(2)) row_tile = int(match.group(4)) arch, isa = xnncommon.parse_target_name(target_name=match.group(3)) return channel_tile, row_tile, arch, isa 
models|embeddings|OpenKE|ccorr|HolE def _ccorr(self, a, b): a = tf.cast(a, tf.complex64) b = tf.cast(b, tf.complex64) return tf.real(tf.ifft(tf.conj(tf.fft(a)) * tf.fft(b))) 
avod|tf|4c|3d|encoder|core|box|to def tf_box_4c_to_box_3d(boxes_4c, ground_plane): """Vectorized box_4c to box_3d conversion  Args: boxes_4c: Tensor of boxes_4c (N, 10) ground_plane: Tensor of ground plane coefficients (4,)  Returns: Tensor of boxes_3d (N, 7) """ format_checker.check_box_4c_format(boxes_4c) corners = tf.reshape(boxes_4c[:, 0:8], [-1, 2, 4]) p1 = corners[:, :, (0)] p2 = corners[:, :, (1)] p3 = corners[:, :, (2)] p4 = corners[:, :, (3)] midpoint_12 = (p1 + p2) / 2.0 midpoint_23 = (p2 + p3) / 2.0 midpoint_34 = (p3 + p4) / 2.0 midpoint_14 = (p1 + p4) / 2.0 vec_34_12 = midpoint_12 - midpoint_34 vec_34_12_mag = tf.norm(vec_34_12, axis=1) vec_23_14 = midpoint_14 - midpoint_23 vec_23_14_mag = tf.norm(vec_23_14, axis=1) vec_34_12_centroid, vec_34_12_length, vec_34_12_width, vec_34_12_ry = ( calculate_box_3d_info(vec_34_12, vec_34_12_mag, p1, p2, p3, p4, midpoint=midpoint_34)) vec_23_14_centroid, vec_23_14_length, vec_23_14_width, vec_23_14_ry = ( calculate_box_3d_info(vec_23_14, vec_23_14_mag, p1, p2, p3, p4, midpoint=midpoint_23)) vec_34_12_mask = tf.greater(vec_34_12_mag, vec_23_14_mag) vec_23_14_mask = tf.logical_not(vec_34_12_mask) vec_34_12_float_mask = tf.reshape(tf.cast(vec_34_12_mask, tf.float32), [-1, 1]) vec_23_14_float_mask = tf.reshape(tf.cast(vec_23_14_mask, tf.float32), [-1, 1]) centroid_xz = (vec_34_12_centroid * vec_34_12_float_mask + vec_23_14_centroid * vec_23_14_float_mask) length_out = (vec_34_12_length * vec_34_12_float_mask + vec_23_14_length * vec_23_14_float_mask) width_out = (vec_34_12_width * vec_34_12_float_mask + vec_23_14_width * vec_23_14_float_mask) ry_out = (vec_34_12_ry * vec_34_12_float_mask + vec_23_14_ry * vec_23_14_float_mask) a = ground_plane[0] b = ground_plane[1] c = ground_plane[2] d = ground_plane[3] h1 = boxes_4c[:, (8)] h2 = boxes_4c[:, (9)] centroid_x = centroid_xz[:, (0)] centroid_z = centroid_xz[:, (1)] length_out = tf.squeeze(length_out) width_out = tf.squeeze(width_out) ry_out = tf.squeeze(ry_out) ground_y = -(a * centroid_x + c * centroid_z + d) / b centroid_y = ground_y - h1 height_out = h2 - h1 box_3d_out = tf.stack([centroid_x, centroid_y, centroid_z, length_out, width_out, height_out, ry_out], axis=1) return box_3d_out 
seld|scores|update|dcase2|19|SELDMetrics|sed|metrics|master|evaluation def update_sed_scores(self, pred, gt): """ Computes SED metrics for one second segments  :param pred: predicted matrix of dimension [nb_frames, nb_classes], with 1 when sound event is active else 0 :param gt:  reference matrix of dimension [nb_frames, nb_classes], with 1 when sound event is active else 0 :param nb_frames_1s: integer, number of frames in one second :return: """ self.f1_overall_1sec(pred, gt) self.er_overall_1sec(pred, gt) 
output|best|nbl|Output|model|save|top5 def save_model_nbl_best_top5(self, session, epoch, g_step): self.tf_saver_best_nbl_top5.save(session, '{}-{}-nbl_best_top5'.format( self.model_file_base, epoch), global_step=g_step) 
conv|transpose|ops def conv_transpose(x, outputShape, name): with tf.variable_scope(name): w = tf.get_variable('w', [5, 5, outputShape[-1], x.get_shape()[-1]], initializer=tf.truncated_normal_initializer(stddev=0.02)) b = tf.get_variable('b', [outputShape[-1]], initializer=tf. constant_initializer(0.0)) convt = tf.nn.conv2d_transpose(x, w, output_shape=outputShape, strides=[1, 2, 2, 1]) return convt 
arguments|get|inference def get_arguments(): """Parse all the arguments provided from the CLI.  Returns: A list of parsed arguments. """ parser = argparse.ArgumentParser(description= 'Discovering Class Specific Pixels (DCSP) Network Inference.') parser.add_argument('img_path', type=str, help= 'Path to the RGB image file.') parser.add_argument('model_weights', type=str, help= 'Path to the file with model weights.') parser.add_argument('--save-dir', type=str, default=SAVE_DIR, help= 'Where to save predicted mask.') return parser.parse_args() 
goodness|SVAE|DC|dc|svae|transform def goodness_transform(self, raw_goodness, debug=False): goodness = torch.sigmoid((raw_goodness - self.good_th) * 25.0) if debug: logging.info('goodness') logging.info(raw_goodness.squeeze().detach().cpu().numpy()) logging.info('transformed_goodness') logging.info(goodness.squeeze().detach().cpu().numpy()) return goodness 
lines|position|from|translate|read|utils def read_lines_from_position(paths, from_position=None, binary=None): binary = binary or [False] * len(paths) from_position = from_position or [None] * len(paths) iterators = [(read_binary_features(path, from_position_) if binary_ else read_text_from_position(path, from_position_)) for path, binary_, from_position_ in zip(paths, binary, from_position)] for data in zip(*iterators): yield tuple(zip(*data)) 
generator|avod|grid|generate|3d|anchor|GridAnchor3dGenerator|core|generators def _generate(self, **params): """ Generates 3D anchors in a grid in the provided 3d area and places them on the ground_plane.  Args: **params: area_3d: [[min_x, max_x], [min_y, max_y], [min_z, max_z]]  Returns: list of 3D anchors in the form N x [x, y, z, l, w, h, ry] """ area_3d = params.get('area_3d') anchor_3d_sizes = params.get('anchor_3d_sizes') anchor_stride = params.get('anchor_stride') ground_plane = params.get('ground_plane') return tile_anchors_3d(area_3d, anchor_3d_sizes, anchor_stride, ground_plane) 
tICA|tica|fit def _fit(self, X): X = np.asarray(array2d(X), dtype=np.float64) if X.shape[1] > X.shape[0]: warnings.warn( 'The number of features (%d) is greater than the length of the data (%d). The covariance matrix is not guaranteed to be positive definite.' % (X.shape[1], X.shape[0])) self._initialize(X.shape[1]) if not len(X) > self.lag_time: warnings.warn( 'length of data (%d) is too short for the lag time (%d)' % (len (X), self.lag_time)) return self.n_observations_ += X.shape[0] self.n_sequences_ += 1 self._outer_0_to_T_lagged += np.dot(X[:-self.lag_time].T, X[self.lag_time:] ) self._sum_0_to_TminusTau += X[:-self.lag_time].sum(axis=0) self._sum_tau_to_T += X[self.lag_time:].sum(axis=0) self._sum_0_to_T += X.sum(axis=0) self._outer_0_to_TminusTau += np.dot(X[:-self.lag_time].T, X[:-self. lag_time]) self._outer_offset_to_T += np.dot(X[self.lag_time:].T, X[self.lag_time:]) self._is_dirty = True 
prefab|gym|sprites|parts|teleport|pycolab|MazeWalker def _teleport(self, virtual_position): """Set the new virtual position of the agent, applying side-effects.  This method is a somewhat "low level" method: it doesn't check whether the new location has an impassible character in it, nor does it apply any scrolling orders that may be current (if called during a game iteration). This method is only grudgingly "protected" (and not "private"), mainly to allow `MazeWalker` subclasses to initialise their location at a place somewhere off the board. Use at your own risk.  This method does handle entering and exiting the board in the conventional way. Virtual positions off of the board yield a true position of `(0, 0)`, and `_on_board_exit` and `_on_board_enter` are called as appropriate.  Args: virtual_position: A 2-tuple containing the intended virtual position for this `MazeWalker`. """ new_row, new_col = virtual_position old_row, old_col = self._virtual_row, self._virtual_col old_on_board = self._on_board(old_row, old_col) new_on_board = self._on_board(new_row, new_col) if old_on_board and not new_on_board: self._on_board_exit() self._virtual_row, self._virtual_col = new_row, new_col if new_on_board: self._position = self.Position(new_row, new_col) else: self._position = self.Position(0, 0) if not old_on_board and new_on_board: self._on_board_enter() 
forward|deepMOT|slice|features|models|SST|master|DAN|stacker def forward_stacker_features_slice(self, xp, xn, fill_up_column=True, toNumpy=True): pre_rest_num_final = self.max_object - xp.shape[1] next_rest_num_final = self.max_object - xn.shape[1] pre_num_final = xp.shape[1] next_num_final = xn.shape[1] tmp = [] output = [] for i in range(0, xp.shape[1], self.max_object): tmp.append(xp[:, i:i + self.max_object, :]) for xp_tmp in tmp: pre_rest_num = self.max_object - xp_tmp.shape[1] next_rest_num = self.max_object - xn.shape[1] pre_num = xp_tmp.shape[1] next_num = xn.shape[1] x = self.forward_stacker2(self.resize_dim(xp_tmp, pre_rest_num, dim =1), self.resize_dim(xn, next_rest_num, dim=1)) x = self.final_dp(x) x = self.forward_final(x, self.final_net) x = x.contiguous() if next_num < self.max_object: x[(0), (0), :, next_num:] = 0 if pre_num < self.max_object: x[(0), (0), pre_num:, :] = 0 x = x[:, :, :pre_num, :next_num] x = x[(0), (0), :] output.append(x) output = torch.cat(output, dim=0) output = self.resize_dim(output, 1, dim=0, constant=self.false_constant) output = self.resize_dim(output, 1, dim=1, constant=self.false_constant) x_f = F.softmax(output, dim=1) x_t = F.softmax(output, dim=0) last_row, last_col = x_f.shape row_slice = list(range(pre_num_final)) + [last_row - 1] col_slice = list(range(next_num_final)) + [last_col - 1] x_f = x_f[(row_slice), :] x_f = x_f[:, (col_slice)] x_t = x_t[(row_slice), :] x_t = x_t[:, (col_slice)] x = Variable(torch.zeros(pre_num_final, next_num_final + 1).cuda()) x[0:pre_num_final, 0:next_num_final] = (x_f[0:pre_num_final, 0: next_num_final] + x_t[0:pre_num_final, 0:next_num_final]) / 2.0 x[:, next_num_final:next_num_final + 1] = x_f[:pre_num_final, next_num_final:next_num_final + 1] if fill_up_column and pre_num > 1: x = torch.cat([x, x[:, next_num_final:next_num_final + 1].repeat(1, pre_num_final - 1)], dim=1) if toNumpy: if self.use_gpu: y = x.data.cpu().numpy() else: y = x.data.numpy() return y return x 
models|forward|layers|DenseBlock def forward(self, x): if self.upsample: new_features = [] for layer in self.layers: out = layer(x) x = torch.cat([x, out], 1) new_features.append(out) return torch.cat(new_features, 1) else: for layer in self.layers: out = layer(x) x = torch.cat([x, out], 1) return x 
CheckRedundantOverrideOrFinal|cpplint def CheckRedundantOverrideOrFinal(filename, clean_lines, linenum, error): """Check if line contains a redundant "override" or "final" virt-specifier.  Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found. """ line = clean_lines.elided[linenum] declarator_end = line.rfind(')') if declarator_end >= 0: fragment = line[declarator_end:] elif linenum > 1 and clean_lines.elided[linenum - 1].rfind(')') >= 0: fragment = line else: return if Search('\\boverride\\b', fragment) and Search('\\bfinal\\b', fragment): error(filename, linenum, 'readability/inheritance', 4, '"override" is redundant since function is already declared as "final"' ) 
eval|list|NPRFDRMM|qid|by|nprf|helper|drmm def eval_by_qid_list_helper(self, qid_list, pair_generator): relevance_dict = load_pickle(self.config.relevance_dict_path) qid_list = sorted(qid_list) qualified_qid_list = [] res_dict = OrderedDict() for qid in qid_list: relevance = relevance_dict.get(qid) supervised_docid_list = relevance.get_supervised_docid_list() if len(supervised_docid_list) < self.config.nb_supervised_doc: score_list = relevance.get_supervised_score_list() res = Result(qid, supervised_docid_list, score_list, self. config.runid) res_dict.update({qid: res}) logging.warn('query {0} not to be rerank'.format(qid)) else: qualified_qid_list.append(qid) dd_q, dd_d, score_gate, len_indicator = pair_generator.generate_list_batch( qualified_qid_list, self.config.rerank_topk) return [dd_q, dd_d, score_gate ], len_indicator, res_dict, qualified_qid_list 
official|test|logs|params|run|logger|BenchmarkFileLoggerTest|utils|collect def test_collect_run_params(self): run_info = {} run_parameters = {'batch_size': 32, 'synthetic_data': True, 'train_epochs': 100.0, 'dtype': 'fp16', 'resnet_size': 50, 'random_tensor': tf.constant(2.0)} logger._collect_run_params(run_info, run_parameters) self.assertEqual(len(run_info['run_parameters']), 6) self.assertEqual(run_info['run_parameters'][0], {'name': 'batch_size', 'long_value': 32}) self.assertEqual(run_info['run_parameters'][1], {'name': 'dtype', 'string_value': 'fp16'}) self.assertEqual(run_info['run_parameters'][2], {'name': 'random_tensor', 'string_value': 'Tensor("Const:0", shape=(), dtype=float32)'}) self.assertEqual(run_info['run_parameters'][3], {'name': 'resnet_size', 'long_value': 50}) self.assertEqual(run_info['run_parameters'][4], {'name': 'synthetic_data', 'bool_value': 'True'}) self.assertEqual(run_info['run_parameters'][5], {'name': 'train_epochs', 'float_value': 100.0}) 
nets|nasnet|cifar|config def cifar_config(): return tf.contrib.training.HParams(stem_multiplier=3.0, drop_path_keep_prob=0.6, num_cells=18, use_aux_head=1, num_conv_filters=32, dense_dropout_keep_prob=1.0, filter_scaling_rate=2.0, num_reduction_layers=2, data_format='NHWC', skip_reduction_layer_input=0, total_training_steps=937500) 
python|grpc|OpsTest|string|ops|test|seed|rl|master @parameterized.parameters(([], False), ([1], True)) def test_string(self, dim, batched): address = self.get_unix_address() server = ops.Server([address])  @tf.function(input_signature=[tf.TensorSpec(dim, tf.string)]) def hello(x): return tf.strings.join([x, ' world']) server.bind(hello, batched=batched) server.start() client = ops.Client(address) self.assertAllEqual(b'hello world', client.hello('hello')) server.shutdown() 
loss|get|fns|ac def get_ac_loss(learner_agent_output, env_output, actor_agent_output, actor_action, reward_clipping, discounting, baseline_cost, entropy_cost, num_steps): """Computes actor-critic loss.  Args: learner_agent_output: A nested structure of type `AgentOutput`. The tensors are expected to have shape [num_timesteps, batch, ....] env_output: A nested structure of type `EnvOutput`. The tensors are expected to have shape [num_timesteps, batch, ...]. actor_agent_output: A nested structure of type `AgentOutput`. The tensors are expected to have shape [num_timesteps, batch, ....] actor_action: An instance of `ActorAction` containing indices of the actions chosen by actor. The total number of actions available to actor at any point is equal to actor_agent_output.policy_logits.shape()[-1]. reward_clipping: A string denoting the clipping strategy to be applied to rewards. An empty string means no clipping is applied. discounting: The discount factor. baseline_cost: A multiplier for baseline loss. entropy_cost: A multiplier for entropy. num_steps: An int to be used as step arg for summaries.  Returns: A tensor of shape [num_timesteps - 1, batch_size] which contains the computed actor-critic loss per timestep per element. """ bootstrap_value = learner_agent_output.baseline[-1] actor_agent_output = tf.nest.map_structure(lambda t: t[1:], actor_agent_output) rewards, done, _, _ = tf.nest.map_structure(lambda t: t[1:], env_output) actor_action_idx = actor_action.chosen_action_idx[1:] learner_agent_output = tf.nest.map_structure(lambda t: t[:-1], learner_agent_output) clipped_rewards = rewards if reward_clipping == 'abs_one': clipped_rewards = tf.clip_by_value(rewards, -1, 1) elif reward_clipping == 'soft_asymmetric': squeezed = tf.tanh(rewards / 5.0) clipped_rewards = tf.where(rewards < 0, 0.3 * squeezed, squeezed) * 5.0 discounts = tf.cast(~done, tf.float32) * discounting vtrace_returns = vtrace.from_logits(behaviour_policy_logits= actor_agent_output.policy_logits, target_policy_logits= learner_agent_output.policy_logits, actions=actor_action_idx, discounts=discounts, rewards=clipped_rewards, values= learner_agent_output.baseline, bootstrap_value=bootstrap_value) pg_advantages = vtrace_returns.pg_advantages v_advantages = vtrace_returns.vs - learner_agent_output.baseline tf.summary.histogram('pg_advantages', pg_advantages, step=num_steps) tf.summary.histogram('v_advantages', v_advantages, step=num_steps) pg_loss = _compute_policy_gradient_loss(learner_agent_output. policy_logits, actor_action_idx, pg_advantages, step=num_steps) baseline_loss = _compute_baseline_loss(v_advantages, step=num_steps) entropy = _compute_entropy_loss(learner_agent_output.policy_logits, step=num_steps) total_loss = (pg_loss + baseline_cost * baseline_loss + entropy_cost * entropy) tf.summary.scalar('loss/ac_loss', tf.reduce_mean(total_loss), step= num_steps) return total_loss 
ludwig|plato|LudwigNLU|del|component|agent|nlu def __del__(self): """ Close the Ludwig model.  :return: """ if self.model: self.model.close() 
models|LeNet|forward|lenet def forward(self, sess, input_feature): """Feed-forward pass in the autoencoder @param sess (tf.Session) the current session @param input_feature (np.array) matrix of features @return (np.array) the output of the autoencoder (reconstruction) """ output = sess.run([self.output], feed_dict={self.x: input_feature}) return output 
tangents|predict|get|dependency|neural def _get_dependency(get, compute_cov): """Figure out dependency for get.""" _, get = canonicalize_get(get) for g in get: if g not in ['nngp', 'ntk']: raise NotImplementedError( 'Can only get either "nngp" or "ntk" predictions, got %s.' % g) get_dependency = () if 'nngp' in get or 'ntk' in get and compute_cov: get_dependency += 'nngp', if 'ntk' in get: get_dependency += 'ntk', return get_dependency 
tangents|prod|get|stax|neural|normalising def _get_normalising_prod(var1, var2, marginal, axis=()): """Returns three tensors, `prod11`, `prod12` and `prod22` which contain products of marginal variances of `var1`, `nngp` and `var2` respectively.  `prod12` is a 6D tensor where an entry [x1, x2, a, b, c, d] equals k_{ab}(x1, x1) * k_{cd}(x2, x2), if `marginal` is `OVER_POINTS` or `NO`, or a 4D tensor k_{aa}(x1, x1) k_{cc}(x2, x2) if `marginal` is `OVER_PIXELS`, or a 2D tensor k(x1, x1) k(x2, x2) if `marginal` is `OVER_ALL`. In the last two cases, both `prod11` and `prod22` will be None. Otherwise they will be 5D tensors k_{ab}(x1, x1) k_{cd}(x1, x1) in the `marginal == OVER_POINTS` case, or 6D tensors akin to the one for `prod12` if `marginal == NO`. """ axis = (axis,) if isinstance(axis, int) else tuple(axis) same_input = var2 is None if same_input: var2 = var1 elif var1.shape[1:] != var2.shape[1:]: raise ValueError(var1.shape, var2.shape) if marginal in (M.OVER_ALL, M.OVER_PIXELS): if marginal == M.OVER_ALL and len(axis) > 0: raise ValueError( 'Required normalisation over axis={} is impossible when {}. Maybe axis=()?' .format(axis, marginal)) sqnorms1, sqnorms2 = var1, var2 sqnorms1 = np.mean(sqnorms1, axis=axis, keepdims=True) if same_input: sqnorms2 = sqnorms1 else: sqnorms2 = np.mean(sqnorms2, axis=axis, keepdims=True) prod12 = np.expand_dims(sqnorms1, 1) * np.expand_dims(sqnorms2, 0) prod11 = sqnorms1 ** 2.0 prod22 = sqnorms2 ** 2.0 if not same_input else prod11 elif marginal in (M.OVER_POINTS, M.NO):  def outer_prod_full(sqnorms1, sqnorms2): sqnorms1 = sqnorms1[:, (None), :, (None), :, (None)] sqnorms2 = sqnorms2[(None), :, (None), :, (None), :] return sqnorms1 * sqnorms2 sqnorms1 = _get_dimensionwise_marg_var(var1, marginal) sqnorms1 = np.mean(sqnorms1, axis=axis, keepdims=True) if same_input: sqnorms2 = sqnorms1 else: sqnorms2 = _get_dimensionwise_marg_var(var2, marginal) sqnorms2 = np.mean(sqnorms2, axis=axis, keepdims=True) prod12 = outer_prod_full(sqnorms1, sqnorms2) if marginal == M.OVER_POINTS:  def outer_prod_pix(sqnorms1, sqnorms2): sqnorms1 = sqnorms1[:, :, (None), :, (None)] sqnorms2 = sqnorms2[:, (None), :, (None), :] return sqnorms1 * sqnorms2 prod11 = outer_prod_pix(sqnorms1, sqnorms1) prod22 = outer_prod_pix(sqnorms2, sqnorms2 ) if not same_input else prod11 else: prod11 = outer_prod_full(sqnorms1, sqnorms1) prod22 = outer_prod_full(sqnorms2, sqnorms2 ) if not same_input else prod11 else: raise NotImplementedError( 'Only implemented for `OVER_ALL`, `OVER_PIXELS`, `OVER_POINTS` and `NO`; supplied {}' .format(marginal)) return prod11, prod12, prod22 
pad|datacode|DataCreation|before|ner def pad_before(self, a_list, target_length): return (target_length - len(a_list)) * [self.padding_word_string] + a_list 
Node|kaffe|parameters|graph @property def parameters(self): if self.layer is not None: return self.layer.parameters return None 
get|tgt|estimator|nmt|eos|id|sos def _get_tgt_sos_eos_id(hparams): with tf.Session() as sess: _, tgt_vocab_table = vocab_utils.create_vocab_tables(hparams. src_vocab_file, hparams.tgt_vocab_file, hparams.share_vocab) tgt_sos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(hparams.sos )), tf.int32) tgt_eos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(hparams.eos )), tf.int32) sess.run(tf.tables_initializer()) tgt_sos_id = sess.run(tgt_sos_id, {}) tgt_eos_id = sess.run(tgt_eos_id, {}) return tgt_sos_id, tgt_eos_id 
dilated|fn1|edge|tf def fn1(): idxs = tf.range(tf.shape(neigh_idx)[2]) ridxs = tf.random_shuffle(idxs)[:k] neigh_idx_1 = tf.gather(neigh_idx, ridxs, axis=2) return tf.identity(neigh_idx_1) 
io|str|img|IOException def __str__(self): return repr(self.value) 
compute|attr|attributions|and|bim|save def compute_and_save_attr(model, data, indices, num_threads): """Given the name of a model and a set of data, select a set of images based on provided indices, and compute and save their saliency maps and object attributions.""" base_dir = os.getcwd() data_dir = os.path.join(base_dir, 'data', data, 'val') model_dir = os.path.join(base_dir, 'models', model) sal_output_dir = os.path.join(base_dir, SAL_DIR, model + '-' + data) attr_output_dir = os.path.join(base_dir, ATTR_DIR, model + '-' + data) if not tf.gfile.Exists(sal_output_dir): tf.gfile.MakeDirs(sal_output_dir) if not tf.gfile.Exists(attr_output_dir): tf.gfile.MakeDirs(attr_output_dir) img_names = [sorted(tf.gfile.ListDirectory(data_dir))[i] for i in indices] img_paths = [os.path.join(data_dir, img_name) for img_name in img_names] imgs = load_imgs(img_paths, num_threads) input_name = 'input_tensor:0' logit_name = 'resnet_model/final_dense:0' conv_name = 'resnet_model/block_layer4:0' with tf.Session(graph=tf.Graph()) as sess: tf.saved_model.loader.load(sess, ['serve'], model_dir) graph = tf.get_default_graph() input_tensor = graph.get_tensor_by_name(input_name) logit_tensor = graph.get_tensor_by_name(logit_name) neuron_selector = tf.placeholder(tf.int32) y = logit_tensor[:, (neuron_selector)] pred_tensor = tf.argmax(logit_tensor, 1) vg = saliency.GradientSaliency(graph, sess, y, input_tensor) gb = saliency.GuidedBackprop(graph, sess, y, input_tensor) ig = saliency.IntegratedGradients(graph, sess, y, input_tensor) gc = saliency.GradCam(graph, sess, y, input_tensor, graph. get_tensor_by_name(conv_name))  def single_map(img, img_name): pred = sess.run(pred_tensor, feed_dict={input_tensor: [img]})[0] vg_mask = vg.GetMask(img, feed_dict={neuron_selector: pred}) vgs_mask = vg.GetSmoothedMask(img, feed_dict={neuron_selector: pred}, num_threads=50) gb_mask = gb.GetMask(img, feed_dict={neuron_selector: pred}) gbs_mask = gb.GetSmoothedMask(img, feed_dict={neuron_selector: pred}, num_threads=50) baseline = np.zeros(img.shape) - np.expand_dims(np.expand_dims( _CHANNEL_MEANS, 0), 0) ig_mask = ig.GetMask(img, feed_dict={neuron_selector: pred}, x_baseline=baseline) igs_mask = ig.GetSmoothedMask(img, feed_dict={neuron_selector: pred}, x_baseline=baseline, num_threads=50) gc_mask = gc.GetMask(img, feed_dict={neuron_selector: pred}) gcs_mask = gc.GetSmoothedMask(img, feed_dict={neuron_selector: pred}, num_threads=50) gbgc_mask = gb_mask * gc_mask gbgcs_mask = gbs_mask * gcs_mask masks = np.array([vg_mask, vgs_mask, gb_mask, gbs_mask, ig_mask, igs_mask, gc_mask, gcs_mask, gbgc_mask, gbgcs_mask, vg_mask * img, vgs_mask * img]) return masks, pred sal_maps = [] preds = [] for img, img_name in zip(imgs, img_names): sal_path = tf.gfile.Glob(os.path.join(sal_output_dir, img_name[ :-4] + '*')) if len(sal_path) > 0: sal_maps.append(np.load(tf.gfile.GFile(sal_path[0], 'rb'))) preds.append(sal_path[0].split('_')[-1]) tf.logging.info('Loaded saliency maps for {}.'.format(img_name) ) else: masks, pred = single_map(img, img_name) sal_maps.append(masks) preds.append(pred) out_path = os.path.join(sal_output_dir, img_name[:-4] + '_' + str(pred)) np.save(tf.gfile.GFile(out_path, 'w'), masks) tf.logging.info('Saved saliency maps for {}.'.format(img_name)) loc_fpath = os.path.join(base_dir, 'data', data, 'val_loc.txt') lines = [tf.gfile.Open(loc_fpath).readlines()[i] for i in indices] locs = np.array([[int(int(l) * float(RESNET_SHAPE[0]) / IMG_SHAPE[0]) for l in line.rstrip('\n').split(' ')[-1].split(',')] for line in lines]) pool = multiprocessing.Pool(num_threads) maps_3d = np.array(sal_maps).reshape(-1, RESNET_SHAPE[0], RESNET_SHAPE[ 1], 3) maps_2d = np.array(pool.map(visualize_pos_attr, maps_3d)) maps_2d = maps_2d.reshape(len(indices), int(maps_2d.shape[0] // len( indices)), RESNET_SHAPE[0], RESNET_SHAPE[1]) mask_fpath = os.path.join(base_dir, 'data', data, 'val_mask') obj_masks = [np.load(tf.gfile.GFile(mask_fpath, 'rb'), allow_pickle= True)[i] for i in indices] attrs = [] for i in range(len(indices)): attr = single_attr(maps_2d[i], locs[i], obj_masks[i]) attrs.append(attr) out_path = os.path.join(attr_output_dir, img_names[i][:-4] + '_' + str(preds[i])) np.save(tf.gfile.GFile(out_path, 'w'), attr) 
attribution|init|TiedEmbeddingSoftmax|source def __init__(self, vocab_size=vocab_size, embedding_size=embedding_dim, ** kwargs): super(TiedEmbeddingSoftmax, self).__init__() self.w = self.add_weight(name='w', shape=(vocab_size, embedding_size), initializer='random_normal', trainable=True) self.b = self.add_weight(name='b', shape=(vocab_size,), initializer= 'zeros', trainable=True) 
stdout|utils|redirect @contextmanager def redirect_stdout(new_target): old_target, sys.stdout = sys.stdout, new_target try: yield new_target finally: sys.stdout = old_target 
one|encoded|dataset|hot def one_hot_encoded(class_numbers, num_classes=None): """ Generate the One-Hot encoded class-labels from an array of integers.  For example, if class_number=2 and num_classes=4 then the one-hot encoded label is the float array: [0. 0. 1. 0.]  :param class_numbers: Array of integers with class-numbers. Assume the integers are from zero to num_classes-1 inclusive.  :param num_classes: Number of classes. If None then use max(class_numbers)+1.  :return: 2-dim array of shape: [len(class_numbers), num_classes] """ if num_classes is None: num_classes = np.max(class_numbers) + 1 return np.eye(num_classes, dtype=float)[class_numbers] 
construct|graph|delta|build|patch def build_graph(sess, num_steps, step_size, dog_imgs, loc, dog_mask, func_name='resnet_model/final_dense:0', input_name='input_tensor:0', model_name='scene_only', image_bounds=[0 - _B_MEAN, 255 - _R_MEAN], alpha=0.01, beta=0.01): """Construct the graph to compute ||f(x) - f(x+d)|| num_steps times.  During each iteration, delta is updated using gradient descent to minimize the difference between f(x) and f(x+d). """ dog_mask_resized = img_as_bool(resize(dog_mask.astype(np.bool), (loc[2] - loc[0], loc[3] - loc[1]))) dog_mask_resized = np.expand_dims(dog_mask_resized, axis=2) dog_mask_resized = np.array([np.tile(dog_mask_resized, [1, 1, 3])]) tf.saved_model.loader.load(sess, ['serve'], os.path.join(os.getcwd(), 'models', model_name)) graph = tf.get_default_graph() gdef_1 = tf.graph_util.convert_variables_to_constants(sess, tf. get_default_graph().as_graph_def(), ['softmax_tensor'], variable_names_blacklist=[input_name, func_name])  def update_delta(delta, i): tf.import_graph_def(gdef_1, input_map={input_name: tf. get_default_graph().get_tensor_by_name(input_name) + delta}) graph = tf.get_default_graph() image = graph.get_tensor_by_name(input_name) fx = graph.get_tensor_by_name(func_name) fxd = graph.get_tensor_by_name('while/import/' + func_name) loss = tf.norm(fxd - fx) loss = loss - alpha * tf.norm(tf.reshape(delta, [-1])) relu = tf.reduce_sum(tf.nn.relu(image + delta - image_bounds[0]) + tf.nn.relu(tf.fill(tf.shape(image), image_bounds[1]) - image - delta)) loss = loss + beta * relu new_delta = delta - step_size * tf.squeeze(tf.gradients(loss, delta), 0 ) new_delta = tf.pad(new_delta[:, loc[0]:loc[2], loc[1]:loc[3], :] * dog_mask_resized, tf.constant([[0, 0], [loc[0], 224 - loc[2]], [loc[1], 224 - loc[3]], [0, 0]])) return new_delta, i + 1  def cond(unused_delta, i): return tf.less(i, num_steps) i = tf.constant(0) delta = tf.constant(dog_imgs) new_delta, _ = tf.while_loop(cond, update_delta, loop_vars=[delta, i]) image = tf.get_default_graph().get_tensor_by_name(input_name) new_image = image + new_delta return tf.stop_gradient(new_image) 
core|avod|update|evaluator|cls|losses|loc|Evaluator|box|orient def _update_avod_box_cls_loc_orient_losses(self, eval_avod_losses, eval_losses, eval_total_loss, global_step): """Helper function to calculate the evaluation average losses.  Note: This function evaluates classification, regression/offsets and orientation losses.  Args: eval_avod_losses: A dictionary containing all the average losses. eval_losses: A dictionary containing the current evaluation losses. eval_total_loss: A scalar loss of model total loss. global_step: Global step at which the metrics are computed. """ sum_avod_cls_loss = eval_avod_losses[KEY_SUM_AVOD_CLS_LOSS] sum_avod_reg_loss = eval_avod_losses[KEY_SUM_AVOD_REG_LOSS] sum_avod_total_loss = eval_avod_losses[KEY_SUM_AVOD_TOTAL_LOSS] assert len(eval_losses) > 2 avod_classification_loss = eval_losses[AvodModel.LOSS_FINAL_CLASSIFICATION] avod_regression_loss = eval_losses[AvodModel.LOSS_FINAL_REGRESSION] avod_localization_loss = eval_losses[AvodModel.LOSS_FINAL_LOCALIZATION] avod_orientation_loss = eval_losses[AvodModel.LOSS_FINAL_ORIENTATION] sum_avod_cls_loss += avod_classification_loss sum_avod_reg_loss += avod_regression_loss sum_avod_total_loss += eval_total_loss eval_avod_losses.update({KEY_SUM_AVOD_CLS_LOSS: sum_avod_cls_loss}) eval_avod_losses.update({KEY_SUM_AVOD_REG_LOSS: sum_avod_reg_loss}) eval_avod_losses.update({KEY_SUM_AVOD_TOTAL_LOSS: sum_avod_total_loss}) if avod_localization_loss > 0.0 and avod_orientation_loss > 0.0: sum_avod_loc_loss = eval_avod_losses[KEY_SUM_AVOD_LOC_LOSS] sum_avod_ang_loss = eval_avod_losses[KEY_SUM_AVOD_ANG_LOSS] sum_avod_loc_loss += avod_localization_loss sum_avod_ang_loss += avod_orientation_loss eval_avod_losses.update({KEY_SUM_AVOD_LOC_LOSS: sum_avod_loc_loss}) eval_avod_losses.update({KEY_SUM_AVOD_ANG_LOSS: sum_avod_ang_loss}) num_valid_regression_samples = eval_avod_losses[ KEY_NUM_VALID_REG_SAMPLES] num_valid_regression_samples += 1 eval_avod_losses.update({KEY_NUM_VALID_REG_SAMPLES: num_valid_regression_samples}) print( 'Step {}: Eval AVOD Loss: classification {:.3f}, regression {:.3f}, total {:.3f}' .format(global_step, avod_classification_loss, avod_regression_loss, eval_total_loss)) print('Step {}: Eval AVOD Loss: localization {:.3f}, orientation {:.3f}' .format(global_step, avod_localization_loss, avod_orientation_loss)) 
remove|utils|none def remove_none(l): return [e for e in l if e is not None] 
unpickle|providers|data def unpickle(file): import cPickle with open(file, 'rb') as fo: dict = cPickle.load(fo) return dict 
gen|Gen|MiniImgNet|omni|init|data def __init__(self, path='/tmp/data/miniimagenet', data_path=None): if data_path is None: self.path = path self.train_paths = [('train/' + x) for x in os.listdir(path + '/train') ] self.test_paths = [('test/' + x) for x in os.listdir(path + '/test')] self.val_paths = [('val/' + x) for x in os.listdir(path + '/val')] self.data_path = data_path self.meta_train = None self.meta_test = None self.meta_val = None 
models|encoder|rnnsearch|gru|thumt def _gru_encoder(cell, inputs, sequence_length, initial_state, dtype=None): output_size = cell.output_size dtype = dtype or inputs.dtype batch = tf.shape(inputs)[0] time_steps = tf.shape(inputs)[1] zero_output = tf.zeros([batch, output_size], dtype) if initial_state is None: initial_state = cell.zero_state(batch, dtype) input_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'input_array') output_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'output_array') input_ta = input_ta.unstack(tf.transpose(inputs, [1, 0, 2]))  def loop_func(t, out_ta, state): inp_t = input_ta.read(t) cell_output, new_state = cell(inp_t, state) cell_output = _copy_through(t, sequence_length, zero_output, cell_output) new_state = _copy_through(t, sequence_length, state, new_state) out_ta = out_ta.write(t, cell_output) return t + 1, out_ta, new_state time = tf.constant(0, dtype=tf.int32, name='time') loop_vars = time, output_ta, initial_state outputs = tf.while_loop(lambda t, *_: t < time_steps, loop_func, loop_vars, parallel_iterations=32, swap_memory=True) output_final_ta = outputs[1] final_state = outputs[2] all_output = output_final_ta.stack() all_output.set_shape([None, None, output_size]) all_output = tf.transpose(all_output, [1, 0, 2]) return all_output, final_state 
scorer|override|parameters def override_parameters(params, args): if args.parameters: params.parse(args.parameters) params.vocabulary = {'source': vocabulary.load_vocabulary(args. vocabulary[0]), 'target': vocabulary.load_vocabulary(args. vocabulary[1])} params.vocabulary['source'] = vocabulary.process_vocabulary(params. vocabulary['source'], params) params.vocabulary['target'] = vocabulary.process_vocabulary(params. vocabulary['target'], params) control_symbols = [params.pad, params.bos, params.eos, params.unk] params.mapping = {'source': vocabulary.get_control_mapping(params. vocabulary['source'], control_symbols), 'target': vocabulary. get_control_mapping(params.vocabulary['target'], control_symbols)} return params 
main|fn|model|infer def model_fn(features, labels, mode, config, params): return las_model_fn(features, labels, mode, config, params, binf2phone= binf2phone_np) 
texar|decoders|added|length|TextDataDecoder|data @property def added_length(self): """The added text length due to appended bos and eos tokens. """ return self._added_length 
kernels|params|Sequential|rank|GPSig|low|validate def _validate_low_rank_params(self, low_rank, num_components, rank_bound, sparsity): """ Validates the low-rank related arguments. """ if low_rank is not None and low_rank == True: if not type(low_rank) == bool: raise ValueError( 'Unknown low-rank argument: %s. It should be True of False.' % low_rank) if sparsity not in ['log', 'sqrt', 'subsample', 'subsample_gauss']: raise ValueError( "Unknown sparsity argument %s. Possible values are 'sqrt', 'log', 'subsample', 'subsample_gauss'." % sparsity) if rank_bound is not None and rank_bound <= 0: raise ValueError( 'The rank-bound in the low-rank algorithm must be either None or a positiv integer.' ) if num_components is None or num_components <= 0: raise ValueError( 'The number of components in the kernel approximation must be a positive integer.' ) else: low_rank = False return low_rank, num_components, rank_bound, sparsity 
Model|enas|models|cifar1|rl|valid|build def build_valid_rl(self, shuffle=False): print('-' * 80) print('Build valid graph on shuffled data') with tf.device('/cpu:0'): if not shuffle and self.data_format == 'NCHW': self.images['valid_original'] = np.transpose(self.images[ 'valid_original'], [0, 3, 1, 2]) x_valid_shuffle, y_valid_shuffle = tf.train.shuffle_batch([self. images['valid_original'], self.labels['valid_original']], batch_size=self.batch_size, capacity=25000, enqueue_many=True, min_after_dequeue=0, num_threads=16, seed=self.seed, allow_smaller_final_batch=True)  def _pre_process(x): x = tf.pad(x, [[4, 4], [4, 4], [0, 0]]) x = tf.random_crop(x, [32, 32, 3], seed=self.seed) x = tf.image.random_flip_left_right(x, seed=self.seed) if self.data_format == 'NCHW': x = tf.transpose(x, [2, 0, 1]) return x if shuffle: x_valid_shuffle = tf.map_fn(_pre_process, x_valid_shuffle, back_prop=False) logits = self._model(x_valid_shuffle, False, reuse=True) valid_shuffle_preds = tf.argmax(logits, axis=1) valid_shuffle_preds = tf.to_int32(valid_shuffle_preds) self.valid_shuffle_acc = tf.equal(valid_shuffle_preds, y_valid_shuffle) self.valid_shuffle_acc = tf.to_int32(self.valid_shuffle_acc) self.valid_shuffle_acc = tf.reduce_sum(self.valid_shuffle_acc) 
test|ResnetCompleteNetworkTest|nets|testFullyConvolutionalUnknownHeightWidth|v2|resnet def testFullyConvolutionalUnknownHeightWidth(self): batch = 2 height, width = 65, 65 global_pool = False inputs = create_test_input(batch, None, None, 3) with slim.arg_scope(resnet_utils.resnet_arg_scope()): output, _ = self._resnet_small(inputs, None, global_pool=global_pool) self.assertListEqual(output.get_shape().as_list(), [batch, None, None, 32]) images = create_test_input(batch, height, width, 3) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) output = sess.run(output, {inputs: images.eval()}) self.assertEqual(output.shape, (batch, 3, 3, 32)) 
compute|models|ge|dist|struc2vec|dtw def compute_dtw_dist(part_list, degreeList, dist_func): dtw_dist = {} for v1, nbs in part_list: lists_v1 = degreeList[v1] for v2 in nbs: lists_v2 = degreeList[v2] max_layer = min(len(lists_v1), len(lists_v2)) dtw_dist[v1, v2] = {} for layer in range(0, max_layer): dist, path = fastdtw(lists_v1[layer], lists_v2[layer], radius=1, dist=dist_func) dtw_dist[v1, v2][layer] = dist return dtw_dist 
call|custom|BilinearUpsampling|layer def call(self, inputs): return K.tf.image.resize_bilinear(inputs, (int(inputs.shape[1] * self. upsampling[0]), int(inputs.shape[2] * self.upsampling[1]))) 
logger|Trainer|train|set def set_logger(self): if cfg.toy: self.logger = SummaryWriter(utils.make_name(self.opt, prefix= 'garbage/logs/', eval_=True, do_epoch=False)) else: self.logger = SummaryWriter(utils.make_name(self.opt, prefix= 'logs/', eval_=True, do_epoch=False)) print('Logging Tensorboard Files at: {}'.format(self.logger.log_dir)) 
trainer|main def main(args): if args.distribute: distribute.enable_distributed_training() tf.logging.set_verbosity(tf.logging.INFO) model_cls = models.get_model(args.model) params = default_parameters() params = merge_parameters(params, model_cls.get_parameters()) params = import_params(args.output, args.model, params) override_parameters(params, args) if distribute.rank() == 0: export_params(params.output, 'params.json', params) export_params(params.output, '%s.json' % args.model, collect_params (params, model_cls.get_parameters())) with tf.Graph().as_default(): if not params.record: features = dataset.get_training_input(params.input, params) else: features = record.get_input_features(os.path.join(params.record, '*train*'), 'train', params) initializer = get_initializer(params) regularizer = tf.contrib.layers.l1_l2_regularizer(scale_l1=params. scale_l1, scale_l2=params.scale_l2) model = model_cls(params) global_step = tf.train.get_or_create_global_step() dtype = tf.float16 if args.half else None sharded_losses = parallel.parallel_model(model.get_training_func( initializer, regularizer, dtype), features, params.device_list) loss = tf.add_n(sharded_losses) / len(sharded_losses) loss = loss + tf.losses.get_regularization_loss() if distribute.rank() == 0: print_variables() learning_rate = get_learning_rate_decay(params.learning_rate, global_step, params) learning_rate = tf.convert_to_tensor(learning_rate, dtype=tf.float32) tf.summary.scalar('loss', loss) tf.summary.scalar('learning_rate', learning_rate) if params.optimizer == 'Adam': opt = tf.train.AdamOptimizer(learning_rate, beta1=params. adam_beta1, beta2=params.adam_beta2, epsilon=params. adam_epsilon) elif params.optimizer == 'LazyAdam': opt = tf.contrib.opt.LazyAdamOptimizer(learning_rate, beta1= params.adam_beta1, beta2=params.adam_beta2, epsilon=params. adam_epsilon) else: raise RuntimeError('Optimizer %s not supported' % params.optimizer) opt = optimizers.MultiStepOptimizer(opt, params.update_cycle) if args.half: opt = optimizers.LossScalingOptimizer(opt, params.loss_scale) grads_and_vars = opt.compute_gradients(loss, colocate_gradients_with_ops=True) if params.clip_grad_norm: grads, var_list = list(zip(*grads_and_vars)) grads, _ = tf.clip_by_global_norm(grads, params.clip_grad_norm) grads_and_vars = zip(grads, var_list) train_op = opt.apply_gradients(grads_and_vars, global_step=global_step) if params.validation and params.references[0]: files = [params.validation] + list(params.references) eval_inputs = dataset.sort_and_zip_files(files) eval_input_fn = dataset.get_evaluation_input else: eval_input_fn = None train_hooks = [tf.train.StopAtStepHook(last_step=params.train_steps ), tf.train.NanTensorHook(loss), tf.train.LoggingTensorHook({ 'step': global_step, 'loss': loss, 'source': tf.shape(features[ 'source']), 'target': tf.shape(features['target'])}, every_n_iter=1)] broadcast_hook = distribute.get_broadcast_hook() if broadcast_hook: train_hooks.append(broadcast_hook) if distribute.rank() == 0: save_vars = tf.trainable_variables() + [global_step] saver = tf.train.Saver(var_list=save_vars if params. only_save_trainable else None, max_to_keep=params. keep_checkpoint_max, sharded=False) tf.add_to_collection(tf.GraphKeys.SAVERS, saver) train_hooks.append(hooks.MultiStepHook(tf.train. CheckpointSaverHook(checkpoint_dir=params.output, save_secs =params.save_checkpoint_secs or None, save_steps=params. save_checkpoint_steps or None, saver=saver), step=params. update_cycle)) if eval_input_fn is not None: train_hooks.append(hooks.MultiStepHook(hooks.EvaluationHook (lambda f: inference.create_inference_graph([model], f, params), lambda : eval_input_fn(eval_inputs, params), lambda x: decode_target_ids(x, params), params.output, session_config(params), params.keep_top_checkpoint_max, eval_secs=params.eval_secs, eval_steps=params. eval_steps), step=params.update_cycle)) checkpoint_dir = params.output else: checkpoint_dir = None restore_op = restore_variables(args.checkpoint)  def restore_fn(step_context): step_context.session.run(restore_op) with tf.train.MonitoredTrainingSession(checkpoint_dir= checkpoint_dir, hooks=train_hooks, save_checkpoint_secs=None, config=session_config(params)) as sess: sess.run_step_fn(restore_fn) while not sess.should_stop(): sess.run(train_op) 
defend|defense|png def defend_png(input_array): pil_image = PIL.Image.fromarray(np.around(input_array * 255.0).astype( np.uint8)) f = BytesIO() pil_image.save(f, format='png') png_image = np.asarray(PIL.Image.open(f)).astype(np.float32) / 255.0 return png_image 
texar|list|decoders|items|data|ScalarDataDecoder def list_items(self): """Returns the list of item names that the decoder can produce.  Returns: A list of strings can be passed to :meth:`decode()`. """ return [self._data_name] 
utils|get|sha def get_sha(repo='.'): """ Grabs the current SHA-1 hash of the given directory's git HEAD-revision. The output of this is equivalent to calling git rev-parse HEAD.  Be aware that a missing git repository will make this return an error message, which is not a valid hash. """ sha = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=repo) return sha.decode('ascii').strip() 
list|from|embs|vocab|main def main(): parser = argparse.ArgumentParser() parser.add_argument('-i', '--embeddings', nargs='+', required=True) parser.add_argument('-o', '--output', type=str, required=True) args = parser.parse_args() printTrace('Loading vocabulary from embeddings...') vocab_embeddings = [vocab_from_path(x) for x in args.embeddings] union_vocab = set.union(*vocab_embeddings) printTrace('Te union of the vocabulary has ' + str(len(union_vocab)) + ' words.') printTrace('Printing vocabulary in ' + args.output + '...') with open(args.output, 'w+') as file: for word in union_vocab: print(word, file=file) 
attention|nmt|AttentionWrapper|utils|call def call(self, inputs, state): """Perform a step of attention-wrapped RNN.  - Step 1: Mix the `inputs` and previous step's `attention` output via `cell_input_fn`. - Step 2: Call the wrapped `cell` with this input and its previous state. - Step 3: Score the cell's output with `attention_mechanism`. - Step 4: Calculate the alignments by passing the score through the `normalizer`. - Step 5: Calculate the context vector as the inner product between the alignments and the attention_mechanism's values (memory). - Step 6: Calculate the attention output by concatenating the cell output and context through the attention layer (a linear layer with `attention_layer_size` outputs).  Args: inputs: (Possibly nested tuple of) Tensor, the input at this time step. state: An instance of `AttentionWrapperState` containing tensors from the previous time step.  Returns: A tuple `(attention_or_cell_output, next_state)`, where:  - `attention_or_cell_output` depending on `output_attention`. - `next_state` is an instance of `AttentionWrapperState` containing the state calculated at this time step.  Raises: TypeError: If `state` is not an instance of `AttentionWrapperState`. """ if not isinstance(state, AttentionWrapperState): raise TypeError( 'Expected state to be instance of AttentionWrapperState. Received type %s instead.' % type(state)) cell_inputs = self._cell_input_fn(inputs, state.attention) cell_state = state.cell_state cell_output, next_cell_state = self._cell(cell_inputs, cell_state) cell_batch_size = cell_output.shape[0].value or array_ops.shape(cell_output )[0] error_message = ('When applying AttentionWrapper %s: ' % self.name + 'Non-matching batch sizes between the memory (encoder output) and the query (decoder output).  Are you using the BeamSearchDecoder?  You may need to tile your memory input via the tf.contrib.seq2seq.tile_batch function with argument multiple=beam_width.' ) with ops.control_dependencies(self._batch_size_checks(cell_batch_size, error_message)): cell_output = array_ops.identity(cell_output, name= 'checked_cell_output') if self._is_multi: previous_attention_state = state.attention_state previous_alignment_history = state.alignment_history else: previous_attention_state = [state.attention_state] previous_alignment_history = [state.alignment_history] all_alignments = [] all_attentions = [] all_attention_states = [] maybe_all_histories = [] for i, attention_mechanism in enumerate(self._attention_mechanisms): attention, alignments, next_attention_state = _compute_attention( attention_mechanism, cell_output, previous_attention_state[i], self._attention_layers[i] if self._attention_layers else None) alignment_history = previous_alignment_history[i].write(state.time, alignments) if self._alignment_history else () all_attention_states.append(next_attention_state) all_alignments.append(alignments) all_attentions.append(attention) maybe_all_histories.append(alignment_history) attention = array_ops.concat(all_attentions, 1) next_state = AttentionWrapperState(time=state.time + 1, cell_state= next_cell_state, attention=attention, attention_state=self. _item_or_tuple(all_attention_states), alignments=self. _item_or_tuple(all_alignments), alignment_history=self. _item_or_tuple(maybe_all_histories)) if self._output_attention: return attention, next_state else: return cell_output, next_state 
fisher|compute|ops|grads|classification|tensors|blocks|FullyConnectedDiagonalFB|to def tensors_to_compute_grads(self): """Tensors to compute derivative of loss with respect to.""" return self._outputs 
collection|get|ops|LayerCollection|scale|classification|make|or|layer|factor def make_or_get_scale_factor(self, cls, args): try: hash(args) except TypeError: raise TypeError( 'Unable to use (cls, args) = ({}, {}) as a key in LayerCollection.fisher_factors. The pair cannot be hashed.' .format(cls, args)) key = cls, args if key not in self.scale_factors: colo = self._colocate_cov_ops_with_inputs with variable_scope.variable_scope(self._var_scope): self.scale_factors[key] = cls(*args, colocate_cov_ops_with_inputs=colo) return self.scale_factors[key] 
log|set|MaskRCNN|model|dir def set_log_dir(self, model_path=None): """Sets the model log directory and epoch counter.  model_path: If None, or a format different from what this code uses then set a new log directory and start epochs from 0. Otherwise, extract the log directory and the epoch counter from the file name. """ self.epoch = 0 now = datetime.datetime.now() if model_path: regex = ( '.*/\\w+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})/mask\\_rcnn\\_\\w+(\\d{4})\\.h5' ) m = re.match(regex, model_path) if m: now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m .group(3)), int(m.group(4)), int(m.group(5))) self.epoch = int(m.group(6)) self.log_dir = os.path.join(self.model_dir, '{}{:%Y%m%dT%H%M}'.format( self.config.NAME.lower(), now)) if not os.path.exists(self.log_dir): os.makedirs(self.log_dir) self.checkpoint_path = os.path.join(self.log_dir, 'mask_rcnn_{}_*epoch*.h5'.format(self.config.NAME.lower())) self.checkpoint_path = self.checkpoint_path.replace('*epoch*', '{epoch:04d}') 
ukernel|test|generate|split|name|argmaxpool def split_ukernel_name(name): match = re.match( '^xnn_(f16|f32)_argmaxpool_ukernel_((\\d+)p)?(\\d+)x__(.+)_c(\\d+)$', name) if match is None: raise ValueError('Unexpected microkernel name: ' + name) if match.group(2): primary_tile = int(match.group(3)) incremental_tile = int(match.group(4)) else: primary_tile = int(match.group(4)) incremental_tile = 0 channel_tile = int(match.group(6)) arch, isa = xnncommon.parse_target_name(target_name=match.group(5)) return primary_tile, incremental_tile, channel_tile, arch, isa 
pad|labels|image|and|reader|crop|random|deeplab|resnet def random_crop_and_pad_image_and_labels(image, label, crop_h, crop_w, ignore_label=255): """ Randomly crop and pads the input images.  Args: image: Training image to crop/ pad. label: Segmentation mask to crop/ pad. crop_h: Height of cropped segment. crop_w: Width of cropped segment. ignore_label: Label to ignore during the training. """ label = tf.cast(label, dtype=tf.float32) label = label - ignore_label combined = tf.concat(axis=2, values=[image, label]) image_shape = tf.shape(image) combined_pad = tf.image.pad_to_bounding_box(combined, 0, 0, tf.maximum( crop_h, image_shape[0]), tf.maximum(crop_w, image_shape[1])) last_image_dim = tf.shape(image)[-1] last_label_dim = tf.shape(label)[-1] combined_crop = tf.random_crop(combined_pad, [crop_h, crop_w, 4]) img_crop = combined_crop[:, :, :last_image_dim] label_crop = combined_crop[:, :, last_image_dim:] label_crop = label_crop + ignore_label label_crop = tf.cast(label_crop, dtype=tf.uint8) img_crop.set_shape((crop_h, crop_w, 3)) label_crop.set_shape((crop_h, crop_w, 1)) return img_crop, label_crop 
WS353|get|all|datasets|ENES|similarity def get_WS353_ENES_all(): data = pd.read_csv(WS353_ENES_all_path, sep='\t', header=None).values return Bunch(X=data[:, 0:2].astype('object'), y=data[:, (2)].astype(np. float)) 
lottery|prune|nmt|generic|amc|functions def prune_amc_generic(iteration):  def pruner(names, weights, masks):  def do_prune(name, weights, masks): percent = iteration[name] weights = weights * masks weights = np.abs(weights) threshold = np.percentile(weights.ravel(), (1 - percent) * 100) old_frac = masks.sum() / np.prod(masks.shape) masks[weights < threshold] = 0 print('{}: {}->{}'.format(name, old_frac, masks.sum() / np.prod (masks.shape)), flush=True) return masks return [do_prune(name, weight, mask) for name, weight, mask in zip( names, weights, masks)] return pruner 
cleverhans|attacks|generate|Attack|np def generate_np(self, x_val, **kwargs): """ Generate adversarial examples and return them as a NumPy array. Sub-classes *should not* implement this method unless they must perform special handling of arguments. :param x_val: A NumPy array with the original inputs. :param **kwargs: optional parameters used by child classes. :return: A NumPy array holding the adversarial examples. """ if self.sess is None: raise ValueError('Cannot use `generate_np` when no `sess` was provided' ) fixed = dict((k, v) for k, v in kwargs.items() if k in self. structural_kwargs) feedable = dict((k, v) for k, v in kwargs.items() if k in self. feedable_kwargs) if len(fixed) + len(feedable) < len(kwargs): warnings.warn( 'Supplied extra keyword arguments that are not used in the graph computation. They have been ignored.' ) if not all(isinstance(value, collections.Hashable) for value in fixed. values()): hash_key = None else: hash_key = tuple(sorted(fixed.items())) if hash_key not in self.graphs: self.construct_graph(fixed, feedable, x_val, hash_key) x, new_kwargs, x_adv = self.graphs[hash_key] feed_dict = {x: x_val} for name in feedable: feed_dict[new_kwargs[name]] = feedable[name] return self.sess.run(x_adv, feed_dict) 
train|dataflow|text|get def get_train_dataflow(roidb): """ Tensorpack text dataflow. """ ds = DataFromList(roidb, shuffle=True) preprocess = TextDataPreprocessor(cfg) buffer_size = cfg.num_threads * 10 ds = MultiThreadMapData(ds, cfg.num_threads, preprocess, buffer_size= buffer_size) ds = PrefetchData(ds, 100, multiprocessing.cpu_count() // 4) return ds 
texar|decoders|name|TextDataDecoder|data|tensor|text @text_tensor_name.setter def text_tensor_name(self, name): self._text_tensor_name = name 
pad|features|xlnet|labels|and|PaddingSignals|estimator|tpu|master|tensor|single def pad_single_tensor(tensor): """Pads out the batch dimension of a tensor to the complete batch_size.""" rank = len(tensor.shape) assert rank > 0 padding = array_ops.stack([[0, missing_count]] + [[0, 0]] * (rank - 1)) padded_shape = (batch_size,) + tuple(tensor.shape[1:]) padded_tensor = array_ops.pad(tensor, padding) padded_tensor.set_shape(padded_shape) return padded_tensor 
BasicRNNDecoder|rnn|texar|modules|decoders|step def step(self, time, inputs, state, name=None): cell_outputs, cell_state = self._cell(inputs, state) logits = self._output_layer(cell_outputs) sample_ids = self._helper.sample(time=time, outputs=logits, state= cell_state) reach_max_time = tf.equal(time + 1, self.max_decoding_length) finished, next_inputs, next_state = self._helper.next_inputs(time=time, outputs=logits, state=cell_state, sample_ids=sample_ids, reach_max_time=reach_max_time) outputs = BasicRNNDecoderOutput(logits, sample_ids, cell_outputs) return outputs, next_state, next_inputs, finished 
shuffle|corpus|main def main(args): name = args.corpus suffix = '.' + args.suffix stream = [_open(item, 'r') for item in name] data = [fd.readlines() for fd in stream] minlen = min([len(lines) for lines in data]) count = 0 if args.seed: numpy.random.seed(args.seed) indices = numpy.arange(minlen) numpy.random.shuffle(indices) if args.num_shards == 1: newstream = [[_open(item + suffix, 'w') for item in name]] else: newstream = [[_open(item + '-%s-of-%s' % (i, args.num_shards), 'w') for item in name] for i in range(args.num_shards)] for idx in indices.tolist(): lines = [item[idx] for item in data] for line, fd in zip(lines, newstream[count % args.num_shards]): fd.write(line) count += 1 for fdr in stream: fdr.close() for fds in newstream: for fd in fds: fd.close() 
models|chaos|MacArthur|init def __init__(self, r=None, k=None, c=None, d=None, s=None, m=None): """ Inputs """ if r == None: self.set_defaults() else: assert len(s) == k.shape[0], "vector 's' has improper dimensionality" assert k.shape == c.shape, 'K and C matrices must have matching dimensions' self.r = r self.k = k self.c = c self.d = d self.s = s self.m = m self.n_resources, self.n_species = self.k.shape 
AgentsTest|test|cell|agents|unroll def test_unroll_cell(self): zero_state = (tf.constant([[0]]),) * 3 output, state = agents._unroll_cell(inputs=[[[1]]], done=[[False]], start_state=zero_state, zero_state=zero_state, recurrent_cell=stack_fn) self.assertAllEqual(output, [[[0, 0, 0, 1]]]) output, state = agents._unroll_cell(inputs=[[[2]]], done=[[False]], start_state=state, zero_state=zero_state, recurrent_cell=stack_fn) self.assertAllEqual(output, [[[0, 0, 1, 2]]]) output, state = agents._unroll_cell(inputs=[[[3]], [[4]], [[5]], [[6]], [[7]], [[8]]], done=[[False]] * 6, start_state=state, zero_state= zero_state, recurrent_cell=stack_fn) self.assertEqual(output.shape[0], 6) self.assertAllEqual(output[0], [[0, 1, 2, 3]]) self.assertAllEqual(output[5], [[5, 6, 7, 8]]) 
ablation|posterior|distribution|agents|SimpleModelDistributionNetwork|model|network|sample|slac def sample_posterior(self, images, actions, step_types, features=None): sequence_length = step_types.shape[1].value - 1 actions = actions[:, :sequence_length] if features is None: features = self.compressor(images) features = tf.transpose(features, [1, 0, 2]) actions = tf.transpose(actions, [1, 0, 2]) step_types = tf.transpose(step_types, [1, 0]) latent_dists = [] latent_samples = [] for t in range(sequence_length + 1): if t == 0: latent_dist = self.latent_first_posterior(features[t]) latent_sample = latent_dist.sample() else: reset_mask = tf.equal(step_types[t], ts.StepType.FIRST) latent_first_dist = self.latent_first_posterior(features[t]) latent_dist = self.latent_posterior(features[t], latent_samples [t - 1], actions[t - 1]) latent_dist = nest_utils.map_distribution_structure(functools. partial(tf.where, reset_mask), latent_first_dist, latent_dist) latent_sample = latent_dist.sample() latent_dists.append(latent_dist) latent_samples.append(latent_sample) latent_dists = nest_utils.map_distribution_structure(lambda *x: tf. stack(x, axis=1), *latent_dists) latent_samples = tf.stack(latent_samples, axis=1) return latent_samples, latent_dists 
attention|nmt|LuongAttention|utils|call def __call__(self, query, state): """Score the query based on the keys and values.  Args: query: Tensor of dtype matching `self.values` and shape `[batch_size, query_depth]`. state: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`).  Returns: alignments: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). """ with variable_scope.variable_scope(None, 'luong_attention', [query]): score = _luong_score(query, self._keys, self._scale) alignments = self._probability_fn(score, state) next_state = alignments return alignments, next_state 
testNTKGDPrediction|predict|test|PredictTest @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train={}_test={}_network={}_logits={}_{}'.format(train, test, network, out_logits, name), 'train_shape': train, 'test_shape': test, 'network': network, 'out_logits': out_logits, 'fn_and_kernel': fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for out_logits in OUTPUT_LOGITS for name, fn in KERNELS.items())) def testNTKGDPrediction(self, train_shape, test_shape, network, out_logits, fn_and_kernel): key = random.PRNGKey(0) key, split = random.split(key) x_train = random.normal(split, train_shape) key, split = random.split(key) y_train = np.array(random.bernoulli(split, shape=(train_shape[0], out_logits)), np.float32) key, split = random.split(key) x_test = random.normal(split, test_shape) params, f, ntk = fn_and_kernel(key, train_shape[1:], network, out_logits) loss = lambda y, y_hat: 0.5 * np.mean((y - y_hat) ** 2) grad_loss = jit(grad(lambda params, x: loss(f(params, x), y_train))) g_dd = ntk(x_train, None, 'ntk') g_td = ntk(x_test, x_train, 'ntk') predictor = predict.gradient_descent(g_dd, y_train, loss, g_td) atol = ATOL rtol = RTOL step_size = 0.5 if len(train_shape) > 2: atol = ATOL * 2 rtol = RTOL * 2 step_size = 0.1 train_time = 100.0 steps = int(train_time / step_size) opt_init, opt_update, get_params = optimizers.sgd(step_size) opt_state = opt_init(params) fx_initial_train = f(params, x_train) fx_initial_test = f(params, x_test) fx_pred_train, fx_pred_test = predictor(0.0, fx_initial_train, fx_initial_test) self.assertAllClose(fx_initial_train, fx_pred_train, True) self.assertAllClose(fx_initial_test, fx_pred_test, True) for i in range(steps): params = get_params(opt_state) opt_state = opt_update(i, grad_loss(params, x_train), opt_state) params = get_params(opt_state) fx_train = f(params, x_train) fx_test = f(params, x_test) fx_pred_train, fx_pred_test = predictor(train_time, fx_initial_train, fx_initial_test) fx_disp_train = np.sqrt(np.mean((fx_train - fx_initial_train) ** 2)) fx_disp_test = np.sqrt(np.mean((fx_test - fx_initial_test) ** 2)) fx_error_train = (fx_train - fx_pred_train) / fx_disp_train fx_error_test = (fx_test - fx_pred_test) / fx_disp_test self.assertAllClose(fx_error_train, np.zeros_like(fx_error_train), True, rtol, atol) self.assertAllClose(fx_error_test, np.zeros_like(fx_error_test), True, rtol, atol) 
texar|get|FeedableDataIterator|iterators|next|data def get_next(self): """Returns the next element of the activated dataset. """ return self._iterator.get_next() 
get|XLNetModel|xlnet|master|embedding|table def get_embedding_table(self): """ Returns: float32 Tensor in shape [n_token, d_model]. The embedding lookup table. Used for tying embeddings between input and output layers. """ return self.lookup_table 
rnn|beamsearch|output|size|model|PCGNBeamSearchDecoder|PCGN def _rnn_output_size(self): size = self._cell.output_size if self._output_layer is None: return size else: output_shape_with_unknown_batch = nest.map_structure(lambda s: tensor_shape.TensorShape([None]).concatenate(s), size) layer_output_shape = self._output_layer._compute_output_shape( output_shape_with_unknown_batch) return nest.map_structure(lambda s: s[1:], layer_output_shape) 
init|layers|TernaryFullyConnectedLayer def __init__(self, out_dim, reg_factor=1e-07, ema_decay_rate=0.5, rho=0.1, dtype=tf.float32): """Initializer  Arguments: out_dim {int} -- output dimension  Keyword Arguments: reg_factor {float} -- regularization coef for sparsity (default: {1e-7}) ema_decay_rate {float} -- decay rate (default: {0.5}) rho {float} -- rho (default: {1e-1}) dtype {tf dtype} -- data type (default: {tf.float32}) """ super(TernaryFullyConnectedLayer, self).__init__(out_dim, ema_decay_rate, rho, dtype) self.reg_factor = reg_factor self.init_func = lambda shape: np.random.choice([-1.0, 0.0, 1.0], size= shape) self.name = 'Ternary-fully-connected' 
Encoder|init|transformer def __init__(self, num_layers=48, d_model_size=1280, num_heads=16, dff=8192, input_vocab_size=50000, rate=0.1, **kwargs): super(Encoder, self).__init__() self.d_model_size = d_model_size self.num_layers = num_layers self.pos_encoding = positional_encoding(input_vocab_size, self.d_model_size ) for i in range(num_layers): setattr(self, 'layer%i' % i, EncoderLayer(d_model_size, num_heads, dff, rate)) self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-06) self.dropout = tf.keras.layers.Dropout(rate) 
rnn|texar|test|cell|modules|decoders|AttentionRNNDecoderTest|search|beam def test_beam_search_cell(self): """Tests :meth:`texar.modules.AttentionRNNDecoder._get_beam_search_cell` """ seq_length = np.random.randint(self._max_time, size=[self._batch_size]) + 1 encoder_values_length = tf.constant(seq_length) hparams = {'attention': {'kwargs': {'num_units': self._attention_dim, 'probability_fn': 'sparsemax'}}} decoder = AttentionRNNDecoder(memory=self._encoder_output, memory_sequence_length=encoder_values_length, vocab_size=self. _vocab_size, hparams=hparams) helper_train = get_helper(decoder.hparams.helper_train.type, inputs= self._inputs, sequence_length=[self._max_time] * self._batch_size, **decoder.hparams.helper_train.kwargs.todict()) _, _, _ = decoder(helper=helper_train) self.assertEqual(len(decoder.trainable_variables), 5) beam_width = 3 beam_cell = decoder._get_beam_search_cell(beam_width) cell_input = tf.random_uniform([self._batch_size * beam_width, self. _emb_dim]) cell_state = beam_cell.zero_state(self._batch_size * beam_width, tf.float32 ) _ = beam_cell(cell_input, cell_state) for tvar in beam_cell.trainable_variables: self.assertTrue(tvar in decoder.trainable_variables) 
envs|space|Environment|hac|hbaselines|context @property def context_space(self): """Return the shape and bounds of the contextual term.""" if self.use_contexts: if self.random_contexts: context_low = [] context_high = [] for context_i in self.context_range: low, high = context_i context_low.append(low) context_high.append(high) return Box(low=np.asarray(context_low), high=np.asarray( context_high), dtype=np.float32) else: return Box(low=np.asarray(self.context_range), high=np.asarray( self.context_range), dtype=np.float32) else: return None 
distribution|agents|model|network|call|Bernoulli|slac def __call__(self, *inputs): if len(inputs) > 1: inputs = tf.concat(inputs, axis=-1) else: inputs, = inputs out = self.dense1(inputs) out = self.dense2(out) out = self.output_layer(out) logits = tf.squeeze(out, axis=-1) return tfd.Bernoulli(logits=logits) 
update|plato|SGUIController|controller|text|sgui def update_text(self, key, history, utterance): self.window.Element(key).Update(history) if utterance and self.tts: self.speak(utterance) 
def|get|amb|dir|expt def get_expt_dir(hparams): dataset_dir = '{}/'.format(hparams.dataset) task_dir = get_task_dir(hparams) mode_dir = get_mode_dir(hparams) expt_dir = dataset_dir + task_dir + mode_dir return expt_dir 
plot|deepMOT|master|main|results def main(args, colorList): seqList = os.listdir(args.results_path) for seq_name in seqList: if os.path.exists(args.data_root + args.dataset + '/train/' + seq_name[:-4]): path_data = args.data_root + args.dataset + '/train/' + seq_name[: -4] + '/' elif os.path.exists(args.data_root + args.dataset + 'test/' + seq_name[:-4]): path_data = args.data_root + args.dataset + 'test/' + seq_name[:-4 ] + '/' else: continue path_res = args.results_path + seq_name path_images = path_data + 'img1/' save_path = args.save_path + seq_name[:-4] + '/' if not os.path.exists(save_path): os.makedirs(save_path) res_raw = pd.read_csv(path_res, sep=',', header=None) res_raw = np.array(res_raw).astype(np.float32) res_raw[:, 0:6] = np.array(res_raw[:, 0:6]).astype(np.int) N_frame = max(res_raw[:, (0)]) print('total number of frames: ', N_frame) for t in range(1, int(N_frame)): if os.path.exists(save_path + str(t).zfill(6) + '.jpg'): continue print('t = ' + str(t)) img_name = path_images + str(t).zfill(6) + '.jpg' print(img_name) img = cv2.imread(img_name) overlay = img.copy() row_ind = np.where(res_raw[:, (0)] == t)[0] for i in range(0, row_ind.shape[0]): id = int(max(res_raw[row_ind[i], 1], 0)) color_ind = id % len(colorList) row_ind_line = np.where((res_raw[:, (0)] > t - 50) & ( res_raw[:, (0)] < t + 1) & (res_raw[:, (1)] == id))[0] for j in range(0, row_ind_line.shape[0], 5): line_xc = int(res_raw[row_ind_line[j], 2] + 0.5 * res_raw[row_ind_line[j], 4]) line_yc = int(res_raw[row_ind_line[j], 3] + res_raw[ row_ind_line[j], 5]) bb_w = 5 line_x1 = line_xc - bb_w line_y1 = line_yc - bb_w line_x2 = line_xc + bb_w line_y2 = line_yc + bb_w cv2.rectangle(overlay, (line_x1, line_y1), (line_x2, line_y2), colorList[color_ind], -1) t_past = res_raw[row_ind_line[j], 0] alpha = 1 - (t - t_past) / 80 img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0) overlay = img.copy() for i in range(0, row_ind.shape[0]): id = int(res_raw[row_ind[i], 1]) bb_x1 = int(res_raw[row_ind[i], 2]) bb_y1 = int(res_raw[row_ind[i], 3]) bb_x2 = int(res_raw[row_ind[i], 2] + res_raw[row_ind[i], 4]) bb_y2 = int(res_raw[row_ind[i], 3] + res_raw[row_ind[i], 5]) str_tmp = str(i) + ' ' + str('{0:.2f}'.format(res_raw[ row_ind[i], 6])) color_ind = id % len(colorList) cv2.rectangle(overlay, (bb_x1, bb_y1), (bb_x2, bb_y2), colorList[color_ind], 3) save_name = save_path + str(t).zfill(6) + '.jpg' cv2.imwrite(save_name, overlay) 
texar|decoders|ScalarDataDecoder|data|decode def decode(self, data, items): """Decodes the data to return the tensors specified by the list of items.  Args: data: The scalar data to decode. items: A list of strings, each of which is the name of the resulting tensors to retrieve.  Returns: A list of tensors, each of which corresponds to each item. """ data = tf.reshape(data, shape=[]) if data.dtype is tf.string: decoded_data = tf.string_to_number(data, out_type=self._dtype) else: decoded_data = tf.cast(data, self._dtype) outputs = {self._data_name: decoded_data} return [outputs[item] for item in items] 
rnn|time|transpose|deepctr|batch|contrib def _transpose_batch_time(x): """Transpose the batch and time dimensions of a Tensor.    Retains as much of the static shape information as possible.    Args:  x: A tensor of rank 2 or higher.    Returns:  x transposed along the first two dimensions.    Raises:  ValueError: if `x` is rank 1 or lower.  """ x_static_shape = x.get_shape() if x_static_shape.ndims is not None and x_static_shape.ndims < 2: raise ValueError( 'Expected input tensor %s to have rank at least 2, but saw shape: %s' % (x, x_static_shape)) x_rank = array_ops.rank(x) x_t = array_ops.transpose(x, array_ops.concat(([1, 0], math_ops.range(2, x_rank)), axis=0)) x_t.set_shape(tensor_shape.TensorShape([x_static_shape[1].value, x_static_shape[0].value]).concatenate(x_static_shape[2:])) return x_t 
child|enas|fixed|general|cifar1|GeneralChild|layer def _fixed_layer(self, layer_id, prev_layers, start_idx, out_filters, is_training): """ Args: layer_id: current layer prev_layers: cache of previous layers. for skip connections start_idx: where to start looking at. technically, we can infer this from layer_id, but why bother... is_training: for batch_norm """ inputs = prev_layers[-1] if self.whole_channels: if self.data_format == 'NHWC': inp_c = inputs.get_shape()[3].value elif self.data_format == 'NCHW': inp_c = inputs.get_shape()[1].value count = self.sample_arc[start_idx] if count in [0, 1, 2, 3]: size = [3, 3, 5, 5] filter_size = size[count] with tf.variable_scope('conv_1x1'): w = create_weight('w', [1, 1, inp_c, out_filters]) out = tf.nn.relu(inputs) out = tf.nn.conv2d(out, w, [1, 1, 1, 1], 'SAME', data_format=self.data_format) out = batch_norm(out, is_training, data_format=self.data_format ) with tf.variable_scope('conv_{0}x{0}'.format(filter_size)): w = create_weight('w', [filter_size, filter_size, out_filters, out_filters]) out = tf.nn.relu(out) out = tf.nn.conv2d(out, w, [1, 1, 1, 1], 'SAME', data_format=self.data_format) out = batch_norm(out, is_training, data_format=self.data_format ) elif count == 4: pass elif count == 5: pass else: raise ValueError("Unknown operation number '{0}'".format(count)) else: count = self.sample_arc[start_idx:start_idx + 2 * self.num_branches ] * self.out_filters_scale branches = [] total_out_channels = 0 with tf.variable_scope('branch_0'): total_out_channels += count[1] branches.append(self._conv_branch(inputs, 3, is_training, count[1]) ) with tf.variable_scope('branch_1'): total_out_channels += count[3] branches.append(self._conv_branch(inputs, 3, is_training, count [3], separable=True)) with tf.variable_scope('branch_2'): total_out_channels += count[5] branches.append(self._conv_branch(inputs, 5, is_training, count[5]) ) with tf.variable_scope('branch_3'): total_out_channels += count[7] branches.append(self._conv_branch(inputs, 5, is_training, count [7], separable=True)) if self.num_branches >= 5: with tf.variable_scope('branch_4'): total_out_channels += count[9] branches.append(self._pool_branch(inputs, is_training, count[9], 'avg')) if self.num_branches >= 6: with tf.variable_scope('branch_5'): total_out_channels += count[11] branches.append(self._pool_branch(inputs, is_training, count[11], 'max')) with tf.variable_scope('final_conv'): w = create_weight('w', [1, 1, total_out_channels, out_filters]) if self.data_format == 'NHWC': branches = tf.concat(branches, axis=3) elif self.data_format == 'NCHW': branches = tf.concat(branches, axis=1) out = tf.nn.relu(branches) out = tf.nn.conv2d(out, w, [1, 1, 1, 1], 'SAME', data_format= self.data_format) out = batch_norm(out, is_training, data_format=self.data_format) if layer_id > 0: if self.whole_channels: skip_start = start_idx + 1 else: skip_start = start_idx + 2 * self.num_branches skip = self.sample_arc[skip_start:skip_start + layer_id] total_skip_channels = np.sum(skip) + 1 res_layers = [] for i in range(layer_id): if skip[i] == 1: res_layers.append(prev_layers[i]) prev = res_layers + [out] if self.data_format == 'NHWC': prev = tf.concat(prev, axis=3) elif self.data_format == 'NCHW': prev = tf.concat(prev, axis=1) out = prev with tf.variable_scope('skip'): w = create_weight('w', [1, 1, total_skip_channels * out_filters, out_filters]) out = tf.nn.relu(out) out = tf.nn.conv2d(out, w, [1, 1, 1, 1], 'SAME', data_format= self.data_format) out = batch_norm(out, is_training, data_format=self.data_format) return out 
generator|list|pair|nozip|perquery|nprf|batch|drmm|utils|NPRFDRMMPairGenerator def list_batch_nozip_perquery(self, qid, topk): relevance = self.relevance_dict.get(qid) supervised_docid_list = relevance.get_supervised_docid_list() rerank_docid_list = supervised_docid_list[:topk] len_indicator = len(rerank_docid_list) dd_d = np.zeros((len_indicator, self.nb_supervised_doc, self. doc_topk_term, self.hist_size), dtype=np.float32) topk_score_list = relevance.get_supervised_score_list()[:self. nb_supervised_doc] max_score, min_score = topk_score_list[0], topk_score_list[-1] topk_score_list = np.asarray(topk_score_list, dtype=np.float32) topk_score_list = 0.5 * (topk_score_list - min_score) / (max_score - min_score) + 0.5 score = np.tile(topk_score_list, len_indicator) score = score.astype(np.float32).reshape((len_indicator, self. nb_supervised_doc, 1)) dd_q_dict = self.dd_q_gating_dict.get(qid) topk_supervised_docid_list = supervised_docid_list[:self.nb_supervised_doc] dd_q_feat = [dd_q_dict.get(d)[:self.doc_topk_term] for d in topk_supervised_docid_list] dd_q_feat = np.asarray(dd_q_feat, dtype=np.float32) dd_q_feat = np.tile(dd_q_feat, (len_indicator, 1)) dd_q = dd_q_feat.reshape((len_indicator, self.nb_supervised_doc, self. doc_topk_term, 1)) for i, docid in enumerate(rerank_docid_list): dd_d_file = os.path.join(self.dd_d_feature_path, str(qid), 'q{0}_d{1}.npy'.format(qid, docid)) dd_d[i] = np.load(dd_d_file)[:self.nb_supervised_doc, :self. doc_topk_term] return dd_q, dd_d, score, len_indicator 
of|common|method|order def order_of_method(method): if SAFE_RANGE in method: return -1 elif REWIND_LOW_LR in method: return 3 elif REWIND_NORMAL in method: return 1 elif FINETUNE_HIGH_LR in method: return 0 elif FINETUNE_NORMAL in method: return 2 elif REINITIALIZE in method: return 4 else: return 5 
gen|tuple|chunkparser|ChunkParser def tuple_gen(self, gen): """ Take a generator producing v2 records and convert them to tuples. applying a random symmetry on the way. """ for r in gen: yield self.convert_v2_to_tuple(r) 
scores|get|avod|core|evaluator|and|rpn|Evaluator|proposals def get_rpn_proposals_and_scores(self, predictions): """Returns the proposals and scores stacked for saving to file.  Args: predictions: A dictionary containing the model outputs.  Returns: proposals_and_scores: A numpy array of shape (number_of_proposals, 8), containing the rpn proposal boxes and scores. """ top_anchors = predictions[RpnModel.PRED_TOP_ANCHORS] top_proposals = box_3d_encoder.anchors_to_box_3d(top_anchors) softmax_scores = predictions[RpnModel.PRED_TOP_OBJECTNESS_SOFTMAX] proposals_and_scores = np.column_stack((top_proposals, softmax_scores)) return proposals_and_scores 
init|AffineCouplingSdnEx2 def __init__(self, x_shape, shift_and_log_scale_fn, layer_id=0, last_layer= False, validate_args=False, name='real_nvp', gain_init=0.0): super(AffineCouplingSdnEx2, self).__init__(forward_min_event_ndims=1, is_constant_jacobian=False, validate_args=validate_args, name=name) self.x_shape = x_shape self.i0, self.i1, self.ic = x_shape self._last_layer = last_layer self.id = layer_id self._shift_and_log_scale_fn = shift_and_log_scale_fn self.scale = tf.get_variable('rescaling_scale{}'.format(self.id), [], dtype=DTYPE, initializer=tf.constant_initializer(0.0001)) self.gain_init = gain_init 
task|language|data|utils|find def find_language_task(task: str) ->LanguageTask: if task == 'lambada': return LambadaTask() else: raise NotImplementedError("Task '{task}' not supported".format(task =task)) 
dc|svae|limit|logvar|nets|LogvarLimitScheduler def logvar_limit(self): return (LogvarLimitScheduler.MAX_LOGVAR - self.logvar_optimizer. param_groups[0]['lr']) 
official|test|logs|LoggingMetricHookTest|hook|metric|setUp|utils def setUp(self): super(LoggingMetricHookTest, self).setUp() self._log_dir = tempfile.mkdtemp(dir=self.get_temp_dir()) self._logger = mock_lib.MockBenchmarkLogger() 
layers|multihead|attention|thumt def multihead_attention(queries, memories, bias, num_heads, key_size, value_size, output_size, keep_prob=None, output=True, state=None, summary=True, dtype=None, scope=None, max_relative_dis=None): """ Multi-head scaled-dot-product attention with input/output transformations.  :param queries: A tensor with shape [batch, length_q, depth_q] :param memories: A tensor with shape [batch, length_m, depth_m] :param bias: A tensor (see attention_bias) :param num_heads: An integer dividing key_size and value_size :param key_size: An integer :param value_size: An integer :param output_size: An integer :param keep_prob: A floating point number in (0, 1] :param output: Whether to use output transformation :param state: An optional dictionary used for incremental decoding :param summary: Use image summary :param dtype: An optional instance of tf.DType :param scope: An optional string :param max_relative_dis: An integer  :returns: A dict with the following keys: weights: A tensor with shape [batch, heads, length_q, length_kv] outputs: A tensor with shape [batch, length_q, depth_v] """ if key_size % num_heads != 0: raise ValueError( 'Key size (%d) must be divisible by the number of attention heads (%d).' % (key_size, num_heads)) if value_size % num_heads != 0: raise ValueError( 'Value size (%d) must be divisible by the number of attention heads (%d).' % (value_size, num_heads)) with tf.variable_scope(scope, default_name='multihead_attention', values=[queries, memories], dtype=dtype): next_state = {} if memories is None: size = key_size * 2 + value_size combined = linear(queries, size, True, True, scope='qkv_transform') q, k, v = tf.split(combined, [key_size, key_size, value_size], axis=-1) if state is not None: k = tf.concat([state['key'], k], axis=1) v = tf.concat([state['value'], v], axis=1) next_state['key'] = k next_state['value'] = v else: q = linear(queries, key_size, True, True, scope='q_transform') combined = linear(memories, key_size + value_size, True, scope= 'kv_transform') k, v = tf.split(combined, [key_size, value_size], axis=-1) q = split_heads(q, num_heads) k = split_heads(k, num_heads) v = split_heads(v, num_heads) length_q = tf.shape(q)[2] length_kv = tf.shape(k)[2] key_depth_per_head = key_size // num_heads q *= key_depth_per_head ** -0.5 if max_relative_dis and memories is None: rpr_k = tf.get_variable('rpr_k', [2 * max_relative_dis + 1, key_size // num_heads]) rpr_v = tf.get_variable('rpr_v', [2 * max_relative_dis + 1, value_size // num_heads]) rpr_k = create_rpr(rpr_k, length_q, length_kv, max_relative_dis) rpr_v = create_rpr(rpr_v, length_q, length_kv, max_relative_dis) rpr = {'rpr_k': rpr_k, 'rpr_v': rpr_v} results = multiplicative_attention(q, k, v, bias, keep_prob, rpr=rpr) else: results = multiplicative_attention(q, k, v, bias, keep_prob) weights = results['weights'] x = combine_heads(results['outputs']) if output: outputs = linear(x, output_size, True, True, scope= 'output_transform') else: outputs = x if should_generate_summaries() and summary: attention_image_summary(weights) outputs = {'weights': weights, 'outputs': outputs} if state is not None: outputs['state'] = next_state return outputs 
slice|xlnet|PaddingSignals|estimator|tpu|master|or|tensor|single|dict def slice_single_tensor(tensor): rank = len(tensor.shape) assert rank > 0 real_batch_size = batch_size - math_ops.reduce_sum(padding_mask) return verify_batch_size(tensor)[0:real_batch_size] 
bert|tokenization|convert|by|vocab def convert_by_vocab(vocab, items): """Converts a sequence of [tokens|ids] using the vocab.""" output = [] for item in items: output.append(vocab[item]) return output 
tf|Logger|testing|init|mac def init_tf(self, log_dir): self.writer = tf.summary.FileWriter(log_dir) self.acc_loss_pl = tf.placeholder(dtype=tf.float32, name='loss_accumulator' ) 
BatchNorm|create|borealisflows|layers|vars def _create_vars(self): with tf.variable_scope(self.name): self.train_m = tf.get_variable('mean', [self.ic], dtype=DTYPE, initializer=tf.constant_initializer(0.0), trainable=False) self.train_v = tf.get_variable('var', [self.ic], dtype=DTYPE, initializer=tf.constant_initializer(1.0), trainable=False) 
old|graphics|raster|borealisflows|to def to_raster_old(x, rescale=False, width=None): x = np.transpose(x, (0, 3, 1, 2)) if len(x.shape) == 3: x = x.reshape((x.shape[0], 1, x.shape[1], x.shape[2])) if x.shape[1] == 1: x = np.repeat(x, 3, axis=1) if rescale: x = (x - x.min()) / (x.max() - x.min()) * 255.0 x = np.clip(x, 0, 255) assert len(x.shape) == 4 assert x.shape[1] == 3 n_patches = x.shape[0] if width is None: width = int(np.ceil(np.sqrt(n_patches))) height = int(n_patches / width) tile_height = x.shape[2] tile_width = x.shape[3] result = np.zeros((3, int(height * tile_height), int(width * tile_width )), dtype='uint8') for i in range(height): for j in range(width): result[:, i * tile_height:(i + 1) * tile_height, j * tile_width :(j + 1) * tile_width] = x[i] return result 
predict|test|official|fn|cifar1|BaseTest|model|mode|v2|resnet def test_cifar10_model_fn_predict_mode_v2(self): self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT, resnet_version=2, dtype=tf.float32) 
size|utils|distribute|thumt def size(): return _ENGINE.size() if _ENGINE is not None else 1 
rates|optimizers|learning|get def get_learning_rates(args): """ :return: constant or decaying learning rates per epoch, depending on settings """ w_lr = args.w_learning_rate z_lr = args.z_learning_rate global_w_step = None if args.w_lr_decay is not None: global_w_step = tf.get_variable('global_w_step', dtype=tf.int32, initializer=0) epoch_steps = args.train_set_size / args.batch_size if args.w_optim != 'nested': epoch_steps *= len(args.topology) + 1 w_lr = tf.train.exponential_decay(w_lr, global_w_step, epoch_steps, args.w_lr_decay, staircase=True) if args.lr_decay_stop is not None: min_lr = args.w_learning_rate * args.w_lr_decay ** (args. lr_decay_stop - 1) w_lr = tf.maximum(w_lr, min_lr) return w_lr, z_lr, global_w_step 
init|Embedding|embedding def __init__(self, vocabulary=Vocabulary(), vectors=[]): self.vocabulary = vocabulary self.vectors = np.array(vectors) if len(self.vocabulary) != self.vectors.shape[0]: raise ValueError( 'We have a different number of words and vectors. We have {} words and {} vectors' .format(len(self.vocabulary), self.vectors.shape[0])) if len(self.vocabulary) != len(set(self.vocabulary.words)): logging.warning('Vocabulary has duplicates') 
bert|fn|builder|input|master|run|classifier def input_fn(params): """The actual input function.""" batch_size = params['batch_size'] num_examples = len(features) d = tf.data.Dataset.from_tensor_slices({'input_ids': tf.constant( all_input_ids, shape=[num_examples, seq_length], dtype=tf.int32), 'input_mask': tf.constant(all_input_mask, shape=[num_examples, seq_length], dtype=tf.int32), 'segment_ids': tf.constant( all_segment_ids, shape=[num_examples, seq_length], dtype=tf.int32), 'label_ids': tf.constant(all_label_ids, shape=[num_examples], dtype =tf.int32)}) if is_training: d = d.repeat() d = d.shuffle(buffer_size=100) d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder) return d 
MultiStepHook|init|utils|hooks|thumt def __init__(self, hook, step=1): self._hook = hook self._step = step self._iter = 0 if step == 1 else 1 
models|cae|test|Autoencoder def test(self, sess, input_features, input_labels): """Single step test of the autoencoder @param sess (tf.Session) the current session @param input_feature (np.array) matrix or array of feature @param lambda_e explicit mixing coefficient @param lambda_i implicit mixing coefficient @return (float) the losses: loss, loss_r, loss_c, acc_c, loss_e, loss_i """ loss, loss_r = sess.run([self.loss, self.loss_reconstruction], feed_dict={self.x: input_features, self.labels_placeholder: input_labels}) return loss 
darkflow|im|imcv2|transform|utils|recolor def imcv2_recolor(im, a=0.1): t = [np.random.uniform()] t += [np.random.uniform()] t += [np.random.uniform()] t = np.array(t) * 2.0 - 1.0 im = im * (1 + t * a) mx = 255.0 * (1 + a) up = np.random.uniform() * 2 - 1 im = cv2.pow(im / mx, 1.0 + up * 0.5) return np.array(im * 255.0, np.uint8) 
BlurAddNoise|measure|commons|init def __init__(self, hparams): MeasurementDevice.__init__(self, hparams) self.output_type = 'image' 
EigenBasisProvidingFactor|fisher|get|ops|classification|eigen|factors|value def get_eigen_value(self, damping): return self._eigen_value[damping] 
providers|DataProvider|data|next def next(self): """Returns next data batch or raises `StopIteration` if at end.""" if self._curr_batch + 1 > self.num_batches: self.new_epoch() raise StopIteration() batch_slice = slice(self._curr_batch * self.batch_size, (self. _curr_batch + 1) * self.batch_size) inputs_batch = self.inputs[batch_slice] targets_batch = self.targets[batch_slice] self._curr_batch += 1 return inputs_batch, targets_batch 
curriculum|test|len|cmp|path|env def cmp_path_len(path1, path2): return len(path1['path']) <= len(path2['path']) 
test|official|BaseTest|make|layer|projection|resnet def make_projection(self, filters_out, strides, data_format): """1D convolution with stride projector.  Args: filters_out: Number of filters in the projection. strides: Stride length for convolution. data_format: channels_first or channels_last  Returns: A CNN projector function with kernel_size 1. """  def projection_shortcut(inputs): return resnet_model.conv2d_fixed_padding(inputs=inputs, filters= filters_out, kernel_size=1, strides=strides, data_format= data_format) return projection_shortcut 
defend|quilt|defense|make def make_defend_quilt(sess): quilt_db = np.load('data/quilt_db.npy') quilt_db_reshaped = quilt_db.reshape(1000000, -1) TILE_SIZE = 5 TILE_OVERLAP = 2 tile_skip = TILE_SIZE - TILE_OVERLAP K = 10 db_tensor = tf.placeholder(tf.float32, quilt_db_reshaped.shape) query_imgs = tf.placeholder(tf.float32, (TILE_SIZE * TILE_SIZE * 3, None)) norms = tf.reduce_sum(tf.square(db_tensor), axis=1)[:, (tf.newaxis) ] - 2 * tf.matmul(db_tensor, query_imgs) _, topk_indices = tf.nn.top_k(-tf.transpose(norms), k=K, sorted=False)  def min_error_table(arr, direction): assert direction in ('horizontal', 'vertical') y, x = arr.shape cum = np.zeros_like(arr) if direction == 'horizontal': cum[:, (-1)] = arr[:, (-1)] for ix in range(x - 2, -1, -1): for iy in range(y): m = arr[iy, ix + 1] if iy > 0: m = min(m, arr[iy - 1, ix + 1]) if iy < y - 1: m = min(m, arr[iy + 1, ix + 1]) cum[iy, ix] = arr[iy, ix] + m elif direction == 'vertical': cum[(-1), :] = arr[(-1), :] for iy in range(y - 2, -1, -1): for ix in range(x): m = arr[iy + 1, ix] if ix > 0: m = min(m, arr[iy + 1, ix - 1]) if ix < x - 1: m = min(m, arr[iy + 1, ix + 1]) cum[iy, ix] = arr[iy, ix] + m return cum  def index_exists(arr, index): if arr.ndim != len(index): return False return all(i > 0 for i in index) and all(index[i] < arr.shape[i] for i in range(arr.ndim))  def assign_block(ix, iy, tile, synth): posx = tile_skip * ix posy = tile_skip * iy if ix == 0 and iy == 0: synth[posy:posy + TILE_SIZE, posx:posx + TILE_SIZE, :] = tile elif iy == 0: tile_left = tile[:, :TILE_OVERLAP, :] synth_right = synth[:TILE_SIZE, posx:posx + TILE_OVERLAP, :] errors = np.sum(np.square(tile_left - synth_right), axis=2) table = min_error_table(errors, direction='vertical') xoff = np.argmin(table[(0), :]) synth[(posy), posx + xoff:posx + TILE_SIZE] = tile[(0), xoff:] for yoff in range(1, TILE_SIZE): candidates = [(yoff, xoff), (yoff, xoff - 1), (yoff, xoff + 1)] index = min((i for i in candidates if index_exists(table, i )), key=lambda i: table[i]) xoff = index[1] synth[(posy + yoff), posx + xoff:posx + TILE_SIZE] = tile[( yoff), xoff:] elif ix == 0: tile_up = tile[:TILE_OVERLAP, :, :] synth_bottom = synth[posy:posy + TILE_OVERLAP, :TILE_SIZE, :] errors = np.sum(np.square(tile_up - synth_bottom), axis=2) table = min_error_table(errors, direction='horizontal') yoff = np.argmin(table[:, (0)]) synth[posy + yoff:posy + TILE_SIZE, (posx)] = tile[yoff:, (0)] for xoff in range(1, TILE_SIZE): candidates = [(yoff, xoff), (yoff - 1, xoff), (yoff + 1, xoff)] index = min((i for i in candidates if index_exists(table, i )), key=lambda i: table[i]) yoff = index[0] synth[posy + yoff:posy + TILE_SIZE, (posx + xoff)] = tile[yoff :, (xoff)] else: tile_up = tile[:TILE_OVERLAP, :, :] synth_bottom = synth[posy:posy + TILE_OVERLAP, :TILE_SIZE, :] errors_up = np.sum(np.square(tile_up - synth_bottom), axis=2) table_up = min_error_table(errors_up, direction='horizontal') tile_left = tile[:, :TILE_OVERLAP, :] synth_right = synth[:TILE_SIZE, posx:posx + TILE_OVERLAP, :] errors_left = np.sum(np.square(tile_left - synth_right), axis=2) table_left = min_error_table(errors_left, direction='vertical') glue_index = -1 glue_value = np.inf for i in range(TILE_OVERLAP): e = table_up[i, i] + table_left[i, i] if e < glue_value: glue_value = e glue_index = i xoff = glue_index synth[(posy + glue_index), posx + xoff:posx + TILE_OVERLAP] = tile[ (glue_index), xoff:TILE_OVERLAP] for yoff in range(glue_index + 1, TILE_SIZE): candidates = [(yoff, xoff), (yoff, xoff - 1), (yoff, xoff + 1)] index = min((i for i in candidates if index_exists( table_left, i)), key=lambda i: table_left[i]) xoff = index[1] synth[(posy + yoff), posx + xoff:posx + TILE_OVERLAP] = tile[( yoff), xoff:TILE_OVERLAP] yoff = glue_index synth[posy + yoff:posy + TILE_OVERLAP, (posx + glue_index)] = tile[ yoff:TILE_OVERLAP, (glue_index)] for xoff in range(glue_index + 1, TILE_SIZE): candidates = [(yoff, xoff), (yoff - 1, xoff), (yoff + 1, xoff)] index = min((i for i in candidates if index_exists(table_up, i)), key=lambda i: table_up[i]) yoff = index[0] synth[posy + yoff:posy + TILE_OVERLAP, (posx + xoff)] = tile[ yoff:TILE_OVERLAP, (xoff)] synth[posy + TILE_OVERLAP:posy + TILE_SIZE, posx + TILE_OVERLAP :posx + TILE_SIZE] = tile[TILE_OVERLAP:, TILE_OVERLAP:] KNN_MAX_BATCH = 1000  def quilt(arr, graphcut=True): h, w, c = arr.shape assert (h - TILE_SIZE) % tile_skip == 0 assert (w - TILE_SIZE) % tile_skip == 0 horiz_blocks = (w - TILE_SIZE) // tile_skip + 1 vert_blocks = (h - TILE_SIZE) // tile_skip + 1 num_patches = horiz_blocks * vert_blocks patches = np.zeros((TILE_SIZE * TILE_SIZE * 3, num_patches)) idx = 0 for iy in range(vert_blocks): for ix in range(horiz_blocks): posx = tile_skip * ix posy = tile_skip * iy patches[:, (idx)] = arr[posy:posy + TILE_SIZE, posx:posx + TILE_SIZE, :].ravel() idx += 1 ind = [] for chunk in range(num_patches // KNN_MAX_BATCH + (1 if num_patches % KNN_MAX_BATCH != 0 else 0)): start = KNN_MAX_BATCH * chunk end = start + KNN_MAX_BATCH indices_ = sess.run(topk_indices, {db_tensor: quilt_db_reshaped, query_imgs: patches[:, start:end]}) for i in indices_: ind.append(np.random.choice(i)) synth = np.zeros((299, 299, 3)) idx = 0 for iy in range(vert_blocks): for ix in range(horiz_blocks): posx = tile_skip * ix posy = tile_skip * iy tile = quilt_db[ind[idx]] if not graphcut: synth[posy:posy + TILE_SIZE, posx:posx + TILE_SIZE, : ] = tile else: assign_block(ix, iy, tile, synth) idx += 1 return synth return quilt 
of|get|PruneResNet32|params|and|prune|resnet32|whole|model|computation def _get_params_and_computation_of_whole_model(self, weights_dict): all_computation = 0 all_params = 0 for weight_name in weights_dict: if ('conv' in weight_name or 'dense' in weight_name ) and 'kernel' in weight_name: output_size = self._get_output_size_from_layer_name(weight_name) computation = get_computation(weights_dict, weight_name, {}, '', output_size) params = get_parameters(weights_dict, weight_name, {}, '') all_computation += computation all_params += params return all_params, all_computation 
ExamplesPerSecondHook|cifar1|after|utils|run def after_run(self, run_context, run_values): _ = run_context global_step = run_values.results if self._timer.should_trigger_for_step(global_step): elapsed_time, elapsed_steps = self._timer.update_last_triggered_step( global_step) if elapsed_time is not None: steps_per_sec = elapsed_steps / elapsed_time self._step_train_time += elapsed_time self._total_steps += elapsed_steps average_examples_per_sec = self._batch_size * (self. _total_steps / self._step_train_time) current_examples_per_sec = steps_per_sec * self._batch_size logging.info('%s: %g (%g), step = %g', 'Average examples/sec', average_examples_per_sec, current_examples_per_sec, self. _total_steps) 
neural|tangents|affine|stax def _affine(nngp, W_std, b_std): """Get [co]variances of affine outputs if inputs have [co]variances `nngp`.  The output is assumed to be `xW + b`, where `x` is the input, `W` is a matrix of i.i.d. Gaussian weights with std `W_std`, `b` is a vector of i.i.d. Gaussian biases with std `b_std`.  Args: nngp: a 2D or 1D `np.ndarray` containing either a) sample-sample covariances of shape `[batch_size_1, batch_size_2]` or b) sample variances of shape `[batch_size]`. W_std: a float, standard deviation of a fully-connected layer weights. b_std: a float, standard deviation of a fully-connected layer biases.  Returns: a 2D or 1D `np.ndarray` containing sample[-sample] [co]variances of FC outputs. Has the same shape as `nngp`. """ if nngp is None: return nngp return W_std ** 2 * nngp + b_std ** 2 
bbans|pixelvae|codec|pop|pvae|obs def pop_(msg): msg, (data, _) = pop(msg) return msg, data 
queue|A3C|from|pull|batch|a3c def pull_batch_from_queue(self): """ self explanatory:  take a rollout from the queue of the thread runner. """ rollout = self.runner.queue.get(timeout=600.0) while not rollout.terminal: try: rollout.extend(self.runner.queue.get_nowait()) except queue.Empty: break return rollout 
fisher|eigendecomp|ops|InverseProvidingFactor|classification|factors|register def register_eigendecomp(self): """Registers an eigendecomposition. Unlike register_damp_inverse and register_matpower this doesn't create any variables or inverse ops.  Instead it merely makes tensors containing the eigendecomposition available to anyone that wants them.  They will be recomputed (once) for each session.run() call (when they needed by some op). """ if not self._eigendecomp: eigenvalues, eigenvectors = linalg_ops.self_adjoint_eig(self._cov) clipped_eigenvalues = math_ops.maximum(eigenvalues, EIGENVALUE_CLIPPING_THRESHOLD) self._eigendecomp = clipped_eigenvalues, eigenvectors 
pool|max|make|cnn|3x3|helpers def make_max_pool_3x3(op_name, in_tensor, padding='VALID'): with tf.name_scope(op_name): return tf.nn.pool(input=in_tensor, window_shape=[3, 3], strides=[2, 2], pooling_type='MAX', padding=padding, name=op_name) 
checkpoint|nmt|before|AsyncCheckpointSaverHook|async|run def before_run(self, run_context): return SessionRunArgs(self._global_step_tensor) 
models|DSIN|tests|test def test_DSIN(): model_name = 'DSIN' x, y, feature_dim_dict, behavior_feature_list = get_xy_fd(True) model = DSIN(feature_dim_dict, behavior_feature_list, sess_max_count=2, sess_len_max=4, embedding_size=4, dnn_hidden_units=[4, 4, 4], dnn_dropout=0.5) check_model(model, model_name, x, y) 
bert|matrix|reshape|from|master|modeling def reshape_from_matrix(output_tensor, orig_shape_list): """Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""" if len(orig_shape_list) == 2: return output_tensor output_shape = get_shape_list(output_tensor) orig_dims = orig_shape_list[0:-1] width = output_shape[-1] return tf.reshape(output_tensor, orig_dims + [width]) 
2|train|inception|v3|baseline|validate def validate(out, sess, epoch, validation_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, learning_rate, is_training_ph, is_validating_nbl_ph): g_step, acc_top1, acc_top5, test_loss, lr = 0, 0, 0, 0, 0 for i in range(validation_steps): out.validation_step_begin(i, validation_steps) g_step, l, acc1, acc5, lr = sess.run([global_step, loss_op, acc_top_1_op, acc_top_5_op, learning_rate], feed_dict={ is_training_ph: False, is_validating_nbl_ph: False}) acc_top1 = (acc1 + i * acc_top1) / (i + 1) acc_top5 = (acc5 + i * acc_top5) / (i + 1) test_loss = (l + i * test_loss) / (i + 1) out.validation_end(sess, epoch, g_step, False, test_loss, lr, acc_top1, acc_top5) 
texar|google|file|extract|data|id|utils|drive def _extract_google_drive_file_id(url): url_suffix = url[url.find('/d/') + 3:] file_id = url_suffix[:url_suffix.find('/')] return file_id 
python|grpc|OpsTest|in|while|ops|test|shutdown|seed|rl|master|call @parameterized.parameters(([], False), ([1], True)) def test_shutdown_while_in_call(self, dim, batched): address = self.get_unix_address() server = ops.Server([address]) is_waiting = threading.Event()  @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): tf.py_function(is_waiting.set, [], []) tf.py_function(time.sleep, [1], []) return x + 1 server.bind(foo, batched=batched) server.start() client = ops.Client(address) with futures.ThreadPoolExecutor(max_workers=1) as executor: f = executor.submit(client.foo, 42) is_waiting.wait() with self.assertRaisesRegexp(tf.errors.UnavailableError, 'server closed'): server.shutdown() f.result() 
Default|task|init def __init__(self, task: str, max_input_digit: int): self.task = task self.max_input_digit = max_input_digit 
tangents|utils|empirical|fn|neural|nngp def empirical_nngp_fn(f): """Returns a function to draw a single sample the NNGP of a given network `f`.  This method assumes that slices of the random network outputs along the last dimension are i.i.d. (which is true for e.g. classifiers with a dense readout layer, or true for outputs of a CNN layer with the `NHWC` data format. As a result it treats outputs along that dimension as independent samples and only reports covariance along other dimensions.  Note that the `ntk_monte_carlo` makes no such assumption and returns the full covariance.  Args: f: a function computing the output of the neural network. From `jax.experimental.stax`: "takes params, inputs, and an rng key and applies the layer".  Returns: A function to draw a single sample the NNGP of a given network `f`. """  def nngp_fn(x1, x2, params): """Sample a single NNGP of a given network `f` on given inputs and `params`.  This method assumes that slices of the random network outputs along the last dimension are i.i.d. (which is true for e.g. classifiers with a dense readout layer, or true for outputs of a CNN layer with the `NHWC` data format. As a result it treats outputs along that dimension as independent samples and only reports covariance along other dimensions.  Note that the `ntk` method makes no such assumption and returns the full covariance.  Args: x1: a `np.ndarray` of shape `[batch_size_1] + input_shape`. x2: an optional `np.ndarray` with shape `[batch_size_2] + input_shape`. `None` means `x2 == x1`. params: A PyTree of parameters about which we would like to compute the NNGP.  Returns: A Monte Carlo estimate of the NNGP, a `np.ndarray` of shape `[batch_size_1] + output_shape[:-1] + [batch_size_2] + output_shape[:-1]`. """ out1 = f(params, x1) out2 = f(params, x2) if x2 is not None else out1 out2 = np.expand_dims(out2, -1) nngp_12 = np.dot(out1, out2) / out1.shape[-1] return np.squeeze(nngp_12, -1) return nngp_fn 
of|bias|lottery|name|base def bias_name_of_base_name(base_name): return base_name + _BIAS_SUFFIX 
avod|tf|offsets|4c|encoder|core|box|to def tf_offsets_to_box_4c(boxes_4c, offsets): """Applies box_4c offsets to boxes_4c  Args: boxes_4c: boxes_4c to apply offsets to offsets: box_4c offsets to apply  Returns: regressed boxes_4c """ return boxes_4c + offsets 
extensions|hved|net|init|u|GaussianSampler def __init__(self, name='gaussian_sampler'): super(GaussianSampler, self).__init__(name=name) 
concat|init|custom|layer def __init__(self, axis=-1, **kwargs): super(custom_concat, self).__init__(**kwargs) self.axis = axis 
commons|save|data|inception|utils def save_inception_data(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals, inf_net, x_sample_val): y_hat_val_list = [] for _ in range(16): x_sample_val, _ = get_samples(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals) y_hat_val = inf_net.get_y_hat_val(x_sample_val) y_hat_val_list.append(y_hat_val) data = get_inception_data(y_hat_val_list) basic_utils.save_to_pickle(data, hparams.incpt_pkl) print('Saved inception data') 
compute|LocalActivationUnit|mask|deepctr|core|layers def compute_mask(self, inputs, mask): return mask 
graph|run|model|MaskRCNN def run_graph(self, images, outputs): """Runs a sub-set of the computation graph that computes the given outputs.  outputs: List of tuples (name, tensor) to compute. The tensors are symbolic TensorFlow tensors and the names are for easy tracking.  Returns an ordered dict of results. Keys are the names received in the input and values are Numpy arrays. """ model = self.keras_model outputs = OrderedDict(outputs) for o in outputs.values(): assert o is not None inputs = model.inputs if model.uses_learning_phase and not isinstance(K.learning_phase(), int): inputs += [K.learning_phase()] kf = K.function(model.inputs, list(outputs.values())) molded_images, image_metas, windows = self.mold_inputs(images) model_in = [molded_images, image_metas] if model.uses_learning_phase and not isinstance(K.learning_phase(), int): model_in.append(0.0) outputs_np = kf(model_in) outputs_np = OrderedDict([(k, v) for k, v in zip(outputs.keys(), outputs_np)]) for k, v in outputs_np.items(): log(k, v) return outputs_np 
wolf|plato|reinforcement|WoLFPHCPolicy|component|agent|phc|init|policy|dialogue|learning def __init__(self, args): """ Initialize parameters and internal structures  :param args: dictionary containing the dialogue_policy's settings """ super(WoLFPHCPolicy, self).__init__() self.ontology = None if 'ontology' in args: ontology = args['ontology'] if isinstance(ontology, Ontology): self.ontology = ontology elif isinstance(ontology, str): self.ontology = Ontology(ontology) else: raise ValueError('WoLFPHCPolicy Unacceptable ontology type %s ' % ontology) else: raise ValueError('WoLFPHCPolicy: No ontology provided') self.database = None if 'database' in args: database = args['database'] if isinstance(database, DataBase): self.database = database elif isinstance(database, str): self.database = DataBase(database) else: raise ValueError( 'WoLFPHCPolicy: Unacceptable database type %s ' % database) else: raise ValueError('WoLFPHCPolicy: No database provided') self.agent_role = args['agent_role'] if 'agent_role' in args else 'system' self.alpha = args['alpha'] if 'alpha' in args else 0.2 self.gamma = args['gamma'] if 'gamma' in args else 0.95 self.epsilon = args['epsilon'] if 'epsilon' in args else 0.95 self.alpha_decay_rate = args['alpha_decay' ] if 'alpha_decay' in args else 0.995 self.exploration_decay_rate = args['epsilon_decay' ] if 'epsilon_decay' in args else 0.9995 self.IS_GREEDY_POLICY = False self.d_win = 0.0025 self.d_lose = 0.01 self.is_training = False self.Q = {} self.pi = {} self.mean_pi = {} self.state_counter = {} self.pp = pprint.PrettyPrinter(width=160) self.warmup_policy = None self.warmup_simulator = None if self.agent_role == 'system': self.warmup_policy = slot_filling_policy.HandcraftedPolicy({ 'ontology': self.ontology}) elif self.agent_role == 'user': usim_args = dict(zip(['ontology', 'database'], [self.ontology, self .database])) self.warmup_simulator = AgendaBasedUS(usim_args) self.dstc2_acts_sys = self.dstc2_acts_usr = None self.dstc2_acts_sys = ['offer', 'canthelp', 'affirm', 'deny', 'ack', 'bye', 'reqmore', 'welcomemsg', 'expl-conf', 'select', 'repeat', 'confirm-domain', 'confirm'] self.dstc2_acts_usr = ['affirm', 'negate', 'deny', 'ack', 'thankyou', 'bye', 'reqmore', 'hello', 'expl-conf', 'repeat', 'reqalts', 'restart', 'confirm'] self.informable_slots = deepcopy(list(self.ontology.ontology[ 'informable'].keys())) self.requestable_slots = deepcopy(self.ontology.ontology['requestable']) self.system_requestable_slots = deepcopy(self.ontology.ontology[ 'system_requestable']) if self.dstc2_acts_sys: if self.agent_role == 'system': self.NActions = len(self.dstc2_acts_sys) + len(self. requestable_slots) + len(self.system_requestable_slots) self.NOtherActions = len(self.dstc2_acts_usr) + len(self. requestable_slots) + len(self.system_requestable_slots) elif self.agent_role == 'user': self.NActions = len(self.dstc2_acts_usr) + len(self. requestable_slots) + len(self.system_requestable_slots) self.NOtherActions = len(self.dstc2_acts_sys) + len(self. requestable_slots) + len(self.system_requestable_slots) elif self.agent_role == 'system': self.NActions = 5 + len(self.ontology.ontology['system_requestable'] ) + len(self.ontology.ontology['requestable']) self.NOtherActions = 4 + 2 * len(self.ontology.ontology['requestable']) elif self.agent_role == 'user': self.NActions = 4 + 2 * len(self.ontology.ontology['requestable']) self.NOtherActions = 5 + len(self.ontology.ontology[ 'system_requestable']) + len(self.ontology.ontology['requestable']) self.statistics = {'supervised_turns': 0, 'total_turns': 0} 
python|grpc|OpsTest|failing|ops|test|seed|rl|master|function|foo @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): tf.assert_equal(1, x) return x 
specs|utils|read def read_specs(logdir): """Reads specs from given location.""" specs_path = os.path.join(logdir, 'specs') while not tf.io.gfile.exists(specs_path): logging.info('Waiting for tensor specs.') time.sleep(5) with tf.io.gfile.GFile(specs_path, 'rb') as f: logging.info('Reading from specs file at %s.', specs_path) return pickle.load(f) 
preprocess|train|batch|dataset def batch_train(self): self.num_batches = len(self.data_lab) // self.batch_size for i in range(self.num_batches): yield self.data_lab[i * self.batch_size:(i + 1) * self.batch_size ], self.label[i * self.batch_size:(i + 1) * self.batch_size] 
training|params|utils|load def load_training_params(path): with open(os.path.join(path, 'tensorflow.log'), 'r') as f: return load_params(f.readline()) 
test|testFullyConvolutional|nets|OverFeatTest|overfeat def testFullyConvolutional(self): batch_size = 1 height, width = 281, 281 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze= False) self.assertEquals(logits.op.name, 'overfeat/fc8/BiasAdd') self.assertListEqual(logits.get_shape().as_list(), [batch_size, 2, 2, num_classes]) 
variables|models|create|model|Model def _create_variables(self): raise NotImplementedError( '_create_variables() method should be implemented in concrete model') 
tests|test|sequence|KMaxPooling|layers def test_KMaxPooling(): with CustomObjectScope({'KMaxPooling': sequence.KMaxPooling}): layer_test(sequence.KMaxPooling, kwargs={'k': 3, 'axis': 1}, input_shape=(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE, 2)) 
get|rates|train|learning|Trainer def get_learning_rates(self): """Get learning rates  Returns: list -- learning rates (None means no lr) """ learning_rates = [(self.sess.run(obj.learning_rate) if hasattr(obj, 'learning_rate') else None) for obj in self.network.layers] return learning_rates 
get|infer|pcgn|config|utils def get_pcgn_infer_config(config): is_beam_search = config['inference']['is_beam_search'] beam_size = config['inference']['beam_size'] batch_size = config['inference']['infer_batch_size'] infer_file = config['inference']['infer_file'] infer_source_max_length = config['inference']['infer_source_max_length'] infer_target_max_length = config['inference']['infer_target_max_length'] infer_desc_max_length = config['inference']['infer_desc_max_length'] infer_max_iter = config['inference']['infer_max_iter'] output_path = config['inference']['output_path'] gpu_fraction = config['training']['gpu_fraction'] gpu_id = config['training']['gpu_id'] return (infer_file, batch_size, is_beam_search, beam_size, infer_source_max_length, infer_target_max_length, infer_desc_max_length, infer_max_iter, output_path, gpu_fraction, gpu_id) 
strQ2B|dataset def strQ2B(uchar): """ Convert full-width character to half-width character. """ inside_code = ord(uchar) if inside_code == 12288: inside_code = 32 elif inside_code >= 65281 and inside_code <= 65374: inside_code -= 65248 return chr(inside_code) 
test|EmpiricalTest|empirical|testNTKAgainstDirect @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train_shape={}_test_shape={}_network={}_{}'.format(train, test, network, name), 'train_shape': train, 'test_shape': test, 'network': network, 'name': name, 'kernel_fn': kernel_fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for name, kernel_fn in KERNELS. items())) def testNTKAgainstDirect(self, train_shape, test_shape, network, name, kernel_fn): key = random.PRNGKey(0) key, self_split, other_split = random.split(key, 3) data_self = random.normal(self_split, train_shape) data_other = random.normal(other_split, test_shape) implicit, direct = kernel_fn(key, train_shape[1:], network) g = implicit(data_self, None) g_direct = direct(data_self, None) self.assertAllClose(g, g_direct, check_dtypes=False) g = implicit(data_other, data_self) g_direct = direct(data_other, data_self) self.assertAllClose(g, g_direct, check_dtypes=False) 
gcn|accuracy|metrics|master|masked|text def masked_accuracy(preds, labels, mask): """Accuracy with masking.""" correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1)) accuracy_all = tf.cast(correct_prediction, tf.float32) mask = tf.cast(mask, dtype=tf.float32) mask /= tf.reduce_mean(mask) accuracy_all *= mask return tf.reduce_mean(accuracy_all) 
add|variable|nmt|MaskedLSTMCell|model|helper def add_variable(self, name, shape=None, **kwargs): variable = super().add_variable(lottery.weight_name_of_base_name(name), shape, **kwargs) mask = super().add_variable(lottery.mask_name_of_base_name(name), shape, trainable=False, initializer=tf.initializers.ones()) return tf.math.multiply(variable, mask) 
sparse|gen|lth|plot|resnet11|oneshot def plot_resnet110_sparse_oneshot_lth(): common.lth_plot(network=common.RESNET110, is_iterative=False, prune_method=common.UNSTRUCTURED, min_max_y=(-0.03, 0.01)) 
losses|optimal|ineq|calc def calc_optimal_ineq(x, y, tx, ty, phi, psi, cost, lambda_ineq, loss): ux, vtx = phi(x), psi(tx) uty, vy = phi(ty), psi(y) return loss(x, tx, ux, vtx, cost, lambda_ineq) + ineq_loss(ty, y, uty, vy, cost, lambda_ineq) 
mnist|setup|TwoLayerMNISTModel|predict def predict(self, data): return self.model(data) 
attack|block|parsimonious|attacks|ParsimoniousAttack|split def _split_block(self, upper_left, lower_right, block_size): """Split an image into a set of blocks. Note that a block consists of [upper_left, lower_right, channel]  Args: upper_left: [x, y], the coordinate of the upper left of an image lower_right: [x, y], the coordinate of the lower right of an image block_size: int, the size of a block  Return: blocks: list, a set of blocks """ blocks = [] xs = np.arange(upper_left[0], lower_right[0], block_size) ys = np.arange(upper_left[1], lower_right[1], block_size) for x, y in itertools.product(xs, ys): for c in range(3): blocks.append([[x, y], [x + block_size, y + block_size], c]) return blocks 
models|init|DelayedModelCheckpoint def __init__(self, filepath, monitor='val_acc', delay=50, verbose=0, weights=False): super(DelayedModelCheckpoint, self).__init__() self.monitor = monitor self.verbose = verbose self.filepath = filepath self.delay = delay if self.monitor == 'val_acc': self.best = -np.Inf else: self.best = np.Inf self.weights = weights 
devtools|cleverhans|tests|parse|docscrape|NumpyFunctionDocString def _parse(self): self._parsed_data = {'Signature': '', 'Summary': '', 'Extended Summary': [], 'Parameters': [], 'Other Parameters': [], 'Returns': [], 'Raises': [], 'Warns': [], 'See Also': [], 'Notes': [], 'References': '', 'Examples': '', 'index': {}} return NumpyDocString._parse(self) 
plot|get|test|nll|train def get_train_test_nll(df_train, df_test): xtr = df_train['epoch'] ytr = df_train['NLL'] xts = df_test['epoch'] yts = df_test['NLL'] nll_g = df_test['NLL_G'] nll_nlf = df_test['NLL_SDN'] return xtr, ytr, xts, yts, nll_g, nll_nlf 
speak|darkflow|ops|convolution|conv|net|extract def speak(self): l = self.lay args = [l.ksize] * 2 + [l.pad] + [l.stride] args += [l.batch_norm * '+bnorm'] args += [l.activation] msg = 'extr {}x{}p{}_{}  {}  {}'.format(*args) return msg 
heatmap|src|get|gaussian def gaussian(heatmap, center_x, center_y, sigma): th = 4.6052 delta = math.sqrt(th * 2) height = heatmap.shape[0] width = heatmap.shape[1] x0 = int(max(0, center_x - delta * sigma + 0.5)) y0 = int(max(0, center_y - delta * sigma + 0.5)) x1 = int(min(width - 1, center_x + delta * sigma + 0.5)) y1 = int(min(height - 1, center_y + delta * sigma + 0.5)) arr_heat = heatmap[y0:y1 + 1, x0:x1 + 1] exp_factor = 1 / 2.0 / sigma / sigma x_vec = (np.arange(x0, x1 + 1) - center_x) ** 2 y_vec = (np.arange(y0, y1 + 1) - center_y) ** 2 xv, yv = np.meshgrid(x_vec, y_vec) arr_sum = exp_factor * (xv + yv) arr_exp = np.exp(-arr_sum) arr_exp[arr_sum > th] = 0 heatmap[y0:y1 + 1, x0:x1 + 1] = np.maximum(arr_heat, arr_exp) return heatmap 
DeeppolyTanhNodeLast|deeppoly|nodes|transformer def transformer(self, nn, man, element, nlb, nub, use_area_heuristic): """ transformer for a fully connected layer if it's the last layer in the network  Arguments --------- man : ElinaManagerPtr man to which element belongs element : ElinaAbstract0Ptr abstract element onto which the transformer gets applied  Return ------ output : ElinaAbstract0Ptr abstract element after the transformer """ ffn_handle_last_tanh_layer(man, element, *self.get_arguments(), self. tanh_present, use_area_heuristic) return element 
NatSR|gradients|generate def gradients(x): return np.mean((x[:-1, :-1, :] - x[1:, :-1, :]) ** 2 + (x[:-1, :-1, :] - x[:-1, 1:, :]) ** 2) 
CamRestDST|initialize|applications|camrest|dst|restaurants|cambridge def initialize(self, args): """ Initialize the dialogue state  :return: nothing """ super(CamRestDST, self).initialize(args) self.DState.initialize() 
points|end|with|gather|utils @functools.wraps(model) def gather_end_points(*args, **kwargs): logits = model(*args, **kwargs) predictions = prediction(logits) prob = slim.softmax(logits, scope='prob') conf = tf.reduce_max(prob, axis=1) end_points = {'logits': logits, 'pred': predictions, 'prob': prob, 'conf': conf} return end_points 
deepMOT|models|init|master|siamrpn|SiamRPNotb def __init__(self): super(SiamRPNotb, self).__init__(size=1, feature_out=256) self.cfg = {'lr': 0.3, 'window_influence': 0.4, 'penalty_k': 0.22, 'instance_size': 271, 'adaptive': False} 
FeedForwardPolicy|conditioned|actor|goal|policy|make|hbaselines def make_actor(self, obs, reuse=False, scope='pi'): """Create an actor tensor.  Parameters ---------- obs : tf.compat.v1.placeholder the input observation placeholder reuse : bool whether or not to reuse parameters scope : str the scope name of the actor  Returns ------- tf.Variable the output from the actor """ with tf.compat.v1.variable_scope(scope, reuse=reuse): pi_h = obs if self.zero_fingerprint: ob_dim = self.ob_space.shape[0] co_dim = self.co_space.shape[0] pi_h *= tf.constant([1.0] * (ob_dim - FINGERPRINT_DIM) + [0.0] * FINGERPRINT_DIM + [1.0] * co_dim) for i, layer_size in enumerate(self.layers): pi_h = tf.layers.dense(pi_h, layer_size, name='fc' + str(i), kernel_initializer=slim.variance_scaling_initializer(factor =1.0 / 3.0, mode='FAN_IN', uniform=True)) if self.layer_norm: pi_h = tf.contrib.layers.layer_norm(pi_h, center=True, scale=True) pi_h = self.act_fun(pi_h) policy = tf.nn.tanh(tf.layers.dense(pi_h, self.ac_space.shape[0], name='output', kernel_initializer=tf.random_uniform_initializer (minval=-0.003, maxval=0.003))) ac_means = (self.ac_space.high + self.ac_space.low) / 2.0 ac_magnitudes = (self.ac_space.high - self.ac_space.low) / 2.0 policy = ac_means + ac_magnitudes * policy return policy 
plato|ConversationalSingleAgent|agent|conversational|dialogue|continue|single def continue_dialogue(self): """ Perform next dialogue turn.  :return: nothing """ usr_utterance = '' sys_utterance = '' if self.USE_USR_SIMULATOR: usr_input = self.user_simulator.respond() if isinstance(self.user_simulator, DTLUserSimulator): print('USER (nlg) > %s \n' % usr_input) usr_input = self.nlu.process_input(usr_input, self. dialogue_manager.get_state()) elif self.USER_SIMULATOR_NLG: print('USER > %s \n' % usr_input) if self.nlu: usr_input = self.nlu.process_input(usr_input) else: print('USER (DACT) > %s \n' % '; '.join([str(ui) for ui in usr_input])) else: if self.USE_SPEECH: with speech_rec.Microphone() as source: print('(listening...)') audio = self.asr.listen(source, phrase_time_limit=3) try: usr_utterance = self.asr.recognize_google(audio) print('Google ASR: ' + usr_utterance) except speech_rec.UnknownValueError: print('Google ASR did not understand you') except speech_rec.RequestError as e: print('Google ASR request error: {0}'.format(e)) else: usr_utterance = input('USER > ') if self.nlu: usr_input = self.nlu.process_input(usr_utterance, self. dialogue_manager.get_state()) else: raise EnvironmentError( 'agent: No nlu defined for text-based interaction!') self.dialogue_manager.receive_input(usr_input) self.curr_state = deepcopy(self.dialogue_manager.get_state()) if self.dialogue_turn < self.MAX_TURNS: sys_response = self.dialogue_manager.generate_output() else: sys_response = [DialogueAct('bye', [])] if self.USE_NLG: sys_utterance = self.nlg.generate_output({'dacts': sys_response}) print('SYSTEM > %s ' % sys_utterance) if self.USE_SPEECH: try: tts = gTTS(text=sys_utterance, lang='en') tts.save('sys_output.mp3') os.system('afplay sys_output.mp3') except: print( 'WARNING: gTTS encountered an error. Falling back to sys TTS.' ) os.system('say ' + sys_utterance) else: print('SYSTEM > %s ' % '; '.join([str(sr) for sr in sys_response])) if self.USE_USR_SIMULATOR: usim_input = sys_response if self.USER_SIMULATOR_NLU and self.USE_NLG: usim_input = self.user_simulator.nlu.process_input(sys_utterance) self.user_simulator.receive_input(usim_input) rew, success, task_success = self.reward_func.calculate(self. dialogue_manager.get_state(), sys_response, self.user_simulator .goal) else: rew, success, task_success = 0, None, None if self.prev_state: self.recorder.record(self.prev_state, self.curr_state, self. prev_action, self.prev_reward, self.prev_success, input_utterance=self.prev_usr_utterance, output_utterance=self. prev_sys_utterance, task_success=self.prev_task_success) self.dialogue_turn += 1 self.prev_state = deepcopy(self.curr_state) self.prev_action = deepcopy(sys_response) self.prev_usr_utterance = deepcopy(usr_utterance) self.prev_sys_utterance = deepcopy(sys_utterance) self.prev_reward = rew self.prev_success = success self.prev_task_success = task_success 
extensions|hved|Product|application|u|Gaussian def Product_Gaussian(means, logvars, list_mod): mu_prior = tf.zeros(tf.shape(means[list_mod[0]])) log_prior = tf.zeros(tf.shape(means[list_mod[0]])) eps = 1e-07 T = [(1 / (tf.exp(logvars[mod]) + eps)) for mod in list_mod] + [1 + log_prior] mu = [(means[mod] / (tf.exp(logvars[mod]) + eps)) for mod in list_mod] + [ mu_prior] posterior_means = tf.add_n(mu) / tf.add_n(T) var = 1 / tf.add_n(T) posterior_logvars = tf.log(var + eps) return posterior_means, posterior_logvars 
kernels|compute|Kdiag|Sequential|GPSig @autoflow((settings.float_type, [None, None])) def compute_Kdiag(self, X): return self.Kdiag(X) 
testEndPoints|test|vgg|VGGATest|nets def testEndPoints(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = vgg.vgg_a(inputs, num_classes) expected_names = ['vgg_a/conv1/conv1_1', 'vgg_a/pool1', 'vgg_a/conv2/conv2_1', 'vgg_a/pool2', 'vgg_a/conv3/conv3_1', 'vgg_a/conv3/conv3_2', 'vgg_a/pool3', 'vgg_a/conv4/conv4_1', 'vgg_a/conv4/conv4_2', 'vgg_a/pool4', 'vgg_a/conv5/conv5_1', 'vgg_a/conv5/conv5_2', 'vgg_a/pool5', 'vgg_a/fc6', 'vgg_a/fc7', 'vgg_a/fc8'] self.assertSetEqual(set(end_points.keys()), set(expected_names)) 
SparsityFormatter|get|formatter|density|init|interpolation|common def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) 
similarities|l2|negative|models|distance|vkge|square def negative_square_l2_distance(x1, x2, axis=1): """ Negative Square L2 Distance.  .. math:: L = - \\sum_i (x1_i - x2_i)^2  :param x1: First term. :param x2: Second term. :param axis: Reduction Indices. :return: Similarity Value. """ distance = tf.reduce_sum(tf.square(x1 - x2), axis=axis) return -distance 
gym|control|get|DaisyGait11DPolicy|action|daisy|gaits|custom def get_action(self, obs, t): return self.actions[t] 
avod|test|KittiDatasetTest|kitti|splits|datasets|data|dataset def test_data_splits(self): bad_config = DatasetBuilder.copy_config(DatasetBuilder.KITTI_UNITTEST) bad_config.data_split = 'bad' self.assertRaises(ValueError, KittiDataset, bad_config) bad_config.data_split = 'training' self.assertRaises(ValueError, KittiDataset, bad_config) bad_config.data_split = 'validation' self.assertRaises(ValueError, KittiDataset, bad_config) bad_config.data_split = 'testing' self.assertRaises(ValueError, KittiDataset, bad_config) train_dataset = self.get_fake_dataset('train', self.fake_kitti_dir) self.assertEqual(train_dataset.num_samples, 7) validation_dataset = self.get_fake_dataset('val', self.fake_kitti_dir) self.assertEqual(validation_dataset.num_samples, 6) trainval_dataset = self.get_fake_dataset('trainval', self.fake_kitti_dir) self.assertEqual(trainval_dataset.num_samples, 13) test_dataset = self.get_fake_dataset('test', self.fake_kitti_dir) self.assertEqual(test_dataset.num_samples, 10) 
rpn|model|graph def rpn_graph(feature_map, anchors_per_location, anchor_stride): """Builds the computation graph of Region Proposal Network.  feature_map: backbone features [batch, height, width, depth] anchors_per_location: number of anchors per pixel in the feature map anchor_stride: Controls the density of anchors. Typically 1 (anchors for every pixel in the feature map), or 2 (every other pixel).  Returns: rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax) rpn_probs: [batch, W, W, 2] Anchor classifier probabilities. rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors. """ shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map) x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared) rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], - 1, 2]))(x) rpn_probs = KL.Activation('softmax', name='rpn_class_xxx')(rpn_class_logits ) x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid', activation='linear', name='rpn_bbox_pred')(shared) rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x) return [rpn_class_logits, rpn_probs, rpn_bbox] 
prepare|preprocess|poolcontext|d2d @contextmanager def poolcontext(*args, **kwargs): pool = multiprocessing.Pool(*args, **kwargs) yield pool pool.terminate() 
decide|save|train|Trainer|to def decide_to_save(self): to_save = cfg.save and not cfg.toy to_save = to_save or cfg.test_save print(cfg.save_strategy) if cfg.save_strategy == 'best': if self.top_score[0] != self.opt.train.dynamic.epoch: print('DOING IT RIGHT') to_save = False return to_save 
sparse|gen|lth|plot|force|all|resnet56|iterative def plot_resnet56_sparse_iterative_lth_force_all(): common.lth_plot(network=common.RESNET56, is_iterative=True, prune_method=common.UNSTRUCTURED, min_max_y=(-0.03, 0.01), to_ignore=['reinit', 'lr_lottery'], comparison_points=[(0.1497, - 0.0006), (0.1, 0.0016), (0.05, -0.0065), (0.03, -0.0135)], comparison_label=common.CARREIRA, nybins=4, force_single=True, force_all_single=True) 
sdn|params|plot|get def get_sdn_params(df_sdn_params, sdn_param_name): if sdn_param_name is None: sdn_param_name = 'beta' xtr = df_sdn_params['epoch'] ytr = [] for i in range(1, 3): ytr.append(df_sdn_params['beta' + str(i)]) return xtr, ytr 
seld|scores|update|dcase2|19|SELDMetrics|metrics|master|doa|evaluation def update_doa_scores(self, pred_doa_thresholded, gt_doa): """ Compute DOA metrics when DOA is estimated using classification approach  :param pred_doa_thresholded: predicted results of dimension [nb_frames, nb_classes, nb_azi*nb_ele], with value 1 when sound event active, else 0 :param gt_doa: reference results of dimension [nb_frames, nb_classes, nb_azi*nb_ele], with value 1 when sound event active, else 0 :param data_gen_test: feature or data generator class  :return: DOA metrics  """ self._doa_loss_pred_cnt += np.sum(pred_doa_thresholded) self._nb_frames += pred_doa_thresholded.shape[0] for frame in range(pred_doa_thresholded.shape[0]): nb_gt_peaks = int(np.sum(gt_doa[(frame), :])) nb_pred_peaks = int(np.sum(pred_doa_thresholded[(frame), :])) if nb_gt_peaks == nb_pred_peaks: self._nb_good_pks += 1 elif nb_gt_peaks > nb_pred_peaks: self._less_est_frame_cnt += 1 self._less_est_cnt += nb_gt_peaks - nb_pred_peaks elif nb_pred_peaks > nb_gt_peaks: self._more_est_frame_cnt += 1 self._more_est_cnt += nb_pred_peaks - nb_gt_peaks if nb_gt_peaks and nb_pred_peaks: pred_ind = np.where(pred_doa_thresholded[frame] == 1)[1] pred_list_rad = np.array(self._data_gen.get_matrix_index(pred_ind) ) * np.pi / 180 gt_ind = np.where(gt_doa[frame] == 1)[1] gt_list_rad = np.array(self._data_gen.get_matrix_index(gt_ind) ) * np.pi / 180 frame_dist = distance_between_gt_pred(gt_list_rad.T, pred_list_rad.T) self._doa_loss_pred += frame_dist 
avod|test|3d|anchor|Box3dEncoderTest|encoder|core|box|tensor|to def test_anchor_tensor_to_box_3d(self): anchors = np.asarray([[-0.59, 1.9, 25.01, 3.2, 1.66, 1.61], [-0.59, 1.9, 25.01, 1.61, 1.66, 3.2]], dtype=np.float32) exp_3d_box = np.asarray([[-0.59, 1.9, 25.01, 3.2, 1.61, 1.66, 0], [- 0.59, 1.9, 25.01, 3.2, 1.61, 1.66, -1.57]], dtype=np.float32) anchor_tensors = tf.convert_to_tensor(anchors, dtype=tf.float32) boxes_3d = box_3d_encoder.anchors_to_box_3d(anchor_tensors, fix_lw=True) sess = tf.Session() with sess.as_default(): boxes_3d_out = boxes_3d.eval() np.testing.assert_almost_equal(boxes_3d_out, exp_3d_box, decimal=3, err_msg='Wrong tensor anchor to box3D format') 
Micro|fire|module|net|master|micro|Models|Net def fire_module(inputs, fire_i, base_e, freq, squeeze_ratio, pct_3x3, dilation_rate, activation, kernel_initializer, data_format, use_bias= False, decoder=False): e_i, s_1x1, e_1x1, e_3x3 = get_fire_config(i=fire_i, base_e=base_e, freq=freq, squeeze_ratio=squeeze_ratio, pct_3x3=pct_3x3) if decoder: d = 'decoder_' else: d = '' squeeze = Conv2D(s_1x1, (1, 1), use_bias=use_bias, activation= activation, kernel_initializer=kernel_initializer, padding='same', dilation_rate=dilation_rate, name=d + 'fire%d_s1x1' % fire_i, data_format=data_format)(inputs) fire2_expand1 = Conv2D(e_1x1, (1, 1), use_bias=use_bias, activation= activation, kernel_initializer=kernel_initializer, padding='same', dilation_rate=dilation_rate, name=d + 'fire%d_e1x1' % fire_i, data_format=data_format)(squeeze) fire2_expand2 = Conv2D(e_3x3, (3, 3), use_bias=use_bias, activation= activation, kernel_initializer=kernel_initializer, padding='same', dilation_rate=dilation_rate, name=d + 'fire%d_e3x3' % fire_i, data_format=data_format)(squeeze) merge = Concatenate(axis=-1, name=d + 'fire_merge%d' % fire_i)([ fire2_expand1, fire2_expand2]) return merge 
test|testForward|vgg|VGGATest|nets def testForward(self): batch_size = 1 height, width = 224, 224 with self.test_session() as sess: inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = vgg.vgg_a(inputs) sess.run(tf.global_variables_initializer()) output = sess.run(logits) self.assertTrue(output.any()) 
darkflow|utils|init|loader def __init__(self, *args): self.src_key = list() self.vals = list() self.load(*args) 
plato|del|ConversationalSingleAgent|agent|conversational|single def __del__(self): """ Do some house-keeping and save the models.  :return: nothing """ if self.recorder and self.SAVE_LOG: self.recorder.save() if self.nlu: self.nlu.save() if self.dialogue_manager: self.dialogue_manager.save() if self.nlg: self.nlg.save() self.curr_state = None self.prev_state = None self.curr_state = None self.prev_usr_utterance = None self.prev_sys_utterance = None self.prev_action = None self.prev_reward = None self.prev_success = None self.prev_task_success = None 
bert|features|extract|init|master|InputExample def __init__(self, unique_id, text_a, text_b): self.unique_id = unique_id self.text_a = text_a self.text_b = text_b 
gcn|corpus|load|master|utils|text def load_corpus(dataset_str): """ Loads input corpus from gcn/data directory  ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object; ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object; ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object; ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object; ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object; ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object; ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object; ind.dataset_str.train.index => the indices of training docs in original doc list.  All objects above must be saved using python pickle module.  :param dataset_str: Dataset name :return: All data input files loaded (as well the training/test data). """ names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj'] objects = [] for i in range(len(names)): with open('data/ind.{}.{}'.format(dataset_str, names[i]), 'rb') as f: if sys.version_info > (3, 0): objects.append(pkl.load(f, encoding='latin1')) else: objects.append(pkl.load(f)) x, y, tx, ty, allx, ally, adj = tuple(objects) print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape) features = sp.vstack((allx, tx)).tolil() labels = np.vstack((ally, ty)) print(len(labels)) train_idx_orig = parse_index_file('data/{}.train.index'.format(dataset_str) ) train_size = len(train_idx_orig) val_size = train_size - x.shape[0] test_size = tx.shape[0] idx_train = range(len(y)) idx_val = range(len(y), len(y) + val_size) idx_test = range(allx.shape[0], allx.shape[0] + test_size) train_mask = sample_mask(idx_train, labels.shape[0]) val_mask = sample_mask(idx_val, labels.shape[0]) test_mask = sample_mask(idx_test, labels.shape[0]) y_train = np.zeros(labels.shape) y_val = np.zeros(labels.shape) y_test = np.zeros(labels.shape) y_train[(train_mask), :] = labels[(train_mask), :] y_val[(val_mask), :] = labels[(val_mask), :] y_test[(test_mask), :] = labels[(test_mask), :] adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj) return (adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size) 
texar|train|mode|is|py|utils def is_train_mode_py(mode, default=True): """Returns a python boolean indicating whether the mode is TRAIN.  Args: mode: A string taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`. Can be `None`. default (bool): The return value when :attr:`mode` is `None`. Default is `True`.  Returns: A python boolean. """ if mode is None: return default if mode not in context.valid_modes(): raise ValueError('Unknown mode: {}'.format(mode)) return mode == tf.estimator.ModeKeys.TRAIN 
testComposition|test|batch|BatchTest @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train_shape={}_test_shape={}_network={}_{}'.format(train, test, network, name), 'train_shape': train, 'test_shape': test, 'network': network, 'name': name, 'kernel_fn': kernel_fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for name, kernel_fn in KERNELS. items())) def testComposition(self, train_shape, test_shape, network, name, kernel_fn): utils.stub_out_pmap(batch, 2) key = random.PRNGKey(0) key, self_split, other_split = random.split(key, 3) data_self = random.normal(self_split, train_shape) data_other = random.normal(other_split, test_shape) kernel_fn = kernel_fn(key, train_shape[1:], network) kernel_batched = batch._parallel(batch._serial(kernel_fn, batch_size=2)) _test_kernel_against_batched(self, kernel_fn, kernel_batched, data_self, data_other) kernel_batched = batch._serial(batch._parallel(kernel_fn), batch_size=2) _test_kernel_against_batched(self, kernel_fn, kernel_batched, data_self, data_other) 
imgs|plot|save|evaluator def plot_save_imgs(activations, max_index): for cnt in range(max_index): print(cnt) s = 'Images/activation' + str(cnt) + '.png' curLayer = activations[cnt] if curLayer.ndim == 4: plt.clf() plt.cla() plt.close() plt.imshow(curLayer[(0), :, :, (0)], interpolation='none', cmap ='viridis') plt.axis('off') plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]) plt.savefig(s, frameon='false', bbox_inches='tight', transparent=True, pad_inches=0.0) elif curLayer.ndim == 2: plt.axis([0, 1, 0, 1]) plt.savefig(s, frameon='false', bbox_inches='tight', transparent=True, pad_inches=0.0) 
get|update|target|updater|agents|agent|SlacAgent|slac def update(): """Update target network.""" critic_update_1 = common.soft_variables_update(self._critic_network1. variables, self._target_critic_network1.variables, tau) critic_update_2 = common.soft_variables_update(self._critic_network2. variables, self._target_critic_network2.variables, tau) return tf.group(critic_update_1, critic_update_2) 
BeamSampler|append|sampler|batch def append_batch(self, X, beam_toks, mask): next_pos = X[:, -1:, (1)] + 1 next_x = torch.cat((beam_toks.unsqueeze(1), next_pos), -1).unsqueeze(1) next_mask = torch.cat([mask, torch.ones(X.size(0), 1, device=mask. device)], 1) return torch.cat((X, next_x), 1), next_mask 
flow|multithread|train|noise|sample def sample_multithread(sess, ts_batch_que, loss, sd_z, x, x_sample, y, nlf0, nlf1, iso, cam, is_training, sample_epoch_loss_que, sd_z_que, kldiv_que, test_its, nthr=8, requeue=False, sc_sd=1, epoch=0): divs = divide_parts(test_its, nthr) threads = [] for thr_id in range(nthr): threads.append(Thread(target=sample_thread, args=(thr_id, divs[ thr_id], sess, ts_batch_que, loss, sd_z, x, x_sample, y, nlf0, nlf1, iso, cam, is_training, sample_epoch_loss_que, sd_z_que, kldiv_que, requeue, sc_sd, epoch))) threads[thr_id].start() for thr_id in range(nthr): threads[thr_id].join() 
init|Piczak|PiczakCNN|EndToEndClassification|Models|EnvClassification def __init__(self, model_name, num_classes=50, weights_initializer= initializers.xavier_initializer(), biases_initializer=init_ops. zeros_initializer(), weights_regularizer=None, biases_regularizer=None): """ Initializes the PiczakCNN model class.  Args: model_name (str): model name. num_classes (int): number of the classes (i.e. size of the output layer of the classifier). weights_initializer (func): how to initialize the weights of all layers. biases_initializer (func): how to initialize the biases of all layers. weights_regularizer (func): regularization of the weights of all layers. biases_regularizer (func): regularization of the biases of all layers. """ self.model_name = model_name self.num_classes = num_classes self.W_init = weights_initializer self.b_init = biases_initializer self.W_reg = weights_regularizer self.b_reg = biases_regularizer 
operations|BatchNorm def BatchNorm(input_, isTrain, name='BN', decay=0.99): with tf.variable_scope(name) as scope: return tf.contrib.layers.batch_norm(input_, is_training=isTrain, decay=decay) 
CategoricalLogitsNegativeLogProbLoss|ops|classification|input|loss|functions|minibatches @property def input_minibatches(self): return self._logits_components 
elpips|validate|Config def validate(self): assert self.metric in ('vgg_ensemble', 'vgg', 'squeeze', 'squeeze_ensemble_maxpool') assert self.color_multiplication_mode in ('color', 'brightness') assert self.num_scales == len(self.scale_probabilities) 
texar|get|seq|agent|agents|pg|samples|SeqPGAgent def get_samples(self, extra_fetches=None, feed_dict=None): """Returns sequence samples and extra results.  Args: extra_fetches (dict, optional): Extra tensors to fetch values, besides `samples` and `sequence_length`. Same as the `fetches` argument of :tf_main:`tf.Session.run <Session#run>` and tf_main:`partial_run <Session#partial_run>`. feed_dict (dict, optional): A `dict` that maps tensor to values. Note that all placeholder values used in :meth:`get_samples` and subsequent :meth:`observe` calls should be fed here.  Returns: A `dict` with keys **"samples"** and **"sequence_length"** containing the fetched values of :attr:`samples` and :attr:`sequence_length`, as well as other fetched values as specified in :attr:`extra_fetches`.  Example:  .. code-block:: python  extra_fetches = {'truth_ids': data_batch['text_ids']} vals = agent.get_samples() sample_text = tx.utils.map_ids_to_strs(vals['samples'], vocab) truth_text = tx.utils.map_ids_to_strs(vals['truth_ids'], vocab) reward = reward_fn_in_python(truth_text, sample_text) """ if self._sess is None: raise ValueError('`sess` must be specified before sampling.') self._check_extra_fetches(extra_fetches) fetch_values = None if extra_fetches is not None: fetch_values = list(extra_fetches.values()) feeds = None if feed_dict is not None: feeds = list(feed_dict.keys()) self._setup_partial_run(fetches=fetch_values, feeds=feeds) fetches = {'samples': self._samples, 'sequence_length': self. _sequence_length} if extra_fetches is not None: fetches.update(extra_fetches) feed_dict_ = feed_dict vals = self._sess.partial_run(self._partial_run_handle, fetches, feed_dict=feed_dict_) self._samples_py = vals['samples'] self._sequence_length_py = vals['sequence_length'] return vals 
fn|input|nmt|estimator|make def _input_fn(params): """Input function.""" if mode == tf.contrib.learn.ModeKeys.TRAIN: src_file = '%s.%s' % (hparams.train_prefix, hparams.src) tgt_file = '%s.%s' % (hparams.train_prefix, hparams.tgt) else: src_file = '%s.%s' % (hparams.test_prefix, hparams.src) tgt_file = '%s.%s' % (hparams.test_prefix, hparams.tgt) src_vocab_file = hparams.src_vocab_file tgt_vocab_file = hparams.tgt_vocab_file src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables( src_vocab_file, tgt_vocab_file, hparams.share_vocab) src_dataset = tf.data.TextLineDataset(src_file) tgt_dataset = tf.data.TextLineDataset(tgt_file) if mode == tf.contrib.learn.ModeKeys.TRAIN: if 'context' in params: batch_size = params['batch_size'] global_batch_size = batch_size num_hosts = params['context'].num_hosts current_host = params['context'].current_input_fn_deployment()[1] elif 'dataset_index' in params: current_host = params['dataset_index'] num_hosts = params['dataset_num_shards'] batch_size = params['batch_size'] global_batch_size = hparams.batch_size else: num_hosts = 1 current_host = 0 batch_size = hparams.batch_size global_batch_size = batch_size mlperf_log.gnmt_print(key=mlperf_log.INPUT_BATCH_SIZE, value=batch_size ) mlperf_log.gnmt_print(key=mlperf_log.TRAIN_HP_MAX_SEQ_LEN, value= hparams.src_max_len) return iterator_utils.get_iterator(src_dataset, tgt_dataset, src_vocab_table, tgt_vocab_table, batch_size=batch_size, global_batch_size=global_batch_size, sos=hparams.sos, eos= hparams.eos, random_seed=hparams.random_seed, num_buckets= hparams.num_buckets, src_max_len=hparams.src_max_len, tgt_max_len=hparams.tgt_max_len, output_buffer_size=None, skip_count=None, num_shards=num_hosts, shard_index=current_host, reshuffle_each_iteration=True, use_char_encode=hparams. use_char_encode, filter_oversized_sequences=True) else: if 'infer_batch_size' in params: batch_size = params['infer_batch_size'] else: batch_size = hparams.infer_batch_size return iterator_utils.get_infer_iterator(src_dataset, src_vocab_table, batch_size=batch_size, eos=hparams.eos, src_max_len=hparams.src_max_len_infer, use_char_encode=hparams. use_char_encode) 
texar|MemNetBase|dim|modules|memory|network @property def memory_dim(self): """The dimension of embedded memory and all vectors in hops. """ return self._memory_dim 
test|InceptionTest|testGlobalPoolUnknownImageShape|inception|nets|v2|resnet def testGlobalPoolUnknownImageShape(self): batch_size = 1 height, width = 330, 400 num_classes = 1000 with self.test_session() as sess: inputs = tf.placeholder(tf.float32, (batch_size, None, None, 3)) logits, end_points = inception.inception_resnet_v2(inputs, num_classes, create_aux_logits=False) self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits')) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) pre_pool = end_points['Conv2d_7b_1x1'] images = tf.random_uniform((batch_size, height, width, 3)) sess.run(tf.global_variables_initializer()) logits_out, pre_pool_out = sess.run([logits, pre_pool], {inputs: images.eval()}) self.assertTupleEqual(logits_out.shape, (batch_size, num_classes)) self.assertTupleEqual(pre_pool_out.shape, (batch_size, 8, 11, 1536)) 
label|compute|datacode|LabelRepresentation|idx|map|name|to|ner def _compute_label_idx_to_label_name_map(self): self.label_idx_to_label_name_map = {v: k for k, v in self. label_name_to_label_idx_map.items()} 
preprocess|utils|file|read def read_file(path, tok=False): """a generator to generate each line of file.""" with open_file(path) as f: for line in f.readlines(): words = split_sentence(line.strip(), tok) yield words 
deepctr|DNN|core|layers|build def build(self, input_shape): input_size = input_shape[-1] hidden_units = [int(input_size)] + list(self.hidden_units) self.kernels = [self.add_weight(name='kernel' + str(i), shape=( hidden_units[i], hidden_units[i + 1]), initializer=glorot_normal( seed=self.seed), regularizer=l2(self.l2_reg), trainable=True) for i in range(len(self.hidden_units))] self.bias = [self.add_weight(name='bias' + str(i), shape=(self. hidden_units[i],), initializer=Zeros(), trainable=True) for i in range(len(self.hidden_units))] if self.use_bn: self.bn_layers = [tf.keras.layers.BatchNormalization() for _ in range(len(self.hidden_units))] self.dropout_layers = [tf.keras.layers.Dropout(self.dropout_rate, seed= self.seed + i) for i in range(len(self.hidden_units))] self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))] super(DNN, self).build(input_shape) 
fisher|renorm|ops|EigenCorrectedConvKFCBasicFB|classification|coeff|blocks @property def _renorm_coeff(self): return self._num_locations 
generator|topic|pair|pairs|on|nprf|drmm|count|utils|NPRFDRMMPairGenerator def count_on_topic(neg_len, pos_len, sample_size): sample_size = min(neg_len, sample_size) if sample_size == 0: return 0 else: return pos_len * sample_size 
v1|testBuildPreLogitsNetwork|test|InceptionV1Test|inception|nets def testBuildPreLogitsNetwork(self): batch_size = 5 height, width = 224, 224 num_classes = None inputs = tf.random_uniform((batch_size, height, width, 3)) net, end_points = inception.inception_v1(inputs, num_classes) self.assertTrue(net.op.name.startswith('InceptionV1/Logits/AvgPool')) self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024]) self.assertFalse('Logits' in end_points) self.assertFalse('Predictions' in end_points) 
models|Node2VecModel|build|graphsage def _build(self): labels = tf.reshape(tf.cast(self.placeholders['batch2'], dtype=tf.int64 ), [self.batch_size, 1]) self.neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(true_classes =labels, num_true=1, num_sampled=FLAGS.neg_sample_size, unique=True, range_max=len(self.degrees), distortion=0.75, unigrams=self.degrees .tolist()) self.outputs1 = tf.nn.embedding_lookup(self.target_embeds, self.inputs1) self.outputs2 = tf.nn.embedding_lookup(self.context_embeds, self.inputs2) self.outputs2_bias = tf.nn.embedding_lookup(self.context_bias, self.inputs2 ) self.neg_outputs = tf.nn.embedding_lookup(self.context_embeds, self. neg_samples) self.neg_outputs_bias = tf.nn.embedding_lookup(self.context_bias, self. neg_samples) self.link_pred_layer = BipartiteEdgePredLayer(self.hidden_dim, self. hidden_dim, self.placeholders, bilinear_weights=False) 
extensions|hemis|print|net|ConvDecoderImg|u def _print(self, list_of_layers): for op in list_of_layers: print(op) 
minibatch|feed|next|graphsage|EdgeMinibatchIterator|dict def next_minibatch_feed_dict(self): start_idx = self.batch_num * self.batch_size self.batch_num += 1 end_idx = min(start_idx + self.batch_size, len(self.train_edges)) batch_edges = self.train_edges[start_idx:end_idx] return self.batch_feed_dict(batch_edges) 
compute|output|FGCNNLayer|interaction|deepctr|layers|shape def compute_output_shape(self, input_shape): new_features_num = 0 features_num = input_shape[1] for i in range(0, len(self.kernel_width)): pooled_features_num = features_num // self.pooling_width[i] new_features_num += self.new_maps[i] * pooled_features_num features_num = pooled_features_num return None, new_features_num, input_shape[-1] 
fn|train|input|imagenet def train_input_fn(data_dir, batch_size, epochs, **kargs): filenames = [os.path.join(data_dir, 'train/train-%05d-of-01024' % i) for i in range(1024)] for path in filenames: if not os.path.exists(path): raise ValueError(path + ' not found') dataset = tf.data.Dataset.list_files(filenames, shuffle=True) dataset = tf.data.TFRecordDataset(dataset) dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(100 * batch_size, epochs)) dataset = dataset.apply(tf.data.experimental.map_and_batch(lambda record: _parse_one_record(record, True, kargs), batch_size)) return dataset 
call|torch|Round|vae def __call__(self, pic): return torch.round(pic) 
static|gradient|lstm|internal|checkpointed|seq2seq def lstm_seq2seq_internal_static(inputs, targets, hparams, train): """The basic LSTM seq2seq model, main step used for training.""" with tf.variable_scope('lstm_seq2seq'): if inputs is not None: inputs = tf.reverse(common_layers.flatten4d3d(inputs), axis=[1]) input_list = [inputs[:, (i), :] for i in range(21)] _, final_encoder_state = lstm(input_list, hparams, train, 'encoder' ) else: final_encoder_state = None input_list.clear() shifted_trg = common_layers.flatten4d3d(common_layers.shift_right( targets)) target_list = [shifted_trg[:, (i), :] for i in range(21)] decoder_outputs, _ = lstm(target_list, hparams, train, 'decoder', initial_state=final_encoder_state) target_list.clear() tensors = tf.transpose(tf.convert_to_tensor(decoder_outputs), perm= [1, 0, 2]) decoder_outputs.clear() with tf.variable_scope('projection'): projected_outputs = tf.layers.dense(tensors, 2048, activation= None, use_bias=False) return tf.expand_dims(projected_outputs, axis=2) 
metrics|bim|main def main(argv): global_indices = get_global_indices() for metric in FLAGS.metrics: print('Results for {}:'.format(metric)) if metric == 'RMCS': model_prefix, data = METRIC_CONFIG[metric] if FLAGS.scratch: for i in range(1, NUM_RELATIVE_MODELS + 1): compute_and_save_attr(model_prefix + str(i), data, global_indices, FLAGS.num_threads) for i in range(1, NUM_RELATIVE_MODELS): print('MCS between', model_prefix + str(i), 'and', model_prefix + str(NUM_RELATIVE_MODELS)) MCS(model_prefix + str(i), data, model_prefix + str( NUM_RELATIVE_MODELS), data) else: model1, data1, model2, data2 = METRIC_CONFIG[metric] if FLAGS.scratch: compute_and_save_attr(model1, data1, global_indices, FLAGS. num_threads) compute_and_save_attr(model2, data2, global_indices, FLAGS. num_threads) if metric == 'MCS': MCS(model1, data1, model2, data2) if metric == 'IDR': IDR(model1, data1, data2) if metric == 'IIR': IIR(model1, data1, data2) 
filters|init|MasterFilter|fasterai def __init__(self, filters: [IFilter], render_factor: int): self.filters = filters self.render_factor = render_factor 
main|diffBudget def main(): parser = argparse.ArgumentParser() parser.add_argument('--delete_num_1', '-p1', help='level of robustness') parser.add_argument('--delete_num_2', '-p2', help='level of robustness') parser.add_argument('--album_size', '-size', help='album size') args = parser.parse_args() p1 = int(args.delete_num_1) p2 = int(args.delete_num_2) albumSize = int(args.album_size) hm_path = '../dataset/features_npz/' locations_path = hm_path + 'val' + str(albumSize) + '/' locations = os.listdir(locations_path) num_winner_total_list = [] ct = 0 ct_whole = 0 for location in locations: location_path = locations_path + location + '/' albums_path = os.listdir(location_path) success_rate = [] num_winner_list = [] num_deletion_list = [] num_albums = len(albums_path) for i in range(num_albums): ct_whole += 1 album_path = location_path + albums_path[i] f = np.load(album_path) album_this = f['albumFea'] true_winner = f['label'] album_id = f['album_id'] diff = np.transpose(np.transpose(album_this) - album_this[:, ( true_winner)]) diff = np.delete(diff, true_winner, axis=1) num_winners1, num_deletion1, ind_delete1 = IP(diff, p1, albumSize) if math.isnan(num_deletion1): ct += 1 continue no_total = np.sum(ind_delete1) num_winners2, num_deletion2, ind_delete2 = IP(diff, p2, albumSize) no_same = np.sum(ind_delete1[ind_delete1 == ind_delete2]) ratio = (no_total - no_same) / no_total sys.stdout.write('ratio=%.4f\n' % ratio) sys.stdout.flush() 
print|elbo|GPSig|training|run def run(self, ctx): likelihood = ctx.session.run(self.model.likelihood_tensor) print('ELBO: {:.2f}'.format(likelihood), end='\t|\t') 
agents|agent|summary|slac|gif def _gif_summary(name, images, fps, saturate=False, step=None): images = tf.image.convert_image_dtype(images, tf.uint8, saturate=saturate) output = tf.concat(tf.unstack(images), axis=2)[None] gif_utils.gif_summary_v2(name, output, 1, fps, step=step) 
forward|networks|GEN def forward(self, input): output = self.model(input.view(input.size(0), -1)) output = output.view(*input.size()) return 2 * output + torch.clamp(input, min=-1, max=1 ) if self.residual else output 
checkpoint|gpt2|init|model|utils def init_gpt2_checkpoint(sess, init_checkpoint): """ Initializes GPT-2 model parameters from a checkpoint  Args: init_checkpoint (str): Path to the checkpoint. """ tvars = tf.trainable_variables() if init_checkpoint: assignment_map = _get_assignment_map_from_checkpoint(sess, tvars, init_checkpoint) init_fn = tf.contrib.framework.assign_from_checkpoint_fn( init_checkpoint, assignment_map, reshape_variables=True) init_fn(sess) 
envs|gym|PycolabGridWorldsLevel19Env|grid|init|pycolab|worlds|env def __init__(self): super(PycolabGridWorldsEnv, self).__init__() 
devtools|cleverhans|tests|get|errors|docscrape|NumpyClassDocString def get_errors(self): errors = NumpyDocString.get_errors(self) if not self['Parameters'] and self.has_parameters: errors.append('%s class has no Parameters section' % self.class_name) return errors 
array|preprocess|utils|make def make_array(word_id, words): """generate id numpy array from plain text words.""" ids = [word_id.get(word, unk_token_id) for word in words] return np.array(ids, 'i') 
one|scannet|epoch|train def train_one_epoch(sess, ops, next_train_element, train_writer): """ ops: dict mapping from string to tf ops """ log_string(str(datetime.now())) cur_batch_input = np.zeros((BATCH_SIZE, NUM_POINT, INPUT_DIM)) cur_batch_label = np.zeros((BATCH_SIZE, NUM_POINT), dtype=np.int32) cur_batch_inner = np.zeros((BATCH_SIZE, NUM_POINT), dtype=np.int32) total_correct = 0 total_seen = 0 loss_sum = 0 batch_idx = 0 train_time = 0.0 while True: try: padded_all = sess.run(next_train_element) bsize = padded_all.shape[0] batch_input = np.zeros((bsize, NUM_POINT, INPUT_DIM)) batch_label = np.zeros((bsize, NUM_POINT), dtype=np.int32) batch_inner = np.zeros((bsize, NUM_POINT), dtype=np.int32) for b in range(bsize): loc = np.where(padded_all[(b), :, (-1)] < 0) if len(loc[0]) == 0: num = padded_all.shape[1] else: num = loc[0][0] if num == 0: print(loc, padded_all[(b), 0:10, :]) print('problem of train') exit() if num < NUM_POINT: sample_index = np.random.choice(num, NUM_POINT, replace =True) else: sample_index = np.random.choice(num, NUM_POINT, replace =False) batch_input[b, ...] = padded_all[(b), (sample_index), 0:-2] batch_label[(b), :] = padded_all[b, sample_index, -2] batch_inner[(b), :] = padded_all[b, sample_index, -1] batch_input, batch_label, batch_inner = augment_fn(batch_input, batch_label, batch_inner) cur_batch_input[0:bsize, (...)] = batch_input cur_batch_label[0:bsize, :] = batch_label cur_batch_inner[0:bsize, :] = batch_inner feed_dict = {ops['input_pl']: cur_batch_input, ops['label_pl']: cur_batch_label, ops['inner_label_pl']: cur_batch_inner, ops['training_pl']: True} now = time.time() summary, global_step, _, loss_val, pred_val = sess.run([ops[ 'merged'], ops['global_step'], ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict) train_time += time.time() - now train_writer.add_summary(summary, global_step) pred_val = np.argmax(pred_val, 2) for b in range(bsize): inIdx = batch_inner[b] == 1 correct = np.sum(pred_val[b, inIdx] == batch_label[b, inIdx]) total_correct += correct total_seen += sum(inIdx) loss_sum += loss_val if (batch_idx + 1) % 50 == 0: log_string(' ---- batch: %03d ----' % (batch_idx + 1)) log_string('mean loss: %f' % (loss_sum / 10)) log_string('accuracy: %f' % (total_correct / float(total_seen)) ) total_correct = 0 total_seen = 0 loss_sum = 0 batch_idx += 1 except tf.errors.OutOfRangeError: break log_string('training one batch require %.2f milliseconds' % (1000 * train_time / batch_idx)) return 
class|theta|model|my|func|id|to def my_func(class_id): if class_id in [1, 2, 4]: return np.float32(2 * math.pi / 6) else: return np.float32(0) 
bert|get|variable|AdamWeightDecayOptimizer|name|master|optimization def _get_variable_name(self, param_name): """Get the variable name from the tensor name.""" m = re.match('^(.*):\\d+$', param_name) if m is not None: param_name = m.group(1) return param_name 
BertModelTest|bert|test|create|BertModelTester|for|pretraining|modeling def create_bert_for_pretraining(self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels): model = BertForPreTraining(config=config) model.eval() loss = model(input_ids, token_type_ids, input_mask, token_labels, sequence_labels) prediction_scores, seq_relationship_score = model(input_ids, token_type_ids, input_mask) outputs = {'loss': loss, 'prediction_scores': prediction_scores, 'seq_relationship_score': seq_relationship_score} return outputs 
BertModelTest|bert|test|master|modeling|run|tester def run_tester(self, tester): with self.test_session() as sess: ops = tester.create_model() init_op = tf.group(tf.global_variables_initializer(), tf. local_variables_initializer()) sess.run(init_op) output_result = sess.run(ops) tester.check_output(output_result) self.assert_all_tensors_reachable(sess, [init_op, ops]) 
pnasnet|test|testBuildNonExistingLayerLargeModel|nasnet|PNASNetTest|nets def testBuildNonExistingLayerLargeModel(self): """Tests that the model is built correctly without unnecessary layers.""" inputs = tf.random_uniform((5, 331, 331, 3)) tf.train.create_global_step() with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()): pnasnet.build_pnasnet_large(inputs, 1000) vars_names = [x.op.name for x in tf.trainable_variables()] self.assertIn('cell_stem_0/1x1/weights', vars_names) self.assertNotIn('cell_stem_1/comb_iter_0/right/1x1/weights', vars_names) 
texar|networks|Conv1DNetwork|conv|modules|build def _build(self, inputs, sequence_length=None, dtype=None, mode=None): """Feeds forward inputs through the network layers and returns outputs.  Args: inputs: The inputs to the network, which is a 3D tensor. sequence_length (optional): An int tensor of shape `[batch_size]` containing the length of each element in :attr:`inputs`. If given, time steps beyond the length will first be masked out before feeding to the layers. dtype (optional): Type of the inputs. If not provided, infers from inputs automatically. mode (optional): A tensor taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including `TRAIN`, `EVAL`, and `PREDICT`. If `None`, :func:`texar.global_mode` is used.  Returns: The output of the final layer. """ if sequence_length is not None: inputs = mask_sequences(inputs, sequence_length, dtype=dtype, time_major=False, tensor_rank=3) return super(Conv1DNetwork, self)._build(inputs, mode=mode) 
sn|celeba|sgan|256|t|normalization|spectral def spectral_normalization(w): return w / spectral_norm(w) 
gen|local|response|wgan|celebA|utils|norm def local_response_norm(x): return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=0.0001, beta=0.75) 
utils|labels|decode|deeplab|resnet def decode_labels(mask, num_images=1): """Decode batch of segmentation masks.  Args: mask: result of inference after taking argmax. num_images: number of images to decode from the batch.  Returns: A batch with num_images RGB images of the same size as the input. """ n, h, w, c = mask.shape assert n >= num_images, 'Batch size %d should be greater or equal than number of images to save %d.' % ( n, num_images) outputs = np.zeros((num_images, h, w, 3), dtype=np.uint8) for i in range(num_images): img = Image.new('RGB', (len(mask[i, 0]), len(mask[i]))) pixels = img.load() for j_, j in enumerate(mask[(i), :, :, (0)]): for k_, k in enumerate(j): if k < n_classes: pixels[k_, j_] = label_colours[k] outputs[i] = np.array(img) return outputs 
texar|StochasticConnector|connectors|modules|init def __init__(self, output_size, hparams=None): ConnectorBase.__init__(self, output_size, hparams) 
models|nfm|deepctr|NFM def NFM(feature_dim_dict, embedding_size=8, dnn_hidden_units=(128, 128), l2_reg_embedding=1e-05, l2_reg_linear=1e-05, l2_reg_dnn=0, init_std= 0.0001, seed=1024, bi_dropout=0, dnn_dropout=0, dnn_activation='relu', task='binary'): """Instantiates the Neural Factorization Machine architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_linear: float. L2 regularizer strength applied to linear part. :param l2_reg_dnn: float . L2 regularizer strength applied to DNN :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param biout_dropout: When not ``None``, the probability we will drop out the output of BiInteractionPooling Layer. :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param dnn_activation: Activation function to use in deep net :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) deep_emb_list, linear_emb_list, dense_input_dict, inputs_list = ( preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True)) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) fm_input = concat_fun(deep_emb_list, axis=1) bi_out = BiInteractionPooling()(fm_input) if bi_dropout: bi_out = tf.keras.layers.Dropout(bi_dropout)(bi_out, training=None) deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed)(bi_out) deep_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)( deep_out) final_logit = linear_logit if len(dnn_hidden_units) > 0: final_logit = tf.keras.layers.add([final_logit, deep_logit]) output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
RpnModel|avod|inputs|models|fill|anchor|pl|rpn|model|core def _fill_anchor_pl_inputs(self, anchors_info, ground_plane, image_shape, stereo_calib_p2, sample_name, sample_augs): """ Fills anchor placeholder inputs with corresponding data  Args: anchors_info: anchor info from mini_batch_utils ground_plane: ground plane coefficients image_shape: image shape (h, w), used for projecting anchors sample_name: name of the sample, e.g. "000001" sample_augs: list of sample augmentations """ all_anchor_boxes_3d = [] anchors_ious = [] anchor_offsets = [] anchor_classes = [] if len(self.dataset.classes) > 1: for class_idx in range(len(self.dataset.classes)): grid_anchor_boxes_3d = self._anchor_generator.generate(area_3d= self._area_extents, anchor_3d_sizes=self._cluster_sizes[ class_idx], anchor_stride=self._anchor_strides[class_idx], ground_plane=ground_plane) all_anchor_boxes_3d.append(grid_anchor_boxes_3d) all_anchor_boxes_3d = np.concatenate(all_anchor_boxes_3d) else: class_idx = 0 grid_anchor_boxes_3d = self._anchor_generator.generate(area_3d=self ._area_extents, anchor_3d_sizes=self._cluster_sizes[class_idx], anchor_stride=self._anchor_strides[class_idx], ground_plane= ground_plane) all_anchor_boxes_3d = grid_anchor_boxes_3d sample_has_labels = True if self._train_val_test in ['train', 'val']: if anchors_info: (anchor_indices, anchors_ious, anchor_offsets, anchor_classes ) = anchors_info anchor_boxes_3d_to_use = all_anchor_boxes_3d[anchor_indices] else: train_cond = (self._train_val_test == 'train' and self. _train_on_all_samples) eval_cond = (self._train_val_test == 'val' and self. _eval_all_samples) if train_cond or eval_cond: sample_has_labels = False else: sample_has_labels = False if not sample_has_labels: voxel_grid_2d = self.dataset.kitti_utils.create_sliced_voxel_grid_2d( sample_name, self.dataset.bev_source, image_shape=image_shape) anchors_to_use = box_3d_encoder.box_3d_to_anchor(all_anchor_boxes_3d) empty_filter = anchor_filter.get_empty_anchor_filter_2d(anchors_to_use, voxel_grid_2d, density_threshold=1) anchor_boxes_3d_to_use = all_anchor_boxes_3d[empty_filter] anchor_boxes_3d_to_use = np.asarray(anchor_boxes_3d_to_use) anchors_ious = np.asarray(anchors_ious) anchor_offsets = np.asarray(anchor_offsets) anchor_classes = np.asarray(anchor_classes) if kitti_aug.AUG_FLIPPING in sample_augs: anchor_boxes_3d_to_use = kitti_aug.flip_boxes_3d(anchor_boxes_3d_to_use , flip_ry=False) if anchors_info: anchor_offsets[:, (0)] = -anchor_offsets[:, (0)] anchors_to_use = box_3d_encoder.box_3d_to_anchor(anchor_boxes_3d_to_use) num_anchors = len(anchors_to_use) bev_anchors, bev_anchors_norm = anchor_projector.project_to_bev( anchors_to_use, self._bev_extents) img_anchors, img_anchors_norm = anchor_projector.project_to_image_space( anchors_to_use, stereo_calib_p2, image_shape) self._bev_anchors_norm = bev_anchors_norm[:, ([1, 0, 3, 2])] self._img_anchors_norm = img_anchors_norm[:, ([1, 0, 3, 2])] self._placeholder_inputs[self.PL_ANCHORS] = anchors_to_use if self._train_val_test in ['train', 'val'] and len(anchors_ious) > 0: self._placeholder_inputs[self.PL_ANCHOR_IOUS] = anchors_ious self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = anchor_offsets self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = anchor_classes elif self._train_val_test in ['test'] or len(anchors_ious) == 0: self._placeholder_inputs[self.PL_ANCHOR_IOUS] = np.zeros(num_anchors) self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = np.zeros([ num_anchors, 6]) self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = np.zeros(num_anchors ) else: raise ValueError('Got run mode {}, and non-empty anchor info'. format(self._train_val_test)) self._placeholder_inputs[self.PL_BEV_ANCHORS] = bev_anchors self._placeholder_inputs[self.PL_BEV_ANCHORS_NORM] = self._bev_anchors_norm self._placeholder_inputs[self.PL_IMG_ANCHORS] = img_anchors self._placeholder_inputs[self.PL_IMG_ANCHORS_NORM] = self._img_anchors_norm 
tests|test|DNN|core|layers @pytest.mark.parametrize('hidden_units,use_bn', [(hidden_units, use_bn) for hidden_units in [(), (10,)] for use_bn in [True, False]]) def test_DNN(hidden_units, use_bn): with CustomObjectScope({'DNN': layers.DNN}): layer_test(layers.DNN, kwargs={'hidden_units': hidden_units, 'use_bn': use_bn, 'dropout_rate': 0.5}, input_shape=(BATCH_SIZE, EMBEDDING_SIZE)) 
image|warmup|inference|encode|common def _encode_image(image_array, fmt): """encodes an (numpy) image array to string.  Args: image_array: (numpy) image array fmt: image format to use  Returns: encoded image string """ from PIL import Image pil_image = Image.fromarray(image_array) image_io = io.BytesIO() pil_image.save(image_io, format=fmt) return image_io.getvalue() 
GPSig|preprocessing|paths|normalize def normalize_paths(ts_table, num_features=1, percentile=97): """ Takes as input a table of tabulated time-series as a size (num_paths, num_features * max_length) numpy array and normalises all paths to (0,1) dimension-wise applying the same lengthscales to each path Input: :ts_table:      A table of tabulated time-series of size (num_paths, num_features * max_length) :num_features:  The number of feature dimensions in each path Output: :ts_table_normalized:  A table of tabulated time-series of size (num_paths, num_features * max_length) """ ts_concatenated = np.reshape(ts_table, [-1, num_features]) scale = np.percentile(np.abs(ts_concatenated), percentile, axis=0).reshape( [1, -1]) ts_concatenated /= scale ts_table_normalized = np.reshape(ts_concatenated, ts_table.shape) return ts_table_normalized 
bn|conv|wider|net2net def conv_bn_wider(weights, next_weights, inputs, channels, new_channels, noise_std=0, last_block=False, rand=None, dir_alpha=None, verify=False): if new_channels == 0: return weights, next_weights if rand == None: rand = list(range(channels)) rand.extend(np.random.randint(0, channels, new_channels)) rep_factor = np.bincount(rand) factor = np.zeros(len(rand)) if dir_alpha == None: for i in range(len(rand)): factor[i] = 1.0 / rep_factor[rand[i]] else: for i in range(channels): x = np.random.dirichlet([dir_alpha] * rep_factor[i]) e = 0 for j in range(channels + new_channels): if rand[j] == i: factor[j] = x[e] e += 1 w_conv_new = np.array(weights[0]).reshape(channels, inputs, 3, 3)[(rand ), :, :, :] bias = np.array(weights[1])[rand] w_bn_means = np.array(weights[2])[rand] w_bn_vars = np.array(weights[3])[rand] if not last_block: w_filter = 3 else: w_filter = 1 next_weights_new = [] for j in range(len(next_weights)): n = np.array(next_weights[j]).reshape(-1, channels, w_filter, w_filter) next_weights_new.append(n[:, (rand), :, :]) for i in range(len(rand)): noise = 0 if i >= channels: noise = np.random.normal(0, noise_std) next_weights_new[j][:, (i), :, :] *= (1.0 + noise) * factor[i] if noise_std == 0 and verify: x = np.random.random((inputs, 19, 19)) old1 = convolve(np.array(weights[0]).reshape(channels, inputs, 3, 3 ), x, bn=[weights[2], weights[3]]) old2 = convolve(np.array(next_weights[0]).reshape(-1, channels, w_filter, w_filter), old1) new1 = convolve(np.array(w_conv_new).reshape(channels + new_channels, inputs, 3, 3), x, bn=[w_bn_means, w_bn_vars]) new2 = convolve(np.array(next_weights_new[0]).reshape(-1, channels + new_channels, w_filter, w_filter), new1) assert (np.abs(old2 - new2) < 1e-06).all() w_conv_new = w_conv_new.flatten() for j in range(len(next_weights)): next_weights_new[j] = next_weights_new[j].flatten() w_new = [w_conv_new, bias, w_bn_means, w_bn_vars] return w_new, next_weights_new 
negative|test|pix2pix|padding|DiscriminatorTest|nets|layers|four def test_four_layers_negative_padding(self): batch_size = 2 input_size = 256 images = tf.ones((batch_size, input_size, input_size, 3)) with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()): with self.assertRaises(ValueError): pix2pix.pix2pix_discriminator(images, num_filters=[64, 128, 256, 512], padding=-1) 
renorm|unitvariance|training|vkge|constraints def renorm_unitvariance(log_var_matrix, norm=1.0, axis=0): var_matrix = tf.sqrt(tf.exp(log_var_matrix)) scaled = tf.clip_by_norm(var_matrix, 1.0, axes=1) scaled = tf.log(scaled ** 2) return tf.assign(log_var_matrix, scaled) 
distributions|regression|DMatrixVariateNormal|controller|sample def _sample(self, n_samples): mean, u_c, v_c = self.mean, self.u_c, self.v_c if not self.is_reparameterized: mean = tf.stop_gradient(mean) u_c = tf.stop_gradient(u_c) v_c = tf.stop_gradient(v_c) u_c = tile_ntimes(u_c, n_samples) v_c = tile_ntimes(v_c, n_samples) shape = tf.concat([[n_samples], self.batch_shape, self.value_shape], 0) epsilon = tf.random_normal(shape, dtype=self.dtype) v_c_t = transpose_last2dims(v_c) samples = mean + tf.matmul(tf.matmul(u_c, epsilon), v_c_t) static_n_samples = n_samples if isinstance(n_samples, int) else None samples.set_shape(tf.TensorShape([static_n_samples]).concatenate(self. get_batch_shape()).concatenate(self.get_value_shape())) return samples 
scope|arg|inception|nets|v2|resnet def inception_resnet_v2_arg_scope(weight_decay=4e-05, batch_norm_decay= 0.9997, batch_norm_epsilon=0.001, activation_fn=tf.nn.relu, batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS): """Returns the scope with the default parameters for inception_resnet_v2.  Args: weight_decay: the weight decay for weights variables. batch_norm_decay: decay for the moving average of batch_norm momentums. batch_norm_epsilon: small float added to variance to avoid dividing by zero. activation_fn: Activation function for conv2d. batch_norm_updates_collections: Collection for the update ops for batch norm.  Returns: a arg_scope with the parameters needed for inception_resnet_v2. """ with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay), biases_regularizer=slim.l2_regularizer(weight_decay)): batch_norm_params = {'decay': batch_norm_decay, 'epsilon': batch_norm_epsilon, 'updates_collections': batch_norm_updates_collections, 'fused': None} with slim.arg_scope([slim.conv2d], activation_fn=activation_fn, normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params ) as scope: return scope 
texar|inputs|tf|modules|decoders|next|GreedyEmbeddingHelper|helpers def next_inputs(self, time, outputs, state, sample_ids, name=None, reach_max_time=None): """Gets the inputs for next step.""" finished = math_ops.equal(sample_ids, self._end_token) all_finished = math_ops.reduce_all(finished) if reach_max_time is not None: all_finished = tf.logical_or(all_finished, reach_max_time) if self._embedding_args_cnt == 1: del time, outputs next_inputs = control_flow_ops.cond(all_finished, lambda : self. _start_inputs, lambda : self._embedding_fn(sample_ids)) elif self._embedding_args_cnt == 2: del outputs times = tf.ones(self._batch_size, dtype=tf.int32) * (time + 1) next_inputs = control_flow_ops.cond(all_finished, lambda : self. _start_inputs, lambda : self._embedding_fn(sample_ids, times)) return finished, next_inputs, state 
texar|dqn|qnet|qvalues|DQNAgent|from|agent|agents def _qvalues_from_qnet(self, observ): return self._sess.run(self._qnet_outputs['qvalues'], feed_dict={self. _observ_inputs: np.array([observ]), tx.global_mode(): tf.estimator. ModeKeys.PREDICT}) 
VIPPruning|fit def fit(self, X, y): if self.face_verif == True: faces1 = self.conv_net.predict(X[:, (0), :]) faces2 = self.conv_net.predict(X[:, (1), :]) faces1 = self.flatten(faces1) faces2 = self.flatten(faces2) X = np.abs(faces1 - faces2) else: X = self.conv_net.predict(X) X = self.flatten(X) pls_model = PLSRegression(n_components=self.n_comp, scale=True) pls_model.fit(X, y) self.scores = self.vip(X, y, pls_model) self.score_by_filter() return self 
bert|forward|gpt2|pytorch|GPT2LMHead|modeling|pretrained def forward(self, hidden_state): lm_logits = self.decoder(hidden_state) return lm_logits 
models|test|cvae|Autoencoder def test(self, sess, input_features, input_labels): """Single step test of the autoencoder @param sess (tf.Session) the current session @param input_feature (np.array) matrix or array of feature @param lambda_e explicit mixing coefficient @param lambda_i implicit mixing coefficient @return (float) the losses: loss, loss_r, loss_c, acc_c, loss_e, loss_i """ loss, loss_r = sess.run([self.loss, self.loss_reconstruction], feed_dict={self.x: input_features, self.labels_placeholder: input_labels}) return loss_r 
queue|get|MiniBatchSampler def get_queue(self): return self.queue 
prefab|gym|sprites|parts|init|pycolab|MazeWalker def __init__(self, corner, position, character, impassable, confined_to_board=False, egocentric_scroller=False, scrolling_group=''): """Superclass constructor for `MazeWalker`-derived classes.  `MazeWalker` does not define `Sprite.update`, so this constructor will fail if you attempt to build a `MazeWalker` on its own.  Args: corner: required by `Sprite`. position: required by `Sprite`. character: required by `Sprite`. impassable: an indexable of ASCII characters that this `MazeWalker` is not able to traverse. A string value containing these characters works. confined_to_board: whether this `MazeWalker` is allowed to walk off of the game board, or to be scrolled off of the game board. egocentric_scroller: whether this `MazeWalker` should behave as an egocentric scroller with respect to `scrolling_group` (see `protocols/scrolling.py`). If your game does not feature a scrolling game board, you don't need to worry about this argument. scrolling_group: the scrolling group that this `MazeWalker` should participate in, if not the default (`''`). If your game does not feature a scrolling game world, you don't need to worry about this argument.  Raises: TypeError: `impassable` contains values that are not ASCII characters. ValueError: `impassable` contains the character that represents this `MazeWalker` on the game board. """ super(MazeWalker, self).__init__(corner, position, character) _character_check(impassable, 'impassable', 'the MazeWalker constructor') if character in impassable: raise ValueError( 'A MazeWalker must not designate its own character {} as impassable.' .format(repr(character))) self._impassable = set(impassable) self._confined_to_board = confined_to_board self._egocentric_scroller = egocentric_scroller self._scrolling_group = scrolling_group self._virtual_row, self._virtual_col = position self._prior_visible = None 
utilities|classification|accuracy|Utilities|EndToEndClassification def classification_accuracy(test_data, predicted_labels): """ Majority-voting based classification accuracy computation.  Args: test_data (pd.DataFrame): data subset for which the accuracy needs to be calculated. predicted_labels (np.array): labels predicted by the model.  Returns: (float): the accuracy based on majority-voting for each of the segments corresponding to a sound file. """ test_data_with_preds = test_data.copy() test_data_with_preds.loc[:, ('prediction')] = predicted_labels group = test_data_with_preds.groupby('filename', sort=False) group = group[['category', 'prediction']].agg(lambda x: x.value_counts( ).index[0]) accuracy = np.sum(group['category'] == group['prediction']) / float(len (group['category'])) return accuracy 
queued|ThreadSafeIterator|init|trainer def __init__(self, iterator_or_generator): self._iterator_or_generator = iterator_or_generator self._lock = threading.Lock() 
get|tensor|nested|row|utils def get_row_nested_tensor(nest, row_idx): return tf.nest.map_structure(lambda t: t[row_idx], nest) 
DifferentialEvolutionSolver|models|differential|rand1|evolution def _rand1(self, samples): """ rand1bin, rand1exp """ r0, r1, r2 = samples[:3] return self.population[r0] + self.scale * (self.population[r1] - self. population[r2]) 
1|networks|elpips|squeezenet1|init def __init__(self, trainable=False, use_net_dropout=False, net_dropout_keep_prob=0.99, custom_net_weights=None, dtype=tf.float32): super(squeezenet1_1, self).__init__(use_net_dropout, net_dropout_keep_prob, dtype=dtype) feature_path = os.path.join(DATA_DIR, 'squeeze_pytorch_transposed_nonlinear_features.npy') self.features = np.load(feature_path).item( ) if custom_net_weights is None else custom_net_weights self.trainable = trainable self.features = make_trainable(self.features, trainable, 'squeezenet') 
nmt|lottery|add|flags def add_flags(flags): flags.DEFINE_string('lottery_results_dir', default=None, help= 'Directory to save results to.') flags.DEFINE_string('lottery_checkpoint_iters', default=None, help= 'Iterations to checkpoint the model at.') flags.DEFINE_string('lottery_prune_at', default=None, help= 'Checkpoint to prune at ("reinitialize" to randomly prune)') flags.DEFINE_string('lottery_pruning_method', default=None, help= 'Pruning method to use (something in prune_functions)') flags.DEFINE_string('lottery_reset_to', default=None, help= 'Checkpoint to reset to ("reinitialize" to train from scratch)') flags.DEFINE_string('lottery_reset_global_step_to', default=None, help= 'Checkpoint to reset global step to (for rewinding just LR)') flags.DEFINE_float('lottery_force_learning_rate', default=None, help= 'Learning rate to force to set to (rather than a schedule; for fine-tuning)' ) 
sn|deconv|sgan|t|sample def sample(path): n = 9 figure = np.zeros((img_dim * n, img_dim * n, 3)) for i in range(n): for j in range(n): z_sample = np.random.randn(1, z_dim) x_sample = g_model.predict(z_sample) digit = x_sample[0] figure[i * img_dim:(i + 1) * img_dim, j * img_dim:(j + 1) * img_dim ] = digit figure = (figure + 1) / 2 * 255 figure = np.round(figure, 0).astype(int) imageio.imwrite(path, figure) 
evaluators|eval|Evaluator|evaluator def eval(self, reco_items, k=50, on_train=False): raise NotImplementedError( 'eval method should be implemented in concrete evaluator') 
get|dev|examples|data|utils|XnliProcessor def get_dev_examples(self, data_dir): """See base class.""" lines = self._read_tsv(os.path.join(data_dir, 'xnli.dev.tsv')) examples = [] for i, line in enumerate(lines): if i == 0: continue guid = 'dev-%d' % i language = tokenization.convert_to_unicode(line[0]) if language != tokenization.convert_to_unicode(self.language): continue text_a = tokenization.convert_to_unicode(line[6]) text_b = tokenization.convert_to_unicode(line[7]) label = tokenization.convert_to_unicode(line[1]) examples.append(InputExample(guid=guid, text_a=text_a, text_b= text_b, label=label)) return examples 
large|pnasnet|scope|nasnet|arg|nets def pnasnet_large_arg_scope(weight_decay=4e-05, batch_norm_decay=0.9997, batch_norm_epsilon=0.001): """Default arg scope for the PNASNet Large ImageNet model.""" return nasnet.nasnet_large_arg_scope(weight_decay, batch_norm_decay, batch_norm_epsilon) 
compute|sequence|mask|BiasEncoding|deepctr|layers def compute_mask(self, inputs, mask=None): return mask 
deepMOT|master|train|mot|main def main(args, sot_tracker, deepMunkres, optimizer, mota_writer, motp_writer, clasf_writer): """ train a sot to perform MOT using DeepMOT Loss :param args: parameters, argparse :param sot_tracker: single object tracker, torch network :param deepMunkres: deep Hungarian Net, torch network :param optimizer: training optimizer, torch optim :param mota_writer: record MOTA loss, tensorboardX writer :param motp_writer: record MOTP loss, tensorboardX writer :param clasf_writer: record classification loss, tensorboardX writer """ iterations = 0 chunks = {} old_loss = 100 for epoch in range(args.epochs): pth = args.data_root + args.dataset + '/train/' print('training...') print('Dataset from: ', pth) videos = os.listdir(pth) random.shuffle(videos) for vname in videos: if 'flip' in vname or 'rot' in vname: continue print( '***************************************************************' ) print(vname) print( '***************************************************************' ) frames_gt = read_txt_gtV2(pth + vname + '/gt/gt.txt') if len(frames_gt.keys()) == 0: print('cannot load gt') break imgs_path = pth + vname + '/img1/' imgs = sorted(os.listdir(imgs_path)) if epoch == 0: tem = [] for i in range(0, len(imgs), args.seq_len): tem.append([i, imgs[i:i + args.seq_len]]) chunks[vname] = tem + [] del tem random.shuffle(chunks[vname]) for i in range(len(chunks[vname])): first_idx, subset = chunks[vname][i] count_ids = 0 bbox_track = dict() id_track = list() states = dict() prev_frame_id = 0 pre_id = dict() no_tracks_flag = False no_detections_flag = False for frameid, im_pth in enumerate(subset): distance_matrix = 0 frameid += first_idx if str(frameid + 1) not in frames_gt.keys(): no_detections_flag = True continue if (frameid == first_idx or no_tracks_flag or no_detections_flag): no_detections_flag = False no_tracks_flag = False img_prev = cv2.imread(os.path.join(imgs_path, im_pth)) gt_bboxes = np.array(frames_gt[str(frameid + 1)], dtype=np.float32) bbox_track[frameid] = torch.tensor(gt_bboxes[:, 1:], dtype=torch.float32).cuda() for k, bbox in enumerate(frames_gt[str(frameid + 1)]): cx, cy, target_w, target_h = 0.5 * (bbox[1] + bbox[3]), 0.5 * (bbox[2] + bbox[4]), bbox[3 ] - bbox[1], bbox[4] - bbox[2] target_pos, target_sz = np.array([cx, cy] ), np.array([target_w, target_h]) state = SiamRPN_init(img_prev, target_pos, target_sz, sot_tracker, bbox[0], train_bool =True) states[count_ids + k] = state id_track = list(range(count_ids, count_ids + len( frames_gt[str(frameid + 1)]))) count_ids += len(frames_gt[str(frameid + 1)]) prev_frame_id = frameid pre_id = dict(zip(gt_bboxes[:, (0)].astype(np.int32 ).tolist(), id_track)) del gt_bboxes continue img_curr = cv2.imread(os.path.join(imgs_path, im_pth)) h, w, _ = img_curr.shape for repeats in range(args.num_repeats): tmp = [] gt_boxes = np.array(frames_gt[str(frameid + 1)], dtype=np.float32) gt_ids = gt_boxes[:, (0)].astype(np.int32).tolist() focal_loss = 0.0 for key, state_curr in states.items(): target_pos, target_sz, state_curr, [score_tensor, ancrs] = SiamRPN_track(state_curr, img_curr, sot_tracker, train=True, noisy_bool=True) tmp.append(torch.stack([target_pos[0] - target_sz[0] * 0.5, target_pos[1] - target_sz[1] * 0.5, target_pos[0] + target_sz[0] * 0.5, target_pos[1] + target_sz[1] * 0.5], dim=0).unsqueeze(0)) focal_loss += focaLoss(score_tensor, ancrs, state_curr, gt_ids, gt_boxes, args) bbox_track[frameid] = torch.cat(tmp, dim=0) focal_loss = focal_loss / len(states.keys()) gt_ids, distance_matrix = ( make_single_matrix_torchV2_fast(frames_gt[str( frameid + 1)], bbox_track[frameid], h, w)) deepMunkres.hidden_row = deepMunkres.init_hidden(1) deepMunkres.hidden_col = deepMunkres.init_hidden(1) output_track_gt = deepMunkres(distance_matrix) softmaxed_row = rowSoftMax(output_track_gt, scale= args.smax_scale).contiguous() softmaxed_col = colSoftMax(output_track_gt, scale= args.smax_scale).contiguous() fn = missedObjectPerframe(softmaxed_col) fp = falsePositivePerFrame(softmaxed_row) if repeats == args.num_repeats - 1: mm, motp_mask, pre_id = missedMatchErrorV3(pre_id, gt_ids, id_track, softmaxed_col, states, toUpdate=True) else: mm, motp_mask, _ = missedMatchErrorV3(pre_id, gt_ids, id_track, softmaxed_col, states, toUpdate=False) total_objects = float(distance_matrix.size(2)) sum_distance, matched_objects = deepMOTPperFrame( distance_matrix, motp_mask) total_matched_objs = float(matched_objects) motp = sum_distance / total_matched_objs mota = (fn + fp + mm) / total_objects loss = mota + 5.0 * motp + 10.0 * focal_loss sot_tracker.zero_grad() loss.backward() optimizer.step() if repeats < args.num_repeats - 1: del mm del fp del fn del sum_distance del output_track_gt del softmaxed_col del softmaxed_row del _ torch.cuda.empty_cache() del bbox_track[prev_frame_id] torch.cuda.empty_cache() prev_frame_id = frameid if (frameid + 1) % args.ref_freq == 0: states = update_target_image_train(motp_mask.detach ().cpu().numpy().copy(), id_track, frames_gt[ str(frameid + 1)], img_curr, states, sot_tracker) bbox_track[frameid ], count_ids, no_tracks_flag = easy_birth_deathV4_rpn( motp_mask.detach().cpu().numpy().copy(), bbox_track [frameid], frames_gt[str(frameid + 1)], img_curr, id_track, count_ids, states, sot_tracker, pre_id) if (iterations + 1 ) % args.save_freq == 0 and old_loss > loss.item(): old_loss = float(loss.item()) print('best model is saved into:', args.save_path + 'best_model_' + str(epoch) + '.pth') torch.save(sot_tracker.state_dict(), args.save_path + 'best_model_' + str(epoch) + '.pth') if (iterations + 1) % args.print_freq == 0: print('Epoch: [{}] Iterations: [{}]\tLoss {:.4f}'. format(epoch, iterations, float(loss.item()))) mota_writer.add_scalar('Loss', mota.item(), iterations) motp_writer.add_scalar('Loss', motp.item(), iterations) clasf_writer.add_scalar('Loss', focal_loss.item(), iterations) if (iterations + 1) % (args.save_freq * 20) == 0: print('model is saved into:', args.save_path + 'model_' + str(epoch) + '.pth') torch.save(sot_tracker.state_dict(), args. save_path + 'model_' + str(epoch) + '.pth') iterations += 1 del mm del fp del fn del sum_distance del output_track_gt del softmaxed_col del softmaxed_row del distance_matrix torch.cuda.empty_cache() 
training|w|logging|step|mac def w_step_logging(sess, train_batch_count): batch_log_ops = 'hist_dt_norms', 'median_dt_norms' for op_name in batch_log_ops: if op_name in logger.ops: log_ops = logger.ops[op_name] if not isinstance(log_ops, list): log_ops = [log_ops] if isinstance(log_ops[0], list): log_ops = [k[i] for k in logger.ops[op_name] for i in range (len(k)) if k[i] is not None] else: log_ops = [k for k in log_ops if k is not None] log_strings = sess.run(log_ops) for log in log_strings: logger.writer.add_summary(log, train_batch_count) 
checkpoints|checkpoint|get|averaging def get_checkpoints(path): if not tf.gfile.Exists(os.path.join(path, 'checkpoint')): raise ValueError('Cannot find checkpoints in %s' % path) checkpoint_names = [] with tf.gfile.GFile(os.path.join(path, 'checkpoint')) as fd: fd.readline() for line in fd: name = line.strip().split(':')[-1].strip()[1:-1] key = int(name.split('-')[-1]) checkpoint_names.append((key, os.path.join(path, name))) sorted_names = sorted(checkpoint_names, key=operator.itemgetter(0), reverse=True) return [item[-1] for item in sorted_names] 
phi|utils|uqim|plip def plip_phi(g): plip_lambda = 1026.0 plip_beta = 1.0 return -plip_lambda * math.pow(math.log(1 - g / plip_lambda), plip_beta) 
envs|gym|grid|PycolabConnect5RotatedEnv|pycolab|format|worlds|env|observation def _format_observation(self, state): state = super(PycolabConnect5RotatedEnv, self)._format_observation(state) return np.rot90(state, k=1) 
l2|cnn|regularizer|helpers def regularizer(t): return weight * tf.nn.l2_loss(t) 
onehot|feature|bytes|tfrecord|make|shapenet def _bytes_feature(value): """Returns a bytes_list from a string / byte.""" return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) 
Monitor|europilot|height|screen @property def height(self): return self._height 
gan|in|dim|JointLatent|latent|reg @property def reg_in_dim(self): return self._reg_in_dim 
init|utils|gcn|EdgeLayer def __init__(self, layer, distance_metric): self.layer = layer self.distance_metric = distance_metric 
run|classifier|main def main(): parser = argparse.ArgumentParser() parser.add_argument('--data_dir', default=None, type=str, required=True, help= 'The input data dir. Should contain the .tsv files (or other data files) for the task.' ) parser.add_argument('--bert_model', default=None, type=str, required= True, help= 'Bert pre-trained model selected in the list: bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese.' ) parser.add_argument('--task_name', default=None, type=str, required= True, help='The name of the task to train.') parser.add_argument('--output_dir', default=None, type=str, required= True, help= 'The output directory where the model predictions and checkpoints will be written.' ) parser.add_argument('--cache_dir', default='', type=str, help= 'Where do you want to store the pre-trained models downloaded from s3') parser.add_argument('--max_seq_length', default=128, type=int, help= """The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.""" ) parser.add_argument('--do_train', action='store_true', help= 'Whether to run training.') parser.add_argument('--do_eval', action='store_true', help= 'Whether to run eval on the dev set.') parser.add_argument('--do_lower_case', action='store_true', help= 'Set this flag if you are using an uncased model.') parser.add_argument('--train_batch_size', default=32, type=int, help= 'Total batch size for training.') parser.add_argument('--eval_batch_size', default=8, type=int, help= 'Total batch size for eval.') parser.add_argument('--learning_rate', default=5e-05, type=float, help= 'The initial learning rate for Adam.') parser.add_argument('--num_train_epochs', default=3.0, type=float, help ='Total number of training epochs to perform.') parser.add_argument('--warmup_proportion', default=0.1, type=float, help= 'Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10%% of training.' ) parser.add_argument('--no_cuda', action='store_true', help= 'Whether not to use CUDA when available') parser.add_argument('--local_rank', type=int, default=-1, help= 'local_rank for distributed training on gpus') parser.add_argument('--seed', type=int, default=42, help= 'random seed for initialization') parser.add_argument('--gradient_accumulation_steps', type=int, default= 1, help= 'Number of updates steps to accumulate before performing a backward/update pass.' ) parser.add_argument('--fp16', action='store_true', help= 'Whether to use 16-bit float precision instead of 32-bit') parser.add_argument('--loss_scale', type=float, default=0, help= """Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value. """ ) parser.add_argument('--server_ip', type=str, default='', help= 'Can be used for distant debugging.') parser.add_argument('--server_port', type=str, default='', help= 'Can be used for distant debugging.') args = parser.parse_args() if args.server_ip and args.server_port: import ptvsd print('Waiting for debugger attach') ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True) ptvsd.wait_for_attach() processors = {'cola': ColaProcessor, 'mnli': MnliProcessor, 'mrpc': MrpcProcessor} num_labels_task = {'cola': 2, 'mnli': 3, 'mrpc': 2} if args.local_rank == -1 or args.no_cuda: device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu') n_gpu = torch.cuda.device_count() else: torch.cuda.set_device(args.local_rank) device = torch.device('cuda', args.local_rank) n_gpu = 1 torch.distributed.init_process_group(backend='nccl') logger.info( 'device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}' .format(device, n_gpu, bool(args.local_rank != -1), args.fp16)) if args.gradient_accumulation_steps < 1: raise ValueError( 'Invalid gradient_accumulation_steps parameter: {}, should be >= 1' .format(args.gradient_accumulation_steps)) args.train_batch_size = (args.train_batch_size // args. gradient_accumulation_steps) random.seed(args.seed) np.random.seed(args.seed) torch.manual_seed(args.seed) if n_gpu > 0: torch.cuda.manual_seed_all(args.seed) if not args.do_train and not args.do_eval: raise ValueError( 'At least one of `do_train` or `do_eval` must be True.') if os.path.exists(args.output_dir) and os.listdir(args.output_dir ) and args.do_train: raise ValueError( 'Output directory ({}) already exists and is not empty.'.format (args.output_dir)) if not os.path.exists(args.output_dir): os.makedirs(args.output_dir) task_name = args.task_name.lower() if task_name not in processors: raise ValueError('Task not found: %s' % task_name) processor = processors[task_name]() num_labels = num_labels_task[task_name] label_list = processor.get_labels() tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) train_examples = None num_train_optimization_steps = None if args.do_train: train_examples = processor.get_train_examples(args.data_dir) num_train_optimization_steps = int(len(train_examples) / args. train_batch_size / args.gradient_accumulation_steps ) * args.num_train_epochs if args.local_rank != -1: num_train_optimization_steps = (num_train_optimization_steps // torch.distributed.get_world_size()) cache_dir = args.cache_dir if args.cache_dir else os.path.join( PYTORCH_PRETRAINED_BERT_CACHE, 'distributed_{}'.format(args.local_rank) ) model = BertForSequenceClassification.from_pretrained(args.bert_model, cache_dir=cache_dir, num_labels=num_labels) if args.fp16: model.half() model.to(device) if args.local_rank != -1: try: from apex.parallel import DistributedDataParallel as DDP except ImportError: raise ImportError( 'Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.' ) model = DDP(model) elif n_gpu > 1: model = torch.nn.DataParallel(model) param_optimizer = list(model.named_parameters()) no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}] if args.fp16: try: from apex.optimizers import FP16_Optimizer from apex.optimizers import FusedAdam except ImportError: raise ImportError( 'Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.' ) optimizer = FusedAdam(optimizer_grouped_parameters, lr=args. learning_rate, bias_correction=False, max_grad_norm=1.0) if args.loss_scale == 0: optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True) else: optimizer = FP16_Optimizer(optimizer, static_loss_scale=args. loss_scale) else: optimizer = BertAdam(optimizer_grouped_parameters, lr=args. learning_rate, warmup=args.warmup_proportion, t_total= num_train_optimization_steps) global_step = 0 nb_tr_steps = 0 tr_loss = 0 if args.do_train: train_features = convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer) logger.info('***** Running training *****') logger.info('  Num examples = %d', len(train_examples)) logger.info('  Batch size = %d', args.train_batch_size) logger.info('  Num steps = %d', num_train_optimization_steps) all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long) all_input_mask = torch.tensor([f.input_mask for f in train_features ], dtype=torch.long) all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long) all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long) train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids) if args.local_rank == -1: train_sampler = RandomSampler(train_data) else: train_sampler = DistributedSampler(train_data) train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size) model.train() for _ in trange(int(args.num_train_epochs), desc='Epoch'): tr_loss = 0 nb_tr_examples, nb_tr_steps = 0, 0 for step, batch in enumerate(tqdm(train_dataloader, desc= 'Iteration')): batch = tuple(t.to(device) for t in batch) input_ids, input_mask, segment_ids, label_ids = batch loss = model(input_ids, segment_ids, input_mask, label_ids) if n_gpu > 1: loss = loss.mean() if args.gradient_accumulation_steps > 1: loss = loss / args.gradient_accumulation_steps if args.fp16: optimizer.backward(loss) else: loss.backward() tr_loss += loss.item() nb_tr_examples += input_ids.size(0) nb_tr_steps += 1 if (step + 1) % args.gradient_accumulation_steps == 0: if args.fp16: lr_this_step = args.learning_rate * warmup_linear( global_step / num_train_optimization_steps, args.warmup_proportion) for param_group in optimizer.param_groups: param_group['lr'] = lr_this_step optimizer.step() optimizer.zero_grad() global_step += 1 if args.do_train: model_to_save = model.module if hasattr(model, 'module') else model output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME) torch.save(model_to_save.state_dict(), output_model_file) output_config_file = os.path.join(args.output_dir, CONFIG_NAME) with open(output_config_file, 'w') as f: f.write(model_to_save.config.to_json_string()) config = BertConfig(output_config_file) model = BertForSequenceClassification(config, num_labels=num_labels) model.load_state_dict(torch.load(output_model_file)) else: model = BertForSequenceClassification.from_pretrained(args. bert_model, num_labels=num_labels) model.to(device) if args.do_eval and (args.local_rank == -1 or torch.distributed. get_rank() == 0): eval_examples = processor.get_dev_examples(args.data_dir) eval_features = convert_examples_to_features(eval_examples, label_list, args.max_seq_length, tokenizer) logger.info('***** Running evaluation *****') logger.info('  Num examples = %d', len(eval_examples)) logger.info('  Batch size = %d', args.eval_batch_size) all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long) all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long) all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long) all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long) eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids) eval_sampler = SequentialSampler(eval_data) eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size) model.eval() eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 for input_ids, input_mask, segment_ids, label_ids in tqdm( eval_dataloader, desc='Evaluating'): input_ids = input_ids.to(device) input_mask = input_mask.to(device) segment_ids = segment_ids.to(device) label_ids = label_ids.to(device) with torch.no_grad(): tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids) logits = model(input_ids, segment_ids, input_mask) logits = logits.detach().cpu().numpy() label_ids = label_ids.to('cpu').numpy() tmp_eval_accuracy = accuracy(logits, label_ids) eval_loss += tmp_eval_loss.mean().item() eval_accuracy += tmp_eval_accuracy nb_eval_examples += input_ids.size(0) nb_eval_steps += 1 eval_loss = eval_loss / nb_eval_steps eval_accuracy = eval_accuracy / nb_eval_examples loss = tr_loss / nb_tr_steps if args.do_train else None result = {'eval_loss': eval_loss, 'eval_accuracy': eval_accuracy, 'global_step': global_step, 'loss': loss} output_eval_file = os.path.join(args.output_dir, 'eval_results.txt') with open(output_eval_file, 'w') as writer: logger.info('***** Eval results *****') for key in sorted(result.keys()): logger.info('  %s = %s', key, str(result[key])) writer.write('%s = %s\n' % (key, str(result[key]))) 
deepMOT|death|master|mot|tracking|utils|birth def tracking_birth_death(distance, bbox_track, det_boxes, curr_img, id_track, count_ids, frameid, birth_candidates, toinit_tracks, death_candidates, states, sot_tracker, collect_prev_pos, sst, th, birth_iou=0.4, death_count=30, iou_th_overlap=0.4, to_refine=False, DAN_th=0.5, birth_wait=3, to_interpolate=None, interpolate_flag=-1.0, loose_assignment=False, case1_interpolate=True): """ :param distance: IOU between [track, det]  [num_track, num_dets] :param velocity_track: {person_id: velocity...}, velocity_track at time t, keep track of current target velocity :param bbox_track: torch tensor of shape [num_track, 4], keep track of current target bboxes :param det_boxes: current detection [gt] boxes [[(id,) bbox], [(id,) bbox]] :param curr_img: numpy array current img to get target crop to give birth :param id_track: track ids for current frame :param count_ids: total num track ids counter :param frameid: current frameid id (int, starts with 0) :param birth_candidates: dict for recording candidates to birth, {frameid:[det_id, ...],...} :param toinit_tracks: record to birth for previous frames (near online process) [[frameid, det_box_id, track_id], ...] :param th: threshold to do :return: updated bbox_track, count_ids, no_tracks_flag """ det = det_boxes[str(frameid + 1)] max_frameid = np.max(np.array(list(det_boxes.keys()), dtype=np.float32)) im_h, im_w, _ = curr_img.shape im_curr_features = TrackUtil.convert_image(curr_img.copy()) to_die = [] to_recover_track_idxes = [] not_a_birth_candidate = [] if distance.shape[0] != 0: track_dets = dict() det_ids_iou_lower_th = list() for detection_id in range(distance.shape[1]): track_indexes = np.where(distance[:, (detection_id)] >= th)[0] if len(track_indexes) > 0: if len(track_indexes) > 1: s = list() for track_index in track_indexes: s.append(collect_prev_pos[id_track[track_index]][5]) if 'inactive' in s and 'active' in s: for idxx in range(len(s)): if s[idxx] == 'inactive': track_indexes = np.delete(track_indexes, [idxx] ) else: pass else: pass if not loose_assignment: rest_IOUs = [distance[track_index, detection_id] for track_index in track_indexes] max_iou_idx = track_indexes[np.argmax(rest_IOUs)] if max_iou_idx not in track_dets.keys(): track_dets[max_iou_idx] = [[detection_id, np.max( rest_IOUs)]] else: track_dets[max_iou_idx].append([detection_id, np. max(rest_IOUs)]) else: for max_iou_idx in track_indexes: if max_iou_idx not in track_dets.keys(): track_dets[max_iou_idx] = [[detection_id, distance[max_iou_idx, detection_id]]] else: track_dets[max_iou_idx].append([detection_id, distance[max_iou_idx, detection_id]]) else: det_ids_iou_lower_th.append(detection_id) to_allocate_index = np.argmax(distance[:, (detection_id)]) iou_max = distance[to_allocate_index, detection_id] if to_allocate_index not in track_dets.keys(): track_dets[to_allocate_index] = [[detection_id, iou_max]] else: track_dets[to_allocate_index].append([detection_id, iou_max]) for track_idx in range(distance.shape[0]): if track_idx not in track_dets.keys(): track_dets[track_idx] = [[None, None]] for i in range(distance.shape[0]): ious_for_i = track_dets[i] if ious_for_i[0][1] is None: max_iou = None associated_det_id = None else: where_max_iou = np.argmax(np.array(ious_for_i)[:, (1)]) associated_det_id, max_iou = ious_for_i[where_max_iou] if max_iou is None or max_iou < th: if interpolate_flag > 0: if collect_prev_pos[id_track[i]][6][-1] != -1 and (id_track [i] not in death_candidates or death_candidates[ id_track[i]] < interpolate_flag) and id_track[i ] not in to_interpolate: center_velo, avg_h, avg_w = collect_prev_pos[id_track[i]][6 ] to_check_box = bbox_track[(i), :].cpu().numpy().copy() condition_left = to_check_box[0] < 0 and to_check_box[2 ] > 0 condition_top = to_check_box[1] < 0 and to_check_box[3] > 0 condition_right = to_check_box[2] > im_w and to_check_box[0 ] < im_w condition_bottom = to_check_box[1] < im_h and to_check_box[ 3] > im_h if (condition_left or condition_top or condition_bottom or condition_right): if id_track[i] not in death_candidates: max_comp = interpolate_flag begin_frameid = frameid else: max_comp = interpolate_flag + death_candidates[ id_track[i]] begin_frameid = frameid - death_candidates[ id_track[i]] com_counter = 0 complete_OofV = complete_out_of_view(to_check_box, im_w, im_h) pre_pos = collect_prev_pos[id_track[i]][0][-1][-1 ].copy() to_comp = list() while (com_counter < max_comp and not complete_OofV and begin_frameid + com_counter < max_frameid): cx, cy = 0.5 * (pre_pos[0] + pre_pos[2] ) + com_counter * center_velo[0], 0.5 * ( pre_pos[1] + pre_pos[3] ) + com_counter * center_velo[1] comp_box = np.array([cx - avg_w * 0.5, cy - avg_h * 0.5, cx + avg_w * 0.5, cy + avg_h * 0.5]) complete_OofV = complete_out_of_view(comp_box, im_w, im_h) to_comp.append([[begin_frameid + com_counter, comp_box, id_track[i]]]) com_counter += 1 if (com_counter <= max_comp and begin_frameid + com_counter < max_frameid): to_interpolate[id_track[i]] = copy.deepcopy(to_comp ) collect_prev_pos[id_track[i]][5] = 'inactive' if id_track[i] not in death_candidates.keys(): death_candidates[id_track[i]] = 1 else: death_candidates[id_track[i]] += 1 if death_candidates[id_track[i]] >= death_count: to_die.append(i) del death_candidates[id_track[i]] del collect_prev_pos[id_track[i]] else: if collect_prev_pos[id_track[i]][6][-1] != -1: center_velo, avg_h, avg_w = collect_prev_pos[id_track[i]][6 ] if collect_prev_pos[id_track[i]][7][-1] == -1: pre_pos = collect_prev_pos[id_track[i]][0][-1][-1 ].copy() else: pre_pos = collect_prev_pos[id_track[i]][7].copy() cx, cy = 0.5 * (pre_pos[0] + pre_pos[2]), 0.5 * (pre_pos [1] + pre_pos[3]) states[id_track[i]]['target_pos'] = np.array([cx, cy] ) + center_velo states[id_track[i]]['target_sz'] = np.array([avg_w, avg_h]) inactive_pos = [cx + center_velo[0] - avg_w * 0.5, cy + center_velo[1] - avg_h * 0.5, cx + center_velo[0] + avg_w * 0.5, cy + center_velo[1] + avg_h * 0.5] collect_prev_pos[id_track[i]][7] = np.array(inactive_pos ).copy() collect_prev_pos[id_track[i]][4].append([frameid, bbox_track[(i), :].detach().cpu().numpy().copy()]) if max_iou is not None and 0.3 < max_iou < 0.5: if DAN_th > 0.0: detection = np.array([det[associated_det_id][-4:]], dtype=np.float32) detection[:, (2)] = detection[:, (2)] - detection[:, (0)] detection[:, (3)] = detection[:, (3)] - detection[:, (1)] detection[:, ([0, 2])] /= float(im_w) detection[:, ([1, 3])] /= float(im_h) detection_center = TrackUtil.convert_detection( detection) det_features = sst.forward_feature_extracter( im_curr_features, detection_center).detach_() affinity_mat_after_softmax = 0 penalty = 0 for old_frame, i_features in collect_prev_pos[ id_track[i]][1]: penalty += 0.995 ** ((frameid - old_frame) / 3.0) affinity_mat_after_softmax += 0.995 ** (( frameid - old_frame) / 3.0 ) * sst.forward_stacker_features(i_features, det_features, False, toNumpy=True)[:, :-1] affinity_mat_after_softmax /= penalty else: affinity_mat_after_softmax = 1.0 if float(affinity_mat_after_softmax) >= DAN_th: collect_prev_pos[id_track[i]][2] += 1 collect_prev_pos[id_track[i]][3].append([frameid, associated_det_id]) if collect_prev_pos[id_track[i]][2] == 3: collected_dets = collect_prev_pos[id_track[i]][3] avg_box = None for f_id, det_index in collected_dets: if avg_box is None: avg_box = np.array(det_boxes[str(f_id + 1)][det_index][-4:]).astype(np.float32) else: avg_box += np.array(det_boxes[str(f_id + 1)][det_index][-4:]).astype(np.float32) if f_id in birth_candidates.keys( ) and det_index in birth_candidates[f_id]: del birth_candidates[f_id][ birth_candidates[f_id].index(det_index) ] if (f_id == frameid and det_index not in not_a_birth_candidate): not_a_birth_candidate.append(det_index) avg_box /= 3.0 cx, cy, w, h = 0.5 * (avg_box[0] + avg_box[2] ), 0.5 * (avg_box[1] + avg_box[3]), avg_box[2 ] - avg_box[0], avg_box[3] - avg_box[1] states[id_track[i]] = SiamRPN_init(curr_img, np .array([cx, cy]), np.array([w, h]), sot_tracker, states[id_track[i]]['gt_id']) collect_prev_pos[id_track[i]][4][-1] = [frameid, avg_box] collect_prev_pos[id_track[i]][7] = avg_box.copy() bbox_track[i][0] = float(avg_box[0]) bbox_track[i][1] = float(avg_box[1]) bbox_track[i][2] = float(avg_box[2]) bbox_track[i][3] = float(avg_box[3]) else: pass else: collect_prev_pos[id_track[i]][2] = 0 collect_prev_pos[id_track[i]][3] = list() else: collect_prev_pos[id_track[i]][2] = 0 collect_prev_pos[id_track[i]][3] = list() elif id_track[i] in death_candidates: if DAN_th > 0.0: detection = np.array([det[associated_det_id][-4:]], dtype= np.float32) detection[:, (2)] = detection[:, (2)] - detection[:, (0)] detection[:, (3)] = detection[:, (3)] - detection[:, (1)] detection[:, ([0, 2])] /= float(im_w) detection[:, ([1, 3])] /= float(im_h) detection_center = TrackUtil.convert_detection(detection) det_features = sst.forward_feature_extracter(im_curr_features, detection_center).detach_() affinity_mat_after_softmax = 0 penalty = 0 for old_frame, i_features in collect_prev_pos[id_track[i]][1]: penalty += 0.995 ** ((frameid - old_frame) / 3.0) affinity_mat_after_softmax += 0.995 ** ((frameid - old_frame) / 3.0) * sst.forward_stacker_features( i_features, det_features, False, toNumpy=True)[:, :-1] affinity_mat_after_softmax /= penalty else: affinity_mat_after_softmax = 1.0 if affinity_mat_after_softmax >= DAN_th: collect_prev_pos[id_track[i]][5] = 'active' collected_dets = collect_prev_pos[id_track[i]][3] for f_id, det_index in collected_dets: if f_id in birth_candidates.keys( ) and det_index in birth_candidates[f_id]: del birth_candidates[f_id][birth_candidates[f_id]. index(det_index)] not_a_birth_candidate.append(associated_det_id) if id_track[i] in to_interpolate.keys(): del to_interpolate[id_track[i]] to_recover_track_idxes.append(i) else: collect_prev_pos[id_track[i]][5] = 'inactive' if id_track[i] not in death_candidates.keys(): death_candidates[id_track[i]] = 1 else: death_candidates[id_track[i]] += 1 if death_candidates[id_track[i]] >= death_count: to_die.append(i) del death_candidates[id_track[i]] del collect_prev_pos[id_track[i]] else: collect_prev_pos[id_track[i]][4].append([frameid, bbox_track[(i), :].detach().cpu().numpy().copy()]) collect_prev_pos[id_track[i]][2] = 0 collect_prev_pos[id_track[i]][3] = list() else: pass if bbox_track is not None: now_track = bbox_track.detach().cpu().numpy().copy() candidates_index = [] for indx in range(now_track.shape[0]): if indx in to_die or collect_prev_pos[id_track[indx]][5 ] == 'inactive': continue else: candidates_index.append(indx) if len(candidates_index) > 0: self_apperance_scores = [(-1.0 * id_track[corres_index]) for corres_index in candidates_index] _, pick = nms_fast_apperance_as_score(now_track[( candidates_index), :], self_apperance_scores, 0.5) to_deactivate_index = list(set(pick) ^ set(list(range(len( candidates_index))))) to_deactivate = [candidates_index[to_pick] for to_pick in to_deactivate_index] for inactive_idx in to_deactivate: if inactive_idx in to_recover_track_idxes: del to_recover_track_idxes[to_recover_track_idxes.index (inactive_idx)] collect_prev_pos[id_track[inactive_idx]][5] = 'inactive' if id_track[inactive_idx] not in death_candidates.keys(): death_candidates[id_track[inactive_idx]] = 1 else: death_candidates[id_track[inactive_idx]] += 1 if death_candidates[id_track[inactive_idx]] >= death_count: to_die.append(inactive_idx) del death_candidates[id_track[inactive_idx]] del collect_prev_pos[id_track[inactive_idx]] else: if collect_prev_pos[id_track[inactive_idx]][6][-1] != -1: center_velo, avg_h, avg_w = collect_prev_pos[ id_track[inactive_idx]][6] if collect_prev_pos[id_track[inactive_idx]][7][-1 ] == -1: pre_pos = collect_prev_pos[id_track[inactive_idx]][ 0][-1][-1].copy() else: pre_pos = collect_prev_pos[id_track[inactive_idx]][ 7].copy() cx, cy = 0.5 * (pre_pos[0] + pre_pos[2]), 0.5 * ( pre_pos[1] + pre_pos[3]) states[id_track[inactive_idx]]['target_pos' ] = np.array([cx, cy]) + center_velo states[id_track[inactive_idx]]['target_sz'] = np.array( [avg_w, avg_h]) inactive_pos = [cx + center_velo[0] - avg_w * 0.5, cy + center_velo[1] - avg_h * 0.5, cx + center_velo[0] + avg_w * 0.5, cy + center_velo[ 1] + avg_h * 0.5] collect_prev_pos[id_track[inactive_idx]][7] = np.array( inactive_pos).copy() collect_prev_pos[id_track[inactive_idx]][4].append([ frameid, bbox_track[(inactive_idx), :].detach().cpu ().numpy().copy()]) collect_prev_pos[id_track[inactive_idx]][2] = 0 collect_prev_pos[id_track[inactive_idx]][3] = list() for to_recover in to_recover_track_idxes: first_frame = collect_prev_pos[id_track[to_recover]][4][0][0 ] - 1 current_frame = frameid + 0 first_pos = collect_prev_pos[id_track[to_recover]][0][-1][-1 ].copy() if to_refine: ious_for_to_recover = track_dets[to_recover] where_max_iou = np.argmax(np.array(ious_for_to_recover) [:, (1)]) associated_det_id, _ = ious_for_to_recover[where_max_iou] curr_pos = np.array(det[associated_det_id]) cx, cy, w, h = 0.5 * (curr_pos[0] + curr_pos[2]), 0.5 * ( curr_pos[1] + curr_pos[3]), curr_pos[2] - curr_pos[0 ], curr_pos[3] - curr_pos[1] states[id_track[to_recover]] = SiamRPN_init(curr_img, np.array([cx, cy]), np.array([w, h]), sot_tracker, states[id_track[to_recover]]['gt_id']) bbox_track[to_recover][0] = float(curr_pos[0]) bbox_track[to_recover][1] = float(curr_pos[1]) bbox_track[to_recover][2] = float(curr_pos[2]) bbox_track[to_recover][3] = float(curr_pos[3]) else: curr_pos = bbox_track[(to_recover), :].detach().cpu( ).numpy() velocity = (curr_pos - first_pos) / (current_frame - first_frame) if case1_interpolate: for framerate in range(frameid - first_frame - 1): toinit_tracks.append([[framerate + first_frame + 1, first_pos + (framerate + 1) * velocity, id_track[to_recover]]]) collect_prev_pos[id_track[to_recover]][5] = 'active' collect_prev_pos[id_track[to_recover]][2] = 0 collect_prev_pos[id_track[to_recover]][3] = list() collect_prev_pos[id_track[to_recover]][4] = list() del death_candidates[id_track[to_recover]] for i in range(distance.shape[0]): if i not in to_die: to_check_sz = states[id_track[i]]['target_sz'] if to_check_sz[0] > im_w and to_check_sz[1] > im_h: to_die.append(i) del death_candidates[id_track[i]] del collect_prev_pos[id_track[i]] to_birth = [] if distance.shape[0] != 0: candidates_index = [] for indx in range(distance.shape[0]): if indx in to_die or collect_prev_pos[id_track[indx]][5 ] == 'inactive': continue else: candidates_index.append(indx) if len(candidates_index) > 0: distance_pick = distance[(candidates_index), :].copy() for j in range(distance_pick.shape[1]): if np.max(distance_pick[:, (j)] ) < th and j not in not_a_birth_candidate: to_birth.append(j) else: to_birth = list(range(len(det))) new_tobirth = [] if len(birth_candidates.keys()) < birth_wait: birth_candidates[frameid] = to_birth + [] else: not_assigned = [] associated_bbox_detid = {} for idx in to_birth: associated_bbox_detid[idx] = [] bbox = [] + det[idx][-4:] for frame, compare_lst in birth_candidates.items(): tmp = np.array([]) if len(compare_lst): old_bbox = [det_boxes[str(frame + 1)][histo_det_idx] for histo_det_idx in compare_lst] tmp = bb_fast_IOU_v1(bbox, old_bbox) if tmp.shape[0] == 0 or np.max(tmp) < birth_iou: not_assigned.append(idx) del associated_bbox_detid[idx] break else: associated_bbox_detid[idx].append([frame, compare_lst[ np.argmax(tmp)]]) to_remove = [] for current_id, v in associated_bbox_detid.items(): for frame, histo_detid in v: if histo_detid in birth_candidates[frame]: birth_candidates[frame].remove(histo_detid) else: to_remove.append(current_id) break for x in to_remove: del associated_bbox_detid[x] not_assigned += to_remove new_tobirth = list(set(not_assigned) ^ set(to_birth)) birth_candidates[frameid] = not_assigned + [] if len(list(birth_candidates.keys())) != 0: del birth_candidates[list(birth_candidates.keys())[0]] for idx in new_tobirth: bbox = det[idx][-4:] cx, cy, w, h = 0.5 * (bbox[0] + bbox[2]), 0.5 * (bbox[1] + bbox[3] ), bbox[2] - bbox[0], bbox[3] - bbox[1] target_pos, target_sz = np.array([cx, cy]), np.array([w, h]) state = SiamRPN_init(curr_img, target_pos, target_sz, sot_tracker, count_ids) states[count_ids] = state if bbox_track is not None: bbox_track = torch.cat([bbox_track, torch.FloatTensor(bbox). cuda().unsqueeze(0)], dim=0) else: bbox_track = torch.FloatTensor(bbox).cuda().unsqueeze(0) id_track.append(count_ids) for element in associated_bbox_detid[idx]: toinit_tracks.append([element + [count_ids]]) count_ids += 1 if len(to_die) > 0: to_die = sorted(to_die, reverse=True) for idx in to_die: id = id_track[idx] if id in to_interpolate: toinit_tracks += to_interpolate[id] del states[id] id_track.pop(idx) if bbox_track is not None: res = list(set(to_die) ^ set(list(range(bbox_track.size(0))))) else: res = [] if len(res) != 0: bbox_track = torch.index_select(bbox_track, 0, torch.LongTensor(res ).cuda()) else: print('all tracks died.') bbox_track = None return bbox_track, count_ids 
GlorotIndexGenerator|training|init|index|vkge def __init__(self, random_state=None): self.random_state = (random_state if random_state is not None else np. random.RandomState(0)) 
experimentalsettings|ExperimentalSettings|setitem def __setitem__(self, key, value): raise Exception('ExperimentalSettings object can not be changed.') 
envs|gym|game|grid|PycolabConnect5Env|make|pycolab|worlds|env def _make_game(self): self._setup() return connect_four.make_game(shape=self._shape, connect_n=5) 
rel|env|get|enc def get_rel_enc(enc, heading, pitch): """Replaces absolute angles with relative angles in `enc`.  Args: enc: A 2-d numpy array of shape [N, ?]; where N is the number of images and second dim is the encoding of each of the images. heading: A scalar, the current heading in radians relative to which the new heading is computed and replaced in the `enc`. pitch: A scalar, the current pitch in radians relative to which the new pitch is computed and replaced in the `enc`.  Returns: A 2-d numpy array of same shape as `enc`. """ rel_heading = _get_heading(enc) - heading rel_pitch = _get_pitch(enc) - pitch rel_signature = np.concatenate([np.sin(rel_heading), np.cos(rel_heading ), np.sin(rel_pitch), np.cos(rel_pitch)], axis=1) return np.concatenate([rel_signature, enc[:, 4:]], axis=1).astype(np. float32) 
bert|gcn|init|BERT|GCN def __init__(self, sequence_length, target_sequence_length, targets_num_max, num_classes, word_embedding_dim, l2_reg_lambda=0.0, num_hidden=100): rand_base = 0.01 self.input_x = tf.placeholder(tf.float32, [None, sequence_length, word_embedding_dim], name='input_x') self.input_target = tf.placeholder(tf.float32, [None, target_sequence_length, word_embedding_dim], name='input_x') self.input_targets_all = tf.placeholder(tf.float32, [None, targets_num_max, target_sequence_length, word_embedding_dim], name= 'input_x') self.sen_len = tf.placeholder(tf.int32, None, name='sen_len') self.target_len = tf.placeholder(tf.int32, None, name='target_len') with tf.name_scope('targets_all_len'): self.targets_all_len_a = tf.placeholder(tf.int32, [None, targets_num_max], name='targets_all_len') batch_size = tf.shape(self.input_x)[0] self.targets_all_len = [] for i in range(targets_num_max): targets_i_len = tf.slice(self.targets_all_len_a, [0, i], [ batch_size, 1]) self.targets_all_len.append(tf.squeeze(targets_i_len)) self.targets_num = tf.placeholder(tf.int32, None, name='targets_num') self.relate_cross = tf.placeholder(tf.float32, [None, targets_num_max, targets_num_max], name='relate_cross') self.relate_self = tf.placeholder(tf.float32, [None, targets_num_max, targets_num_max], name='relate_self') self.target_which = tf.placeholder(tf.float32, [None, targets_num_max], name='which_position') self.target_position = tf.placeholder(tf.float32, [None, sequence_length], name='target_position') with tf.name_scope('targets_all_position'): self.targets_all_position_a = tf.placeholder(tf.float32, [None, targets_num_max, sequence_length], name='targets_all_position') self.targets_all_position = [] for i in range(targets_num_max): targets_i_len = self.targets_all_position_a[:, (i), :] self.targets_all_position.append(tf.squeeze(targets_i_len)) self.input_y = tf.placeholder(tf.float32, [None, num_classes], name= 'input_y') self.dropout_keep_prob = tf.placeholder(tf.float32, name= 'dropout_keep_prob') l2_loss = tf.constant(0.0) with tf.name_scope('embedded_sen'): self.embedded_sen = self.input_x self.embedded_sen = tf.nn.dropout(self.embedded_sen, keep_prob=self .dropout_keep_prob) embedding_size = word_embedding_dim print('embedding_size {}'.format(embedding_size)) num_hidden = word_embedding_dim with tf.name_scope('embedding_target'): self.embedded_target = self.input_target self.embedded_target = tf.nn.dropout(self.embedded_target, keep_prob=self.dropout_keep_prob) with tf.name_scope('embedding_targets'): self.embedded_targets_all = list(range(targets_num_max)) for i in range(targets_num_max): self.embedded_target_i = self.input_targets_all[:, (i), :, :] self.embedded_target_i = tf.nn.dropout(self.embedded_target_i, keep_prob=self.dropout_keep_prob) self.embedded_targets_all[i] = self.embedded_target_i num_hidden = 300 with tf.name_scope('Bi-LSTM_sentence'): cell = tf.nn.rnn_cell.LSTMCell self.LSTM_Hiddens_sen = bi_dynamic_rnn(cell, self.embedded_sen, num_hidden, self.sen_len, sequence_length, 'bi-lstm-sentence', 'all', dropout=True, dropout_prob=self.dropout_keep_prob) pool_sen = reduce_mean_with_len(self.LSTM_Hiddens_sen, self.sen_len) with tf.variable_scope('Bi-LSTM_targets') as scope: self.LSTM_targets_all = list(range(targets_num_max)) poor_targets_all = list(range(targets_num_max)) for i in range(targets_num_max): cell = tf.nn.rnn_cell.LSTMCell self.LSTM_targets_all[i] = bi_dynamic_rnn(cell, self. embedded_targets_all[i], num_hidden, self.targets_all_len[i ], target_sequence_length, 'bi-lstm-targets', 'all', dropout=True, dropout_prob=self.dropout_keep_prob) poor_targets_all[i] = reduce_mean_with_len(self. LSTM_targets_all[i], self.targets_all_len[i]) scope.reuse_variables() with tf.variable_scope('Attention-targets_all2sentence') as scope: self.outputs_ss = list(range(targets_num_max)) self.outputs_ts = list(range(targets_num_max)) for i in range(targets_num_max): att_s_i = bilinear_attention_layer(self.LSTM_targets_all[i], pool_sen, self.targets_all_len[i], 2 * num_hidden, l2_reg_lambda, random_base=rand_base, layer_id='tar') self.outputs_ss[i] = tf.squeeze(tf.matmul(att_s_i, self. LSTM_targets_all[i]), axis=1) target_position_i = tf.expand_dims(self.targets_all_position[i], 2) LSTM_Hiddens_sen_i = tf.multiply(self.LSTM_Hiddens_sen, target_position_i) att_s_i = bilinear_attention_layer(LSTM_Hiddens_sen_i, self. outputs_ss[i], self.sen_len, 2 * num_hidden, l2_reg_lambda, random_base=rand_base, layer_id='sen') self.outputs_ts[i] = tf.squeeze(tf.matmul(att_s_i, self. LSTM_Hiddens_sen), axis=1) scope.reuse_variables() with tf.name_scope('targets_gather'): self.targets_concat = tf.concat([tf.expand_dims(i, axis=2) for i in self.outputs_ts], axis=2) with tf.name_scope('GCN_layer1'): W_cross = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_cross') b_cross = tf.Variable(tf.random.uniform([2 * num_hidden], - rand_base, rand_base), name='b_cross') W_self = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_self') b_self = tf.Variable(tf.random.uniform([2 * num_hidden], -rand_base, rand_base), name='b_self') GCN1_cross = WXbA_Relu(self.targets_concat, self.relate_cross, W_cross, b_cross) GCN1_self = WXbA_Relu(self.targets_concat, self.relate_self, W_self, b_self) GCN1_out = GCN1_cross + GCN1_self with tf.name_scope('GCN_layer2'): W_cross = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_cross') b_cross = tf.Variable(tf.random.uniform([2 * num_hidden], - rand_base, rand_base), name='b_cross') W_self = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_self') b_self = tf.Variable(tf.random.uniform([2 * num_hidden], -rand_base, rand_base), name='b_self') GCN2_cross = WXbA_Relu(GCN1_out, self.relate_cross, W_cross, b_cross) GCN2_self = WXbA_Relu(GCN1_out, self.relate_self, W_self, b_self) GCN2_out = GCN2_cross + GCN2_self target_which = tf.expand_dims(self.target_which, 1) self.GCN2_out = tf.multiply(GCN2_out, target_which) self.targets_representation = tf.reduce_sum(self.GCN2_out, 2) W = tf.Variable(tf.random_normal([2 * num_hidden, num_classes])) b = tf.Variable(tf.random_normal([num_classes])) with tf.name_scope('output'): self.scores = tf.nn.xw_plus_b(self.targets_representation, W, b, name='scores') l2_loss += tf.nn.l2_loss(W) l2_loss += tf.nn.l2_loss(b) self.predictions = tf.argmax(self.scores, 1, name='predictions') self.true_y = tf.argmax(self.input_y, 1, name='true_y') self.softmax = tf.nn.softmax(self.scores, name='softmax') with tf.name_scope('loss'): self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=self. scores, labels=self.input_y) self.loss = tf.reduce_mean(self.losses, name='loss' ) + l2_reg_lambda * l2_loss with tf.name_scope('accuracy'): self.correct_pred = tf.equal(self.predictions, self.true_y) self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, 'float'), name='accuracy') print('LOADED LSTM-Att-GCN2!') 
len|SVAEDataset|dc|svae|utils def __len__(self): return self.x.size(0) 
get|list|score|Relevance|relevance|utils|info|supervised def get_supervised_score_list(self): return self._supervised_score_list 
evaluate|modelnet def evaluate(num_votes): testset = input_fn(testlist, BATCH_SIZE, 10000) test_iterator = testset.make_initializable_iterator() next_test_element = test_iterator.get_next() with tf.device('/gpu:' + str(GPU_INDEX)): xyz_pl, label_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT) training_pl = tf.placeholder(tf.bool, shape=()) pred, end_points = MODEL.get_model(xyz_pl, training_pl, config= net_config) MODEL.get_loss(pred, label_pl, end_points) losses = tf.get_collection('losses') total_loss = tf.add_n(losses, name='total_loss') saver = tf.train.Saver() config = tf.ConfigProto() config.gpu_options.allow_growth = True config.allow_soft_placement = True config.log_device_placement = False with tf.Session(config=config) as sess: saver.restore(sess, os.path.join(LOG_DIR, MODEL_NAME)) log_string('Model restored.') ops = {'xyz_pl': xyz_pl, 'label_pl': label_pl, 'training_pl': training_pl, 'pred': pred, 'loss': total_loss} sess.run(test_iterator.initializer) eval_one_epoch(sess, ops, next_test_element, num_votes) 
batches|max|providers|data|num|DataProvider @max_num_batches.setter def max_num_batches(self, value): if value == 0 or value < -1: raise ValueError('max_num_batches must be -1 or > 0') self._max_num_batches = value self._update_num_batches() 
tangents|predict|on|cpu|neural|is def _is_on_cpu(x): return tree_all(tree_map(_arr_is_on_cpu, x)) 
texar|sequences|mask|utils|shapes def mask_sequences(sequence, sequence_length, dtype=None, time_major=False, tensor_rank=2): """Masks out sequence entries that are beyond the respective sequence lengths. Masks along the time dimension.  :attr:`sequence` and :attr:`sequence_length` can either be python arrays or Tensors, respectively. If both are python arrays (or None), the return will be a python array as well.  Args: sequence: A Tensor or python array of sequence values. If `time_major==False` (default), this must be a Tensor of shape `[batch_size, max_time, ...]`. The batch and time dimension is exchanged if `time_major==True`. sequence_length: A Tensor or python array of shape `[batch_size]`. Time steps beyond the respective sequence lengths will be made zero. dtype (dtype): Type of :attr:`sequence`. If `None`, infer from :attr:`sequence` automatically. time_major (bool): The shape format of the inputs. If `True`, :attr:`sequence` must have shape `[max_time, batch_size, ...]`. If `False` (default), :attr:`sequence` must have shape `[batch_size, max_time, ...]`. tensor_rank (int): The number of dimensions of :attr:`sequence`. Default is 2, i.e., :attr:`sequence` is a 2D Tensor consisting of batch and time dimensions. Ignored if both :attr:`sequence` and :attr:`sequence_length` are python arrays.  Returns: The masked sequence, i.e., a Tensor or python array of the same shape as :attr:`sequence` but with masked-out entries (set to zero).  If both :attr:`sequence` and :attr:`sequence_length` are python arrays, the returned value is a python array as well. """ is_tensor = tf.contrib.framework.is_tensor if is_tensor(sequence) or is_tensor(sequence_length): return _mask_sequences_tensor(sequence, sequence_length, dtype, time_major, tensor_rank) else: return _mask_sequences_py(sequence, sequence_length, dtype, time_major) 
beamsearch|search|step|model|beam|PCGN def _beam_search_step(real_vocab_size, time, logits, next_cell_state, beam_state, batch_size, beam_width, end_token, length_penalty_weight): """Performs a single step of Beam Search Decoding.  Args: time: Beam search time step, should start at 0. At time 0 we assume that all beams are equal and consider only the first beam for continuations. logits: Logits at the current time step. A tensor of shape `[batch_size, beam_width, vocab_size]` next_cell_state: The next state from the cell, e.g. an instance of AttentionWrapperState if the cell is attentional. beam_state: Current state of the beam search. An instance of `BeamSearchDecoderState`. batch_size: The batch size for this input. beam_width: Python int.  The size of the beams. end_token: The int32 end token. length_penalty_weight: Float weight to penalize length. Disabled with 0.0.  Returns: A new beam state. """ static_batch_size = tensor_util.constant_value(batch_size) prediction_lengths = beam_state.lengths previously_finished = beam_state.finished step_log_probs = nn_ops.log_softmax(logits) step_log_probs = _mask_probs(step_log_probs, end_token, previously_finished ) total_probs = array_ops.expand_dims(beam_state.log_probs, 2 ) + step_log_probs vocab_size = logits.shape[-1].value or array_ops.shape(logits)[-1] lengths_to_add = array_ops.one_hot(indices=array_ops.tile(array_ops. reshape(end_token, [1, 1]), [batch_size, beam_width]), depth= vocab_size, on_value=constant_op.constant(0, dtype=dtypes.int64), off_value=constant_op.constant(1, dtype=dtypes.int64), dtype=dtypes .int64) add_mask = 1 - math_ops.to_int64(previously_finished) lengths_to_add = array_ops.expand_dims(add_mask, 2) * lengths_to_add new_prediction_lengths = lengths_to_add + array_ops.expand_dims( prediction_lengths, 2) scores = _get_scores(log_probs=total_probs, sequence_lengths= new_prediction_lengths, length_penalty_weight=length_penalty_weight) time = ops.convert_to_tensor(time, name='time') scores_shape = array_ops.shape(scores) scores_flat = control_flow_ops.cond(time > 0, lambda : array_ops. reshape(scores, [batch_size, -1]), lambda : scores[:, (0)]) num_available_beam = control_flow_ops.cond(time > 0, lambda : math_ops. reduce_prod(scores_shape[1:]), lambda : math_ops.reduce_prod( scores_shape[2:])) next_beam_size = math_ops.minimum(ops.convert_to_tensor(beam_width, dtype=dtypes.int32, name='beam_width'), num_available_beam) next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size ) next_beam_scores.set_shape([static_batch_size, beam_width]) word_indices.set_shape([static_batch_size, beam_width]) next_beam_probs = _tensor_gather_helper(gather_indices=word_indices, gather_from=total_probs, batch_size=batch_size, range_size= beam_width * vocab_size, gather_shape=[-1], name='next_beam_probs') raw_next_word_ids = math_ops.mod(word_indices, vocab_size, name= 'next_beam_word_ids') next_word_ids = math_ops.to_int32(raw_next_word_ids) next_word_ids = next_word_ids % real_vocab_size next_beam_ids = math_ops.to_int32(word_indices / vocab_size, name= 'next_beam_parent_ids') previously_finished = _tensor_gather_helper(gather_indices= next_beam_ids, gather_from=previously_finished, batch_size= batch_size, range_size=beam_width, gather_shape=[-1]) next_finished = math_ops.logical_or(previously_finished, math_ops.equal (next_word_ids, end_token), name='next_beam_finished') lengths_to_add = math_ops.to_int64(math_ops.not_equal(next_word_ids, end_token)) lengths_to_add = (1 - math_ops.to_int64(next_finished)) * lengths_to_add next_prediction_len = _tensor_gather_helper(gather_indices= next_beam_ids, gather_from=beam_state.lengths, batch_size= batch_size, range_size=beam_width, gather_shape=[-1]) next_prediction_len += lengths_to_add next_cell_state = nest.map_structure(lambda gather_from: _maybe_tensor_gather_helper(gather_indices=next_beam_ids, gather_from=gather_from, batch_size=batch_size, range_size= beam_width, gather_shape=[batch_size * beam_width, -1]), next_cell_state) next_state = BeamSearchDecoderState(cell_state=next_cell_state, log_probs=next_beam_probs, lengths=next_prediction_len, finished= next_finished) output = BeamSearchDecoderOutput(scores=next_beam_scores, predicted_ids =next_word_ids, parent_ids=next_beam_ids) return output, next_state 
bert|fn|builder|input|master|pretraining|run def input_fn(params): """The actual input function.""" batch_size = params['batch_size'] name_to_features = {'input_ids': tf.FixedLenFeature([max_seq_length], tf.int64), 'input_mask': tf.FixedLenFeature([max_seq_length], tf. int64), 'segment_ids': tf.FixedLenFeature([max_seq_length], tf. int64), 'masked_lm_positions': tf.FixedLenFeature([ max_predictions_per_seq], tf.int64), 'masked_lm_ids': tf. FixedLenFeature([max_predictions_per_seq], tf.int64), 'masked_lm_weights': tf.FixedLenFeature([max_predictions_per_seq], tf.float32), 'next_sentence_labels': tf.FixedLenFeature([1], tf.int64)} if is_training: d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files)) d = d.repeat() d = d.shuffle(buffer_size=len(input_files)) cycle_length = min(num_cpu_threads, len(input_files)) d = d.apply(tf.contrib.data.parallel_interleave(tf.data. TFRecordDataset, sloppy=is_training, cycle_length=cycle_length)) d = d.shuffle(buffer_size=100) else: d = tf.data.TFRecordDataset(input_files) d = d.repeat() d = d.apply(tf.contrib.data.map_and_batch(lambda record: _decode_record (record, name_to_features), batch_size=batch_size, num_parallel_batches=num_cpu_threads, drop_remainder=True)) return d 
initialise|extensions|multi|MultiModalApplication|modal|application|uniform|sampler def initialise_uniform_sampler(self): self.sampler = [[UniformSampler(reader=reader, window_sizes=self. data_param, batch_size=self.net_param.batch_size, windows_per_image =self.action_param.sample_per_volume, queue_length=self.net_param. queue_length) for reader in self.readers]] 
forward|SepConv|models|layers|sto def forward(self, x): return self.op(x) 
get|weights|pruned|prune|resnet18|cared|PruneResNet18 def get_pruned_cared_weights(self, weights_dict): return collections.OrderedDict([(key, weight) for key, weight in weights_dict.items() if 'kernel' in key and 'shortcut' not in key]) 
EigenMultivariateNormal|distributions|regression|mean|controller @property def mean(self): """The mean of the MatrixVariateNormal distribution.""" return self._mean 
fpn|coord|model|bins|graph|delta|build def build_fpn_coord_bins_delta_graph(rois, feature_maps, image_shape, pool_size, num_classes, num_bins, net_name): """Builds the computation graph of the coordinate map head of Feature Pyramid Network.  rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: Coordinate maps [batch, roi_count, height, width, num_classes, num_bins] """ x = PyramidROIAlign([pool_size, pool_size], image_shape, name= 'roi_align_{}'.format(net_name))([rois] + feature_maps) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv1'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn1'.format( net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv2'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn2'.format( net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv3'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn3'.format( net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv4'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn4'.format( net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation='relu'), name='mrcnn_{}_deconv'.format(net_name))(x) x1 = KL.TimeDistributed(KL.Conv2D(num_bins * num_classes, (1, 1), strides=1), name='mrcnn_{}_conv_bins'.format(net_name))(x) x2 = KL.TimeDistributed(KL.Conv2D(num_bins * num_classes, (1, 1), strides=1), name='mrcnn_{}_conv_delta'.format(net_name))(x) x1 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, num_bins]), name= 'mrcnn_{}_bins_reshape'.format(net_name))(x1) x1 = KL.Activation('softmax', name='mrcnn_{}_bins'.format(net_name))(x1) x2 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, num_bins]), name= 'mrcnn_{}_delta_reshape'.format(net_name))(x2) x2 = KL.Activation('sigmoid', name='mrcnn_{}_delta_bins'.format(net_name))( x2) return x1, x2 
neural|tangents|stax|Dense @_layer def Dense(out_dim, W_std=1.0, b_std=0.0, W_init=_randn(1.0), b_init=_randn(1.0) ): """Layer constructor function for a dense (fully-connected) layer.  Based on `jax.experimental.stax.Dense`. Has a similar API. """ init_fn, _ = ostax.Dense(out_dim, W_init, b_init)  def apply_fn(params, inputs, **kwargs): W, b = params norm = W_std / np.sqrt(inputs.shape[-1]) return norm * np.dot(inputs, W) + b_std * b  def kernel_fn(kernels): """Compute the transformed kernels after a dense layer.""" var1, nngp, var2, ntk = (kernels.var1, kernels.nngp, kernels.var2, kernels.ntk)  def fc(x): return _affine(x, W_std, b_std) var1, nngp, var2, ntk = map(fc, (var1, nngp, var2, ntk)) if ntk is not None: ntk += nngp - b_std ** 2 return kernels._replace(var1=var1, nngp=nngp, var2=var2, ntk=ntk, is_gaussian=True) setattr(kernel_fn, _COVARIANCES_REQ, {'marginal': M.OVER_ALL, 'cross': M.OVER_ALL}) return init_fn, apply_fn, kernel_fn 
fisher|compute|ops|classification|new|factors|cov|FullyConnectedEigenBasisFactor def _compute_new_cov(self, idx=0): with _maybe_colocate_with(self._tensors[idx], self. _colocate_cov_ops_with_inputs): tensor = self._tensors[idx] if self._has_bias: tensor = _append_homog(tensor) return _compute_cov(tensor) 
ImageNetModel|init|setup|imagenet def __init__(self, sess, use_softmax=False, model_name='resnet_v2_50', create_prediction=True): global CREATED_GRAPH self.sess = sess self.use_softmax = use_softmax model_param = model_params[model_name] maybe_download_and_extract(model_param) if not CREATED_GRAPH: create_graph(model_param) CREATED_GRAPH = True self.num_channels = 3 self.output_name = model_param['prob' ] if self.use_softmax else model_param['logit'] self.input_name = model_param['input'] self.shape_name = model_param['shape'] self.model_name = model_param['name'] self.num_labels = (1000 if 'vgg' in self.model_name or 'densenet' in self.model_name or 'alexnet' in self.model_name else 1001) self.image_size = model_param['size'] self.use_softmax = use_softmax if create_prediction: self.model = ImageNetModelPrediction(sess, use_softmax, model_name) 
predict|xlnet|single|estimator|tpu|convert|step|master|ModelFnWrapper|to def convert_to_single_tpu_predict_step(self, dequeue_fn): """Converts user provided model_fn` as a single predict step on TPU.  Args: dequeue_fn: The function to retrieve inputs, features and labels, from TPU infeed dequeue channel.  Returns: A tuple of predict_fn, host_calls, and captured scaffold_fn. The predict_fn representing the predict step for TPU. """ host_calls = _OutfeedHostCall(self._ctx) captured_scaffold_fn = _CapturedObject() captured_predict_hooks = _CapturedObject()  def predict_step(unused_scalar_stopping_signal): """Evaluation step function for use inside a while loop.""" inputs = dequeue_fn() features, labels = inputs.features_and_labels() stopping_signals = inputs.signals() assert stopping_signals is not None, 'Internal Error: `signals` is missing.' tpu_estimator_spec = self._call_model_fn(features, labels, is_export_mode=False) if not isinstance(tpu_estimator_spec, model_fn_lib._TPUEstimatorSpec): raise RuntimeError( 'estimator_spec used by TPU prediction must have type`TPUEstimatorSpec`. Got {}' .format(type(tpu_estimator_spec))) self._verify_tpu_spec_predictions(tpu_estimator_spec.predictions) captured_scaffold_fn.capture(tpu_estimator_spec.scaffold_fn) captured_predict_hooks.capture(tpu_estimator_spec.prediction_hooks) to_record = {} identity_fn = lambda **kwargs: kwargs to_record['predictions'] = [identity_fn, tpu_estimator_spec.predictions ] to_record['signals'] = [identity_fn, stopping_signals] if tpu_estimator_spec.host_call is not None: to_record['host_call'] = tpu_estimator_spec.host_call host_calls.record(to_record) with ops.control_dependencies(host_calls.create_enqueue_op()): return _StopSignals.as_scalar_stopping_signal(stopping_signals) return (predict_step, host_calls, captured_scaffold_fn, captured_predict_hooks) 
plato|BasicController|basic|controller|run def run_controller(self, args): """ This function will create and run a controller. It iterates over the desired number of tests and prints some basic results.  :param args: the parsed configuration file :return: nothing """ cfg_parser = args['cfg_parser'] tests = args['tests'] num_dialogues = args['dialogues'] interaction_mode = args['interaction_mode'] num_agents = args['num_agents'] for test in range(tests): print('\n\n=======================================') print('# Running {0} dialogues (test {1} of {2}) #'.format( num_dialogues, test + 1, tests)) print('=======================================\n') try: if interaction_mode in ['simulation', 'text', 'speech']: statistics = self.run_single_agent(cfg_parser, num_dialogues) elif interaction_mode == 'multi_agent': statistics = self.run_multi_agent(cfg_parser, num_dialogues, num_agents) else: ValueError('Unknown interaction mode: {0}'.format( interaction_mode)) return -1 except (ValueError, FileNotFoundError, TypeError, AttributeError ) as err: print('\nPlato error! {0}\n'.format(err)) return -1 print(f'Results:\n{statistics}') return 0 
tangents|variance|get|stax|neural def _get_variance(x, marginal_type): if marginal_type in (M.OVER_ALL, M.OVER_PIXELS): ret = np.sum(x ** 2, axis=-1, keepdims=False) elif marginal_type == M.OVER_POINTS: ret = _point_marg(x) elif marginal_type == M.NO: ret = np.squeeze(np.dot(x, x[..., None]), -1) ret = np.transpose(ret, (0, 3, 1, 4, 2, 5)) else: raise NotImplementedError( 'Only implemented for `OVER_ALL`, `OVER_PIXELS`, `OVER_POINTS` and `NO`; supplied {}' .format(marginal_type)) return ret / x.shape[-1] 
get|activation|deepctr|config|layers|Dice def get_config(self): config = {'axis': self.axis, 'epsilon': self.epsilon} base_config = super(Dice, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
KittiDataset|avod|kitti|names|datasets|load|sample|dataset def load_sample_names(self, data_split): """Load the sample names listed in this dataset's set file (e.g. train.txt, validation.txt)  Args: data_split: override the sample list to load (e.g. for clustering)  Returns: A list of sample names (file names) read from the .txt file corresponding to the data split """ set_file = self.dataset_dir + '/' + data_split + '.txt' with open(set_file, 'r') as f: sample_names = f.read().splitlines() return np.array(sample_names) 
texar|discard|test|MonoTextDataTest|length|data|mono|text def test_length_discard(self): """Tests discard lenghy seq. """ hparams = copy.copy(self._hparams) hparams['dataset'].update({'max_seq_length': 4, 'length_filter_mode': 'discard'}) self._run_and_test(hparams) 
eval|save|data|file def save_eval_file(opt, stats, eval_type='losses', split='dev', ext='pickle'): if cfg.test_save: name = '{}/{}.{}'.format(utils.make_name(opt, prefix='garbage/{}/'. format(eval_type), is_dir=True, eval_=True), split, ext) else: name = '{}/{}.{}'.format(utils.make_name(opt, prefix='results/{}/'. format(eval_type), is_dir=True, eval_=True), split, ext) print('Saving {} {} to {}'.format(split, eval_type, name)) if ext == 'pickle': with open(name, 'wb') as f: pickle.dump(stats, f) elif ext == 'txt': with open(name, 'w') as f: f.write(stats) elif ext == 'json': with open(name, 'w') as f: json.dump(stats, f) else: raise 
format|axes|common def format_axes(ax, twinx=False): SPINE_COLOR = 'gray' if twinx: visible_spines = ['bottom', 'right'] invisible_spines = ['top', 'left'] else: visible_spines = ['bottom', 'left'] invisible_spines = ['top', 'right'] for spine in invisible_spines: ax.spines[spine].set_visible(False) for spine in visible_spines: ax.spines[spine].set_color(SPINE_COLOR) ax.spines[spine].set_linewidth(0.5) ax.xaxis.set_ticks_position('bottom') if twinx: ax.yaxis.set_ticks_position('right') else: ax.yaxis.set_ticks_position('left') for axis in [ax.xaxis, ax.yaxis]: axis.set_tick_params(direction='out', color=SPINE_COLOR) return ax 
enqueue|host|xlnet|ops|fn|per|generate|estimator|tpu|master|for|v2 def generate_per_host_v2_enqueue_ops_fn_for_host(ctx, input_fn, inputs_structure_recorder, device, host_id): """Generates infeed enqueue ops for per-host input_fn on a single host.""" captured_infeed_queue = _CapturedObject() dataset_initializer = None with ops.device(device): user_context = tpu_context.TPUContext(internal_ctx=ctx, input_device=device, invocation_index=host_id) inputs = _Inputs.from_input_fn(input_fn(user_context)) is_dataset = inputs.is_dataset if not is_dataset: raise TypeError( '`input_fn` must return a `Dataset` for the PER_HOST_V2 input pipeline configuration.' ) if ctx.mode == model_fn_lib.ModeKeys.PREDICT: inputs = _InputsWithStoppingSignals(dataset=inputs.dataset, batch_size=ctx.batch_size_for_input_fn, add_padding=True, num_invocations_per_step=ctx.num_of_replicas_per_host) dataset_initializer = inputs.dataset_initializer() tpu_ordinal_function_impl = ctx.tpu_ordinal_function(host_id)  def enqueue_ops_fn(): """Generates the per_host enqueue ops.""" control_deps = [] per_host_sharded_inputs = [] num_replicas_per_host = ctx.num_of_replicas_per_host cached_signals = None with ops.device(device): if not inputs.is_dataset: raise TypeError( '`input_fn` must return a `Dataset` for this mode.') for _ in range(num_replicas_per_host): with ops.control_dependencies(control_deps): features, labels = inputs.features_and_labels() signals = inputs.signals() if cached_signals: signals['stopping'] = cached_signals['stopping'] else: cached_signals = signals inputs_structure_recorder.validate_and_record_structure( features, labels) flattened_inputs = (inputs_structure_recorder. flatten_features_and_labels(features, labels, signals)) control_deps.extend(flattened_inputs) per_host_sharded_inputs.append(flattened_inputs) if inputs_structure_recorder.flattened_input_dims: input_partition_dims = (inputs_structure_recorder. flattened_input_dims) if signals: input_partition_dims += [None] * len(signals) infeed_queue = tpu_feed._PartitionedInfeedQueue( number_of_tuple_elements=len(per_host_sharded_inputs[0] ), host_id=host_id, input_partition_dims= input_partition_dims, device_assignment=ctx. device_assignment) per_host_enqueue_ops = infeed_queue.generate_enqueue_ops( per_host_sharded_inputs) else: infeed_queue = tpu_feed.InfeedQueue(number_of_tuple_elements =len(per_host_sharded_inputs[0])) per_host_enqueue_ops = infeed_queue.generate_enqueue_ops( per_host_sharded_inputs, tpu_ordinal_function= tpu_ordinal_function_impl) captured_infeed_queue.capture(infeed_queue) if signals is None: return per_host_enqueue_ops else: return {'ops': per_host_enqueue_ops, 'signals': signals} return enqueue_ops_fn, captured_infeed_queue, dataset_initializer 
EnvTest|env|testReset|test def testReset(self): scan_id = 0 verify_env_output(self, self._env.reset(), expected_reward=0, expected_done=False, expected_info='', expected_time_step=0, expected_path_id=1304, expected_pano_name= '80929af5cf234ae38ac3a2a4e60e4342', expected_heading=6.101, expected_pitch=0.0, expected_scan_id=scan_id, expected_oracle_action=self._get_pano_id( 'ba27da20782d4e1a825f0a133ad84da9', scan_id)) 
utils|repr|DD def __repr__(self): return str(self) 
cleverhans|cw|attacks|wrap|generate|CarliniWagnerL def cw_wrap(x_val, y_val): return np.array(attack.attack(x_val, y_val), dtype=np.float32) 
IsInitializerList|cpplint def IsInitializerList(clean_lines, linenum): """Check if current line is inside constructor initializer list.  Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. Returns: True if current line appears to be inside constructor initializer list, False otherwise. """ for i in xrange(linenum, 1, -1): line = clean_lines.elided[i] if i == linenum: remove_function_body = Match('^(.*)\\{\\s*$', line) if remove_function_body: line = remove_function_body.group(1) if Search('\\s:\\s*\\w+[({]', line): return True if Search('\\}\\s*,\\s*$', line): return True if Search('[{};]\\s*$', line): return False return False 
spec|texar|get|utils|ith|DataSpec|data|dataset def get_ith_data_spec(self, i): """Returns an instance of :class:`_DataSpec` that contains the `i`-th specifications. """ kwargs = {} for k, v in six.iteritems(self.__dict__): kwargs[k] = v[i] if isinstance(v, (tuple, list)) else v return _DataSpec(**kwargs) 
eval|compute|train|summaries def compute_summaries(metrics, environment, policy, num_episodes=1, num_episodes_to_render=1, images_ph=None, images_summary=None, render_images_summary=None): for metric in metrics: metric.reset() if num_episodes_to_render: environment.start_rendering() time_step = environment.reset() policy_state = policy.get_initial_state(environment.batch_size) render_images = [] if num_episodes_to_render and 'pixels' in time_step.observation: images = [[time_step.observation['pixels']]] else: images = [] step = 0 episode = 0 while episode < num_episodes: action_step = policy.action(time_step, policy_state) next_time_step = environment.step(action_step.action) traj = trajectory.from_transition(time_step, action_step, next_time_step) for observer in metrics: observer(traj) if episode < num_episodes_to_render: if traj.is_last(): render_images.append(list(environment.frames)) environment.frames[:] = [] if episode + 1 >= num_episodes_to_render: environment.stop_rendering() if 'pixels' in time_step.observation: if traj.is_boundary(): images.append([]) images[-1].append(next_time_step.observation['pixels']) episode += np.sum(traj.is_last()) step += np.sum(~traj.is_boundary()) time_step = next_time_step policy_state = action_step.state py_metric.run_summaries(metrics) if render_images: render_images = pad_and_concatenate_videos(render_images) session = tf.compat.v1.get_default_session() session.run(render_images_summary, feed_dict={images_ph: [ render_images]}) if images: images = pad_and_concatenate_videos(images) session = tf.compat.v1.get_default_session() session.run(images_summary, feed_dict={images_ph: [images]}) 
list|get|bounds|ours|weights def get_weights_list(model): weights = [] bias = [] U = model.U for i, Ui in enumerate(U): [weight_Ui, bias_Ui] = Ui.get_weights() print('Hidden layer {} weight shape: {}'.format(i, weight_Ui.shape)) weights.append(np.ascontiguousarray(np.transpose(weight_Ui))) bias.append(np.ascontiguousarray(np.transpose(bias_Ui))) print('Hidden layer {} bias shape: {}'.format(i, bias_Ui.shape)) [W, bias_W] = model.W.get_weights() weights.append(np.ascontiguousarray(np.transpose(W))) bias.append(np.ascontiguousarray(np.transpose(bias_W))) print('Last layer weight shape: {}'.format(W.shape)) print('Last layer bias shape: {}'.format(bias_W.shape)) return weights, bias 
privacy|accountants|eps|delta|mu def delta_eps_mu(eps, mu): return norm.cdf(-eps / mu + mu / 2) - np.exp(eps) * norm.cdf(-eps / mu - mu / 2) 
log|distributions|regression|DMatrixVariateNormal|determinant|u|controller|c @property def u_c_log_determinant(self): """ The log determinant of the cholesky decomposition matrix of the row variance matrix. """ return self._u_c_log_determinant 
init|dataset|train|Dataset def __init__(self, directory, dataset_mode, load_size): self.directory = directory if not os.path.isdir('dataset'): raise Exception( "Could not find the dataset directory 'dataset'. Download the dataset first." ) self.full_path = os.path.join('dataset', '2afc', directory) self.dataset_mode = dataset_mode self.load_size = load_size self.cache = {'judge': [], 'p0': [], 'p1': [], 'ref': [], 'judge_path': [], 'p0_path': [], 'p1_path': [], 'ref_path': []} darc_path = self.getDarcPath() if not os.path.exists(darc_path): self.createDarc() self.darc = darc.DataArchive(darc_path) 
LayerParametersDict|ops|classification|getitem|utils def __getitem__(self, key): key = self._canonicalize_key(key) return super(LayerParametersDict, self).__getitem__(key) 
linear|Layer|init|layers|gae def __init__(self, **kwargs): allowed_kwargs = {'name', 'logging'} for kwarg in kwargs.keys(): assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg name = kwargs.get('name') if not name: layer = self.__class__.__name__.lower() name = layer + '_' + str(get_layer_uid(layer)) self.name = name self.vars = {} logging = kwargs.get('logging', False) self.logging = logging self.issparse = False 
experiment|setup|test|carlini def setup_experiment(): np.random.seed(FLAGS.seed) if not tf.gfile.Exists(FLAGS.data_dir) or not tf.gfile.IsDirectory(FLAGS .data_dir): raise ValueError('Could not find folder %s' % FLAGS.data_dir) assert FLAGS.batch_size % FLAGS.carlini_batch_size == 0 if not tf.gfile.Exists(FLAGS.load_dir) or not tf.gfile.IsDirectory(FLAGS .load_dir): raise ValueError('Could not find folder %s' % FLAGS.load_dir) FLAGS.working_dir = os.path.join(FLAGS.working_dir, os.path.basename(os .path.normpath(FLAGS.load_dir))) FLAGS.adv_data_dir = os.path.join(FLAGS.working_dir, FLAGS.adv_data_dir) FLAGS.samples_dir = os.path.join(FLAGS.working_dir, FLAGS.samples_dir) FLAGS.git_revision = get_sha() if tf.gfile.Exists(FLAGS.working_dir): tf.gfile.DeleteRecursively(FLAGS.working_dir) tf.gfile.MakeDirs(FLAGS.working_dir) tf.gfile.MakeDirs(FLAGS.adv_data_dir) tf.gfile.MakeDirs(FLAGS.samples_dir) train_params = load_training_params(FLAGS.load_dir) FLAGS.model = train_params['model'] FLAGS.model_name = train_params['model_name'] FLAGS.activation_fn = train_params['activation_fn'] FLAGS.num_classes = train_params['num_classes'] FLAGS.layer_dims = train_params['layer_dims'] logger = getLogger('tensorflow') tf.logging.set_verbosity(tf.logging.INFO) file_hndl = FileHandler(os.path.join(FLAGS.working_dir, 'tensorflow.log')) file_hndl.setLevel(logging.DEBUG) logger.addHandler(file_hndl) logging.info({k: v.value for k, v in FLAGS._flags().items()}) logging.info('Training params: %s', train_params) 
BiLSTM|compute|output|sequence|deepctr|layers|shape def compute_output_shape(self, input_shape): print(self.merge_mode) if self.merge_mode is None: return [input_shape, input_shape] elif self.merge_mode == 'concat': return input_shape[:-1] + (input_shape[-1] * 2,) else: return input_shape 
PartialRollout|extend|a3c def extend(self, other): assert not self.terminal self.states.extend(other.states) self.actions.extend(other.actions) self.rewards.extend(other.rewards) self.values.extend(other.values) self.r = other.r self.task.extend(other.task) self.terminal = other.terminal 
extents|avod|test|project|bev|projector|anchor|AnchorProjectorTest|core|to def test_project_to_bev_extents(self): anchors = np.asarray([[0, 0, 3, 2, 0, 6], [3, 0, 3, 2, 0, 2]], dtype=np .float64) bev_extents = [[-5, 5], [0, 10]] bev_extents_range = np.diff(bev_extents, axis=1) bev_extents_range = np.stack([bev_extents_range, bev_extents_range] ).flatten() expected_boxes = np.asarray([[0 - -5 - 1, 4, 0 - -5 + 1, 10], [3 - -5 - 1, 6, 3 - -5 + 1, 8]], dtype=np.float64) expected_boxes_norm = expected_boxes / bev_extents_range boxes, boxes_norm = anchor_projector.project_to_bev(anchors, bev_extents) for box, box_norm, exp_box, exp_box_norm in zip(boxes, boxes_norm, expected_boxes, expected_boxes_norm): np.testing.assert_allclose(box, exp_box, rtol=1e-05) np.testing.assert_allclose(box_norm, exp_box_norm, rtol=1e-05) 
should|generate|attention|summaries|layers|thumt def should_generate_summaries(): """Is this an appropriate context to generate summaries. :returns: a boolean """ if 'while/' in tf.contrib.framework.get_name_scope(): return False if tf.get_variable_scope().reuse: return False return True 
lmu|init|LMUCell def __init__(self, units, order, theta, method='zoh', realizer=Identity(), factory=LegendreDelay, trainable_input_encoders=True, trainable_hidden_encoders=True, trainable_memory_encoders=True, trainable_input_kernel=True, trainable_hidden_kernel=True, trainable_memory_kernel=True, trainable_A=False, trainable_B=False, input_encoders_initializer='lecun_uniform', hidden_encoders_initializer ='lecun_uniform', memory_encoders_initializer=Constant(0), input_kernel_initializer='glorot_normal', hidden_kernel_initializer= 'glorot_normal', memory_kernel_initializer='glorot_normal', hidden_activation='tanh', **kwargs): super().__init__(**kwargs) self.units = units self.order = order self.theta = theta self.method = method self.realizer = realizer self.factory = factory self.trainable_input_encoders = trainable_input_encoders self.trainable_hidden_encoders = trainable_hidden_encoders self.trainable_memory_encoders = trainable_memory_encoders self.trainable_input_kernel = trainable_input_kernel self.trainable_hidden_kernel = trainable_hidden_kernel self.trainable_memory_kernel = trainable_memory_kernel self.trainable_A = trainable_A self.trainable_B = trainable_B self.input_encoders_initializer = initializers.get( input_encoders_initializer) self.hidden_encoders_initializer = initializers.get( hidden_encoders_initializer) self.memory_encoders_initializer = initializers.get( memory_encoders_initializer) self.input_kernel_initializer = initializers.get(input_kernel_initializer) self.hidden_kernel_initializer = initializers.get(hidden_kernel_initializer ) self.memory_kernel_initializer = initializers.get(memory_kernel_initializer ) self.hidden_activation = activations.get(hidden_activation) self._realizer_result = realizer(factory(theta=theta, order=self.order)) self._ss = cont2discrete(self._realizer_result.realization, dt=1.0, method=method) self._A = self._ss.A - np.eye(order) self._B = self._ss.B self._C = self._ss.C assert np.allclose(self._ss.D, 0) self.state_size = self.units, self.order self.output_size = self.units 
models|LeNet|lenet|save def save(self, sess, verbose=True): """Save the model @param sess (tf.Session) the current session @param verbose (bool) if True print information on terminal """ if not os.path.exists(self.dir_header + '/model/'): os.makedirs(self.dir_header + '/model/') model_id = strftime('%H%M%S_%d%m%Y', gmtime()) model_folder = self.dir_header + '/model/' + model_id + '_' + str(self. train_iteration) + '/model.ckpt' if verbose: print('Saving network in: ' + str(model_folder)) save_path = self.tf_saver.save(sess, model_folder) 
neural|tangents|randn|stax def _randn(stddev=0.01): """`jax.experimental.stax.randn` for implicitly-typed results."""  def init(rng, shape): return stddev * random.normal(rng, shape) return init 
switch|shuffle|exchange|linear2|gated|network|layer def gated_linear2(input, suffix, bias_start_reset, in_units, out_units): input = tf.reshape(input, [batch_size, length // 2, in_units * 2]) reset1 = gate_fn(conv_linear(input, kernel_width, in_units * 2, in_units * 2, bias_start_reset, prefix + '/reset1/' + suffix)) reset2 = gate_fn(conv_linear(input, kernel_width, in_units * 2, in_units * 2, bias_start_reset, prefix + '/reset2/' + suffix)) res1 = conv_linear(input * reset1, kernel_width, in_units * 2, out_units, 0.0, prefix + '/cand1/' + suffix) res2 = conv_linear(input * reset2, kernel_width, in_units * 2, out_units, 0.0, prefix + '/cand2/' + suffix) res = tf.concat([res1, res2], axis=2) res = tf.reshape(res, [batch_size, length, out_units]) return activation_fn(res), tf.reshape(reset1, [batch_size, length, in_units]) 
problem|create|summary|ndh|NDHProblem def create_summary(self, step, info): sum_episode_reward = 0.0 sum_episode_num_steps = 0.0 num_infos = 0 for infos in [pickle.loads(t.numpy()) for t in info]: for episode_undisc_reward, episode_num_steps in infos: sum_episode_reward += episode_undisc_reward sum_episode_num_steps += episode_num_steps num_infos += 1 if num_infos: tf.summary.scalar('train_debug/episode_undiscounted_reward', sum_episode_reward / num_infos, step=step) tf.summary.scalar('train_debug/episode_num_steps', sum_episode_num_steps / num_infos, step=step) 
vocabulary|load|lambada|embedding def load_embedding_vocabulary() ->dict: if not tf.gfile.Exists(cnf.emb_word_dictionary): utils.prepare_embeddings() with open(cnf.emb_word_dictionary, 'rb') as dict_file: return pickle.load(dict_file) 
init|utils|LoopLogger def __init__(self, max_value=None, step_size=1, n_steps=25, print_time=True): self.max_value = max_value if n_steps is not None: self.step_size = max(1, max_value // n_steps) else: self.step_size = step_size self.print_time = print_time self.n = 0 self.start_time = time.time() 
DTAN|plot|RDTAN|model|layer|outputs def plot_RDTAN_outputs(self, model, X, y, ratio=[8, 6], name='movie.gif'): """  :param model: trained DTAN Keras model :param X: :param p_samples: Bool. Plot samples :param p_mean: bool. plot mean :return: """ plot_all_layers(model, X, y, self.n_recurrences, ratio, name) nb_points = 1000 
Config|log|config def log(self, output_path): with open(output_path, 'w') as f: for a in dir(self): if not a.startswith('__') and not callable(getattr(self, a)): f.write('{:30} {}\n'.format(a, getattr(self, a))) 
mnist|setup|TwoLayerMNISTModel|predict def predict(self, data): return self.model(data) 
get|list|input|cvusa|train|data|InputData def get_train_list(self): return self.id_list 
query|UncertaintyEntropySampling|methods def query(self, X_train, Y_train, labeled_idx, amount): unlabeled_idx = get_unlabeled_idx(X_train, labeled_idx) predictions = self.model.predict(X_train[(unlabeled_idx), :]) unlabeled_predictions = np.sum(predictions * np.log(predictions + 1e-10 ), axis=1) selected_indices = np.argpartition(unlabeled_predictions, amount)[:amount] return np.hstack((labeled_idx, unlabeled_idx[selected_indices])) 
ExtractPatch|measure|commons def measure(self, hparams, x, theta_ph): k = hparams.patch_size patch_list = [] for t in range(hparams.batch_size): i, j = theta_ph[t, 0], theta_ph[t, 1] patch = x[(t), i:i + k, j:j + k, :] patch = tf.reshape(patch, [1, k, k, hparams.image_dims[-1]]) patch_list.append(patch) patches = tf.concat(patch_list, axis=0) paddings = measure_utils.get_padding_ep(hparams) x_measured = tf.pad(patches, paddings, 'CONSTANT', name='x_measured') return x_measured 
init|MaxPoolingAggregator|aggregators|graphsage def __init__(self, input_dim, output_dim, model_size='small', neigh_input_dim=None, dropout=0.0, bias=False, act=tf.nn.relu, name= None, concat=False, **kwargs): super(MaxPoolingAggregator, self).__init__(**kwargs) self.dropout = dropout self.bias = bias self.act = act self.concat = concat if neigh_input_dim is None: neigh_input_dim = input_dim if name is not None: name = '/' + name else: name = '' if model_size == 'small': hidden_dim = self.hidden_dim = 512 elif model_size == 'big': hidden_dim = self.hidden_dim = 1024 self.mlp_layers = [] self.mlp_layers.append(Dense(input_dim=neigh_input_dim, output_dim= hidden_dim, act=tf.nn.relu, dropout=dropout, sparse_inputs=False, logging=self.logging)) with tf.variable_scope(self.name + name + '_vars'): self.vars['neigh_weights'] = glorot([hidden_dim, output_dim], name= 'neigh_weights') self.vars['self_weights'] = glorot([input_dim, output_dim], name= 'self_weights') if self.bias: self.vars['bias'] = zeros([self.output_dim], name='bias') if self.logging: self._log_vars() self.input_dim = input_dim self.output_dim = output_dim self.neigh_input_dim = neigh_input_dim 
enqueue|initialize|get|level|ops|fn|nmt|runner|low|TrainLowLevelRunner def get_enqueue_ops_fn(host_id): """Generate the enqueue ops graph function.""" params['dataset_num_shards'] = num_hosts params['dataset_index'] = host_id output = input_fn(params) device = device_for_host(get_host(self.resolver, host_id)) with tf.device(device): if self.per_host_v1: iterator = input_fn._iterator else: iterator = output.make_initializable_iterator() self.dataset_initializer.append(iterator.initializer)  def enqueue_ops_fn_v1(): """Enqueue ops function for one host..""" features = output self.feature_structure['features'] = features self.feature_structure['labels'] = {} flattened_inputs = data_nest.flatten(self.feature_structure) infeed = tpu.InfeedQueue(tuple_types=[t.dtype for t in flattened_inputs], tuple_shapes=[t.shape for t in flattened_inputs], shard_dimensions=None) infeed.set_number_of_shards(self.hparams.num_shards_per_host) self.infeed_queue.append(infeed)  def tpu_ordinal_fn(shard_index_in_host): return shard_index_in_host % self.hparams.num_shards_per_host per_host_enqueue_ops = (infeed. split_inputs_and_generate_enqueue_ops(flattened_inputs, placement_function=lambda x: device, tpu_ordinal_function= tpu_ordinal_fn)) return per_host_enqueue_ops  def enqueue_ops_fn_v2(): """Enqueue ops function for one host.""" per_host_sharded_inputs = [] control_deps = [] for _ in range(self.hparams.num_shards_per_host): with tf.control_dependencies(control_deps): features = iterator.get_next() self.feature_structure['features'] = features self.feature_structure['labels'] = {} flattened_inputs = data_nest.flatten(self.feature_structure) control_deps.extend(flattened_inputs) per_host_sharded_inputs.append(flattened_inputs) infeed = tpu.InfeedQueue(number_of_tuple_elements=len( per_host_sharded_inputs[0])) self.infeed_queue.append(infeed)  def tpu_ordinal_fn(shard_index_in_host): return shard_index_in_host % self.hparams.num_shards_per_host return infeed.generate_enqueue_ops(per_host_sharded_inputs, tpu_ordinal_function=tpu_ordinal_fn) if self.per_host_v1: return enqueue_ops_fn_v1 else: return enqueue_ops_fn_v2 
NPRFKNRM|build|knrm|nprf def build(self): dd_input = Input((self.config.nb_supervised_doc, self.config. kernel_size), name='dd_input') z = Dense(self.config.hidden_size, activation='tanh', name='dd_hidden')( dd_input) dd_init_out = Dense(self.config.out_size, name='dd_init_out')(z) dd_gate = Input((self.config.nb_supervised_doc, 1), name= 'baseline_doc_score') dd_w = Dense(1, kernel_initializer=self.initializer_gate, use_bias= False, name='dd_gate')(dd_gate) dd_w = Reshape((self.config.nb_supervised_doc,))(dd_w) dd_init_out = Reshape((self.config.nb_supervised_doc,))(dd_init_out) if self.config.method in [1, 3]: z = dd_init_out elif self.config.method == 2: logging.info('Apply doc gating') z = Multiply(name='dd_out')([dd_init_out, dd_w]) else: raise ValueError('Method not initialized, please check config file') if self.config.method in [1, 2]: logging.info('Dense layer on top') z = Dense(self.config.merge_hidden, activation='tanh', name= 'merge_hidden')(z) out = Dense(self.config.merge_out, name='score')(z) else: logging.info('Apply doc gating, No dense layer on top, sum up scores') out = Dot(axes=[1, 1], name='score')([z, dd_w]) model = Model(inputs=[dd_input, dd_gate], outputs=[out]) print(model.summary()) return model 
end|plato|ConversationalSingleAgent|agent|conversational|dialogue|single def end_dialogue(self): """ Perform final dialogue turn. Train and save models if applicable.  :return: nothing """ self.recorder.record(self.curr_state, self.curr_state, self.prev_action, self.prev_reward, self.prev_success, input_utterance=self. prev_usr_utterance, output_utterance=self.prev_sys_utterance, task_success=self.prev_task_success, force_terminate=True) self.dialogue_episode += 1 if self.IS_TRAINING: if self.dialogue_episode % self.train_interval == 0 and len(self. recorder.dialogues) >= self.minibatch_length: for epoch in range(self.train_epochs): print('Training epoch {0} of {1}'.format(epoch + 1, self. train_epochs)) minibatch = random.sample(self.recorder.dialogues, self. minibatch_length) if self.nlu and self.nlu.training: self.nlu.train(minibatch) if self.dialogue_manager.is_training(): self.dialogue_manager.train(minibatch) if self.nlg and self.nlg.training: self.nlg.train(minibatch) self.cumulative_rewards += self.recorder.dialogues[-1][-1][ 'cumulative_reward'] print('CUMULATIVE REWARD: {0}'.format(self.recorder.dialogues[-1][-1][ 'cumulative_reward'])) if self.dialogue_turn > 0: self.total_dialogue_turns += self.dialogue_turn if self.dialogue_episode % self.SAVE_INTERVAL == 0: if self.nlu: self.nlu.save() if self.dialogue_manager: self.dialogue_manager.save() if self.nlg: self.nlg.save() if self.recorder.dialogues[-1][-1]['success']: print('SUCCESS (Subjective)!') self.num_successful_dialogues += int(self.recorder.dialogues[-1][-1 ]['success']) else: print('FAILURE (Subjective).') if self.recorder.dialogues[-1][-1]['task_success']: self.num_task_success += int(self.recorder.dialogues[-1][-1][ 'task_success']) print('OBJECTIVE TASK SUCCESS: {0}'.format(self.recorder.dialogues[-1][ -1]['task_success'])) 
attacks|params|parse|fda|FDA def parse_params(self, eps=0.3, eps_iter=0.01, nb_iter=40, y=None, ord=np. inf, clip_min=None, clip_max=None, y_target=None, rand_init=True, **kwargs ): """ Take in a dictionary of parameters and applies attack-specific checks before saving them as attributes.  Attack-specific parameters:  :param eps: (required float) maximum distortion of adversarial example compared to original input :param eps_iter: (required float) step size for each attack iteration :param nb_iter: (required int) Number of attack iterations. :param y: (optional) A tensor with the model labels. :param ord: (optional) Order of the norm (mimics Numpy). Possible values: np.inf, 1 or 2. :param y_target: (optional) A tensor with the labels to target. Leave y_target=None if y is also set. Labels should be one-hot-encoded. :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value :param rand_init: (optional bool) If True, an initial random perturbation is added. """ self.eps = eps self.eps_iter = eps_iter self.nb_iter = nb_iter self.y = y self.ord = ord self.y_target = y_target self.clip_min = clip_min self.clip_max = clip_max self.rand_init = rand_init if self.y is not None and self.y_target is not None: raise ValueError('Must not set both y and y_target') return True 
GaussianDiagDistr|init|dc|svae|utils|prob def __init__(self, mu, logvar, logvar_limit): super(GaussianDiagDistr, self).__init__() self.mu = mu nl = torch.nn.Hardtanh(-logvar_limit, logvar_limit) self.logvar = nl(logvar) GaussianDiagDistr.check_param_tensors(mu, logvar) 
assert|tica|all|finite def _assert_all_finite(X): """Like assert_all_finite, but only for ndarray.""" X = np.asanyarray(X) if X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum() ) and not np.isfinite(X).all(): raise ValueError( 'Input contains NaN, infinity or a value too large for %r.' % X .dtype) 
raml|stdout|print|and|file|main def print_stdout_and_file(content, file): print(content) print(content, file=file) 
hbaselines|envs|end|project|state|hac|goal|init|UR5|to def project_state_to_end_goal(sim, *_): return np.array([bound_angle(sim.data.qpos[i]) for i in range(len(sim. data.qpos))]) 
pool|avg|make|cnn|3x3|helpers def make_avg_pool_3x3(op_name, in_tensor, padding='VALID'): with tf.name_scope(op_name): return tf.nn.pool(input=in_tensor, window_shape=[3, 3], strides=[3, 3], pooling_type='AVG', padding=padding, name=op_name) 
test|tmp|seed|master|facenet|train|run def run_train(): with tf.Graph().as_default(): tf.set_random_seed(666) images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS. batch_size, FLAGS.image_size, FLAGS.image_size, 3), name='input') embeddings = inference_conv_test(images_placeholder) anchor, positive, negative = tf.split(0, 3, embeddings) pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1) neg_dist = tf.reduce_sum(tf.square(tf.sub(anchor, negative)), 1) basic_loss = tf.add(tf.sub(pos_dist, neg_dist), FLAGS.alpha) loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0) opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate) grads = opt.compute_gradients(loss) train_op = opt.apply_gradients(grads) init = tf.global_variables_initializer() sess = tf.Session() sess.run(init) np.random.seed(666) with sess.as_default(): grads_eval = [] all_vars = [] for step in xrange(1): batch = np.random.random((FLAGS.batch_size, FLAGS. image_size, FLAGS.image_size, 3)) feed_dict = {images_placeholder: batch} var_names = tf.global_variables() all_vars += sess.run(var_names, feed_dict=feed_dict) grad_tensors, grad_vars = zip(*grads) grads_eval += sess.run(grad_tensors, feed_dict=feed_dict) sess.run(train_op, feed_dict=feed_dict) sess.close() return var_names, all_vars, grad_vars, grads_eval 
rnn|texar|test|decoder|modules|decoders|HelpersTest|embedding|softmax|helpers def test_softmax_embedding_helpers(self): """Tests softmax helpers. """  def _test_fn(helper): _, next_inputs, _ = helper.next_inputs(time=1, outputs=tf.ones([ self._batch_size, self._vocab_size]), state=None, sample_ids=tf .ones([self._batch_size, self._vocab_size])) self.assertEqual(helper.sample_ids_shape, tf.TensorShape(self. _vocab_size)) self.assertEqual(next_inputs.get_shape(), tf.TensorShape([self. _batch_size, self._emb_dim])) output_layer = tf.layers.Dense(self._vocab_size) decoder = BasicRNNDecoder(vocab_size=self._vocab_size, output_layer =output_layer) outputs, final_state, sequence_lengths = decoder(helper=helper, max_decoding_length=self._max_seq_length) cell_dim = decoder.hparams.rnn_cell.kwargs.num_units with self.test_session() as sess: sess.run(tf.global_variables_initializer()) outputs_, final_state_, sequence_lengths_ = sess.run([outputs, final_state, sequence_lengths]) max_length = max(sequence_lengths_) self.assertEqual(outputs_.logits.shape, (self._batch_size, max_length, self._vocab_size)) self.assertEqual(outputs_.sample_id.shape, (self._batch_size, max_length, self._vocab_size)) self.assertEqual(final_state_[0].shape, (self._batch_size, cell_dim)) helper = SoftmaxEmbeddingHelper(self._embedding, self._start_tokens, self._end_token, 0.7) _test_fn(helper) embedder = WordEmbedder(self._embedding) helper = SoftmaxEmbeddingHelper(embedder, self._start_tokens, self. _end_token, 0.7, embedding_size=self._vocab_size) _test_fn(helper) word_embedder = WordEmbedder(self._embedding) pos_embedder = PositionEmbedder(position_size=self._max_seq_length)  def _emb_fn(soft_ids, times): return word_embedder(soft_ids=soft_ids) + pos_embedder(times) helper = SoftmaxEmbeddingHelper(_emb_fn, self._start_tokens, self. _end_token, 0.7, embedding_size=self._vocab_size) _test_fn(helper) helper = GumbelSoftmaxEmbeddingHelper(self._embedding, self. _start_tokens, self._end_token, 0.7) _test_fn(helper) 
get|fn|network|factory|nets @functools.wraps(func) def network_fn(images, **kwargs): arg_scope = arg_scopes_map[name](weight_decay=weight_decay) with slim.arg_scope(arg_scope): return func(images, num_classes, is_training=is_training, **kwargs) 
parseGames|analysis|resign def parseGames(filenames, resignthr, verbose, prefixes): gsd = {} for filename in filenames: training_filename = filename.replace('.debug', '') with open(filename) as fh, open(training_filename) as tfh: version = fh.readline().rstrip() assert version == '2' cfg_resignpct, network = fh.readline().split() if prefixes: net_name = os.path.basename(network) matches = filter(lambda n: net_name.startswith(n), prefixes) if not list(matches): continue cfg_resignpct = int(cfg_resignpct) if cfg_resignpct == 0: gsd[filename] = parseGameBody(filename, fh, tfh, verbose, resignthr) elif verbose >= 2: print('{} was played with -r {}, skipping'.format(filename, cfg_resignpct)) return gsd 
BertModelTest|bert|test|create|BertModelTester|model|modeling def create_bert_model(self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels): model = BertModel(config=config) model.eval() all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask) outputs = {'sequence_output': all_encoder_layers[-1], 'pooled_output': pooled_output, 'all_encoder_layers': all_encoder_layers} return outputs 
models|decoder|transformer|thumt def transformer_decoder(inputs, memory, bias, mem_bias, params, state=None, dtype=None, scope=None): with tf.variable_scope(scope, default_name='decoder', dtype=dtype, values=[inputs, memory, bias, mem_bias]): x = inputs next_state = {} for layer in range(params.num_decoder_layers): layer_name = 'layer_%d' % layer with tf.variable_scope(layer_name): layer_state = state[layer_name] if state is not None else None max_relative_dis = (params.max_relative_dis if params. position_info_type == 'relative' else None) with tf.variable_scope('self_attention'): y = layers.attention.multihead_attention(_layer_process (x, params.layer_preprocess), None, bias, params. num_heads, params.attention_key_channels or params. hidden_size, params.attention_value_channels or params.hidden_size, params.hidden_size, 1.0 - params.attention_dropout, state=layer_state, max_relative_dis=max_relative_dis) if layer_state is not None: next_state[layer_name] = y['state'] y = y['outputs'] x = _residual_fn(x, y, 1.0 - params.residual_dropout) x = _layer_process(x, params.layer_postprocess) with tf.variable_scope('encdec_attention'): y = layers.attention.multihead_attention(_layer_process (x, params.layer_preprocess), memory, mem_bias, params.num_heads, params.attention_key_channels or params.hidden_size, params.attention_value_channels or params.hidden_size, params.hidden_size, 1.0 - params.attention_dropout, max_relative_dis= max_relative_dis) y = y['outputs'] x = _residual_fn(x, y, 1.0 - params.residual_dropout) x = _layer_process(x, params.layer_postprocess) with tf.variable_scope('feed_forward'): y = _ffn_layer(_layer_process(x, params. layer_preprocess), params.filter_size, params. hidden_size, 1.0 - params.relu_dropout) x = _residual_fn(x, y, 1.0 - params.residual_dropout) x = _layer_process(x, params.layer_postprocess) outputs = _layer_process(x, params.layer_preprocess) if state is not None: return outputs, next_state return outputs 
kwargs|train|app|to def to_train_kwargs(args): """Parse command-line training arguments.  Parameters ---------- args : argparse.Namespace Namespace of an argument parser that was created with create_default_argument_parser.  Returns ------- Dict[str, T] Returns a dictionary of named arguments to be passed on to train_loop.  """ kwargs_dict = {'batch_size': args.batch_size, 'learning_rate': args. learning_rate, 'log_dir': args.log_dir, 'loss_mode': args.loss_mode, 'number_of_steps': args.number_of_steps, 'restore_path': args. restore_path, 'run_id': args.run_id} return kwargs_dict 
LearnableGaussianDiagCondDistr|init|dc|svae|nets def __init__(self, cond_size, output_size, hidden_size, pr): super(LearnableGaussianDiagCondDistr, self).__init__() self.nn = nn.Sequential(nn.Linear(cond_size, hidden_size), pr.nl, nn. Linear(hidden_size, hidden_size), pr.nl) self.mu = nn.Sequential(nn.Linear(hidden_size, output_size), pr.mu_nl) self.logvar = nn.Sequential(nn.Linear(hidden_size, output_size)) self.logvar_scheduler = pr.logvar_scheduler 
meanteacher|variables|update|LA|ema|train|certainty def update_ema_variables(model, ema_model, alpha, global_step): alpha = min(1 - 1 / (global_step + 1), alpha) for ema_param, param in zip(ema_model.parameters(), model.parameters()): ema_param.data.mul_(alpha).add_(1 - alpha, param.data) 
UtilsNetwork|linear|regression def linear_regression(keyword, variable_name, sample, level_c, level_f, level_single, n_input, norm, scaler, point): if 'diff' in keyword: X, y, X_test, y_test, min_val, max_val = get_data_diff(keyword, sample, variable_name, level_c, level_f, n_input, normalize= norm, scaler=scaler, point=point) else: X, y, X_test, y_test, min_val, max_val = get_data(keyword, sample, variable_name, level_single, n_input, normalize=norm, scaler= scaler, point=point) reg = LinearRegression().fit(X, y) y_pred = reg.predict(X_test) y_test = y_test * (max_val - min_val) + min_val y_pred = y_pred * (max_val - min_val) + min_val mean_error = compute_mean_prediction_error(y_test, y_pred, 2) * 100 stdv_error = compute_prediction_error_variance(y_test, y_pred, 2) * 100 print(colored('\nEvaluate linearity data:', 'green', attrs=['bold'])) print(str(mean_error) + '%') print(str(stdv_error) + '%') return mean_error, stdv_error, reg 
cnn|nn|layer def cnn_layer(inputs, filter_size, strides, padding, random_base, l2_reg, active_func=None, scope_name='conv'): w = tf.get_variable(name='conv' + scope_name, shape=filter_size, initializer=tf.random_uniform_initializer(-random_base, random_base ), regularizer=tf.contrib.layers.l2_regularizer(l2_reg)) b = tf.get_variable(name='softmax_b' + scope_name, shape=[filter_size[- 1]], initializer=tf.random_uniform_initializer(-random_base, random_base), regularizer=tf.contrib.layers.l2_regularizer(l2_reg)) x = tf.nn.conv2d(inputs, w, strides, padding) + b if active_func is None: active_func = tf.nn.relu return active_func(x) 
bert|test|tokenization|lower|TokenizationTest|master|basic|tokenizer def test_basic_tokenizer_lower(self): tokenizer = tokenization.BasicTokenizer(do_lower_case=True) self.assertAllEqual(tokenizer.tokenize(' \tHeLLo!how  \n Are yoU?  '), ['hello', '!', 'how', 'are', 'you', '?']) self.assertAllEqual(tokenizer.tokenize('Hllo'), ['hello']) 
learner|rescaling|function|value|inverse def inverse_value_function_rescaling(x): """See Proposition A.2 in paper "Observe and Look Further".""" eps = FLAGS.value_function_rescaling_epsilon return tf.math.sign(x) * (tf.math.square((tf.math.sqrt(1.0 + 4.0 * eps * (tf.math.abs(x) + 1.0 + eps)) - 1.0) / (2.0 * eps)) - 1.0) 
texar|PairedTextData|paired|make|data|padded|shapes|text def _make_padded_shapes(self, dataset, src_decoder, tgt_decoder): src_text_and_id_shapes = {} if self._hparams.source_dataset.pad_to_max_seq_length: src_text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes( dataset, self._hparams.source_dataset, src_decoder, self. source_text_name, self.source_text_id_name) tgt_text_and_id_shapes = {} if self._hparams.target_dataset.pad_to_max_seq_length: tgt_text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes( dataset, self._hparams.target_dataset, tgt_decoder, self. target_text_name, self.target_text_id_name) padded_shapes = dataset.output_shapes padded_shapes.update(src_text_and_id_shapes) padded_shapes.update(tgt_text_and_id_shapes) return padded_shapes 
enas|get|ops|update|train|utils def _update(): update_last_reset = tf.assign(last_reset, curr_epoch, use_locking=True) update_T_i = tf.assign(T_i, T_i * lr_T_mul, use_locking=True) with tf.control_dependencies([update_last_reset, update_T_i]): rate = tf.to_float(T_curr) / tf.to_float(T_i) * 3.1415926 lr = lr_min + 0.5 * (lr_max - lr_min) * (1.0 + tf.cos(rate)) return lr 
test|freeze|TrainTest|train|graph def test_freeze_graph(self): print('test_freeze_graph') argv = ['python', 'src/freeze_graph.py', self.pretrained_model, self. frozen_graph_filename] subprocess.call(argv) 
weighted|PatchStatsCalculator|baselines def weighted_baselines(self, out_que): sum_g = 0 sum_s = 0 sum_v = 0 qsz = out_que.qsize() for i in range(qsz): d = out_que.get() sum_g += d['sum_g'] sum_v += d['sum_v'] sum_s += d['sum_s'] n = self.hps.test_its * self.hps.n_batch_test print('***** n (patches) = %f' % n) ndims = self.hps.patch_height * self.patch_height * 4 nll_gauss = ndims / 2.0 * np.log(2 * np.pi * self.stats['sc_in_vr'] ) + sum_g / n nll_sdn = ndims / 2.0 * np.log(2 * np.pi) + sum_v / n + sum_s / n bpd_gauss = bpd(nll_gauss, self.hps.n_bins, self.hps.n_dims) bpd_sdn = bpd(nll_sdn, self.hps.n_bins, self.hps.n_dims) return nll_gauss, bpd_gauss, nll_sdn, bpd_sdn 
generator|sobolev|cifar1 def generator(z, reuse): with tf.variable_scope('generator', reuse=reuse): with tf.name_scope('pre_process'): z = tf.layers.dense(z, 4 * 4 * 128) x = tf.reshape(z, [-1, 4, 4, 128]) with tf.name_scope('x1'): x = resblock(x, filters=128, resample='up', normalize=True) x = resblock(x, filters=128, resample='up', normalize=True) x = resblock(x, filters=128, resample='up', normalize=True) with tf.name_scope('post_process'): x = activation(bn(x)) result = apply_conv(x, filters=3, he_init=False) return tf.tanh(result) 
with|adam|neural|minimize|style def minimize_with_adam(sess, net, optimizer, init_img, loss): if args.verbose: print('\nMINIMIZING LOSS USING: ADAM OPTIMIZER') train_op = optimizer.minimize(loss) init_op = tf.global_variables_initializer() sess.run(init_op) sess.run(net['input'].assign(init_img)) iterations = 0 while iterations < args.max_iterations: sess.run(train_op) if iterations % args.print_iterations == 0 and args.verbose: curr_loss = loss.eval() print('At iterate {}\tf=  {}'.format(iterations, curr_loss)) iterations += 1 
Block|resblocks|shortcut def shortcut(self, x): if self.learnable_sc: x = self.c_sc(x) if self.downsample: return _downsample(x) else: return x else: return x 
lanenet|test|init|DataSet|data|processor|dataset def _init_dataset(self): """ :return: """ img_list = [] if not tf.gfile.Exists(self._dataset_info_file): raise ValueError('Failed to find file: ' + self._dataset_info_file) with open(self._dataset_info_file, 'r') as file: for _info in file: info_tmp = _info.strip(' ').split() img_list.append(info_tmp[0][1:]) self._len = len(img_list) return img_list 
StdConv|models|init|layers|sto def __init__(self, C_in, C_out, kernel_size=3, stride=1, padding=1, groups= 1, affine=True, after_norm_type='bn'): super(StdConv, self).__init__() self.op = nn.Sequential(nn.BatchNorm2d(C_in, affine=affine, track_running_stats=False), nn.ReLU(inplace=True), nn.Conv2d(C_in, C_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=False, groups=groups), NORMS[after_norm_type](C_out, groups)) 
avod|project|bev|projector|3d|core|box|to def project_to_bev(boxes_3d, bev_extents): """ Projects an array of 3D boxes into bird's eye view  Args: boxes_3d: list of 3d boxes in the format: N x [x, y, z, l, w, h, ry] bev_extents: xz extents of the 3d area [[min_x, max_x], [min_z, max_z]]  Returns: box_points: counter-clockwise order box points in bev map space N x [[x0, y0], ... [x3, y3]] - (N x 4 x 2) box_points_norm: points normalized as a percentage of the map size N x [[x0, y0], ... [x3, y3]] - (N x 4 x 2) """ format_checker.check_box_3d_format(boxes_3d) boxes_3d = np.array(boxes_3d, dtype=np.float32) x = boxes_3d[:, (0)] z = boxes_3d[:, (2)] l = boxes_3d[:, (3)] w = boxes_3d[:, (4)] ry = boxes_3d[:, (6)] l_2 = l / 2.0 w_2 = w / 2.0 p0 = np.array([l_2, w_2]) p1 = np.array([-l_2, w_2]) p2 = np.array([-l_2, -w_2]) p3 = np.array([l_2, -w_2]) box_points = np.empty((len(boxes_3d), 4, 2)) for box_idx in range(len(boxes_3d)): rot = ry[box_idx] rot_mat = np.reshape([[np.cos(rot), np.sin(rot)], [-np.sin(rot), np .cos(rot)]], (2, 2)) box_x = x[box_idx] box_z = z[box_idx] box_xz = [box_x, box_z] box_p0 = np.dot(rot_mat, p0[:, (box_idx)]) + box_xz box_p1 = np.dot(rot_mat, p1[:, (box_idx)]) + box_xz box_p2 = np.dot(rot_mat, p2[:, (box_idx)]) + box_xz box_p3 = np.dot(rot_mat, p3[:, (box_idx)]) + box_xz box_points[box_idx] = np.array([box_p0, box_p1, box_p2, box_p3]) x_extents_min = bev_extents[0][0] z_extents_min = bev_extents[1][1] points_shifted = box_points - [x_extents_min, z_extents_min] x_extents_range = bev_extents[0][1] - bev_extents[0][0] z_extents_range = bev_extents[1][0] - bev_extents[1][1] box_points_norm = points_shifted / [x_extents_range, z_extents_range] box_points = np.asarray(box_points, dtype=np.float32) box_points_norm = np.asarray(box_points_norm, dtype=np.float32) return box_points, box_points_norm 
models|dien|deepctr|DIEN def DIEN(feature_dim_dict, seq_feature_list, embedding_size=8, hist_len_max =16, gru_type='GRU', use_negsampling=False, alpha=1.0, use_bn=False, dnn_hidden_units=(200, 80), dnn_activation='relu', att_hidden_units=(64, 16), att_activation='dice', att_weight_normalization=True, l2_reg_dnn=0, l2_reg_embedding=1e-06, dnn_dropout=0, init_std=0.0001, seed=1024, task ='binary'): """Instantiates the Deep Interest Evolution Network architecture.  :param feature_dim_dict: dict,to indicate sparse field (**now only support sparse feature**)like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':[]} :param seq_feature_list: list,to indicate  sequence sparse field (**now only support sparse feature**),must be a subset of ``feature_dim_dict["sparse"]`` :param embedding_size: positive integer,sparse feature embedding_size. :param hist_len_max: positive int, to indicate the max length of seq input :param gru_type: str,can be GRU AIGRU AUGRU AGRU :param use_negsampling: bool, whether or not use negtive sampling :param alpha: float ,weight of auxiliary_loss :param use_bn: bool. Whether use BatchNormalization before activation or not in deep net :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN :param dnn_activation: Activation function to use in DNN :param att_hidden_units: list,list of positive integer , the layer number and units in each layer of attention net :param att_activation: Activation function to use in attention net :param att_weight_normalization: bool.Whether normalize the attention score of local activation unit. :param l2_reg_dnn: float. L2 regularizer strength applied to DNN :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance.  """ check_feature_config_dict(feature_dim_dict) (sparse_input, dense_input, user_behavior_input, user_behavior_length ) = get_input(feature_dim_dict, seq_feature_list, hist_len_max) sparse_embedding_dict = {feat.name: Embedding(feat.dimension, embedding_size, embeddings_initializer=RandomNormal(mean=0.0, stddev=init_std, seed=seed), embeddings_regularizer=l2( l2_reg_embedding), name='sparse_emb_' + str(i) + '-' + feat.name) for i, feat in enumerate(feature_dim_dict['sparse'])} query_emb_list = get_embedding_vec_list(sparse_embedding_dict, sparse_input, feature_dim_dict['sparse'], return_feat_list= seq_feature_list) keys_emb_list = get_embedding_vec_list(sparse_embedding_dict, user_behavior_input, feature_dim_dict['sparse'], return_feat_list= seq_feature_list) deep_input_emb_list = get_embedding_vec_list(sparse_embedding_dict, sparse_input, feature_dim_dict['sparse']) query_emb = concat_fun(query_emb_list) keys_emb = concat_fun(keys_emb_list) deep_input_emb = concat_fun(deep_input_emb_list) if use_negsampling: neg_user_behavior_input = OrderedDict() for i, feat in enumerate(seq_feature_list): neg_user_behavior_input[feat] = Input(shape=(hist_len_max,), name='neg_seq_' + str(i) + '-' + feat) neg_uiseq_embed_list = get_embedding_vec_list(sparse_embedding_dict, neg_user_behavior_input, feature_dim_dict['sparse'], seq_feature_list) neg_concat_behavior = concat_fun(neg_uiseq_embed_list) else: neg_concat_behavior = None hist, aux_loss_1 = interest_evolution(keys_emb, query_emb, user_behavior_length, gru_type=gru_type, use_neg=use_negsampling, neg_concat_behavior=neg_concat_behavior, embedding_size= embedding_size, att_hidden_size=att_hidden_units, att_activation= att_activation, att_weight_normalization=att_weight_normalization) deep_input_emb = Concatenate()([deep_input_emb, hist]) deep_input_emb = tf.keras.layers.Flatten()(deep_input_emb) if len(dense_input) > 0: deep_input_emb = Concatenate()([deep_input_emb] + list(dense_input. values())) output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, use_bn, seed)(deep_input_emb) final_logit = Dense(1, use_bias=False)(output) output = PredictionLayer(task)(final_logit) model_input_list = get_inputs_list([sparse_input, dense_input, user_behavior_input]) if use_negsampling: model_input_list += list(neg_user_behavior_input.values()) model_input_list += [user_behavior_length] model = tf.keras.models.Model(inputs=model_input_list, outputs=output) if use_negsampling: model.add_loss(alpha * aux_loss_1) tf.keras.backend.get_session().run(tf.global_variables_initializer()) return model 
tests|end|gym|test|and|do|EngineTest|pycolab|reward|episode|engine def _do_test_reward_and_episode_end(self, q_pre_update, expected_discount): """Core implementation of `testRewardAndEpisodeEndWith*Discount` tests.  Args: q_pre_update: Pre-`update` code to inject for the Q sprite. expected_discount: `discount` we expect to observe after the final game step. """ art = ['.........', '...Q.R...', '.........'] engine = ascii_art.ascii_art_to_game(art=art, what_lies_beneath='.', sprites=dict(Q=tt.TestSprite, R=tt.TestSprite), update_schedule='QR') unused_observation, reward, discount = engine.its_showtime() self.assertIsNone(reward) self.assertEqual(discount, 1.0) self.assertFalse(engine.game_over) tt.pre_update(engine, 'Q', lambda actions, board, layers, backdrop, things, the_plot: the_plot.add_reward('pyco')) tt.pre_update(engine, 'R', lambda actions, board, layers, backdrop, things, the_plot: the_plot.add_reward('lab!')) unused_observation, reward, discount = engine.play('mound of beans') self.assertEqual(reward, 'pycolab!') self.assertEqual(discount, 1.0) self.assertFalse(engine.game_over) tt.pre_update(engine, 'Q', q_pre_update) tt.pre_update(engine, 'R', lambda actions, board, layers, backdrop, things, the_plot: the_plot.add_reward('trousers')) unused_observation, reward, discount = engine.play('mound of beans') self.assertEqual(reward, 'trousers') self.assertEqual(discount, expected_discount) self.assertTrue(engine.game_over) 
optimizer|ops|v|regression|MVGOptimizer|mvg @property def v(self): pi = tf.sqrt((tf.trace(self.fisher_v) / self.shape[1] + 1e-08) / (tf. trace(self.fisher_u) / self.shape[0] + 1e-08)) coeff = self.lam / (self.N * self.ita) v_tmp = tf.matrix_inverse(self.fisher_v + pi * coeff ** 0.5 * tf.eye( self.shape[1])) return (self.lam / self.N) ** 0.5 * v_tmp 
Base|tx|get|model|base def get_tx(self, x, reverse=False): x = Variable(x.data, requires_grad=True) if reverse: ux = self.psi(x) else: ux = self.phi(x) dux = torch.autograd.grad(outputs=ux, inputs=x, grad_outputs=utils. get_ones(ux.size()), create_graph=True, retain_graph=True, only_inputs=True)[0] Tx = x - dux return Tx 
InceptionV3Test|test|v3|inception|nets|testBuildOnlyUptoFinalEndpoint def testBuildOnlyUptoFinalEndpoint(self): batch_size = 5 height, width = 299, 299 endpoints = ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3', 'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c'] for index, endpoint in enumerate(endpoints): with tf.Graph().as_default(): inputs = tf.random_uniform((batch_size, height, width, 3)) out_tensor, end_points = inception.inception_v3_base(inputs, final_endpoint=endpoint) self.assertTrue(out_tensor.op.name.startswith('InceptionV3/' + endpoint)) self.assertItemsEqual(endpoints[:index + 1], end_points.keys()) 
normalize|xyz|SPH3D|shapenet def normalize_xyz(points): points -= tf.reduce_mean(points, axis=1, keepdims=True) scale = tf.reduce_max(tf.reduce_sum(tf.square(points), axis=-1, keepdims=True), axis=1, keepdims=True) scale = tf.sqrt(scale, name='normalize') points /= scale return points 
bert|fn|builder|scaffold|tpu|master|model|squad|run def tpu_scaffold(): tf.train.init_from_checkpoint(init_checkpoint, assignment_map) return tf.train.Scaffold() 
MadryEtAl|cleverhans|attacks|params|parse def parse_params(self, eps=0.3, eps_iter=0.01, nb_iter=40, y=None, ord=np. inf, clip_min=None, clip_max=None, y_target=None, **kwargs): """ Take in a dictionary of parameters and applies attack-specific checks before saving them as attributes.  Attack-specific parameters: :param eps: (required float) maximum distortion of adversarial example compared to original input :param eps_iter: (required float) step size for each attack iteration :param nb_iter: (required int) Number of attack iterations. :param y: (optional) A tensor with the model labels. :param y_target: (optional) A tensor with the labels to target. Leave y_target=None if y is also set. Labels should be one-hot-encoded. :param ord: (optional) Order of the norm (mimics Numpy). Possible values: np.inf, 1 or 2. :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value """ self.eps = eps self.eps_iter = eps_iter self.nb_iter = nb_iter self.y = y self.y_target = y_target self.ord = ord self.clip_min = clip_min self.clip_max = clip_max if self.y is not None and self.y_target is not None: raise ValueError('Must not set both y and y_target') if self.ord not in [np.inf, 1, 2]: raise ValueError('Norm order must be either np.inf, 1, or 2.') return True 
texar|compat|as|dtypes|utils|text def compat_as_text(str_): """Converts strings into `unicode` (Python 2) or `str` (Python 3).  Args: str\\_: A string or other data types convertible to string, or an `n`-D numpy array or (possibly nested) list of such elements.  Returns: The converted strings of the same structure/shape as :attr:`str_`. """  def _recur_convert(s): if isinstance(s, (list, tuple, np.ndarray)): s_ = [_recur_convert(si) for si in s] return _maybe_list_to_array(s_, s) else: try: return tf.compat.as_text(s) except TypeError: return tf.compat.as_text(str(s)) text = _recur_convert(str_) return text 
do|logging|dc|svae|main def do_logging(epoch, elbo, svae_dc_params, optimizer, tb_writer, debug_dict): assert len(optimizer.param_groups) == 1 if tb_writer is not None: tb_writer.add_scalar('elbo', elbo.mean().item(), epoch) tb_writer.add_scalar('optimizer_lr', optimizer.param_groups[0]['lr' ], epoch) tb_writer.add_scalar('logvar_limit', svae_dc_params. logvar_scheduler.logvar_limit(), epoch) for k, v in debug_dict.items(): vv = v.mean().item() if type(v) == torch.Tensor else v tb_writer.add_scalar(k, vv, epoch) logging.info('Train epoch {:d} lr {:.2E} ELBO: {:.4f}'.format(epoch, optimizer.param_groups[0]['lr'], elbo.mean().item())) 
corpus|operation|file|parse|utils def parse_corpus(corpus_file): corpus = [] with open(corpus_file, 'r') as f: for line in f: corpus.append(line.strip()) return corpus 
result|create|SRGANs|master|train|Regularization|GANs|dir|Spectral def create_result_dir(result_dir, config_path, config): if not os.path.exists(result_dir): os.makedirs(result_dir)  def copy_to_result_dir(fn, result_dir): bfn = os.path.basename(fn) shutil.copy(fn, '{}/{}'.format(result_dir, bfn)) copy_to_result_dir(config_path, result_dir) copy_to_result_dir(config.models['generator']['fn'], result_dir) copy_to_result_dir(config.models['discriminator']['fn'], result_dir) copy_to_result_dir(config.dataset['dataset_fn'], result_dir) copy_to_result_dir(config.updater['fn'], result_dir) 
unscaled|tangents|stax|fn|conv|neural|nngp|GeneralConv|kernel def conv_nngp_unscaled(x): if _is_array(x): x = _conv_nngp_5or6d_double_conv(x, filter_shape_nngp, strides_nngp, padding) return x 
def|inf|mnist|model|conv2d def conv2d(x, W): """conv2d returns a 2d convolution layer with full stride.""" return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') 
kernels|bo|KLKernel|dc|svae|utils|dbgforward def dbgforward(self, x1, x2, diag=False, last_dim_is_batch=False, **params): assert last_dim_is_batch == False if diag and torch.equal(x1, x2): return self.postprocess_dists(torch.zeros_like(x1)) was_batch_mode = True if x1.dim() == 2 or x2.dim() == 2: was_batch_mode = False if x1.dim() == 2: x1 = x1.unsqueeze(0) if x2.dim() == 2: x2 = x2.unsqueeze(0) bsz, n, d = x1.size() m = x2.size(1) out = torch.zeros(bsz, n, m).to(device=x1.device) for bid in range(bsz): for nid in range(n): for mid in range(m): _, tau1_dist = self.kernel_transform.apply(x1[(bid), (nid), :].unsqueeze(0)) _, tau2_dist = self.kernel_transform.apply(x2[(bid), (mid), :].unsqueeze(0)) kl_val = GaussianDiagDistr.symmetrized_kl_between(tau1_dist .mu.view(1, -1), tau1_dist.logvar.view(1, -1), tau2_dist.mu.view(1, -1), tau2_dist.logvar.view(1, -1)) out[bid, nid, mid] = kl_val if not was_batch_mode: out = out.squeeze(0) return self.postprocess_dists(out) 
FullTokenizer|init|utils|tokenization def __init__(self, vocab_file, do_lower_case=True): self.vocab = load_vocab(vocab_file) self.inv_vocab = {v: k for k, v in self.vocab.items()} self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case) self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab) 
tmp|inference|master|facenet|nn3 def inference(images, keep_probability, phase_train=True, weight_decay=0.0): """ Define an inference network for face recognition based on inception modules using batch normalization  Args: images: The images to run inference on, dimensions batch_size x height x width x channels phase_train: True if batch normalization should operate in training mode """ endpoints = {} net = network.conv(images, 3, 64, 7, 7, 2, 2, 'SAME', 'conv1_7x7', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay ) endpoints['conv1'] = net net = network.mpool(net, 3, 3, 2, 2, 'SAME', 'pool1') endpoints['pool1'] = net net = network.conv(net, 64, 64, 1, 1, 1, 1, 'SAME', 'conv2_1x1', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay ) endpoints['conv2_1x1'] = net net = network.conv(net, 64, 192, 3, 3, 1, 1, 'SAME', 'conv3_3x3', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay ) endpoints['conv3_3x3'] = net net = network.mpool(net, 3, 3, 2, 2, 'SAME', 'pool3') endpoints['pool3'] = net net = network.inception(net, 192, 1, 64, 96, 128, 16, 32, 3, 32, 1, 'MAX', 'incept3a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept3a'] = net net = network.inception(net, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, 'MAX', 'incept3b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept3b'] = net net = network.inception(net, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, 'MAX', 'incept3c', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept3c'] = net net = network.inception(net, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, 'MAX', 'incept4a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept4a'] = net net = network.inception(net, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, 'MAX', 'incept4b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept4b'] = net net = network.inception(net, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, 'MAX', 'incept4c', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept4c'] = net net = network.inception(net, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, 'MAX', 'incept4d', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept4d'] = net net = network.inception(net, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, 'MAX', 'incept4e', phase_train=phase_train, use_batch_norm=True) endpoints['incept4e'] = net net = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, 'MAX', 'incept5a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept5a'] = net net = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, 'MAX', 'incept5b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay) endpoints['incept5b'] = net net = network.apool(net, 5, 5, 1, 1, 'VALID', 'pool6') endpoints['pool6'] = net net = tf.reshape(net, [-1, 1024]) endpoints['prelogits'] = net net = tf.nn.dropout(net, keep_probability) endpoints['dropout'] = net return net, endpoints 
contributed|face|Detection|master|facenet|faces|find def find_faces(self, image): faces = [] bounding_boxes, _ = align.detect_face.detect_face(image, self.minsize, self.pnet, self.rnet, self.onet, self.threshold, self.factor) for bb in bounding_boxes: face = Face() face.container_image = image face.bounding_box = np.zeros(4, dtype=np.int32) img_size = np.asarray(image.shape)[0:2] face.bounding_box[0] = np.maximum(bb[0] - self.face_crop_margin / 2, 0) face.bounding_box[1] = np.maximum(bb[1] - self.face_crop_margin / 2, 0) face.bounding_box[2] = np.minimum(bb[2] + self.face_crop_margin / 2, img_size[1]) face.bounding_box[3] = np.minimum(bb[3] + self.face_crop_margin / 2, img_size[0]) cropped = image[face.bounding_box[1]:face.bounding_box[3], face. bounding_box[0]:face.bounding_box[2], :] face.image = misc.imresize(cropped, (self.face_crop_size, self. face_crop_size), interp='bilinear') faces.append(face) return faces 
test|testBuildBaseNetwork|inception|nets|v4|InceptionTest def testBuildBaseNetwork(self): batch_size = 5 height, width = 299, 299 inputs = tf.random_uniform((batch_size, height, width, 3)) net, end_points = inception.inception_v4_base(inputs) self.assertTrue(net.op.name.startswith('InceptionV4/Mixed_7d')) self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536]) expected_endpoints = ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3', 'Mixed_3a', 'Mixed_4a', 'Mixed_5a', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d', 'Mixed_5e', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e', 'Mixed_6f', 'Mixed_6g', 'Mixed_6h', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c', 'Mixed_7d'] self.assertItemsEqual(end_points.keys(), expected_endpoints) for name, op in end_points.items(): self.assertTrue(op.name.startswith('InceptionV4/' + name)) 
test|testEvaluation|vgg|nets|VGG16Test def testEvaluation(self): batch_size = 2 height, width = 224, 224 num_classes = 1000 with self.test_session(): eval_inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = vgg.vgg_16(eval_inputs, is_training=False) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) predictions = tf.argmax(logits, 1) self.assertListEqual(predictions.get_shape().as_list(), [batch_size]) 
tangents|kernel|empirical|fn|neural|utils def empirical_kernel_fn(f): """Returns a function that computes single draws from NNGP and NT kernels.""" kernel_fns = {'nngp': empirical_nngp_fn(f), 'ntk': empirical_ntk_fn(f)}  @get_namedtuple('EmpiricalKernel') def kernel_fn(x1, x2, params, get=None): """Returns a draw from the requested empirical kernels.  Args: x1: An ndarray of shape [n1,] + input_shape. x2: An ndarray of shape [n2,] + input_shape. params: A PyTree of parameters for the function `f`. get: either None, a string, a tuple of strings specifying which data should be returned by the kernel function. Can be "nngp" or "ntk". If `None` then both "nngp" and "ntk" are returned.  Returns: If `get` is a string, returns the requested `np.ndarray`. If `get` is a tuple, returns an `EmpiricalKernel` namedtuple containing only the requested information. """ if get is None: get = 'nngp', 'ntk' return {g: kernel_fns[g](x1, x2, params) for g in get} return kernel_fn 
max|grad|pool3d|tf @ops.RegisterGradient('MaxPool3d') def _max_pool3d_grad(op, grad_output, grad_index): input = op.inputs[0] max_index = op.outputs[1] grad_input = pool3d_module.max_pool3d_grad(input, grad_output, max_index) return [grad_input, None, None] 
ESN|reservoir|layer def reservoir_layer(A, Win, input, res_params): states = np.zeros((res_params['N'], res_params['train_length'])) for i in range(res_params['train_length'] - 1): states[:, (i + 1)] = np.tanh(np.dot(A, states[:, (i)]) + np.dot(Win, input[:, (i)])) return states 
normalize|load|text|data def text_normalize(text): text = ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn') text = text.lower() text = re.sub('[^{}]'.format(hp.vocab), ' ', text) text = re.sub('[ ]+', ' ', text) return text 
frames|stack|test|agents @tf.function def stack_frames(frames, frame_stacking_state, done, stack_size): return agents.stack_frames(tf.convert_to_tensor(frames, dtype=tf. float32), frame_stacking_state, tf.convert_to_tensor(done), stack_size) 
size|get|classif|feature|per|file|utils def get_feature_size_per_file(f_name): """ Return the dimensionality of the features in a given file. Typically, this will be the number of bins in a T-F representation """ shape = get_shape(os.path.join(f_name.replace('.data', '.shape'))) return shape[1] 
bert|create|from|attention|mask|input|master|modeling def create_attention_mask_from_input_mask(from_tensor, to_mask): """Create 3D attention mask from a 2D tensor mask.  Args: from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...]. to_mask: int32 Tensor of shape [batch_size, to_seq_length].  Returns: float Tensor of shape [batch_size, from_seq_length, to_seq_length]. """ from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) batch_size = from_shape[0] from_seq_length = from_shape[1] to_shape = get_shape_list(to_mask, expected_rank=2) to_seq_length = to_shape[1] to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32) broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype= tf.float32) mask = broadcast_ones * to_mask return mask 
loss|ops|regularization def regularization_loss(scope_name): """ If you want to use "Regularization" g_loss += regularization_loss('generator') d_loss += regularization_loss('discriminator') """ collection_regularization = tf.get_collection(tf.GraphKeys. REGULARIZATION_LOSSES) loss = [] for item in collection_regularization: if scope_name in item.name: loss.append(item) return tf.reduce_sum(loss) 
ObservationToArray|gym|rendering|pycolab|call def __call__(self, observation): """Derives an array from an `Observation`.  Returns a 2-D or 3-D array whose values at each row, column coordinate come from applying the value mapping supplied to the constructor to `observation`.  Note: the returned array should be accessed in a *read-only* manner exclusively; furthermore, if this method is called again, the contents of the array returned in any prior call to this method are *undefined* (i.e. not guaranteed to be anything---could be blank, random garbage, whatever).  Args: observation: an `Observation` from which this method derives a numpy array.  Returns: a numpy array derived from `observation` as described.  Raises: RuntimeError: `observation` contains a value that is not in the value mapping passed to the constructor. """ if self._array is None or self._array.shape[1:] != observation.board.shape: rows, cols = observation.board.shape self._array = np.zeros((self._depth, rows, cols), dtype=self._dtype) ascii_vals = np.unique(observation.board) for ascii_value in ascii_vals: try: value = self._value_mapping[chr(ascii_value)] except KeyError: raise RuntimeError( 'This ObservationToArray only knows array values for the characters {}, but it received an observation with a character not in that set' .format(str(''.join(self._value_mapping.keys())))) mask = observation.board == ascii_value if self._is_3d: for layer, value_component in enumerate(value): self._array[layer, mask] = value_component else: self._array[:, (mask)] = value result = self._array if self._is_3d else self._array[0] if self._permute is None: return result else: return np.transpose(result, self._permute) 
bert|FullTokenizer|tokenization|init|master def __init__(self, vocab_file, do_lower_case=True): self.vocab = load_vocab(vocab_file) self.inv_vocab = {v: k for k, v in self.vocab.items()} self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case) self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab) 
loss|generation def loss(labels, logits): return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True) 
gan|sp|CDVAECLSGAN|cls|mcc|encoder|cdvae def sp_encoder(self, x): net = self.arch['encoder']['sp'] return self._encoder(x, net) 
plyfile|PlyProperty|write|bin def _write_bin(self, data, stream, byte_order): """ Write data to a binary stream.  """ _np.dtype(self.dtype(byte_order)).type(data).tofile(stream) 
ops|svd|eig|classification|posdef|utils def posdef_eig_svd(mat): """Computes the singular values and left singular vectors of a matrix.""" evals, evecs, _ = linalg_ops.svd(mat) return evals, evecs 
distance|room|config|ndh|env|to def distance_to_room(path_history, next_pano, golden_path, end_of_episode, scan_info, goal_room_panos): """Rewards an agent based on how close it gets to the goal room.  If d(p, g) is the distance of pano `p` from goal room `g`, then  r(p1 --> p2) = 4  if end_of_episode and agent stopped in the goal room = -4 if end_of_episode and agent did not stop in the room = clip(d(p1, g) - d(p2, g), max=1, min=-1) otherwise  Args: path_history: A list of integers specifying pano ids (source) until the current step. next_pano: An integer specifying next pano id (destination). golden_path: A list containing string names of panos on the golden path. Not used in this function. end_of_episode: True if this is the last transition in the episode. scan_info: A `ScanInfo` tuple. See constants.py. goal_room_panos: A list containing string names of panos in the goal room.  Returns: A scalar float immediate reward for the transition current_pano --> next_pano. """ del golden_path current_pano = path_history[-1] if end_of_episode: last_node_id = (next_pano if next_pano != constants.STOP_NODE_ID else current_pano) last_node_name = scan_info.pano_id_to_name[last_node_id] return 4.0 if last_node_name in goal_room_panos else -4.0 current_pano_name = scan_info.pano_id_to_name[current_pano] next_pano_name = scan_info.pano_id_to_name[next_pano]  def get_distance_to_goal_room(pano_name): return min([scan_info.graph.get_distance(pano_name, goal_pano_name) for goal_pano_name in goal_room_panos]) delta_distance = get_distance_to_goal_room(current_pano_name ) - get_distance_to_goal_room(next_pano_name) return min(1.0, max(-1.0, delta_distance)) 
1|networks|expand|elpips|squeezenet1|maxpool|full def _expand(self, input, index, ch_in, ch_out): e1x1 = self._conv(input, self.features['{:d}.expand1x1.weight'.format( index)], self.features['{:d}.expand1x1.bias'.format(index)], w_shape=[1, 1, ch_in, ch_out], padding='VALID', stride=[1, 1, 1, 1]) e3x3 = self._conv(input, self.features['{:d}.expand3x3.weight'.format( index)], self.features['{:d}.expand3x3.bias'.format(index)], w_shape=[3, 3, ch_in, ch_out], padding='SAME', stride=[1, 1, 1, 1]) if isinstance(input, tuple): return tuple(tf.concat([X, Y], 3) for X, Y in zip(e1x1, e3x3)) else: return tf.concat([e1x1, e3x3], 3) 
args|train|parse|PCGN def parse_args(): """ Parse Seq2seq with attention arguments. """ parser = argparse.ArgumentParser(description='Run PCGN training.') parser.add_argument('--config', nargs='?', default= './configs/pcgn_config.yaml', help= 'Configuration file for model specifications') return parser.parse_args() 
render|envs|gym|DaisyCustomEnv|daisy|debug|other|env|custom def render_debug_other(self, width=600): if width is not None or self._projectM is None: if self._projectM is None: self._pixelWidth = self._param_init_camera_width self._pixelHeight = self._param_init_camera_height else: self._pixelWidth = width self._pixelHeight = width nearPlane = 0.01 farPlane = 10 aspect = self._pixelWidth / self._pixelHeight fov = 60 self._projectM = self._p.computeProjectionMatrixFOV(fov, aspect, nearPlane, farPlane) x, y, z = self.robot.body_xyz lookat = [x, y, z] distance = 1.4 yaw = -20 viewM = self._p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition= lookat, distance=distance, yaw=10.0, pitch=yaw, roll=0.0, upAxisIndex=2 ) img_arr = self._p.getCameraImage(self._pixelWidth, self._pixelHeight, viewM, self._projectM, shadow=self._param_COV_ENABLE_SHADOWS, renderer=pybullet.ER_TINY_RENDERER, flags=pybullet. ER_NO_SEGMENTATION_MASK) return img_arr[2] 
models|smooth|SoftLocLoss|tf def smooth(input): shape_in = tf.shape(input) input = tf.reshape(tf.transpose(input, [0, 2, 1]), [shape_in[0] * shape_in[2], shape_in[1], 1]) input = tf.nn.conv1d(input, gaussFilter, stride=1, padding='SAME') input = tf.transpose(tf.reshape(input, [shape_in[0], shape_in[2], shape_in[1]]), [0, 2, 1]) return input 
PackedBitsObservation|init|observation def __init__(self, env): super(PackedBitsObservation, self).__init__(env) self.observation_space = gym.spaces.Box(low=0, high=np.iinfo(np.uint16) .max, shape=env.observation_space.shape[:-1] + ((env. observation_space.shape[-1] + 15) // 16,), dtype=np.uint16) 
rnn|texar|BasicRNNDecoderTest|output|test|modules|decoders|layer def test_output_layer(self): decoder = BasicRNNDecoder(vocab_size=self._vocab_size, output_layer=None) self.assertIsInstance(decoder, BasicRNNDecoder) decoder = BasicRNNDecoder(output_layer=tf.identity) self.assertIsInstance(decoder, BasicRNNDecoder) tensor = tf.random_uniform([self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32) decoder = BasicRNNDecoder(output_layer=tensor) self.assertIsInstance(decoder, BasicRNNDecoder) self.assertEqual(decoder.vocab_size, self._vocab_size) 
grid|generate|and|color|pvae|train|save|samples|vis def color_grid_vis(X, nh, nw, save_path): X = X.transpose(0, 2, 3, 1) h, w = X[0].shape[:2] img = np.zeros((h * nh, w * nw, 3)) for n, x in enumerate(X): j = int(n / nw) i = int(n % nw) img[j * h:j * h + h, i * w:i * w + w, :] = x imsave(save_path, img) 
avod|anchor|offset|encoder|core|to def offset_to_anchor(anchors, offsets): """Decodes the anchor regression predictions with the anchor.  Args: anchors: A numpy array or a tensor of shape [N, 6] representing the generated anchors. offsets: A numpy array or a tensor of shape [N, 6] containing the predicted offsets in the anchor format  [x, y, z, dim_x, dim_y, dim_z].  Returns: anchors: A numpy array of shape [N, 6] representing the predicted anchor boxes. """ fc.check_anchor_format(anchors) fc.check_anchor_format(offsets) x_pred = offsets[:, (0)] * anchors[:, (3)] + anchors[:, (0)] y_pred = offsets[:, (1)] * anchors[:, (4)] + anchors[:, (1)] z_pred = offsets[:, (2)] * anchors[:, (5)] + anchors[:, (2)] tensor_format = isinstance(anchors, tf.Tensor) if tensor_format: dx_pred = tf.exp(tf.log(anchors[:, (3)]) + offsets[:, (3)]) dy_pred = tf.exp(tf.log(anchors[:, (4)]) + offsets[:, (4)]) dz_pred = tf.exp(tf.log(anchors[:, (5)]) + offsets[:, (5)]) anchors = tf.stack((x_pred, y_pred, z_pred, dx_pred, dy_pred, dz_pred), axis=1) else: dx_pred = np.exp(np.log(anchors[:, (3)]) + offsets[:, (3)]) dy_pred = np.exp(np.log(anchors[:, (4)]) + offsets[:, (4)]) dz_pred = np.exp(np.log(anchors[:, (5)]) + offsets[:, (5)]) anchors = np.stack((x_pred, y_pred, z_pred, dx_pred, dy_pred, dz_pred), axis=1) return anchors 
getargspec|texar|utils|inspect def _inspect_getargspec(fn): """Returns `inspect.getargspec(fn)` for Py2 and `inspect.getfullargspec(fn)` for Py3 """ try: return inspect.getfullargspec(fn) except AttributeError: try: return inspect.getargspec(fn) except TypeError: return inspect.getargspec(fn.__call__) 
sample|predict|seq|tag|gumbel|bow|seq2seq def _sample_gumbel(shape, eps=1e-20): U = tf.random_uniform(shape, minval=0, maxval=1) return -tf.log(-tf.log(U + eps) + eps) 
utils|make|gif def make_gif(images, fname, duration=2, true_image=False): import moviepy.editor as mpy  def make_frame(t): try: x = images[int(len(images) / duration * t)] except: x = images[-1] if true_image: return x.astype(np.uint8) else: return ((x + 1) / 2 * 255).astype(np.uint8) clip = mpy.VideoClip(make_frame, duration=duration) clip.write_gif(fname, fps=len(images) / duration) 
simple|hvc|train|validate def validate(out, sess, epoch, validation_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, is_training_ph): g_step, acc_top1, acc_top5, test_loss = 0, 0, 0, 0 for i in range(validation_steps): out.validation_step_begin(i, validation_steps) g_step, l, acc1, acc5 = sess.run([global_step, loss_op, acc_top_1_op, acc_top_5_op], feed_dict={is_training_ph: False}) acc_top1 = (acc1 + i * acc_top1) / (i + 1) acc_top5 = (acc5 + i * acc_top5) / (i + 1) test_loss = (l + i * test_loss) / (i + 1) out.validation_end(sess, epoch, g_step, test_loss, 'DEFAULT', acc_top1, acc_top5) 
conll|reader|data|read def read_data(source_path, word_vocab, char_vocab, ner_vocab, normalize_digits=True): data = [] print('Reading data from %s' % source_path) counter = 0 reader = CoNLLReader(source_path, word_vocab, char_vocab, ner_vocab) inst = reader.getNext(normalize_digits) while inst is not None: counter += 1 sent = inst.sentence data.append([sent.word_ids, sent.char_id_seqs, inst.ner_ids]) inst = reader.getNext(normalize_digits) reader.close() print('Total number of data: %d' % counter) return data 
attributionpriors|mnist|reader|inputs def inputs(train, batch_size, num_epochs): """Reads input data num_epochs times. Args: train: Selects between the training ('train'),  validation ('vald') data and 'test' data. batch_size: Number of examples per returned batch. num_epochs: Number of times to read the input data, or 0/None to train forever. Returns: A tuple (images, labels), where: * images is a float tensor with shape [batch_size, mnist.IMAGE_PIXELS] in the range [-0.5, 0.5]. * labels is an int32 tensor with shape [batch_size] with the true label, a number in the range [0, mnist.NUM_CLASSES). This function creates a one_shot_iterator, meaning that it will only iterate over the dataset once. On the other hand there is no special initialization required. """ if not num_epochs: num_epochs = None if train == 'train': filename = TRAIN_FILE elif train == 'vald': filename = VALIDATION_FILE elif train == 'test': filename = TEST_FILE elif train == 'ref': filename = TRAIN_FILE else: raise ValueError( 'Invalid argument `{}` -  must be one of: train, vald, test'. format(train)) with tf.name_scope('input'): dataset = tf.data.TFRecordDataset(filename) dataset = dataset.map(decode) if train == 'train': dataset = dataset.map(augment) dataset = dataset.map(normalize) if train == 'train' or train == 'ref': dataset = dataset.shuffle(1000 + 3 * batch_size) if num_epochs is not None: dataset = dataset.repeat(num_epochs) elif train == 'ref': dataset = dataset.repeat() dataset = dataset.batch(batch_size, drop_remainder=True) if train == 'ref': return dataset.make_one_shot_iterator() iterator = dataset.make_initializable_iterator() return iterator 
script|generate|test @concat_commands def generate_test(root_dir, working_dir=None, attack_iter=50, attack_clip= 0.5, attack_box_clip=False, attack_overshoot=0.02, hc_confidence=0.8, dataset='test', sort_labels=True, filter_dirs=False): tmpl_str = get_tmpl_str(generate_test, 'test', add_args=[(0, 'load_dir' )], exclude_args=['root_dir', 'filter_dirs']) working_dirs = glob.glob(working_dir + '/*') for load_dir in sorted(glob.glob(root_dir)): if working_dir is None: working_dir = os.path.abspath(os.path.join(load_dir, os.pardir)) working_dir = working_dir.replace('runs', 'test') else: working_dir = working_dir % locals() working_dirs = [os.path.basename(working_path) for working_path in glob.glob(os.path.join(working_dir, '*'))] if not filter_dirs or os.path.basename(load_dir) not in working_dirs: yield tmpl_str % locals() 
ReLULayer|acgan|mnist def ReLULayer(name, n_in, n_out, inputs): output = lib.ops.linear.Linear(name + '.Linear', n_in, n_out, inputs, initialization='he') return tf.nn.relu(output) 
gallery|cmc|create|datasets|and|probe|util def create_cmc_probe_and_gallery(data_y, camera_indices=None, seed=None): """Create probe and gallery images for evaluation of CMC top-k statistics.  For every identity, this function selects one image as probe and one image for the gallery. Cross-view validation is performed when multiple cameras are given.  Parameters ---------- data_y : ndarray Vector of data labels. camera_indices : Optional[ndarray] Optional array of camera indices. If possible, probe and gallery images are selected from different cameras (i.e., cross-view validation). If None given, assumes all images are taken from the same camera. seed : Optional[int] The random seed used to select probe and gallery images.  Returns ------- (ndarray, ndarray) Returns a tuple of indices to probe and gallery images.  """ data_y = np.asarray(data_y) if camera_indices is None: camera_indices = np.zeros_like(data_y, dtype=np.int) camera_indices = np.asarray(camera_indices) random_generator = np.random.RandomState(seed=seed) unique_y = np.unique(data_y) probe_indices, gallery_indices = [], [] for y in unique_y: mask_y = data_y == y unique_cameras = np.unique(camera_indices[mask_y]) if len(unique_cameras) == 1: c = unique_cameras[0] indices = np.where(np.logical_and(mask_y, camera_indices == c))[0] if len(indices) < 2: continue i1, i2 = random_generator.choice(indices, 2, replace=False) else: c1, c2 = random_generator.choice(unique_cameras, 2, replace=False) indices1 = np.where(np.logical_and(mask_y, camera_indices == c1))[0 ] indices2 = np.where(np.logical_and(mask_y, camera_indices == c2))[0 ] i1 = random_generator.choice(indices1) i2 = random_generator.choice(indices2) probe_indices.append(i1) gallery_indices.append(i2) return np.asarray(probe_indices), np.asarray(gallery_indices) 
eval|annotations|load|model def load_annotations(anno_file, return_dict): """Convert annotation JSON file.""" annotations = dict() annotations['image_ids'] = set([]) annotations['annos'] = dict() annotations['delta'] = 2 * np.array([0.01388152, 0.01515228, 0.01057665, 0.01417709, 0.01497891, 0.01402144, 0.03909642, 0.03686941, 0.01981803, 0.03843971, 0.03412318, 0.02415081, 0.01291456, 0.01236173] ) try: annos = json.load(open(anno_file, 'r')) except Exception: return_dict['error' ] = 'Annotation file does not exist or is an invalid JSON file.' exit(return_dict['error']) for anno in annos: annotations['image_ids'].add(anno['image_id']) annotations['annos'][anno['image_id']] = dict() annotations['annos'][anno['image_id']]['human_annos'] = anno[ 'human_annotations'] annotations['annos'][anno['image_id']]['keypoint_annos'] = anno[ 'keypoint_annotations'] return annotations 
MultiStepOptimizer|optimizers|gradients|apply|utils|thumt def _apply_gradients(): op = self._optimizer.apply_gradients(zip(grads, var_list), global_step, name) with tf.control_dependencies([op]): zero_ops = [] for var in var_list: grad_acc = self.get_slot(var, 'grad_acc') zero_ops.append(grad_acc.assign(tf.zeros_like(grad_acc), use_locking=self._use_locking)) zero_op = tf.group(*zero_ops) return tf.group(*[op, zero_op]) 
only|get|Node|parent|kaffe|graph def get_only_parent(self): if len(self.parents) != 1: raise KaffeError('Node (%s) expected to have 1 parent. Found %s.' % (self, len(self.parents))) return self.parents[0] 
utils|data|read def read_data(path, max_size=None): data_set = [] data = json.load(open(path, 'r')) counter = 0 for pair in data: post = pair[0][0] responses = pair[1] source_ids = [int(x) for x in post[0]] for response in responses: if not max_size or counter < max_size: counter += 1 if counter % 100000 == 0: print('    reading data pair %d' % counter) sys.stdout.flush() target_ids = [int(x) for x in response[0]] feat = response[1] desc_ids = [int(x) for x in response[2]] data_set.append([source_ids, target_ids, feat, desc_ids]) return data_set 
get|lottery|prune|name|by|function|functions def get_prune_function_by_name(name): perc_re = '(?P<percent>\\d+(\\.\\d*)?)' regexp_map = {'prune_global(?:_(?:{}|(?P<fc>fc)|(?P<nofc>nofc)))*'. format(perc_re): prune_global_x, 'zprune_global(?:_(?:{}|(?P<fc>fc)|(?P<nofc>nofc)))*'.format( perc_re): zprune_global_x, 'prune_to_global_{}'.format(perc_re): prune_to_global_x, 'prune_all_to_global_{}'.format(perc_re): prune_all_to_global_x, 'zprune_all_to_global_{}'.format(perc_re): zprune_all_to_global_x} regexp_map = {re.compile(k): v for k, v in regexp_map.items()} for regexp, func in regexp_map.items(): match = regexp.match(name) if match is None: continue percentage = float(match.group('percent')) kwargs = {k: v for k, v in match.groupdict().items() if v is not None and k != 'percent'}  def prune(names, weights, masks): return func(names, weights, masks, percentage, **kwargs) return prune return globals()[name] 
conceptnet|shuffle|sequences|GenerationDataLoader def shuffle_sequences(self, split='train', keys=None): if keys is None: keys = self.data[split].keys() for key in keys: if key in ['positive', 'negative']: continue idxs = list(range(len(self.data[split][key]))) random.shuffle(idxs) self.sequences[split][key] = self.sequences[split][key].index_select( 0, torch.LongTensor(idxs)) temp = [self.data[split][key][i] for i in idxs] self.data[split][key] = temp temp = [self.masks[split][key][i] for i in idxs] self.masks[split][key] = temp 
python|texar|io|load|config|utils def _load_config_python(fname): config = {} config_module = importlib.import_module(fname.rstrip('.py')) for key in dir(config_module): if not (key.startswith('__') and key.endswith('__')): config[key] = getattr(config_module, key) return config 
tangents|stax|array|neural|is def _is_array(x): return isinstance(x, np.ndarray) and hasattr(x, 'shape') and x.shape 
test|alexnet|testModelVariables|AlexnetV2Test|nets def testModelVariables(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) alexnet.alexnet_v2(inputs, num_classes) expected_names = ['alexnet_v2/conv1/weights', 'alexnet_v2/conv1/biases', 'alexnet_v2/conv2/weights', 'alexnet_v2/conv2/biases', 'alexnet_v2/conv3/weights', 'alexnet_v2/conv3/biases', 'alexnet_v2/conv4/weights', 'alexnet_v2/conv4/biases', 'alexnet_v2/conv5/weights', 'alexnet_v2/conv5/biases', 'alexnet_v2/fc6/weights', 'alexnet_v2/fc6/biases', 'alexnet_v2/fc7/weights', 'alexnet_v2/fc7/biases', 'alexnet_v2/fc8/weights', 'alexnet_v2/fc8/biases'] model_variables = [v.op.name for v in slim.get_model_variables()] self.assertSetEqual(set(model_variables), set(expected_names)) 
distributions|regression|DMatrixVariateNormal|controller|value|shape def _value_shape(self): return tf.shape(self.mean)[-2:] 
example|converter|to|input def to_example(dictionary): """ Convert python dictionary to tf.train.Example """ features = {} for k, v in six.iteritems(dictionary): if not v: raise ValueError('Empty generated field: %s', str((k, v))) if isinstance(v[0], six.integer_types): int64_list = tf.train.Int64List(value=v) features[k] = tf.train.Feature(int64_list=int64_list) elif isinstance(v[0], float): float_list = tf.train.FloatList(value=v) features[k] = tf.train.Feature(float_list=float_list) elif isinstance(v[0], six.string_types): bytes_list = tf.train.BytesList(value=v) features[k] = tf.train.Feature(bytes_list=bytes_list) else: raise ValueError( 'Value is neither an int nor a float; v: %s type: %s' % ( str(v[0]), str(type(v[0])))) return tf.train.Example(features=tf.train.Features(feature=features)) 
prepare|process|corpora|data def process_corpora(args, corpora, output_corpora, sizes): for corpus in corpora: if corpus is not None: corpus[:] = process_corpus(corpus, args) if any(sizes): logging.info('splitting files') split_corpora = split_corpus(corpora[-1], sizes) for i, split_corpus_ in enumerate(split_corpora): if split_corpus_ is not None: corpora[i] = split_corpus_ for corpus in corpora: if corpus is not None: corpus[:] = filter_corpus(corpus, args) if args.subwords: if args.bpe_path: bpe_filenames = args.bpe_path else: bpe_filenames = [os.path.join(args.output_dir, 'bpe.{}'.format( ext)) for ext in args.extensions] train_corpus = corpora[-1] for ext, filename, bpe_filename, size in zip(args.extensions, train_corpus, bpe_filenames, args.vocab_size): if ext in args.subwords: create_subwords(filename, bpe_filename, size) for corpus in corpora: if corpus is None: continue filenames = [(apply_subwords(filename, bpe_filename) if ext in args.subwords else filename) for ext, filename, bpe_filename in zip(args.extensions, corpus, bpe_filenames)] filenames = filter_corpus(filenames, args) corpus[:] = filenames for corpus, output_corpus in zip(corpora, output_corpora): if corpus is None: continue for filename, output_filename in zip(corpus, output_corpus): shutil.move(filename, output_filename) corpus[:] = output_corpus 
setattr|utils|DD def __setattr__(self, attr, value): assert attr not in ('__getstate__', '__setstate__', '__slots__') self[attr] = value 
ops|kron|classification|2d|eigen|product|utils|basis def eigen_basis_kron_product_2d(left, right, vec, transpose=False): if transpose: return math_ops.matmul(left, math_ops.matmul(vec, right), transpose_a=True) else: return math_ops.matmul(left, math_ops.matmul(vec, right, transpose_b=True)) 
devtools|cleverhans|NumpyDocString|tests|parse|docscrape|summary def _parse_summary(self): """Grab signature (if given) and summary""" summary = self._doc.read_to_next_empty_line() summary_str = '\n'.join([s.strip() for s in summary]) if re.compile('^([\\w. ]+=)?[\\w\\.]+\\(.*\\)$').match(summary_str): self['Signature'] = summary_str if not self._is_at_section(): self['Summary'] = self._doc.read_to_next_empty_line() elif re.compile('^[\\w]+\n[-]+').match(summary_str): self['Summary'] = '' self._doc.reset() else: self['Summary'] = summary if not self._is_at_section(): self['Extended Summary'] = self._read_to_next_section() 
fn|train|shapenet|input def input_fn(filelist, batch_size=16, buffer_size=10000): dataset = tf.data.TFRecordDataset(filelist) dataset = dataset.shuffle(buffer_size=buffer_size) dataset = dataset.map(parse_fn, num_parallel_calls=4) dataset = dataset.padded_batch(batch_size, padded_shapes=(None, INPUT_DIM + 1), padding_values=-1.0, drop_remainder=False) return dataset 
src|layers|call|Dense def _call(self, inputs): x = inputs if self.sparse_inputs: output = tf.sparse_tensor_dense_matmul(x, self.vars['weights']) else: output = tf.matmul(x, self.vars['weights']) if self.bias: output += self.vars['bias'] return self.act(output) 
kl|2d|dsnt def _kl_2d(p, q, eps=24): unsummed_kl = p * (tf.log(p + eps) - tf.log(q + eps)) kl_values = tf.reduce_sum(unsummed_kl, [-1, -2]) return kl_values 
texar|load|data|embedding|glove def load_glove(filename, vocab, word_vecs): """Loads embeddings in the glove text format in which each line is '<word-string> <embedding-vector>'. Dimensions of the embedding vector are separated with whitespace characters.  Args: filename (str): Path to the embedding file. vocab (dict): A dictionary that maps token strings to integer index. Tokens not in :attr:`vocab` are not read. word_vecs: A 2D numpy array of shape `[vocab_size, embed_dim]` which is updated as reading from the file.  Returns: The updated :attr:`word_vecs`. """ with gfile.GFile(filename) as fin: for line in fin: vec = line.strip().split() if len(vec) == 0: continue word, vec = vec[0], vec[1:] word = tf.compat.as_text(word) if word not in vocab: continue if len(vec) != word_vecs.shape[1]: raise ValueError('Inconsistent word vector sizes: %d vs %d' % (len(vec), word_vecs.shape[1])) word_vecs[vocab[word]] = np.array([float(v) for v in vec]) return word_vecs 
Network|face|detect|align|src|master|facenet|softmax @layer def softmax(self, target, axis, name=None): max_axis = tf.reduce_max(target, axis, keepdims=True) target_exp = tf.exp(target - max_axis) normalize = tf.reduce_sum(target_exp, axis, keepdims=True) softmax = tf.div(target_exp, normalize, name) return softmax 
rev2|task|pair|rand|Default def rand_rev2_pair(self, l, nclass): """Random data pair for reverse2 task. Total length should be <= l.""" inp = [(np.random.randint(nclass - 1) + 1, np.random.randint(nclass - 1 ) + 1) for _ in range(l // 2)] res = [i for i in reversed(inp)] return [x for p in inp for x in p], [x for p in res for x in p] 
xlnet|classification|loss|master|modeling def classification_loss(hidden, labels, n_class, initializer, scope, reuse= None, return_logits=False): """ Different classification tasks should use different scope names to ensure different dense layers (parameters) are used to produce the logits.  An exception will be in transfer learning, where one hopes to transfer the classification weights. """ with tf.variable_scope(scope, reuse=reuse): logits = tf.layers.dense(hidden, n_class, kernel_initializer= initializer, name='logit') one_hot_target = tf.one_hot(labels, n_class, dtype=hidden.dtype) loss = -tf.reduce_sum(tf.nn.log_softmax(logits) * one_hot_target, -1) if return_logits: return loss, logits return loss 
fc|td|alg|not|NetworkCreator|or|layer|layers def fc_layer(self, output_size, input_layer=None): with tf.variable_scope(self.network_name): with tf.variable_scope(self._get_name(), reuse=self._reuse): input_layer = self._config_input(input_layer) if len(input_layer.get_shape().as_list()) == 4: input_layer = self._make_flat(input_layer) input_size = input_layer.get_shape().as_list()[1] if output_size == -1: if self._flatting_dimension is None: raise ValueError( 'input_dimension unknown for unflatting fc_layer') output_size = self._flatting_dimension self._flatting_dimension = None init = tf.contrib.layers.variance_scaling_initializer(factor= 1.0, uniform=True) shape = [input_size, output_size] w_fc = tf.get_variable('w', shape=shape, dtype=tf.float32, initializer=init) init = tf.constant_initializer(0.0) shape = [output_size] b_fc = tf.get_variable('b', shape=shape, dtype=tf.float32, initializer=init) output = tf.matmul(input_layer, w_fc) + b_fc if self._print_shape: print(self._layer_number, 'fc', input_layer.get_shape(). as_list(), ' >> ', output.get_shape().as_list()) return self._config_output(output) 
texar|tf|modules|decoders|TrainingHelper|ids|sample|helpers|shape @property def sample_ids_shape(self): return tensor_shape.TensorShape([]) 
python|grpc|OpsTest|ops|test|seed|rl|master|foo|invalid|shape @tf.function(input_signature=[tf.TensorSpec(dim + [4, 3], tf.int32)]) def foo(x): return x 
path|run|get def _get_path(basename, trial, version, other_name=None): path = os.path.join(os.path.join(os.environ.get('DATA_DIR', 'gs://renda'), 'gnmt_results'), version, basename) if other_name: path = os.path.join(path, other_name) path = os.path.join(path, trial) return path 
BertModelTest|test|modeling|run|tester def run_tester(self, tester): config_and_inputs = tester.prepare_config_and_inputs() output_result = tester.create_bert_model(*config_and_inputs) tester.check_bert_model_output(output_result) output_result = tester.create_bert_for_masked_lm(*config_and_inputs) tester.check_bert_for_masked_lm_output(output_result) tester.check_loss_output(output_result) output_result = tester.create_bert_for_next_sequence_prediction(* config_and_inputs) tester.check_bert_for_next_sequence_prediction_output(output_result) tester.check_loss_output(output_result) output_result = tester.create_bert_for_pretraining(*config_and_inputs) tester.check_bert_for_pretraining_output(output_result) tester.check_loss_output(output_result) output_result = tester.create_bert_for_question_answering(* config_and_inputs) tester.check_bert_for_question_answering_output(output_result) tester.check_loss_output(output_result) output_result = tester.create_bert_for_sequence_classification(* config_and_inputs) tester.check_bert_for_sequence_classification_output(output_result) tester.check_loss_output(output_result) output_result = tester.create_bert_for_token_classification(* config_and_inputs) tester.check_bert_for_token_classification_output(output_result) tester.check_loss_output(output_result) 
BertModelTest|bert|test|init|master|BertModelTester|modeling def __init__(self, parent, batch_size=13, seq_length=7, is_training=True, use_input_mask=True, use_token_type_ids=True, vocab_size=99, hidden_size=32, num_hidden_layers=5, num_attention_heads=4, intermediate_size=37, hidden_act='gelu', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=16, initializer_range=0.02, scope=None): self.parent = parent self.batch_size = batch_size self.seq_length = seq_length self.is_training = is_training self.use_input_mask = use_input_mask self.use_token_type_ids = use_token_type_ids self.vocab_size = vocab_size self.hidden_size = hidden_size self.num_hidden_layers = num_hidden_layers self.num_attention_heads = num_attention_heads self.intermediate_size = intermediate_size self.hidden_act = hidden_act self.hidden_dropout_prob = hidden_dropout_prob self.attention_probs_dropout_prob = attention_probs_dropout_prob self.max_position_embeddings = max_position_embeddings self.type_vocab_size = type_vocab_size self.initializer_range = initializer_range self.scope = scope 
preprocess|xngen def preprocess(input_text, input_globals, input_path='codegen'): input_lines = input_text.splitlines() python_lines = ['from __future__ import print_function'] blank_lines = 0 last_line = '' last_indent = '' indent_stack = [('', '')] python_block_start = True for i, input_line in enumerate(input_lines): if input_line == '': blank_lines += 1 continue input_indent = extract_leading_whitespace(input_line) if python_block_start: assert input_indent.startswith(last_indent) extra_python_indent = input_indent[len(last_indent):] python_indent = indent_stack[-1][1] + extra_python_indent indent_stack.append((input_indent, python_indent)) assert input_indent.startswith(indent_stack[-1][0]) else: while not input_indent.startswith(indent_stack[-1][0]): del indent_stack[-1] python_block_start = False python_indent = indent_stack[-1][1] stripped_input_line = input_line.strip() if stripped_input_line.startswith('$' ) and not stripped_input_line.startswith('${'): if stripped_input_line.endswith(':'): python_block_start = True while blank_lines != 0: python_lines.append(python_indent + 'print(file=OUT_STREAM)') blank_lines -= 1 python_lines.append(python_indent + stripped_input_line.replace ('$', '')) else: assert input_line.startswith(python_indent) while blank_lines != 0: python_lines.append(python_indent + 'print(file=OUT_STREAM)') blank_lines -= 1 python_lines.append(python_indent + 'print(%s, file=OUT_STREAM)' % escape(input_line[len( python_indent):])) last_line = input_line last_indent = input_indent while blank_lines != 0: python_lines.append(python_indent + 'print(file=OUT_STREAM)') blank_lines -= 1 exec_globals = dict(input_globals) if sys.version_info > (3, 0): output_stream = io.StringIO() else: output_stream = io.BytesIO() exec_globals['OUT_STREAM'] = output_stream python_bytecode = compile('\n'.join(python_lines), input_path, 'exec') exec(python_bytecode, exec_globals) return output_stream.getvalue() 
json|data|read|utils|mscoco def mscoco_read_json(file_path, bleu_baseline=False): """Read the mscoco dataset  Args: file_path: path to the raw data, a string  Returns: sentence_sets: the sentence sets, a list of paraphrase lists """ print('Reading mscoco raw data .. ') print('  data path: %s' % file_path) with open(file_path, 'r') as fd: data = json.load(fd) print('%d sentences in total' % len(data['annotations'])) image_idx = set([d['image_id'] for d in data['annotations']]) paraphrases = {} for im in image_idx: paraphrases[im] = [] for d in tqdm(data['annotations']): im = d['image_id'] sent = d['caption'] paraphrases[im].append(word_tokenize(sent)) sentence_sets = [paraphrases[im] for im in paraphrases] if bleu_baseline: print('calculating bleu ... ') hypothesis = [s[0] for s in sentence_sets] references = [s[1:] for s in sentence_sets] bleu = corpus_bleu(references, hypothesis) print('bleu on the training set: %.4f' % bleu) return sentence_sets 
testEndPoints|test|nets|OverFeatTest|overfeat def testEndPoints(self): batch_size = 5 height, width = 231, 231 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = overfeat.overfeat(inputs, num_classes) expected_names = ['overfeat/conv1', 'overfeat/pool1', 'overfeat/conv2', 'overfeat/pool2', 'overfeat/conv3', 'overfeat/conv4', 'overfeat/conv5', 'overfeat/pool5', 'overfeat/fc6', 'overfeat/fc7', 'overfeat/fc8'] self.assertSetEqual(set(end_points.keys()), set(expected_names)) 
list|avod|as|BoxList|core|box|tensor|dict def as_tensor_dict(self, fields=None): """Retrieves specified fields as a dictionary of tensors.  Args: fields: (optional) list of fields to return in the dictionary. If None (default), all fields are returned.  Returns: tensor_dict: A dictionary of tensors specified by fields.  Raises: ValueError: if specified field is not contained in boxlist. """ tensor_dict = {} if fields is None: fields = self.get_all_fields() for field in fields: if not self.has_field(field): raise ValueError('boxlist must contain all specified fields') tensor_dict[field] = self.get_field(field) return tensor_dict 
matrix|vec|get|preprocess|word def get_word_vec(word, embeddings, OOV_dict=None): vector_size = embeddings.vector_size try: vec = embeddings.word_vec(word) except KeyError: vec = OOV_dict.get(word) if vec is None: vec = np.random.uniform(low=-1.0, high=1.0, size=vector_size) OOV_dict.update({word: vec}) return vec 
compute|output|sequence|deepctr|DynamicGRU|layers|shape def compute_output_shape(self, input_shape): rnn_input_shape = input_shape[0] if self.return_sequence: return rnn_input_shape else: return None, 1, rnn_input_shape[2] 
bert|v2|classifier|main def main(_): """ Builds the model and runs. """ if FLAGS.distributed: import horovod.tensorflow as hvd hvd.init() tf.logging.set_verbosity(tf.logging.INFO) tx.utils.maybe_create_dir(FLAGS.output_dir) num_train_data = config_data.num_train_data if FLAGS.distributed: config_data.train_hparam['dataset']['num_shards'] = hvd.size() config_data.train_hparam['dataset']['shard_id'] = hvd.rank() config_data.train_hparam['batch_size'] //= hvd.size() train_dataset = tx.data.TFRecordData(hparams=config_data.train_hparam) eval_dataset = tx.data.TFRecordData(hparams=config_data.eval_hparam) test_dataset = tx.data.TFRecordData(hparams=config_data.test_hparam) iterator = tx.data.FeedableDataIterator({'train': train_dataset, 'eval': eval_dataset, 'test': test_dataset}) batch = iterator.get_next() input_ids = batch['input_ids'] segment_ids = batch['segment_ids'] batch_size = tf.shape(input_ids)[0] input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(input_ids, 0)), axis=1) hparams = {'clas_strategy': 'cls_time'} model = tx.modules.BertClassifier(hparams=hparams) logits, preds = model(input_ids, input_length, segment_ids) accu = tx.evals.accuracy(batch['label_ids'], preds) loss = tf.losses.sparse_softmax_cross_entropy(labels=batch['label_ids'], logits=logits) global_step = tf.Variable(0, trainable=False) static_lr = config_downstream.lr['static_lr'] num_train_steps = int(num_train_data / config_data.train_batch_size * config_data.max_train_epoch) num_warmup_steps = int(num_train_steps * config_data.warmup_proportion) lr = model_utils.get_lr(global_step, num_train_steps, num_warmup_steps, static_lr) opt = tx.core.get_optimizer(global_step=global_step, learning_rate=lr, hparams=config_downstream.opt) if FLAGS.distributed: opt = hvd.DistributedOptimizer(opt) train_op = tf.contrib.layers.optimize_loss(loss=loss, global_step= global_step, learning_rate=None, optimizer=opt)  def _is_head(): if not FLAGS.distributed: return True return hvd.rank() == 0  def _train_epoch(sess): """Trains on the training set, and evaluates on the dev set periodically. """ iterator.restart_dataset(sess, 'train') fetches = {'train_op': train_op, 'loss': loss, 'batch_size': batch_size, 'step': global_step} while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'train'), tx.global_mode(): tf.estimator.ModeKeys.TRAIN} rets = sess.run(fetches, feed_dict) step = rets['step'] dis_steps = config_data.display_steps if _is_head() and dis_steps > 0 and step % dis_steps == 0: tf.logging.info('step:%d; loss:%f;' % (step, rets['loss'])) eval_steps = config_data.eval_steps if _is_head() and eval_steps > 0 and step % eval_steps == 0: _eval_epoch(sess) except tf.errors.OutOfRangeError: break  def _eval_epoch(sess): """Evaluates on the dev set. """ iterator.restart_dataset(sess, 'eval') cum_acc = 0.0 cum_loss = 0.0 nsamples = 0 fetches = {'accu': accu, 'loss': loss, 'batch_size': batch_size} while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'eval'), tx.context.global_mode(): tf.estimator. ModeKeys.EVAL} rets = sess.run(fetches, feed_dict) cum_acc += rets['accu'] * rets['batch_size'] cum_loss += rets['loss'] * rets['batch_size'] nsamples += rets['batch_size'] except tf.errors.OutOfRangeError: break tf.logging.info('eval accu: {}; loss: {}; nsamples: {}'.format( cum_acc / nsamples, cum_loss / nsamples, nsamples))  def _test_epoch(sess): """Does predictions on the test set. """ iterator.restart_dataset(sess, 'test') _all_preds = [] while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'test'), tx.context.global_mode(): tf.estimator. ModeKeys.PREDICT} _preds = sess.run(preds, feed_dict=feed_dict) _all_preds.extend(_preds.tolist()) except tf.errors.OutOfRangeError: break output_file = os.path.join(FLAGS.output_dir, 'test_results.tsv') with tf.gfile.GFile(output_file, 'w') as writer: writer.write('\n'.join(str(p) for p in _all_preds)) if FLAGS.distributed: bcast = hvd.broadcast_global_variables(0) session_config = tf.ConfigProto() if FLAGS.distributed: session_config.gpu_options.visible_device_list = str(hvd.local_rank()) with tf.Session(config=session_config) as sess: sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer()) if FLAGS.distributed: bcast.run() saver = tf.train.Saver() if FLAGS.checkpoint: saver.restore(sess, FLAGS.checkpoint) iterator.initialize_dataset(sess) if FLAGS.do_train: for i in range(config_data.max_train_epoch): _train_epoch(sess) saver.save(sess, FLAGS.output_dir + '/model.ckpt') if FLAGS.do_eval: _eval_epoch(sess) if FLAGS.do_test: _test_epoch(sess) 
BiInteractionPooling|interaction|deepctr|layers|call def call(self, inputs, **kwargs): if K.ndim(inputs) != 3: raise ValueError( 'Unexpected inputs dimensions %d, expect to be 3 dimensions' % K.ndim(inputs)) concated_embeds_value = inputs square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis=1, keep_dims=True)) sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True) cross_term = 0.5 * (square_of_sum - sum_of_square) return cross_term 
bias|1x1|conv|no|make|cnn|helpers def make_conv_1x1_no_bias(op_name, in_tensor, filters, strides=(1, 1, 1, 1), padding='VALID', weight_decay=0.0005, stddev=0.1): return make_conv_no_bias(op_name, in_tensor, 1, 1, filters, strides, padding, weight_decay, stddev) 
init|tfprocess|TFProcess def init(self, batch_size, macrobatch=1, gpus_num=None, logbase='leelalogs'): self.batch_size = batch_size self.macrobatch = macrobatch self.logbase = logbase self.planes = tf.placeholder(tf.string, name='in_planes') self.probs = tf.placeholder(tf.string, name='in_probs') self.winner = tf.placeholder(tf.string, name='in_winner') planes = tf.decode_raw(self.planes, tf.uint8) probs = tf.decode_raw(self.probs, tf.float32) winner = tf.decode_raw(self.winner, tf.float32) planes = tf.cast(planes, self.model_dtype) planes = tf.reshape(planes, (batch_size, 18, 19 * 19)) probs = tf.reshape(probs, (batch_size, 19 * 19 + 1)) winner = tf.reshape(winner, (batch_size, 1)) if gpus_num is None: gpus_num = self.gpus_num self.init_net(planes, probs, winner, gpus_num) 
gpu|master|train|xlnet def train(ps_device): train_input_fn, record_info_dict = data_utils.get_input_fn(tfrecord_dir =FLAGS.record_info_dir, split='train', bsz_per_host=FLAGS. train_batch_size, seq_len=FLAGS.seq_len, reuse_len=FLAGS.reuse_len, bi_data=FLAGS.bi_data, num_hosts=1, num_core_per_host=1, perm_size= FLAGS.perm_size, mask_alpha=FLAGS.mask_alpha, mask_beta=FLAGS. mask_beta, uncased=FLAGS.uncased, num_passes=FLAGS.num_passes, use_bfloat16=FLAGS.use_bfloat16, num_predict=FLAGS.num_predict) tf.logging.info('num of batches {}'.format(record_info_dict['num_batch'])) bsz_per_core = FLAGS.train_batch_size // FLAGS.num_core_per_host params = {'batch_size': FLAGS.train_batch_size} train_set = train_input_fn(params) example = train_set.make_one_shot_iterator().get_next() if FLAGS.num_core_per_host > 1: examples = [{} for _ in range(FLAGS.num_core_per_host)] for key in example.keys(): vals = tf.split(example[key], FLAGS.num_core_per_host, 0) for device_id in range(FLAGS.num_core_per_host): examples[device_id][key] = vals[device_id] else: examples = [example] tower_mems, tower_losses, tower_new_mems, tower_grads_and_vars = [], [], [ ], [] for i in range(FLAGS.num_core_per_host): reuse = True if i > 0 else None with tf.device(assign_to_gpu(i, ps_device)), tf.variable_scope(tf. get_variable_scope(), reuse=reuse): mems_i = {} if FLAGS.mem_len: mems_i['mems'] = create_mems_tf(bsz_per_core) loss_i, new_mems_i, grads_and_vars_i = single_core_graph( is_training=True, features=examples[i], mems=mems_i) tower_mems.append(mems_i) tower_losses.append(loss_i) tower_new_mems.append(new_mems_i) tower_grads_and_vars.append(grads_and_vars_i) if len(tower_losses) > 1: loss = tf.add_n(tower_losses) / len(tower_losses) grads_and_vars = average_grads_and_vars(tower_grads_and_vars) else: loss = tower_losses[0] grads_and_vars = tower_grads_and_vars[0] train_op, learning_rate, gnorm = model_utils.get_train_op(FLAGS, None, grads_and_vars=grads_and_vars) global_step = tf.train.get_global_step() tower_mems_np = [] for i in range(FLAGS.num_core_per_host): mems_i_np = {} for key in tower_mems[i].keys(): mems_i_np[key] = initialize_mems_np(bsz_per_core) tower_mems_np.append(mems_i_np) saver = tf.train.Saver() gpu_options = tf.GPUOptions(allow_growth=True) model_utils.init_from_checkpoint(FLAGS, global_vars=True) with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sess: sess.run(tf.global_variables_initializer()) fetches = [loss, tower_new_mems, global_step, gnorm, learning_rate, train_op] total_loss, prev_step = 0.0, -1 while True: feed_dict = {} for i in range(FLAGS.num_core_per_host): for key in tower_mems_np[i].keys(): for m, m_np in zip(tower_mems[i][key], tower_mems_np[i] [key]): feed_dict[m] = m_np fetched = sess.run(fetches, feed_dict=feed_dict) loss_np, tower_mems_np, curr_step = fetched[:3] total_loss += loss_np if curr_step > 0 and curr_step % FLAGS.iterations == 0: curr_loss = total_loss / (curr_step - prev_step) tf.logging.info( '[{}] | gnorm {:.2f} lr {:8.6f} | loss {:.2f} | pplx {:>7.2f}, bpc {:>7.4f}' .format(curr_step, fetched[-3], fetched[-2], curr_loss, math.exp(curr_loss), curr_loss / math.log(2))) total_loss, prev_step = 0.0, curr_step if curr_step > 0 and curr_step % FLAGS.save_steps == 0: save_path = os.path.join(FLAGS.model_dir, 'model.ckpt') saver.save(sess, save_path) tf.logging.info('Model saved in path: {}'.format(save_path)) if curr_step >= FLAGS.train_steps: break 
texar|SequentialLayer|trainable|core|layers|weights @property def trainable_weights(self): return self._trainable_weights 
select|train|data def select_data(temps, poses, all_depths_ir, all_depths_rgb, Kinv, xy, target='rgb'): sel_xyzt = [] sel_deltas = [] for p in poses: if target == 'rgb': depth_target = all_depths_rgb[p, temps[0]] elif target == 'ir': depth_target = all_depths_ir[p, temps[0]] d_target = depth_target[xy[:, (1)], xy[:, (0)]] for t in temps: depth_ir = all_depths_ir[p, t] d_ir = depth_ir[xy[:, (1)], xy[:, (0)]] xyz = sensor_unproject(xy, d_ir, Kinv) xyzt = np.empty((xyz.shape[0], 4), dtype=np.float32) xyzt[:, :3] = xyz xyzt[:, (3)] = t delta = d_target - d_ir mask = d_ir > 0.0 """ plt.imshow(depth_rgb - depth_ir) plt.plot(xy[:,0][mask], xy[:,1][mask], 'k+') plt.colorbar() plt.show() """ sel_xyzt.append(xyzt[mask]) sel_deltas.append(delta[mask]) sel_xyzt = np.concatenate(sel_xyzt) sel_deltas = np.concatenate(sel_deltas) return sel_xyzt, sel_deltas 
wolf|plato|reinforcement|WoLFPHCPolicy|component|agent|phc|action|policy|dialogue|learning|decode def decode_action(self, action_enc, system=True): """ Decode the action, given the role. Note that does not have to match the agent's role, as the agent may be decoding another agent's action (e.g. a system decoding the previous user act).  :param action_enc: action encoding to be decoded :param system: whether the role whose action we are decoding is a 'system' :return: the decoded action """ if system: if action_enc < len(self.dstc2_acts_sys): return [DialogueAct(self.dstc2_acts_sys[action_enc], [])] if action_enc < len(self.dstc2_acts_sys) + len(self. system_requestable_slots): return [DialogueAct('request', [DialogueActItem(self. system_requestable_slots[action_enc - len(self. dstc2_acts_sys)], Operator.EQ, '')])] if action_enc < len(self.dstc2_acts_sys) + len(self. system_requestable_slots) + len(self.requestable_slots): index = action_enc - len(self.dstc2_acts_sys) - len(self. system_requestable_slots) return [DialogueAct('inform', [DialogueActItem(self. requestable_slots[index], Operator.EQ, '')])] else: if action_enc < len(self.dstc2_acts_usr): return [DialogueAct(self.dstc2_acts_usr[action_enc], [])] if action_enc < len(self.dstc2_acts_usr) + len(self.requestable_slots): return [DialogueAct('request', [DialogueActItem(self. requestable_slots[action_enc - len(self.dstc2_acts_usr)], Operator.EQ, '')])] if action_enc < len(self.dstc2_acts_usr) + len(self.requestable_slots ) + len(self.system_requestable_slots): return [DialogueAct('inform', [DialogueActItem(self. system_requestable_slots[action_enc - len(self. dstc2_acts_usr) - len(self.requestable_slots)], Operator.EQ, '')])] print( 'WoLF-PHC dialogue_policy ({0}) policy action decoder warning: Selecting repeat() action (index: {1})!' .format(self.agent_role, action_enc)) return [DialogueAct('repeat', [])] 
spatial|SAFA|net def SAFA(x_sat, x_grd, keep_prob, dimension, trainable): vgg_grd = VGG16() grd_local = vgg_grd.VGG16_conv(x_grd, keep_prob, trainable, 'VGG_grd') grd_local = tf.nn.max_pool(grd_local, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') batch, g_height, g_width, channel = grd_local.get_shape().as_list() grd_w = spatial_aware(grd_local, dimension, trainable, name='spatial_grd') grd_local = tf.reshape(grd_local, [-1, g_height * g_width, channel]) grd_global = tf.einsum('bic, bid -> bdc', grd_local, grd_w) grd_global = tf.reshape(grd_global, [-1, dimension * channel]) vgg_sat = VGG16() sat_local = vgg_sat.VGG16_conv(x_sat, keep_prob, trainable, 'VGG_sat') sat_local = tf.nn.max_pool(sat_local, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') batch, s_height, s_width, channel = sat_local.get_shape().as_list() sat_w = spatial_aware(sat_local, dimension, trainable, name='spatial_sat') sat_local = tf.reshape(sat_local, [-1, s_height * s_width, channel]) sat_global = tf.einsum('bic, bid -> bdc', sat_local, sat_w) sat_global = tf.reshape(sat_global, [-1, dimension * channel]) return tf.nn.l2_normalize(sat_global, dim=1), tf.nn.l2_normalize(grd_global , dim=1) 
get|vgg11|prune|PruneVgg11|related|parameters def _get_parameters_related(self, weight_name, cut_channels): parameters = 0 last_layer_name, next_layer_name = self._get_last_and_next_layer_name( weight_name) parameters += get_parameters(self.weights_dict, weight_name, cut_channels, last_layer_name) if next_layer_name is not None: parameters += get_parameters(self.weights_dict, next_layer_name, cut_channels, weight_name) return parameters 
sn|celeba|sgan|imread|128|t def imread(f): x = misc.imread(f) x = misc.imresize(x, (img_dim, img_dim)) return x.astype(np.float32) / 255 * 2 - 1 
models|lenet5 def lenet5(num_classes, activation_fn=tf.nn.relu, w_init=slim.initializers. variance_scaling_initializer(), name='lenet5'):  @with_end_points def net(inputs, train=True): with slim.arg_scope([slim.conv2d], activation_fn=activation_fn, weights_initializer=w_init, padding='VALID'), slim.arg_scope([ slim.conv2d, slim.max_pool2d], data_format='NCHW'): net = slim.conv2d(inputs, 32, 5) net = slim.max_pool2d(net, 2) net = slim.conv2d(net, 64, 5) net = slim.max_pool2d(net, 2) net = slim.flatten(net) with slim.arg_scope([slim.fully_connected], activation_fn= activation_fn, weights_initializer=w_init): net = slim.fully_connected(net, 512) logits = slim.fully_connected(net, num_classes, activation_fn= None, scope='logits') return logits return tf.make_template(name, net) 
child|enas|get|strides|cifar1|MicroChild|micro def _get_strides(self, stride): """ Args: x: tensor of shape [N, H, W, C] or [N, C, H, W] """ if self.data_format == 'NHWC': return [1, stride, stride, 1] elif self.data_format == 'NCHW': return [1, 1, stride, stride] else: raise ValueError("Unknown data_format '{0}'".format(self.data_format)) 
bert|tokenization|pytorch|is|pretrained|whitespace def _is_whitespace(char): """Checks whether `chars` is a whitespace character.""" if char == ' ' or char == '\t' or char == '\n' or char == '\r': return True cat = unicodedata.category(char) if cat == 'Zs': return True return False 
3d|81|model|func|convbn def convbn_3d(input, out_planes, kernel_size, stride): seq = Conv3D(out_planes, kernel_size, stride, 'same', use_bias=False)(input ) seq = BatchNormalization()(seq) return seq 
la|init|heart|CreateOnehotLabel def __init__(self, num_classes): self.num_classes = num_classes 
set|description|noisematrix|NoiseMatrix def set_description(self, description): self.description = description 
ipa2binf|utils|ipa def ipa2binf(ipa, binf2phone, try_merge_diphtongs=False): binf = np.empty((len(ipa), len(binf2phone.index)), np.float32) for k, phone in enumerate(ipa): if phone in binf2phone.columns: binf_vec = binf2phone[phone].values elif len(phone) > 1 and try_merge_diphtongs: try: binf_vec = np.zeros(len(binf2phone.index), np.int) for p in phone: binf_vec = np.logical_or(binf_vec, binf2phone[p].values ).astype(np.float32) except KeyError: raise IPAError(phone) else: raise IPAError(phone) binf[(k), :] = binf_vec return binf 
get|CheckerboardGenerator|batch|data|loader def get_batch(self): batch = [] for i in range(self.batch_size): point = (np.random.rand(2) - 0.5) * self.eps_noise num = 4 if self.alternate else 5 center = self.centers[i % num] point[0] += center[0] point[1] += center[1] batch.append(point) batch = np.array(batch, dtype='float32') batch = self.float_tensor(batch) batch = batch[(torch.randperm(batch.size(0))), :] return batch 
tangents|predict|fns|neural|eigen|transform def transform(fn): """Generates a transform given a function on the eigenvalues."""  def _(vec, dt): return np.einsum('ji,i,ki,k...->j...', evecs, fn(evals, dt), evecs, vec, optimize=True) return _ 
disk|image|from|reader|images|read|deeplab|segment|resnet def read_images_from_disk(input_queue, input_size, random_scale, random_mirror, seed, n_classes, adapt): """Read one image and its corresponding mask with optional pre-processing.  Args: input_queue: tf queue with paths to the image and its mask. input_size: a tuple with (height, width) values. If not given, return images of original size. random_scale: whether to randomly scale the images prior to random crop. random_mirror: whether to randomly mirror the images prior to random crop. seed: Random seed. n_classes: Total number of classes (including background) in the dataset. adapt: Whether to obtain adapted ground truth cues (True/ False).  Returns: Three tensors: the decoded image, approximate ground-truth mask and binarized category vector. """ img_contents = tf.read_file(input_queue[0]) img = tf.image.decode_jpeg(img_contents, channels=3) img_r, img_g, img_b = tf.split(axis=2, num_or_size_splits=3, value=img) img = tf.cast(tf.concat(axis=2, values=[img_b, img_g, img_r]), dtype=tf .float32) img -= IMG_MEAN label = tf.py_func(get_localization_cues, [input_queue[1], input_queue[ 2], input_queue[3], n_classes, adapt], [tf.int32]) shape = tf.py_func(get_label_shape, label, [tf.int64]) shape = tf.to_int32(tf.reshape(shape, [3])) label = tf.to_int32(tf.reshape(label, shape)) catg = tf.py_func(get_binarized_label_vector, [input_queue[3], n_classes], [tf.float32]) catg = tf.to_float(tf.reshape(catg, [n_classes - 1])) catg.set_shape(n_classes - 1) if input_size is not None: h, w = input_size if random_scale: img, label = image_scaling(img, label, seed) if random_mirror: img, label = image_mirroring(img, label, seed) img, label = random_crop_and_pad_image_and_labels(img, label, h, w, seed) return img, label, catg 
similarities|models|dot|product|vkge def dot_product(x1, x2, axis=1): """ Dot Product.  .. math:: L = \\sum_i x1_i x2_i  :param x1: First term. :param x2: Second term. :param axis: Reduction Indices. :return: Similarity Value. """ similarity = tf.reduce_sum(x1 * x2, axis=axis) return similarity 
device|official|test|GetDistributionStrategyTest|distribution|strategy|one|misc|utils|gpu def test_one_device_strategy_gpu(self): ds = distribution_utils.get_distribution_strategy(1) self.assertEquals(ds.num_replicas_in_sync, 1) self.assertEquals(len(ds.extended.worker_devices), 1) self.assertIn('GPU', ds.extended.worker_devices[0]) 
size|wrong|test|ShuffleBufferTest|shufflebuffer def test_wrong_size(self): sb = ShuffleBuffer(3, 1) try: sb.insert_or_replace(b'1') assert False except: pass 
ops|optimizer|classification|init|NGOptimizer def __init__(self, learning_rate, damping, layer_collection, cov_ema_decay= None, scale_ema_decay=None, var_list=None, momentum=0.0, momentum_type= 'regular', norm_constraint=None, name='NG', estimation_mode='gradients', colocate_gradients_with_ops=False, cov_devices=None, inv_devices=None, opt_type='ekfac'): variables = var_list if variables is None: variables = tf_variables.trainable_variables() self.variables = variables self.damping = damping momentum_type = str(momentum_type).lower() legal_momentum_types = ['regular', 'adam'] if momentum_type not in legal_momentum_types: raise ValueError('Unsupported momentum type {}. Must be one of {}.' .format(momentum_type, legal_momentum_types)) if momentum_type != 'regular' and norm_constraint is not None: raise ValueError( "Update clipping is only supported with momentumtype 'regular'.") self._momentum = momentum self._momentum_type = momentum_type self._norm_constraint = norm_constraint opt_type = str(opt_type).lower() legal_opt_type = ['kfac', 'ekfac', 'diag'] if opt_type not in legal_opt_type: raise ValueError('Unsupported optimization type {}. Must be one of {}.' .format(opt_type, legal_opt_type)) self._opt_type = opt_type self._batch_size = array_ops.shape(layer_collection.losses[0].inputs)[0] self._losses = layer_collection.losses with variable_scope.variable_scope(name): if self._opt_type == 'ekfac': self._fisher_est = est.FisherEstimator(variables, cov_ema_decay, scale_ema_decay, damping, layer_collection, estimation_mode =estimation_mode, colocate_gradients_with_ops= colocate_gradients_with_ops, cov_devices=cov_devices, inv_devices=inv_devices, distribution='emvg') elif self._opt_type == 'kfac': self._fisher_est = est.FisherEstimator(variables, cov_ema_decay, scale_ema_decay, damping, layer_collection, estimation_mode =estimation_mode, colocate_gradients_with_ops= colocate_gradients_with_ops, cov_devices=cov_devices, inv_devices=inv_devices, distribution='mvg') elif self._opt_type == 'diag': self._fisher_est = est.FisherEstimator(variables, cov_ema_decay, scale_ema_decay, damping, layer_collection, estimation_mode =estimation_mode, colocate_gradients_with_ops= colocate_gradients_with_ops, cov_devices=cov_devices, inv_devices=inv_devices, distribution='diag') else: raise NotImplementedError() self.cov_update_op = self._fisher_est.cov_update_op self.eigen_basis_update_op = self._fisher_est.eigen_basis_update_op self.scale_update_op = self._fisher_est.scale_update_op self.inv_update_op = self._fisher_est.inv_update_op self.inv_update_dict = self._fisher_est.inv_updates_dict self.re_init_scale_op = self._fisher_est.re_init_scale_op self.re_init_kfac_scale_op = self._fisher_est.re_init_kfac_scale_op super(NGOptimizer, self).__init__(learning_rate, name=name) 
BiGRU|src|load|relation|network|emb def load_relation_emb(self, path): if path.endswith('npy'): return np.load(path) context = FileUtil.readFile(path) rel_embedding = [] for t in context: data = t.split('\t') data = [float(x) for x in data] rel_embedding.append(data) return np.array(rel_embedding, dtype=np.float32) 
attack|should|confidence|continue|high def should_continue(cond, i, x, r): return tf.logical_and(cond, tf.less(i, max_iter)) 
triplet|models|create|items|score|Triplet def _create_score_items(self): logger.debug('--> Define Triplet ranking scores') users = tf.expand_dims(self.anchors, 1) items = tf.expand_dims(self.item_embeddings, 0) self.scores = -tf.reduce_sum(tf.squared_difference(users, items), axis= 2, name='scores') 
ops|transformation|apply|classification|estimator|FisherEstimator def _apply_transformation(self, vecs_and_vars, transform): vecs = utils.SequenceDict((var, vec) for vec, var in vecs_and_vars) trans_vecs = utils.SequenceDict() for params, fb in self._layers.fisher_blocks.items(): trans_vecs[params] = transform(fb, vecs[params]) return [(trans_vecs[var], var) for _, var in vecs_and_vars] 
W2|calc|w2|dloss|model def calc_dloss(self, x, y, tx, ty, ux, vy, config): d_loss = -torch.mean(ux + vy) if config.ineq: d_loss += losses.ineq_loss(x, y, ux, vy, self.cost, config.lambda_ineq) if config.ineq_interp: d_loss += losses.calc_interp_ineq(x, y, self.phi, self.psi, self. cost, config.lambda_ineq, losses.ineq_loss) if config.eq_phi: d_loss += losses.calc_eq(x, tx, self.phi, self.psi, self.cost, config.lambda_eq) if config.eq_psi: d_loss += losses.calc_eq(ty, y, self.phi, self.psi, self.cost, config.lambda_eq) if config.lambda_eps > 0.0: d_loss += config.lambda_eps * torch.mean(torch.clamp(self.psi(y), min=0) ** 2) return d_loss 
large|rows|core|official|test|io|file|BaseTest|data|low|utils def test_large_rows_low_core(self): self._test_sharding(**_TEST_CASES[2]) 
init|net|DenseFuseNet|densefuse def __init__(self, model_pre_path): self.encoder = Encoder(model_pre_path) self.decoder = Decoder(model_pre_path) 
sn|celeba|sgan|imread|64|t def imread(f): x = misc.imread(f) x = misc.imresize(x, (img_dim, img_dim)) return x.astype(np.float32) / 255 * 2 - 1 
fisher|ops|classification|covariance|factors|initializer def covariance_initializer(shape, dtype, partition_info=None): if INIT_COVARIANCES_AT_ZERO: return array_ops.diag(array_ops.zeros(shape[0], dtype)) return array_ops.diag(array_ops.ones(shape[0], dtype)) 
gym|iter|pycolab|Palette|engine def __iter__(self): return iter(self._legal_characters) 
mnist|get|data|loader def get_mnist_loader(config, batch_size, train=True): tf = transforms.Compose([transforms.Scale(16), transforms.CenterCrop(16 ), transforms.ToTensor(), transforms.Lambda(lambda x: 2 * x - 1)]) mnist = datasets.MNIST(root=config.mnist_path, train=train, download= True, transform=tf) return RealDataGenerator(torch.utils.data.DataLoader(dataset=mnist, batch_size=batch_size, shuffle=True, num_workers=4)) 
demo|play|dc|svae|env def play(env, policy, num_episodes, num_randomized=0, scaled_policy_params= None, rnd_params=None, resample_policy_params=True): if scaled_policy_params is not None: assert rnd_params is None or len(rnd_params) == 0 or len( scaled_policy_params) == len(rnd_params) do_viz = 'Viz' in env.spec.id and env.spec.id.startswith(('Yumi', 'Franka', 'Sawyer')) and hasattr(policy, 'controller') and hasattr( policy.controller, 'waypts') tgt_viz_ids = [] if do_viz: import pybullet waypts = policy.controller.waypts for i in range(waypts.shape[0]): alph = float(i + 1) / waypts.shape[0] tgt_viz_ids.append(env.robot.create_visual_area(pybullet. GEOM_CYLINDER, [0, 0, 0], radius=0.03, rgba=[0, 1, 1, alph])) for epsd in range(num_episodes): if scaled_policy_params is not None: params = policy.unscale_params(scaled_policy_params[epsd]) policy.set_params(params) print('loaded policy') if rnd_params is not None and len(rnd_params) > 0: assert num_randomized == 0 print('setting randomized params', rnd_params[epsd]) env.set_randomize(rnd_params[epsd], debug=True) elif resample_policy_params: policy.resample_params() print('resampled policy params') policy.print() if do_viz: show_waypts(env, policy, tgt_viz_ids) for rnd in range(max(1, num_randomized)): obs = env.reset() if num_randomized > 0: env.randomize(debug=True) step = 0 while True: action = policy.get_action(obs, t=step) next_obs, rwd, done, info = env.step(action) if done: done_reward = info['done_reward'] done_badfrac = info['done_badfrac'] msg = ( 'Play epsd #{:d} done_reward {:0.4f} done_badfrac {:0.2f}' ) logging.info(msg.format(epsd, done_reward, done_badfrac)) if done_badfrac <= 0: input('Not bad! Press Enter to continue') break obs = next_obs step += 1 
AddModel|setup|imagenet def AddModel(name, url, model_filename, image_size, label_filename, input_tensor, logit, prob, shape): global model_params param = {} param['url'] = url param['model_filename'] = model_filename param['size'] = image_size param['input'] = input_tensor param['logit'] = logit param['prob'] = prob param['shape'] = shape param['label_filename'] = label_filename param['name'] = name model_params[name] = param 
embed|texar|get|MemNetBase|fn|modules|memory|network|default def get_default_embed_fn(self, memory_size, embed_fn_hparams): """Creates a default embedding function. Can be used for A, C, or B operation.  For B operation (i.e., query_embed_fn), :attr:`memory_size` must be 1.  The function is a combination of both memory embedding and temporal embedding, with the combination method specified by "combine_mode" in the `embed_fn_hparams`.  .. role:: python(code) :language: python  Args: embed_fn_hparams (dict or HParams): Hyperparameter of the embedding function. See :func:`~texar.modules.default_memnet_embed_fn` for details.  Returns: A tuple `(embed_fn, memory_dim)`, where  - **`memory_dim`** is the dimension of memory entry embedding,             inferred from :attr:`embed_fn_hparams`.  - If `combine_mode` == 'add', `memory_dim` is the                 embedder dimension. - If `combine_mode` == 'concat', `memory_dim` is the sum                 of the memory embedder dimension and the temporal embedder                 dimension.  - **`embed_fn`** is an embedding function that takes in memory             and returns memory embedding.             Specifically, the function has signature             :python:`memory_embedding= embed_fn(memory=None, soft_memory=None)`            where one of `memory` and `soft_memory` is provided (but not both).  Args: memory: An `int` Tensor of shape `[batch_size, memory_size]` containing memory indexes used for embedding lookup. soft_memory: A Tensor of shape `[batch_size, memory_size, raw_memory_dim]` containing soft weights used to mix the embedding vectors.  Returns: A Tensor of shape `[batch_size, memory_size, memory_dim]` containing the memory entry embeddings.  """ embedder = WordEmbedder(vocab_size=self._raw_memory_dim, hparams= embed_fn_hparams['embedding']) temporal_embedder = PositionEmbedder(position_size=memory_size, hparams =embed_fn_hparams['temporal_embedding']) combine = embed_fn_hparams['combine_mode'] if combine == 'add': if embedder.dim != temporal_embedder.dim: raise ValueError( '`embedding` and `temporal_embedding` must have the same dimension for "add" combination.' ) memory_dim = embedder.dim elif combine == 'concat': memory_dim = embedder.dim + temporal_embedder.dim  def _embed_fn(memory, soft_memory, mode=None): if memory is None and soft_memory is None: raise ValueError('Either `memory` or `soft_memory` is required.') if memory is not None and soft_memory is not None: raise ValueError( 'Must not specify `memory` and `soft_memory` at the same time.' ) embedded_memory = embedder(ids=memory, soft_ids=soft_memory, mode=mode) temporal_embedded = temporal_embedder(sequence_length=tf.constant([ memory_size]), mode=mode) temporal_embedded = tf.tile(temporal_embedded, [tf.shape( embedded_memory)[0], 1, 1]) if combine == 'add': return tf.add(embedded_memory, temporal_embedded) elif combine == 'concat': return tf.concat([embedded_memory, temporal_embedded], axis=-1) else: raise ValueError('Unknown combine method: {}'.format(combine)) return _embed_fn, memory_dim 
log2|helper def log2(x): return logBase(x, 2) 
restart|DialogueManagerGeneric|plato|component|agent|generic|manager|dialogue def restart(self, args): """ Restart the relevant structures or variables, e.g. at the beginning of a new dialogue.  :return: Nothing """ self.DSTracker.initialize(args) self.policy.restart(args) self.dialogue_counter += 1 
prefab|gym|board|on|sprites|parts|pycolab|MazeWalker|the @property def on_the_board(self): """True iff the `MazeWalker`'s "virtual position" is on the game board.""" return self._on_board(self._virtual_row, self._virtual_col) 
NBatchLogger|end|on|model|batch def on_batch_end(self, batch, logs={}): self.seen += logs.get('size', 0) self.losses.append(logs.get('loss')) 
chunks|get|parse def get_chunks(data_prefix): return glob.glob(data_prefix + '*.gz') 
size|state|attention|model|PCGNWrapper|PCGN @property def state_size(self): """The `state_size` property of `PCGNWrapper`. Returns: An `PCGNWrapperState` tuple containing shapes used by this object. """ return PCGNWrapperState(cell_state=self._cell.state_size, time= tensor_shape.TensorShape([]), attention=self._attention_layer_size, user_feat_mem=self._user_feat_mem_units, alignments=self. _item_or_tuple(a.alignments_size for a in self. _attention_mechanisms), alignment_history=self._item_or_tuple(() for _ in self._attention_mechanisms)) 
GRUCell|rnn|call|translate def call(self, inputs, state): inputs = tf.concat(inputs, axis=1) input_size = inputs.shape[1] state_size = state.shape[1] dtype = inputs.dtype with tf.variable_scope('gates'): bias_initializer = self._bias_initializer if self._bias_initializer is None and not self._layer_norm: bias_initializer = init_ops.constant_initializer(1.0, dtype=dtype) bias = tf.get_variable('bias', [2 * self._num_units], dtype=dtype, initializer=bias_initializer) weights = tf.get_variable('kernel', [input_size + state_size, 2 * self._num_units], dtype=dtype, initializer=self._kernel_initializer ) inputs_ = tf.matmul(inputs, weights[:input_size]) state_ = tf.matmul(state, weights[input_size:]) if self._layer_norm: inputs_ = tf.contrib.layers.layer_norm(inputs_, scope='inputs') state_ = tf.contrib.layers.layer_norm(state_, scope='state') value = tf.nn.sigmoid(inputs_ + state_ + bias) r, u = tf.split(value=value, num_or_size_splits=2, axis=1) with tf.variable_scope('candidate'): bias = tf.get_variable('bias', [self._num_units], dtype=dtype, initializer=self._bias_initializer) weights = tf.get_variable('kernel', [input_size + state_size, self. _num_units], dtype=dtype, initializer=self._kernel_initializer) c = tf.matmul(tf.concat([inputs, r * state], axis=1), weights) if self._layer_norm: c = tf.contrib.layers.layer_norm(c) c = self._activation(c + bias) new_h = u * state + (1 - u) * c return new_h, new_h 
audio|file|to|transcribe|text def to_text(vocab_list, sample_ids): sym_list = [vocab_list[x] for x in sample_ids] + [utils.EOS] return args.delimiter.join(sym_list[:sym_list.index(utils.EOS)]) 
texar|list|decoders|items|TextDataDecoder|data def list_items(self): """Returns the list of item names that the decoder can produce.  Returns: A list of strings can be passed to :meth:`decode()`. """ return [self._text_tensor_name, self._length_tensor_name, self. _text_id_tensor_name] 
decoder|search|nmt|init|beam|BeamSearchDecoder def __init__(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer=None, max_tgt=None, length_penalty_weight=0.0, coverage_penalty_weight=0.0, reorder_tensor_arrays=True, dtype=tf.float32): """Initialize the BeamSearchDecoder.  Args: cell: An `RNNCell` instance. embedding: A callable that takes a vector tensor of `ids` (argmax ids), or the `params` argument for `embedding_lookup`. start_tokens: `int32` vector shaped `[batch_size]`, the start tokens. end_token: `int32` scalar, the token that marks end of decoding. initial_state: A (possibly nested tuple of...) tensors and TensorArrays. beam_width:  Python integer, the number of beams. output_layer: (Optional) An instance of `tf.variable` to represent the weights for the dense layer to apply to the RNN output prior to storing the result or sampling. max_tgt: maximum prediction length. length_penalty_weight: Float weight to penalize length. Disabled with 0.0. coverage_penalty_weight: Float weight to penalize the coverage of source sentence. Disabled with 0.0. reorder_tensor_arrays: If `True`, `TensorArray`s' elements within the cell state will be reordered according to the beam search path. If the `TensorArray` can be reordered, the stacked form will be returned. Otherwise, the `TensorArray` will be returned as is. Set this flag to `False` if the cell state contains `TensorArray`s that are not amenable to reordering.  Raises: TypeError: if `cell` is not an instance of `RNNCell`, or `output_layer` is not an instance of `tf.layers.Layer`. ValueError: If `start_tokens` is not a vector or `end_token` is not a scalar. """ self._cell = cell self._output_layer = output_layer self._reorder_tensor_arrays = reorder_tensor_arrays if callable(embedding): self._embedding_fn = embedding else: self._embedding_fn = lambda ids: tf.cast(tf.nn.embedding_lookup( embedding, ids), dtype) self._start_tokens = tf.convert_to_tensor(start_tokens, dtype=tf.int32, name='start_tokens') if self._start_tokens.get_shape().ndims != 1: raise ValueError('start_tokens must be a vector') self._end_token = tf.convert_to_tensor(end_token, dtype=tf.int32, name= 'end_token') if self._end_token.get_shape().ndims != 0: raise ValueError('end_token must be a scalar') self._batch_size = start_tokens.shape[0].value self._beam_width = beam_width self._max_tgt = max_tgt self._length_penalty_weight = length_penalty_weight self._coverage_penalty_weight = coverage_penalty_weight self._initial_cell_state = tf.contrib.framework.nest.map_structure(self ._maybe_split_batch_beams, initial_state, self._cell.state_size) self._start_tokens = tf.tile(tf.expand_dims(self._start_tokens, 1), [1, self._beam_width]) self._start_inputs = self._embedding_fn(self._start_tokens) self._finished = tf.one_hot(tf.zeros([self._batch_size], dtype=tf.int32 ), depth=self._beam_width, on_value=False, off_value=True, dtype=tf .bool) 
infov|log|util def _infov(self, msg, *args, **kwargs): self.log(logging.INFO + 1, msg, *args, **kwargs) 
testAnalyticKernelComposeAutomatic|test|batch|BatchTest @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_on_device={}'.format(store_on_device), 'store_on_device': store_on_device} for store_on_device in [True, False])) def testAnalyticKernelComposeAutomatic(self, store_on_device): utils.stub_out_pmap(batch, 2) self._test_analytic_kernel_composition(partial(batch.batch, batch_size= 2, store_on_device=store_on_device)) 
gen|thread|data|patches|loader def gen_patches_thread(file_name, out_que): img = load_raw_image_packed(file_name) dim1, h, w, c = img.shape patches = None cam_iso = file_name[-41:-33] for i in range(0, h - patch_size + 1, stride): for j in range(0, w - patch_size + 1, stride): x = img[(0), i:i + patch_size, j:j + patch_size, :] for k in range(0, aug_times): if patches is None: patches = x[(np.newaxis), :, :, :] else: patches = np.concatenate((patches, x[(np.newaxis), :, :, :]), axis=0) cam_iso_patches = [cam_iso] * len(patches) out_que.put((patches, cam_iso_patches)) 
gan|sp|CDVAECLSGAN|cls|decoder|mcc|cdvae def sp_decoder(self, z, y): net = self.arch['generator']['sp'] return self._generator(z, y, net) 
space|avod|project|tf|image|projector|anchor|core|to def tf_project_to_image_space(anchors, stereo_calib_p2, image_shape): """ Projects 3D tensor anchors into image space  Args: anchors: a tensor of anchors in the shape [N, 6]. The anchors are in the format [x, y, z, dim_x, dim_y, dim_z] stereo_calib_p2: tensor [3, 4] stereo camera calibration p2 matrix image_shape: a float32 tensor of shape [2]. This is dimension of the image [h, w]  Returns: box_corners: a float32 tensor corners in image space - N x [x1, y1, x2, y2] box_corners_norm: a float32 tensor corners as a percentage of the image size - N x [x1, y1, x2, y2] """ if anchors.shape[1] != 6: raise ValueError('Invalid shape for anchors {}, should be (N, 6)'. format(anchors.shape[1])) x = anchors[:, (0)] y = anchors[:, (1)] z = anchors[:, (2)] dim_x = anchors[:, (3)] dim_y = anchors[:, (4)] dim_z = anchors[:, (5)] dim_x_half = dim_x / 2.0 dim_z_half = dim_z / 2.0 x_corners = tf.reshape(tf.transpose(tf.stack([x + dim_x_half, x + dim_x_half, x - dim_x_half, x - dim_x_half, x + dim_x_half, x + dim_x_half, x - dim_x_half, x - dim_x_half])), (1, -1)) y_corners = tf.reshape(tf.transpose(tf.stack([y, y, y, y, y - dim_y, y - dim_y, y - dim_y, y - dim_y])), (1, -1)) z_corners = tf.reshape(tf.transpose(tf.stack([z + dim_z_half, z - dim_z_half, z - dim_z_half, z + dim_z_half, z + dim_z_half, z - dim_z_half, z - dim_z_half, z + dim_z_half])), (1, -1)) anchor_corners = tf.concat([x_corners, y_corners, z_corners], axis=0) pts_2d = project_to_image_tensor(anchor_corners, stereo_calib_p2) i_axis_min_points = tf.reduce_min(tf.reshape(pts_2d[(0), :], (-1, 8)), axis=1) j_axis_min_points = tf.reduce_min(tf.reshape(pts_2d[(1), :], (-1, 8)), axis=1) i_axis_max_points = tf.reduce_max(tf.reshape(pts_2d[(0), :], (-1, 8)), axis=1) j_axis_max_points = tf.reduce_max(tf.reshape(pts_2d[(1), :], (-1, 8)), axis=1) box_corners = tf.transpose(tf.stack([i_axis_min_points, j_axis_min_points, i_axis_max_points, j_axis_max_points], axis=0)) image_shape_h = image_shape[0] image_shape_w = image_shape[1] image_shape_tiled = tf.stack([image_shape_w, image_shape_h, image_shape_w, image_shape_h], axis=0) box_corners_norm = tf.divide(box_corners, image_shape_tiled) return box_corners, box_corners_norm 
ops|deconv2d|Deconv2D|uniform|tflib def uniform(stdev, size): return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt( 3), size=size).astype('float32') 
moments|compute|b|fn|mp|accountant|calculate def b_fn(z): return (mu0(z) / mu(z)) ** lmbd_int - (mu(-z) / mu0(z)) ** lmbd_int 
compute|output|sequence|KMaxPooling|deepctr|layers|shape def compute_output_shape(self, input_shape): output_shape = list(input_shape) output_shape[self.axis] = self.k return tuple(output_shape) 
lr|models|schedule|CNN|cnn def lr_schedule(self, epoch): """Learning Rate Schedule  Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs. Called automatically every epoch as part of callbacks during training.  # Arguments epoch (int): The number of epochs  # Returns lr (float32): learning rate """ lr = 0.001 if epoch > 180: lr *= 0.0005 elif epoch > 160: lr *= 0.001 elif epoch > 120: lr *= 0.01 elif epoch > 80: lr *= 0.1 print('Learning rate: ', lr) return lr 
forward|deepMOT|models|DHN|master|Munkrs def forward(self, Dt): input_row = Dt.view(Dt.size(0), -1, 1).permute(1, 0, 2).contiguous() lstm_R_out, self.hidden_row = self.lstm_row(input_row, self.hidden_row) lstm_R_out = lstm_R_out.view(-1, lstm_R_out.size(2)) lstm_R_out = lstm_R_out.view(Dt.size(1), Dt.size(2), Dt.size(0), -1) input_col = lstm_R_out.permute(1, 0, 2, 3).contiguous() input_col = input_col.view(-1, input_col.size(2), input_col.size(3) ).contiguous() lstm_C_out, self.hidden_col = self.lstm_col(input_col, self.hidden_col) lstm_C_out = lstm_C_out.view(Dt.size(2), Dt.size(1), Dt.size(0), -1 ).permute(1, 0, 2, 3).contiguous() lstm_C_out = lstm_C_out.view(-1, lstm_C_out.size(3)) tag_space = self.hidden2tag_1(lstm_C_out) tag_space = self.hidden2tag_2(tag_space) tag_space = self.hidden2tag_3(tag_space).view(-1, Dt.size(0)) tag_scores = torch.sigmoid(tag_space) return tag_scores.view(Dt.size(1), Dt.size(2), -1).permute(2, 0, 1 ).contiguous() 
tangents|stax|fn|conv|neural|nngp|GeneralConv|kernel def conv_nngp(x): if _is_array(x): x = _conv_nngp_5or6d_double_conv(x, filter_shape_nngp, strides_nngp, padding) x = _affine(x, W_std, b_std) return x 
pair|next|input|cvusa|data|batch|InputData def next_pair_batch(self, batch_size): if self.__cur_id == 0: for i in range(20): random.shuffle(self.id_idx_list) if self.__cur_id + batch_size + 2 >= self.data_size: self.__cur_id = 0 return None, None if self.polar: batch_sat = np.zeros([batch_size, 112, 616, 3], dtype=np.float32) else: batch_sat = np.zeros([batch_size, 256, 256, 3], dtype=np.float32) batch_grd = np.zeros([batch_size, 112, 616, 3], dtype=np.float32) i = 0 batch_idx = 0 while True: if batch_idx >= batch_size or self.__cur_id + i >= self.data_size: break img_idx = self.id_idx_list[self.__cur_id + i] i += 1 img = cv2.imread(self.img_root + self.id_list[img_idx][0]) if not self.polar: if img is None: print('InputData::next_pair_batch: read fail: %s, %d, ' % ( self.img_root + self.id_list[img_idx][0], i), img.shape) continue img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_AREA) elif img is None: print('InputData::next_pair_batch: read fail: %s, %d, ' % (self .img_root + self.id_list[img_idx][0], i), img.shape) continue img = img.astype(np.float32) img[:, :, (0)] -= 103.939 img[:, :, (1)] -= 116.779 img[:, :, (2)] -= 123.6 batch_sat[(batch_idx), :, :, :] = img img = cv2.imread(self.img_root + self.id_list[img_idx][1]) if img is None or img.shape[0] != 224 or img.shape[1] != 1232: print('InputData::next_pair_batch: read fail: %s, %d, ' % (self .img_root + self.id_list[img_idx][1], i), img.shape) continue img = cv2.resize(img, (616, 112), interpolation=cv2.INTER_AREA) img = img.astype(np.float32) img[:, :, (0)] -= 103.939 img[:, :, (1)] -= 116.779 img[:, :, (2)] -= 123.6 batch_grd[(batch_idx), :, :, :] = img batch_idx += 1 self.__cur_id += i return batch_sat, batch_grd 
graph|knn|edge|tf def knn_graph(vertex_features, k, distance_metric=tf_util.pairwise_distance): """ Find the neighbors' indices based on knn """ dists = distance_metric(vertex_features) neigh_idx = tf_util.knn(dists, k=k) return neigh_idx 
tangents|prediction|predict|neural|mean def _mean_prediction(op, g_td, y_train): """Compute the mean prediction of a Gaussian process.  Args: op: Some vector operator that projects the data along the relevant directions, op(vec, dt) = M^{-1} @ (I - E^(-M dt)) @ vec g_td: A kernel relating training data with test data. The kernel should be an `np.ndarray` of shape [n_test * output_dim, n_train * output_dim] or [n_test, n_train]. y_train: An `np.ndarray` of shape [n_train, output_dim] of targets for the training data.  Returns: The mean prediction of the GP. `g_td @ op @ y_train`. """ fl, ufl = _make_flatten_uflatten(g_td, y_train) mean_pred = op(fl(y_train)) mean_pred = np.dot(g_td, mean_pred) return ufl(mean_pred) 
batches|src|generator|utilities def batches_generator(x: np.array, y: np.array, batch_size=100): """ Yield a generator of batches to iterate easily through a dataset. """ N = x.shape[0] n_batches = N // batch_size + (N % batch_size != 0) for i in range(n_batches): lo = i * batch_size hi = (i + 1) * batch_size yield x[lo:hi], y[lo:hi] 
simulate|alias def simulate(N=100, k=10000): truth = gen_prob_dist(N) area_ratio = truth accept, alias = create_alias_table(area_ratio) ans = np.zeros(N) for _ in range(k): i = alias_sample(accept, alias) ans[i] += 1 return ans / np.sum(ans), truth 
models|Node2VecModel|accuracy|graphsage def _accuracy(self): aff = self.link_pred_layer.affinity(self.outputs1, self.outputs2) self.neg_aff = self.link_pred_layer.neg_cost(self.outputs1, self. neg_outputs) self.neg_aff = tf.reshape(self.neg_aff, [self.batch_size, FLAGS. neg_sample_size]) _aff = tf.expand_dims(aff, axis=1) self.aff_all = tf.concat(axis=1, values=[self.neg_aff, _aff]) size = tf.shape(self.aff_all)[1] _, indices_of_ranks = tf.nn.top_k(self.aff_all, k=size) _, self.ranks = tf.nn.top_k(-indices_of_ranks, k=size) self.mrr = tf.reduce_mean(tf.div(1.0, tf.cast(self.ranks[:, (-1)] + 1, tf.float32))) tf.summary.scalar('mrr', self.mrr) 
get|data|loader def get_data(config, train=True): if train: if config.direction == 'usps-mnist': loader = get_usps_loader(config, batch_size=7291) elif config.direction == 'mnist-usps': loader = get_mnist_loader(config, batch_size=60000) elif config.direction == 'usps-mnist': loader = get_mnist_loader(config, batch_size=2007, train=False) elif config.direction == 'mnist-usps': loader = get_usps_loader(config, batch_size=10000, train=False) return next(loader) 
download|tfrecords|and|generate|cifar1|extract def download_and_extract(data_dir): tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir, CIFAR_DOWNLOAD_URL) tarfile.open(os.path.join(data_dir, CIFAR_FILENAME), 'r:gz').extractall( data_dir) 
nets|nasnet|cifar|stem def _cifar_stem(inputs, hparams): """Stem used for models trained on Cifar.""" num_stem_filters = int(hparams.num_conv_filters * hparams.stem_multiplier) net = slim.conv2d(inputs, num_stem_filters, 3, scope='l1_stem_3x3') net = slim.batch_norm(net, scope='l1_stem_bn') return net, [None, net] 
rotate|im|commons|func|py def py_func(func, inp, Tout, stateful=True, name=None, grad=None): rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 100000000.0)) tf.RegisterGradient(rnd_name)(grad) g = tf.get_default_graph() with g.gradient_override_map({'PyFunc': rnd_name}): return tf.py_func(func, inp, Tout, stateful=stateful, name=name) 
tangents|get|kernel|stax|neural|relu|ab def _get_ab_relu_kernel(ker_mat, prod, a, b, do_backprop, ntk=None): cosines = ker_mat / np.sqrt(prod) angles = _arccos(cosines, do_backprop) dot_sigma = (a ** 2 + b ** 2 - (a - b) ** 2 * angles / np.pi) / 2 ker_mat = (a - b) ** 2 * _sqrt(prod - ker_mat ** 2, do_backprop) / (2 * np.pi) + dot_sigma * ker_mat if ntk is not None: ntk *= dot_sigma return ker_mat, ntk 
write|NameSrc|next|training|v2 def next(self): print('Name next') self.n += 1 return self.prefix + '{:0>8d}.gz'.format(self.n) 
ShouldCheckNamespaceIndentation|cpplint def ShouldCheckNamespaceIndentation(nesting_state, is_namespace_indent_item, raw_lines_no_comments, linenum): """This method determines if we should apply our namespace indentation check.  Args: nesting_state: The current nesting state. is_namespace_indent_item: If we just put a new class on the stack, True. If the top of the stack is not a class, or we did not recently add the class, False. raw_lines_no_comments: The lines without the comments. linenum: The current line number we are processing.  Returns: True if we should apply our namespace indentation check. Currently, it only works for classes and namespaces inside of a namespace. """ is_forward_declaration = IsForwardClassDeclaration(raw_lines_no_comments, linenum) if not (is_namespace_indent_item or is_forward_declaration): return False if IsMacroDefinition(raw_lines_no_comments, linenum): return False return IsBlockInNameSpace(nesting_state, is_forward_declaration) 
threads|load|data|loader def load_data_threads(data_dir='/shared-data/SIDD_Medium_Raw/Data/', max_images=0, verbose=False): file_list = glob.glob(os.path.join(data_dir, '*/*GT_RAW_010.MAT')) if max_images != 0: file_list = file_list[:max_images] print('# images pre-filter = %d' % len(file_list)) cam_iso_nlf = load_cam_iso_nlf() cam_iso_vals = cam_iso_nlf['cam_iso'] file_list_copy = [] for f in file_list: if f[-41:-33] in cam_iso_vals: file_list_copy.append(f) file_list = file_list_copy print('# images post-filter = %d' % len(file_list)) data1 = None cam_iso_info = [] tt = time.time() threads = [None] * len(file_list) ques = [None] * len(file_list) for i in range(len(file_list)): ques[i] = queue.Queue(1) threads[i] = Thread(target=gen_patches_thread, args=(file_list[i], ques[i])) threads[i].start() for i in range(len(file_list)): threads[i].join() for i in range(len(file_list)): patches, cam_iso_patches = ques[i].get() if data1 is None: data1 = patches else: data1 = np.concatenate((data1, patches), axis=0) cam_iso_info = cam_iso_info + cam_iso_patches if verbose: print(str(i + 1) + '/' + str(len(file_list)) + ' is done ^_^') discard_n = len(data1) - len(data1) // batch_size * batch_size data1 = np.delete(data1, range(discard_n), axis=0) cam_iso_info = cam_iso_info[discard_n:] assert len(cam_iso_info) == len(data1) tt = time.time() - tt logging.trace('loading data finished, time = %s sec' % str(tt)) return data1, cam_iso_info 
comparison|plot|ModelImageVisualizer|fasterai|visualize def _plot_comparison(self, figsize: (int, int), render_factor: int, display_render_factor: bool, orig: Image, result: Image): fig, axes = plt.subplots(1, 2, figsize=figsize) self._plot_image(orig, axes=axes[0], figsize=figsize, render_factor= render_factor, display_render_factor=False) self._plot_image(result, axes=axes[1], figsize=figsize, render_factor= render_factor, display_render_factor=display_render_factor) 
generator|pvae|imagenet32|make|tflib def make_generator(path, n_files, batch_size, random_state): epoch_count = [1]  def get_epoch(): images = np.zeros((batch_size, 3, 32, 32), dtype='int32') files = list(range(n_files)) random_state.shuffle(files) epoch_count[0] += 1 for n, i in enumerate(files): image = scipy.misc.imread('{}/{}.png'.format(path, str(i + 1). zfill(len(str(n_files))))) images[n % batch_size] = image.transpose(2, 0, 1) if n > 0 and n % batch_size == 0: yield images, return get_epoch 
VirtualAdversarialMethod|cleverhans|generate|attacks def generate(self, x, **kwargs): """ Generate symbolic graph for adversarial examples and return. :param x: The model's symbolic inputs. :param eps: (optional float ) the epsilon (input variation parameter) :param num_iterations: (optional) the number of iterations :param xi: (optional float) the finite difference parameter :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value """ assert self.parse_params(**kwargs) return vatm(self.model, x, self.model.get_logits(x), eps=self.eps, num_iterations=self.num_iterations, xi=self.xi, clip_min=self. clip_min, clip_max=self.clip_max) 
print|start|config def print_config(config): if type(config) == dict: for k in config: if type(config[k]) == dict: print_config(config[k]) else: tf.logging.info('{}:\t {}'.format(k, config[k])) else: return 
minibatch|feed|graphsage|batch|EdgeMinibatchIterator|dict def batch_feed_dict(self, batch_edges): batch1 = [] batch2 = [] for node1, node2 in batch_edges: batch1.append(self.id2idx[node1]) batch2.append(self.id2idx[node2]) feed_dict = dict() feed_dict.update({self.placeholders['batch_size']: len(batch_edges)}) feed_dict.update({self.placeholders['batch1']: batch1}) feed_dict.update({self.placeholders['batch2']: batch2}) return feed_dict 
lrelu|ops def lrelu(x, leak=0.2, name='lrelu'): with tf.variable_scope(name): f1 = 0.5 * (1 + leak) f2 = 0.5 * (1 - leak) return f1 * x + f2 * abs(x) 
gen|tests|utils|sequence def gen_sequence(dim, max_len, sample_size): return np.array([np.random.randint(0, dim, max_len) for _ in range( sample_size)]), np.random.randint(1, max_len + 1, sample_size) 
embed|instance|WordEmbedding|datacode|ner def embed_instance(self, instance): instance.word_emb = self.word_to_embedding(instance.word) instance.left_context_emb = [self.word_to_embedding(word) for word in instance.left_context] instance.right_context_emb = [self.word_to_embedding(word) for word in instance.right_context] 
bert|pytorch|init|modeling|pretrained|BertPreTrainedModel|weights def init_bert_weights(self, module): """ Initialize the weights. """ if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=self.config.initializer_range) elif isinstance(module, BertLayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() 
evaluate|advanced|evaluator def evaluate_advanced(model): global AUC_means, AUC_median, X, Y kfold = StratifiedKFold(n_splits=5, shuffle=True) for index, (train_indices, test_indices) in enumerate(kfold.split(X, Y)): if index == 0: Y = to_categorical(Y, num_classes) xtrain, xtest = X[train_indices], X[test_indices] ytrain, ytest = Y[train_indices], Y[test_indices] model = load_model('complex_model.h5') history = train_advanced(model, xtrain, ytrain, xtest, ytest, index) accuracy_history = history.history['acc'] val_accuracy_history = history.history['val_acc'] print('Last training accuracy: ' + str(accuracy_history[-1]) + ', last validation accuracy: ' + str(val_accuracy_history[-1])) AUC_median = statistics.median(AUC_means) AUC_means = sorted(AUC_means) idx_mid = int(len(AUC_means) / 2) print('AUC MEDIAN: %f' % AUC_median) final_model = load_model('best_model' + str(idx_mid) + '.h5') final_model.save('final_model.h5') print(st.t.interval(0.95, len(AUC_means) - 1, AUC_median, scale=st.sem( AUC_means))) return AUC_median 
envs|gym|grid|PycolabGridWorldsLevel14Env|init|pycolab|worlds|env def __init__(self): super(PycolabGridWorldsEnv, self).__init__() 
nmt|utils|check|vocab def check_vocab(vocab_file, out_dir, check_special_token=True, sos=None, eos=None, unk=None): """Check if vocab_file doesn't exist, create from corpus_file.""" if tf.gfile.Exists(vocab_file): utils.print_out('# Vocab file %s exists' % vocab_file) vocab, vocab_size = load_vocab(vocab_file) if check_special_token: if not unk: unk = UNK if not sos: sos = SOS if not eos: eos = EOS assert len(vocab) >= 3 if vocab[0] != unk or vocab[1] != sos or vocab[2] != eos: utils.print_out( 'The first 3 vocab words [%s, %s, %s] are not [%s, %s, %s]' % (vocab[0], vocab[1], vocab[2], unk, sos, eos)) vocab = [unk, sos, eos] + vocab vocab_size += 3 new_vocab_file = os.path.join(out_dir, os.path.basename( vocab_file)) with codecs.getwriter('utf-8')(tf.gfile.GFile( new_vocab_file, 'wb')) as f: for word in vocab: f.write('%s\n' % word) vocab_file = new_vocab_file else: raise ValueError("vocab_file '%s' does not exist." % vocab_file) vocab_size = len(vocab) return vocab_size, vocab_file 
models|TimeDelayNeuralNetwork|tf def TimeDelayNeuralNetwork(n_nodes, conv_len, n_classes, n_feat, max_len, loss='categorical_crossentropy', causal=False, optimizer='rmsprop', activation='sigmoid', return_param_str=False): n_layers = len(n_nodes) inputs = Input(shape=(max_len, n_feat)) model = inputs inputs_mask = Input(shape=(max_len, 1)) model_masks = [inputs_mask] for i in range(n_layers): if causal: model = ZeroPadding1D((conv_len // 2, 0))(model) model = AtrousConvolution1D(n_nodes[i], conv_len, atrous_rate=i + 1, border_mode='same')(model) if causal: model = Cropping1D((0, conv_len // 2))(model) if activation == 'norm_relu': model = Activation('relu')(model) model = Lambda(channel_normalization, name='encoder_norm_{}'. format(i))(model) elif activation == 'wavenet': model = WaveNet_activation(model) else: model = Activation(activation)(model) model = TimeDistributed(Dense(n_classes, activation='softmax'))(model) model = Model(input=inputs, output=model) model.compile(loss=loss, optimizer=optimizer, sample_weight_mode= 'temporal', metrics=['accuracy']) if return_param_str: param_str = 'TDN_C{}'.format(conv_len) if causal: param_str += '_causal' return model, param_str else: return model 
test|tmp|on|lfw|translate|master|facenet|invariance|images def translate_images(images, offset, image_size): h, v = offset sz1 = images.shape[1] / 2 sz2 = image_size / 2 images_crop = images[:, sz1 - sz2 + v:sz1 + sz2 + v, sz1 - sz2 + h:sz1 + sz2 + h, :] return images_crop 
image|reader|scaling|classfc|deeplab|resnet def image_scaling(img, seed): """ Randomly scales the images between 0.5 to 1.5 times the original size.  Args: img: Training image to scale. seed: Random seed. """ scale = tf.random_uniform([1], minval=0.5, maxval=1.5, dtype=tf.float32, seed=seed) h_new = tf.to_int32(tf.multiply(tf.to_float(tf.shape(img)[0]), scale)) w_new = tf.to_int32(tf.multiply(tf.to_float(tf.shape(img)[1]), scale)) new_shape = tf.squeeze(tf.stack([h_new, w_new]), squeeze_dims=[1]) img = tf.image.resize_images(img, new_shape) return img 
ex2|sdn|params|model|cond|utils def sdn_model_params_ex2(yy, iso, gain_init): c = 0.1 g00100 = tf.get_variable('gain_param_%05d' % 100, [1], tf.float32, initializer=tf.constant_initializer(gain_init / c)) g00400 = tf.get_variable('gain_param_%05d' % 400, [1], tf.float32, initializer=tf.constant_initializer(gain_init / c)) g00800 = tf.get_variable('gain_param_%05d' % 800, [1], tf.float32, initializer=tf.constant_initializer(gain_init / c)) g01600 = tf.get_variable('gain_param_%05d' % 1600, [1], tf.float32, initializer=tf.constant_initializer(gain_init / c)) g03200 = tf.get_variable('gain_param_%05d' % 3200, [1], tf.float32, initializer=tf.constant_initializer(gain_init / c)) gain = tf.cond(tf.equal(iso[0], 100), lambda : tf.exp(c * g00100) * iso, lambda : tf.cond(tf.equal(iso[0], 400), lambda : tf.exp(c * g00400) * iso, lambda : tf.cond(tf.equal(iso[0], 800), lambda : tf.exp(c * g00800) * iso, lambda : tf.cond(tf.equal(iso[0], 1600), lambda : tf .exp(c * g01600) * iso, lambda : tf.cond(tf.equal(iso[0], 3200), lambda : tf.exp(c * g03200) * iso, lambda : tf.exp(c * g00800) * iso))))) b1 = tf.get_variable('b1', [1], tf.float32, initializer=tf. constant_initializer(-3.0)) b1 = tf.nn.sigmoid(b1, 'sigmoid_b1') b2 = tf.get_variable('b2', [1], tf.float32, initializer=tf. constant_initializer(3.0)) b2 = tf.nn.sigmoid(b2, 'sigmoid_b2') scale = tf.sqrt(gain * (b1 * yy / gain + b2)) return scale 
get|agreement|test|stax|with|StaxTest|empirical|check def _get_empirical(n_samples, get): kernel_fn_empirical = monte_carlo.monte_carlo_kernel_fn(init_fn, apply_fn, key, n_samples) return kernel_fn_empirical(x1, x2, get) 
gemm|cases|generate|test def generate_test_cases(ukernel, mr, nr, kr, sr, k_block, is_pipelined, isa): """Generates all tests cases for a GEMM micro-kernel.  Args: ukernel: C name of the micro-kernel function. mr: MR parameter of the GEMM micro-kernel. nr: NR parameter of the GEMM micro-kernel. kr: KR parameter of the GEMM micro-kernel. sr: SR parameter of the GEMM micro-kernel. k_block: Number of K values processed per one iteration of the main loop of the micro-kernel. is_pipelined: Indicates if the micro-kernel is implemented with software pipelining. Additional test cases are generated for software pipelined micro-kernels to separately test prologue + epiloque of the pipelined loop and iteration of the pipelined loop. isa: instruction set required to run the micro-kernel. Generated unit test will skip execution if the host processor doesn't support this ISA.  Returns: Code for the test case. """ _, test_name = ukernel.split('_', 1) _, datatype, ukernel_type, _ = ukernel.split('_', 3) test_args = [ukernel] if not isa or isa == 'psimd': test_args.append('GemmMicrokernelTester::Variant::Scalar') return xngen.preprocess(GEMM_TEST_CODE, {'TEST_NAME': test_name.upper() .replace('UKERNEL_', ''), 'TEST_ARGS': test_args, 'UKERNEL_TYPE': ukernel_type.upper(), 'DATATYPE': datatype, 'MR': mr, 'NR': nr, 'KR': kr, 'SR': sr, 'KBLOCK': k_block, 'ADJKBLOCK': 2 * k_block if is_pipelined else k_block, 'IS_PIPELINED': is_pipelined, 'ISA_CHECK': xnncommon.generate_isa_check_macro(isa), 'next_prime': next_prime}) 
initial|state|agents|DuelingLSTMDQNNet def initial_state(self, batch_size): if self._stack_size > 1: frame_stacking_state = tf.zeros(tf.concat([[batch_size], self. _observation_shape], axis=0)[:-1], dtype=STACKING_STATE_DTYPE) else: frame_stacking_state = () return AgentState(core_state=self._core.get_initial_state(batch_size= batch_size, dtype=tf.float32), frame_stacking_state= frame_stacking_state) 
main|translator def main(args): tf.logging.set_verbosity(tf.logging.INFO) model_cls_list = [models.get_model(model) for model in args.models] params_list = [default_parameters() for _ in range(len(model_cls_list))] params_list = [merge_parameters(params, model_cls.get_parameters()) for params, model_cls in zip(params_list, model_cls_list)] params_list = [import_params(args.checkpoints[i], args.models[i], params_list[i]) for i in range(len(args.checkpoints))] params_list = [override_parameters(params_list[i], args) for i in range (len(model_cls_list))] with tf.Graph().as_default(): model_var_lists = [] for i, checkpoint in enumerate(args.checkpoints): tf.logging.info('Loading %s' % checkpoint) var_list = tf.train.list_variables(checkpoint) values = {} reader = tf.train.load_checkpoint(checkpoint) for name, shape in var_list: if not name.startswith(model_cls_list[i].get_name()): continue if name.find('losses_avg') >= 0: continue tensor = reader.get_tensor(name) values[name] = tensor model_var_lists.append(values) model_list = [] for i in range(len(args.checkpoints)): name = model_cls_list[i].get_name() model = model_cls_list[i](params_list[i], name + '_%d' % i) model_list.append(model) params = params_list[0] sorted_keys, sorted_inputs = dataset.sort_input_file(args.input) features = dataset.get_inference_input(sorted_inputs, params) placeholders = [] for i in range(len(params.device_list)): placeholders.append({'source': tf.placeholder(tf.int32, [None, None], 'source_%d' % i), 'source_length': tf.placeholder(tf .int32, [None], 'source_length_%d' % i)}) if params.generate_samples: inference_fn = sampling.create_sampling_graph else: inference_fn = inference.create_inference_graph predictions = parallel.data_parallelism(params.device_list, lambda f: inference_fn(model_list, f, params), placeholders) assign_ops = [] feed_dict = {} all_var_list = tf.trainable_variables() for i in range(len(args.checkpoints)): un_init_var_list = [] name = model_cls_list[i].get_name() for v in all_var_list: if v.name.startswith(name + '_%d' % i): un_init_var_list.append(v) ops = set_variables(un_init_var_list, model_var_lists[i], name + '_%d' % i, feed_dict) assign_ops.extend(ops) assign_op = tf.group(*assign_ops) init_op = tf.tables_initializer() results = [] tf.get_default_graph().finalize() with tf.Session(config=session_config(params)) as sess: sess.run(assign_op, feed_dict=feed_dict) sess.run(init_op) while True: try: feats = sess.run(features) op, feed_dict = shard_features(feats, placeholders, predictions) results.append(sess.run(op, feed_dict=feed_dict)) message = 'Finished batch %d' % len(results) tf.logging.log(tf.logging.INFO, message) except tf.errors.OutOfRangeError: break vocab = params.vocabulary['target'] outputs = [] scores = [] for result in results: for shard in result: for item in shard[0]: outputs.append(item.tolist()) for item in shard[1]: scores.append(item.tolist()) restored_inputs = [] restored_outputs = [] restored_scores = [] for index in range(len(sorted_inputs)): restored_inputs.append(sorted_inputs[sorted_keys[index]]) restored_outputs.append(outputs[sorted_keys[index]]) restored_scores.append(scores[sorted_keys[index]]) if sys.version_info.major == 2: outfile = open(args.output, 'w') elif sys.version_info.major == 3: outfile = open(args.output, 'w', encoding='utf-8') else: raise ValueError('Unkown python running environment!') count = 0 for outputs, scores in zip(restored_outputs, restored_scores): for output, score in zip(outputs, scores): decoded = [] for idx in output: if idx == params.mapping['target'][params.eos]: break decoded.append(vocab[idx]) decoded = ' '.join(decoded) if not args.verbose: outfile.write('%s\n' % decoded) else: pattern = '%d ||| %s ||| %s ||| %f\n' source = restored_inputs[count] values = count, source, decoded, score outfile.write(pattern % values) count += 1 outfile.close() 
process|post|signature|autogen def post_process_signature(signature): parts = re.split('\\.(?!\\d)', signature) if len(parts) >= 4: if parts[1] == 'api': signature = 'ludwig.' + '.'.join(parts[2:]) return signature 
maze|can|move|x|env|utils def can_move_x(movable): """Return if an object can be moved in the x-direction.  Parameters ---------- movable : int move-ability of the contacting object  Returns ------- bool True if movable, False otherwise """ return movable in [Move.X, Move.XY, Move.XZ, Move.XYZ, Move.SpinXY] 
ablation|distribution|agents|init|model|network|SlacModelDistributionNetwork|slac def __init__(self, observation_spec, action_spec, latent1_first_prior_distribution_ctor=ConstantMultivariateNormalDiag, latent1_prior_distribution_ctor=MultivariateNormalDiag, latent1_posterior_distribution_ctor=MultivariateNormalDiag, latent2_prior_distribution_ctor=MultivariateNormalDiag, latent2_posterior_distribution_ctor=MultivariateNormalDiag, base_depth= 32, latent1_size=32, latent2_size=256, kl_analytic=True, skip_first_kl= False, sequential_latent1_prior=True, sequential_latent2_prior=True, sequential_latent1_posterior=True, sequential_latent2_posterior=True, model_reward=False, model_discount=False, decoder_stddev=np.sqrt(0.1, dtype=np.float32), reward_stddev=None, name=None): super(SlacModelDistributionNetwork, self).__init__(name=name) self.observation_spec = observation_spec self.action_spec = action_spec self.base_depth = base_depth self.latent1_size = latent1_size self.latent2_size = latent2_size self.kl_analytic = kl_analytic self.skip_first_kl = skip_first_kl self.model_reward = model_reward self.model_discount = model_discount self.latent1_first_prior = latent1_first_prior_distribution_ctor( latent1_size) self.latent2_first_prior = latent2_prior_distribution_ctor(8 * base_depth, latent2_size) if sequential_latent1_prior: self.latent1_prior = latent1_prior_distribution_ctor(8 * base_depth, latent1_size) else: self.latent1_prior = (lambda prev_latent, prev_action: self. latent1_first_prior(prev_latent[..., 0])) if sequential_latent2_prior: self.latent2_prior = latent2_prior_distribution_ctor(8 * base_depth, latent2_size) else: self.latent2_prior = (lambda latent1, prev_latent2, prev_action: self.latent2_first_prior(latent1)) self.latent1_first_posterior = latent1_posterior_distribution_ctor(8 * base_depth, latent1_size) if latent2_posterior_distribution_ctor == latent2_prior_distribution_ctor: self.latent2_first_posterior = self.latent2_first_prior else: self.latent2_first_posterior = latent2_posterior_distribution_ctor( 8 * base_depth, latent2_size) if sequential_latent1_posterior: self.latent1_posterior = latent1_posterior_distribution_ctor(8 * base_depth, latent1_size) else: self.latent1_posterior = (lambda feature, prev_latent2, prev_action: self.latent1_first_posterior(feature)) if sequential_latent2_posterior: if (latent2_posterior_distribution_ctor == latent2_prior_distribution_ctor): self.latent2_posterior = self.latent2_prior else: self.latent2_posterior = latent2_posterior_distribution_ctor(8 * base_depth, latent2_size) else: self.latent2_posterior = (lambda latent1, prev_latent2, prev_action: self.latent2_first_posterior(latent1)) self.compressor = Compressor(base_depth, 8 * base_depth) self.decoder = Decoder(base_depth, scale=decoder_stddev) if self.model_reward: self.reward_predictor = Normal(8 * base_depth, scale=reward_stddev) else: self.reward_predictor = None if self.model_discount: self.discount_predictor = Bernoulli(8 * base_depth) else: self.discount_predictor = None 
mod|create|SMILESX|Main|utils def create_mod(params): print('Model: {}'.format(params)) model_tag = data_name K.clear_session() if n_gpus > 1: if bridge_type == 'NVLink': model = LSTMAttModel.create(inputtokens=max_length + 1, vocabsize=vocab_size, lstmunits=int(params[:, (0)][0]), denseunits=int(params[:, (1)]), embedding=int(params[:, (2) ][0])) else: with tf.device('/cpu'): model = LSTMAttModel.create(inputtokens=max_length + 1, vocabsize=vocab_size, lstmunits=int(params[:, (0)][0]), denseunits=int(params[:, (1)]), embedding=int(params[:, (2)][0])) multi_model = ModelMGPU(model, gpus=n_gpus, bridge_type=bridge_type) else: model = LSTMAttModel.create(inputtokens=max_length + 1, vocabsize= vocab_size, lstmunits=int(params[:, (0)][0]), denseunits=int( params[:, (1)]), embedding=int(params[:, (2)][0])) multi_model = model batch_size = int(params[:, (3)][0]) custom_adam = Adam(lr=math.pow(10, -float(params[:, (4)][0]))) multi_model.compile(loss='mse', optimizer=custom_adam, metrics=[metrics .mae, metrics.mse]) history = multi_model.fit_generator(generator=DataSequence( x_train_enum_tokens, vocab=tokens, max_length=max_length, props_set =y_train_enum, batch_size=batch_size), steps_per_epoch=math.ceil( len(x_train_enum_tokens) / batch_size) // bayopt_it_factor, validation_data=DataSequence(x_valid_enum_tokens, vocab=tokens, max_length=max_length, props_set=y_valid_enum, batch_size=min(len( x_valid_enum_tokens), batch_size)), validation_steps=math.ceil(len( x_valid_enum_tokens) / min(len(x_valid_enum_tokens), batch_size)) // bayopt_it_factor, epochs=bayopt_n_epochs, shuffle=True, initial_epoch=0, verbose=0) best_epoch = np.argmin(history.history['val_loss']) mae_valid = history.history['val_mean_absolute_error'][best_epoch] mse_valid = history.history['val_mean_squared_error'][best_epoch] if math.isnan(mse_valid): mae_valid = math.inf mse_valid = math.inf print('Valid MAE: {0:0.4f}, RMSE: {1:0.4f}'.format(mae_valid, mse_valid)) return mse_valid 
lth|common|plot def lth_plot(network, is_iterative, prune_method, min_max_y=None, min_max_x =None, is_delta=True, comparison_points=None, comparison_err=None, comparison_label=None, to_ignore=None, separate_legend=True, nbins=4, nybins=4, only_retrain_epochs=None, dont_plot_x=None, yticks=None, force_single=False, force_all_single=False): data = _PRUNED_DATAFRAME[(_PRUNED_DATAFRAME['network'] == network) & ( _PRUNED_DATAFRAME['is_iterative'] == is_iterative) & ( _PRUNED_DATAFRAME['prune_method'] == prune_method)] fig, ax = plt.subplots() last_density_index_above = None if comparison_points: if comparison_err: ax.errorbar(*zip(*comparison_points), comparison_err, label= comparison_label, color='k', fmt='o--', ms=10, zorder=10, capsize=5, capthick=3) else: ax.scatter(*zip(*comparison_points), label=comparison_label, s= 100, color='k', zorder=10) densities = sorted(set(data.density))[::-1] for retrain_method in sorted(set(data.retrain_method), key=lambda x: order_of_method(x[0])): if to_ignore and retrain_method in to_ignore: continue retrain_method_data = data[data.retrain_method == retrain_method] center = [] below = [] above = [] for density in densities: density_data = retrain_method_data[abs(retrain_method_data. density - density) < _EPS] if force_single: max_rt = density_data['retrain_time'].max() if (retrain_method == FINETUNE_NORMAL or retrain_method == FINETUNE_HIGH_LR): density_data = density_data[density_data['retrain_time' ] == max_rt] elif retrain_method == REWIND_NORMAL: density_data = density_data[density_data['retrain_time' ] == density_data[density_data['retrain_time'] <= 0.9 * max_rt]['retrain_time'].max()] res = density_data.groupby('retrain_time').agg({'test_acc': [ 'median', 'min', 'max'], 'val_acc': 'median'}).sort_values(( 'val_acc', 'median'), ascending=False) best = res.iloc[0].test_acc center.append(best['median']) above.append(best['max'] - best['median']) below.append(best['median'] - best['min']) center = np.array(center) if is_delta: center -= get_base_accuracy(network) m_label = label_of_method(retrain_method) if force_all_single: m_label = 'Re-training with {}'.format(long_label_of_method( retrain_method)) elif force_single: m_label = 'Our pruning algorithm' ax.errorbar(densities, center, [below, above], fmt=fmt_of_method( retrain_method), label=m_label, color=color_of_method( retrain_method), capsize=5, ms=ms_of_method(retrain_method), capthick=3, lw=3) ax.grid(True) ax.set_title('{}{} {} {}'.format('{} '.format('CIFAR-10' if network in CIFAR_NETWORKS else 'ImageNet' if network in IMAGENET_NETWORKS else 'WMT16' if network == GNMT else '???') if is_iterative else '', label_of_network(network), prune_method, ' (iterative)' if is_iterative else '')) ax.set_xlabel('Compression ratio') ax.set_xscale('log') if network == GNMT: score = 'BLEU' else: score = 'Accuracy' if is_delta: ax.set_ylabel('$\\Delta$ {}'.format(score)) else: ax.set_ylabel('{}'.format(score.capitalize())) if min_max_y: ax.set_ylim((min_max_y[0], min_max_y[1])) if min_max_x: min_x, max_x = min_max_x if min_x is None: min_x = ax.get_xlim()[1] if max_x is None: max_x = ax.get_xlim()[0] ax.set_xlim((min_x, max_x)) else: ax.set_xlim((ax.get_xlim()[1], ax.get_xlim()[0])) if dont_plot_x: densities = [densities[i] for i in range(len(densities)) if i not in dont_plot_x] ax.xaxis.set_major_locator(get_major_locator(densities, nbins=nbins)) ax.xaxis.set_minor_locator(matplotlib.ticker.NullLocator()) ax.xaxis.set_major_formatter(get_density_formatter()) ax.yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(nbins=nybins, steps=[1, 2, 2.5, 5, 10])) if network != GNMT: ax.yaxis.set_major_formatter(get_accuracy_formatter(is_delta=is_delta)) h, l = ax.get_legend_handles_labels() sortidx = sorted(range(len(l)), key=lambda i: order_of_method( inv_label_of_method(l[i]))) h_sort = [h[i] for i in sortidx] l_sort = [long_label_of_method(inv_label_of_method(l[i])) for i in sortidx] if force_single and comparison_label and separate_legend: ax.legend(h_sort[:], l_sort[:1], loc='lower left') elif comparison_label and separate_legend: ax.legend(h_sort[-1:], l_sort[-1:], loc='lower left') elif not separate_legend: ax.legend(h_sort, l_sort, loc='lower left') fig.set_tight_layout(True) format_axes(ax) if separate_legend: l_sort = [long_label_of_method(inv_label_of_method(l[i])) for i in sortidx] if force_single: h_sort = h_sort[1:] l_sort = l_sort[1:] elif comparison_label: h_sort = h_sort[:-1] l_sort = l_sort[:-1] legendfig = plt.figure() leg = legendfig.legend(h_sort, l_sort, 'center', fontsize=20, frameon=False) legendfig.set_size_inches(5, 2) legendfig.set_tight_layout(True) hlegendfig = plt.figure() leg = hlegendfig.legend(h_sort, l_sort, 'center', fontsize=20, frameon=False, ncol=2) hlegendfig.set_size_inches(11, 0.8) hlegendfig.set_tight_layout(True) plt.show() 
bert|matrix|reshape|from|modeling def reshape_from_matrix(output_tensor, orig_shape_list): """Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""" if len(orig_shape_list) == 2: return output_tensor output_shape = get_shape_list(output_tensor) orig_dims = orig_shape_list[0:-1] width = output_shape[-1] return tf.reshape(output_tensor, orig_dims + [width]) 
launch|src|model|Model def launch(self): step_time, loss = 0.0, 0.0 current_step = 0 previous_losses = [] writer = tf.summary.FileWriter(self.model_dir, self.sess.graph) if self.phase == 'test': if not distance_loaded: logging.info( 'Warning: distance module not installed. Do whole sequence comparison instead.' ) else: logging.info('Compare word based on edit distance.') num_correct = 0 num_total = 0 for batch in self.s_gen.gen(self.batch_size): start_time = time.time() bucket_id = batch['bucket_id'] img_data = batch['data'] zero_paddings = batch['zero_paddings'] decoder_inputs = batch['decoder_inputs'] target_weights = batch['target_weights'] encoder_masks = batch['encoder_mask'] file_list = batch['filenames'] real_len = batch['real_len'] grounds = [a for a in np.array([decoder_input.tolist() for decoder_input in decoder_inputs]).transpose()] _, step_loss, step_logits, step_attns = self.step(encoder_masks, img_data, zero_paddings, decoder_inputs, target_weights, bucket_id, self.forward_only) curr_step_time = time.time() - start_time step_time += curr_step_time / self.steps_per_checkpoint logging.info('step_time: %f, loss: %f, step perplexity: %f' % ( curr_step_time, step_loss, math.exp(step_loss) if step_loss < 300 else float('inf'))) loss += step_loss / self.steps_per_checkpoint current_step += 1 step_outputs = [b for b in np.array([np.argmax(logit, axis=1). tolist() for logit in step_logits]).transpose()] if self.visualize: step_attns = np.array([[a.tolist() for a in step_attn] for step_attn in step_attns]).transpose([1, 0, 2]) for idx, output, ground in zip(range(len(grounds)), step_outputs, grounds): flag_ground, flag_out = True, True num_total += 1 output_valid = [] ground_valid = [] for j in range(1, len(ground)): s1 = output[j - 1] s2 = ground[j] if s2 != 2 and flag_ground: ground_valid.append(s2) else: flag_ground = False if s1 != 2 and flag_out: output_valid.append(s1) else: flag_out = False if distance_loaded: num_incorrect = distance.levenshtein(output_valid, ground_valid) if self.visualize: self.visualize_attention(file_list[idx], step_attns [idx], output_valid, ground_valid, num_incorrect > 0, real_len) num_incorrect = float(num_incorrect) / len(ground_valid) num_incorrect = min(1.0, num_incorrect) else: if output_valid == ground_valid: num_incorrect = 0 else: num_incorrect = 1 if self.visualize: self.visualize_attention(file_list[idx], step_attns [idx], output_valid, ground_valid, num_incorrect > 0, real_len) num_correct += 1.0 - num_incorrect logging.info('%f out of %d correct' % (num_correct, num_total)) elif self.phase == 'train': total = self.s_gen.get_size() // self.batch_size with tqdm(desc='Train: ', total=total) as pbar: for epoch in range(self.num_epoch): logging.info('Generating first batch)') for i, batch in enumerate(self.s_gen.gen(self.batch_size)): num_total = 0 num_correct = 0 start_time = time.time() batch_len = batch['real_len'] bucket_id = batch['bucket_id'] img_data = batch['data'] zero_paddings = batch['zero_paddings'] decoder_inputs = batch['decoder_inputs'] target_weights = batch['target_weights'] encoder_masks = batch['encoder_mask'] summaries, step_loss, step_logits, _ = self.step( encoder_masks, img_data, zero_paddings, decoder_inputs, target_weights, bucket_id, self. forward_only) grounds = [a for a in np.array([decoder_input.tolist() for decoder_input in decoder_inputs]).transpose()] step_outputs = [b for b in np.array([np.argmax(logit, axis=1).tolist() for logit in step_logits]).transpose() ] for idx, output, ground in zip(range(len(grounds)), step_outputs, grounds): flag_ground, flag_out = True, True num_total += 1 output_valid = [] ground_valid = [] for j in range(1, len(ground)): s1 = output[j - 1] s2 = ground[j] if s2 != 2 and flag_ground: ground_valid.append(s2) else: flag_ground = False if s1 != 2 and flag_out: output_valid.append(s1) else: flag_out = False if distance_loaded: num_incorrect = distance.levenshtein(output_valid, ground_valid) num_incorrect = float(num_incorrect) / len( ground_valid) num_incorrect = min(1.0, num_incorrect) elif output_valid == ground_valid: num_incorrect = 0 else: num_incorrect = 1 num_correct += 1.0 - num_incorrect writer.add_summary(summaries, current_step) curr_step_time = time.time() - start_time step_time += curr_step_time / self.steps_per_checkpoint precision = num_correct / num_total logging.info( 'step %f - time: %f, loss: %f, perplexity: %f, precision: %f, batch_len: %f' % (current_step, curr_step_time, step_loss, math. exp(step_loss) if step_loss < 300 else float('inf'), precision, batch_len)) loss += step_loss / self.steps_per_checkpoint pbar.set_description('Train, loss={:.8f}'.format(step_loss) ) pbar.update() current_step += 1 if current_step % self.steps_per_checkpoint == 0: perplexity = math.exp(loss) if loss < 300 else float( 'inf') logging.info( 'global step %d step-time %.2f loss %f  perplexity %.2f' % (self.global_step.eval(), step_time, loss, perplexity)) previous_losses.append(loss) if not self.forward_only: checkpoint_path = os.path.join(self.model_dir, 'translate.ckpt') logging.info('Saving model, current_step: %d' % current_step) self.saver_all.save(self.sess, checkpoint_path, global_step=self.global_step) step_time, loss = 0.0, 0.0 
greedy|generate|atomic|make|batch def make_batch(X): X = np.array(X) assert X.ndim in [1, 2] if X.ndim == 1: X = np.expand_dims(X, axis=0) pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1]) pos_enc = np.expand_dims(pos_enc, axis=0) batch = np.stack([X, pos_enc], axis=-1) batch = torch.tensor(batch, dtype=torch.long).to(device) return batch 
controller|init|TrainingLog def __init__(self, config): self.model = config.model_name self.output_path = config.output_path self.bow_pred_method = config.bow_pred_method self.vae_seq2seq = config.vae_seq2seq self.is_cheat = config.is_cheat if self.model == 'bow_seq2seq': if self.bow_pred_method == 'seq2seq': self.log = {'loss': [0], 'enc_bow_loss': [0], 'enc_seq2seq_loss': [0], 'enc_loss': [0], 'dec_loss': [0], 'pred_overlap_topk': [0], 'pred_overlap_confident': [0], 'target_support': [0], 'pred_confident_support': [0], 'target_average': [0], 'predict_average_confident': [0], 'precision_confident': [0], 'recall_topk': [0]} elif self.bow_pred_method == 'seq_tag': if self.is_cheat: self.log = {'dec_loss': [0]} else: self.log = {'loss': [0], 'enc_bow_loss': [0], 'enc_loss': [ 0], 'dec_loss': [0], 'pred_overlap_topk': [0], 'pred_overlap_confident': [0], 'target_support': [0], 'pred_topk_support': [0], 'pred_confident_support': [0], 'target_average': [0], 'predict_average_confident': [0], 'precision_confident': [0], 'recall_confident': [0], 'precision_topk': [0], 'recall_topk': [0]} elif self.model == 'latent_bow': self.log = {'loss': [], 'enc_loss': [], 'dec_loss': [], 'pred_overlap_topk': [], 'pred_overlap_confident': [], 'pred_topk_support': [], 'pred_confident_support': [], 'target_support': [], 'predict_average_confident': [], 'target_average': [], 'pointer_ent': [], 'avg_max_ptr': [], 'avg_num_copy': [], 'precision_confident': [], 'recall_confident': [0], 'precision_topk': [], 'recall_topk': []} elif self.model == 'seq2seq': if self.vae_seq2seq: self.log = {'loss': [], 'dec_loss': [], 'kl_loss': []} else: self.log = {'loss': []} elif self.model == 'lm': self.log = {'loss': [], 'ppl': []} 
load|model|util def load_model(path): """Loads model and return it without DataParallel table.""" if os.path.isfile(path): print("=> loading checkpoint '{}'".format(path)) checkpoint = torch.load(path) N = checkpoint['state_dict']['top_layer.bias'].size() sob = 'sobel.0.weight' in checkpoint['state_dict'].keys() model = models.__dict__[checkpoint['arch']](sobel=sob, out=int(N[0]))  def rename_key(key): if not 'module' in key: return key return ''.join(key.split('.module')) checkpoint['state_dict'] = {rename_key(key): val for key, val in checkpoint['state_dict'].items()} model.load_state_dict(checkpoint['state_dict']) print('Loaded') else: model = None print("=> no checkpoint found at '{}'".format(path)) return model 
envs|gym|grid|3Env|init|PycolabGridWorldsLevel1|pycolab|worlds|env def __init__(self): super(PycolabGridWorldsEnv, self).__init__() 
v1|MobilenetV1Test|test|testBuildAndCheckAllEndPointsUptoConv2d|13|nets|mobilenet def testBuildAndCheckAllEndPointsUptoConv2d_13(self): batch_size = 5 height, width = 224, 224 inputs = tf.random_uniform((batch_size, height, width, 3)) with slim.arg_scope([slim.conv2d, slim.separable_conv2d], normalizer_fn =slim.batch_norm): _, end_points = mobilenet_v1.mobilenet_v1_base(inputs, final_endpoint='Conv2d_13_pointwise') _, explicit_padding_end_points = mobilenet_v1.mobilenet_v1_base(inputs, final_endpoint='Conv2d_13_pointwise', use_explicit_padding=True) endpoints_shapes = {'Conv2d_0': [batch_size, 112, 112, 32], 'Conv2d_1_depthwise': [batch_size, 112, 112, 32], 'Conv2d_1_pointwise': [batch_size, 112, 112, 64], 'Conv2d_2_depthwise': [batch_size, 56, 56, 64], 'Conv2d_2_pointwise': [batch_size, 56, 56, 128], 'Conv2d_3_depthwise': [batch_size, 56, 56, 128], 'Conv2d_3_pointwise': [batch_size, 56, 56, 128], 'Conv2d_4_depthwise': [batch_size, 28, 28, 128], 'Conv2d_4_pointwise': [batch_size, 28, 28, 256], 'Conv2d_5_depthwise': [batch_size, 28, 28, 256], 'Conv2d_5_pointwise': [batch_size, 28, 28, 256], 'Conv2d_6_depthwise': [batch_size, 14, 14, 256], 'Conv2d_6_pointwise': [batch_size, 14, 14, 512], 'Conv2d_7_depthwise': [batch_size, 14, 14, 512], 'Conv2d_7_pointwise': [batch_size, 14, 14, 512], 'Conv2d_8_depthwise': [batch_size, 14, 14, 512], 'Conv2d_8_pointwise': [batch_size, 14, 14, 512], 'Conv2d_9_depthwise': [batch_size, 14, 14, 512], 'Conv2d_9_pointwise': [batch_size, 14, 14, 512], 'Conv2d_10_depthwise': [batch_size, 14, 14, 512], 'Conv2d_10_pointwise': [batch_size, 14, 14, 512], 'Conv2d_11_depthwise': [batch_size, 14, 14, 512], 'Conv2d_11_pointwise': [batch_size, 14, 14, 512], 'Conv2d_12_depthwise': [batch_size, 7, 7, 512], 'Conv2d_12_pointwise': [batch_size, 7, 7, 1024], 'Conv2d_13_depthwise': [batch_size, 7, 7, 1024], 'Conv2d_13_pointwise': [batch_size, 7, 7, 1024]} self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys()) for endpoint_name, expected_shape in endpoints_shapes.items(): self.assertTrue(endpoint_name in end_points) self.assertListEqual(end_points[endpoint_name].get_shape().as_list( ), expected_shape) self.assertItemsEqual(endpoints_shapes.keys(), explicit_padding_end_points.keys()) for endpoint_name, expected_shape in endpoints_shapes.items(): self.assertTrue(endpoint_name in explicit_padding_end_points) self.assertListEqual(explicit_padding_end_points[endpoint_name]. get_shape().as_list(), expected_shape) 
gan|Network|network|build def build(self, input): raise NotImplementedError 
BiInteractionPooling|interaction|deepctr|layers|call def call(self, inputs, **kwargs): if K.ndim(inputs) != 3: raise ValueError( 'Unexpected inputs dimensions %d, expect to be 3 dimensions' % K.ndim(inputs)) concated_embeds_value = inputs square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis=1, keep_dims=True)) sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True) cross_term = 0.5 * (square_of_sum - sum_of_square) return cross_term 
parseVec|net|file|read def parseVec(net): return np.array(eval(net.readline()[:-1])) 
OscillatorNetwork|gym|control|update|all|cpg|oscillator|daisy|custom def update_all(self): """ One step of integration of all the oscillators :return: """ for i in range(self.n_oscillators): current = self.oscillators[i] phase_change = 2 * np.pi * current.v for j in range(self.n_oscillators): other = self.oscillators[j] phase_change += self.weights[i][j] * np.sin(other.phase - current.phase - self.phase_biases[i][j]) self.oscillators[i].phase += phase_change * STEP_SIZE a = current.a ddr = a * (0.25 * a * (current.R - current.r) - current.dr) self.oscillators[i].dr += ddr * STEP_SIZE self.oscillators[i].r += self.oscillators[i].dr * STEP_SIZE self.oscillators[i].x = self.oscillators[i].output() 
main|gcs|to|imagenet def main(argv): tf.logging.set_verbosity(tf.logging.INFO) if FLAGS.gcs_upload and FLAGS.project is None: raise ValueError('GCS Project must be provided.') if FLAGS.gcs_upload and FLAGS.gcs_output_path is None: raise ValueError('GCS output path must be provided.') elif FLAGS.gcs_upload and not FLAGS.gcs_output_path.startswith('gs://'): raise ValueError('GCS output path must start with gs://') if FLAGS.local_scratch_dir is None: raise ValueError('Scratch directory path must be provided.') raw_data_dir = FLAGS.raw_data_dir if raw_data_dir is None: raw_data_dir = os.path.join(FLAGS.local_scratch_dir, 'raw_data') tf.logging.info('Downloading data to raw_data_dir: %s' % raw_data_dir) download_dataset(raw_data_dir) training_records, validation_records = convert_to_tf_records(raw_data_dir) if FLAGS.gcs_upload: upload_to_gcs(training_records, validation_records) 
IsDerivedFunction|cpplint def IsDerivedFunction(clean_lines, linenum): """Check if current line contains an inherited function.  Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. Returns: True if current line contains a function with "override" virt-specifier. """ for i in xrange(linenum, max(-1, linenum - 10), -1): match = Match('^([^()]*\\w+)\\(', clean_lines.elided[i]) if match: line, _, closing_paren = CloseExpression(clean_lines, i, len( match.group(1))) return closing_paren >= 0 and Search('\\boverride\\b', line[ closing_paren:]) return False 
UniversalAntMazeEnv|envs|init def __init__(self, maze_id, contextual_reward, use_contexts=False, random_contexts=False, context_range=None, horizon=500): """Initialize the Universal environment.  Parameters ---------- maze_id : str the type of maze environment. One of "Maze", "Push", or "Fall" contextual_reward : function a reward function that takes as input (states, goals, next_states) and returns a float reward and whether the goal has been achieved use_contexts : bool, optional specifies whether to add contexts to the observations and add the contextual rewards random_contexts : bool specifies whether the context is a single value, or a random set of values between some range context_range : list of float or list of (float, float) the desired context / goal, or the (lower, upper) bound tuple for each dimension of the goal horizon : float, optional time horizon  Raises ------ AssertionError If the context_range is not the right form based on whether contexts are a single value or random across a range. """ super(UniversalAntMazeEnv, self).__init__(maze_id=maze_id, maze_height= 0.5, maze_size_scaling=8, n_bins=0, sensor_range=3.0, sensor_span=2 * np.pi, observe_blocks=False, put_spin_near_agent=False, top_down_view=False, manual_collision=False) self.horizon = horizon self.step_number = 0 self.use_contexts = use_contexts self.random_contexts = random_contexts self.context_range = context_range self.contextual_reward = contextual_reward self.current_context = None self.prev_obs = None if self.use_contexts: if self.random_contexts: assert all(isinstance(i, tuple) for i in self.context_range ), 'When using random contexts, every element in context_range, must be a tuple of (min,max) values.' else: assert all(not isinstance(i, tuple) for i in self.context_range ), 'When not using random contexts, every element in context_range, must be a single value.' 
queued|generate|trainer|id|run def _generate_run_id(size=6, chars=None): """Generate a random ID of length `size`.  Parameters ---------- size : int chars : Optional[str] Optional list of characters to use for generating the ID.  Returns ------- str Returns a random identifier of length `size`.  """ if chars is None: chars = string.ascii_uppercase + string.digits import random return ''.join(random.choice(chars) for _ in range(size)) 
transform|utils|3d|coordinates def transform_coordinates_3d(coordinates, RT): """ Input: coordinates: [3, N] RT: [4, 4] Return new_coordinates: [3, N]  """ assert coordinates.shape[0] == 3 coordinates = np.vstack([coordinates, np.ones((1, coordinates.shape[1]), dtype=np.float32)]) new_coordinates = RT @ coordinates new_coordinates = new_coordinates[:3, :] / new_coordinates[(3), :] return new_coordinates 
gan|init|network|VAEEncoder def __init__(self, output_length, stride=2, kernel=4, scope_name= 'VAEEncoder', *args, **kwargs): super(VAEEncoder, self).__init__(*args, scope_name=scope_name, **kwargs) self.output_length = output_length self.stride = stride self.kernel = kernel 
avod|flip|kitti|plane|datasets|ground|aug def flip_ground_plane(ground_plane): """Flips the ground plane by negating the x coefficient (ax + by + cz + d = 0)  Args: ground_plane: ground plane coefficients  Returns: Flipped ground plane coefficients """ flipped_ground_plane = np.copy(ground_plane) flipped_ground_plane[0] = -ground_plane[0] return flipped_ground_plane 
rad2deg|utils def rad2deg(rad): return rad * 360.0 / (2 * np.pi) 
string|NodeLookup|setup|id|to|imagenet def id_to_string(self, node_id): if node_id not in self.node_lookup: return '' return self.node_lookup[node_id] 
initialize|plato|component|based|agent|user|us|AgendaBasedUS|simulator|agenda def initialize(self, args): """ Initializes the user simulator, e.g. before each dialogue episode.  :return: Nothing """ if 'goal' not in args: goal_slot_selection_weights = None if 'goal_slot_selection_weights' in args: goal_slot_selection_weights = args['goal_slot_selection_weights'] self.goal = self.goal_generator.generate(goal_slot_selection_weights =goal_slot_selection_weights) else: self.goal = deepcopy(args['goal']) self.agenda.initialize(deepcopy(self.goal)) if self.nlu: self.nlu.initialize({}) if self.nlg: self.nlg.initialize({}) self.prev_system_acts = None self.curr_patience = self.patience self.dialogue_turn = 0 self.offer_made = False self.prev_offer_name = None 
regression|init|NormalPrior|misc|layers def __init__(self, n_in, n_out, w_name, params=None): super(NormalPrior, self).__init__(n_in, n_out, w_name) if params is None or isinstance(params, dict) and len(params) == 0: mean = 0.0 logstd = 0.0 elif isinstance(params, dict) and 'mean' in params and 'logstd' in params: mean = params['p_mean'] logstd = params['p_logstd'] else: raise ValueError('Not an appropriate param') self._p_mean = mean * tf.ones([self.n_in + 1, self.n_out]) self._p_logstd = logstd * tf.ones([self.n_in + 1, self.n_out]) 
ops|classification|NormalMeanVarianceNegativeLogProbLoss|loss|inner|hessian|functions|factor|static|shape @property def hessian_factor_inner_static_shape(self): raise NotImplementedError() 
create|attention|layers|rpr|thumt def create_rpr(orginal_var, length_q, length_kv, max_relative_dis, name=None): """ Create relative positional representation :param orginal_var: A tensor with shape [2*max_relative_dis+1, depth] :param length_q: An integer :param length_kv: An integer :param max_relative_dis: An integer :returns: A tensor with shape [length_q, length_kv, depth] """ with tf.name_scope(name, default_name='create_rpr', values=[orginal_var]): idxs = tf.reshape(tf.range(length_kv), [-1, 1]) idys = tf.reshape(tf.range(length_kv), [1, -1]) ids = idxs - idys ids = ids + max_relative_dis ids = tf.maximum(ids, 0) ids = tf.minimum(ids, 2 * max_relative_dis) ids = ids[-length_q:, :] rpr = tf.gather(orginal_var, ids) return rpr 
han2one|uni2onehot def uni2onehot(s): res = np.zeros(len(uniquealp)) if s in uniquealp: res[uniquealp.index(s)] = 1 return res 
images|to|carlini def to_carlini_images(x): x = x.transpose(0, 2, 3, 1) x_ca = (2 * x - 1) / 2 return np.asarray(x_ca, dtype=np.float32) 
tensorflow|kaffe|init|TensorFlowNode|transformer def __init__(self, op, *args, **kwargs): self.op = op self.args = args self.kwargs = list(kwargs.items()) self.node = None 
tf|NotBuggySupervisor|prepare|wait|or|for|session|common|utils def prepare_or_wait_for_session(self, master='', config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True ): """Make sure the model is ready to be used.  Create a session on 'master', recovering or initializing the model as needed, or wait for a session to be ready.  If running as the chief and `start_standard_service` is set to True, also call the session manager to start the standard services.  Args: master: name of the TensorFlow master to use.  See the `tf.Session` constructor for how this is interpreted. config: Optional ConfigProto proto used to configure the session, which is passed as-is to create the session. wait_for_checkpoint: Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False. max_wait_secs: Maximum time to wait for the session to become available. start_standard_services: Whether to start the standard services and the queue runners.  Returns: A Session object that can be used to drive the model. """ self._coord.clear_stop() if self._is_chief: sess, initialized = self._session_manager.recover_session(master, self.saver, checkpoint_dir=self._logdir, wait_for_checkpoint= wait_for_checkpoint, max_wait_secs=max_wait_secs, config=config) if start_standard_services: print('Starting queue runners') self.start_queue_runners(sess) if not initialized: if not self.init_op and not self._init_fn: raise RuntimeError( 'Model is not initialized and no init_op or init_fn was given' ) if self.init_op: sess.run(self.init_op, feed_dict=self._init_feed_dict) if self._init_fn: self._init_fn(sess) not_ready = self._session_manager._model_not_ready(sess) if not_ready: raise RuntimeError( 'Init operations did not make model ready.  Init op: %s, init fn: %s, error: %s' % (self.init_op.name, self._init_fn, not_ready)) self._write_graph() if start_standard_services: self.start_standard_services(sess) else: sess = self._session_manager.wait_for_session(master, config=config, max_wait_secs=max_wait_secs) if start_standard_services: self.start_queue_runners(sess) return sess 
prefab|gym|sprites|parts|pycolab|south|MazeWalker def _south(self, board, the_plot): """Try moving one cell downward. Returns `None` on success.""" return self._move(board, the_plot, self._SOUTH) 
utils|printTrace def printTrace(message): print('<' + str(datetime.datetime.now()) + '>  ' + str(message)) 
dact|dtl|plato|component|agent|to|DTLUserSimulator|input|language|user|receive|simulator def receive_input(self, system_acts): """ Process input received and do some housekeeping.  :param system_acts: list containing dialogue acts from the system :return: nothing """ if self.prev_sys_acts and self.prev_sys_acts == system_acts: self.curr_patience -= 1 else: self.curr_patience = self.patience self.prev_sys_acts = deepcopy(system_acts) self.input_system_acts = deepcopy(system_acts) for system_act in system_acts: if system_act.intent == 'offer': self.offer_made = True self.goal.actual_requests = {} for item in self.goal.requests: self.goal.requests[item].value = '' inform_dact = DialogueAct('inform', []) for system_act in system_acts: if system_act.intent in ['inform', 'offer']: inform_dact.params += deepcopy(system_act.params) if self.offer_made: meets_constraints = all([(i.value == self.goal.constraints[i.slot]. value) for i in inform_dact.params if i.slot in self.goal. constraints]) if meets_constraints: for item in inform_dact.params: if item.slot in self.goal.actual_requests: self.goal.actual_requests[item.slot].value = item.value if item.slot in self.goal.requests: self.goal.requests[item.slot].value = item.value self.goal_met = True for slot in self.goal.requests: if not self.goal.requests[slot].value: self.goal_met = False break 
celeba|Attention|t|sagan|build def build(self, input_shape): self.gamma = self.add_weight(name='gamma', shape=[1], initializer= 'zeros', trainable=True) 
matrices|tangents|assert|close|neural|utils def assert_close_matrices(self, expected, actual, rtol): self.assertEqual(expected.shape, actual.shape) relative_error = np.linalg.norm(actual - expected) / np.maximum(np. linalg.norm(expected), 1e-12) if relative_error > rtol or np.isnan(relative_error): self.fail(self.failureException(float(relative_error), expected, actual)) else: print('PASSED with %f relative error.' % relative_error) 
large|rows|official|test|io|file|BaseTest|data|core|utils def test_large_rows_large_core(self): self._test_sharding(**_TEST_CASES[7]) 
minibatch|construct|adj|NodeMinibatchIterator|test|graphsage def construct_test_adj(self): adj = len(self.id2idx) * np.ones((len(self.id2idx) + 1, self.max_degree)) for nodeid in self.G.nodes(): neighbors = np.array([self.id2idx[neighbor] for neighbor in self.G. neighbors(nodeid)]) if len(neighbors) == 0: continue if len(neighbors) > self.max_degree: neighbors = np.random.choice(neighbors, self.max_degree, replace=False) elif len(neighbors) < self.max_degree: neighbors = np.random.choice(neighbors, self.max_degree, replace=True) adj[(self.id2idx[nodeid]), :] = neighbors return adj 
avod|label|indices|mini|mask|class|MiniBatchUtils|batch|core|utils def mask_class_label_indices(self, mb_pos_mask, mb_mask, max_iou_indices, class_indices): """ Samples a mini batch based on anchor ious with ground truth  Args: mb_pos_mask: a boolean tensor mask of size [N] of positive anchors in the mini-batch mb_mask: a boolean tensor mask of size [N] of all anchors in the mini-batch. max_iou_indices: a tensor of shape [N] indicating the indices corresponding to the maximum IoU between predicted anchors and the ground truth anchors. class_indices: a tensor of shape [num_of_classes] indicating the class labels as indices. For instance indices=[0, 1, 2, 3] indicating 'background, car, pedestrian, cyclist' etc.  Returns: masked_class_indices: a tensor of boolean mask for class label indices. This gives the indices for the positive classes and masks negatives or background classes by zero's. """ masked_argmax = tf.boolean_mask(max_iou_indices, mb_mask) masked_labels = tf.gather(class_indices, masked_argmax) mask_pos_mask = tf.boolean_mask(mb_pos_mask, mb_mask) mb_class_indices = tf.multiply(tf.cast(masked_labels, tf.int32), tf. cast(mask_pos_mask, tf.int32)) return mb_class_indices 
fpn|coord|model|graph|build def build_fpn_coord_graph(rois, feature_maps, image_shape, pool_size, num_classes, use_bn): """Builds the computation graph of the coordinate map head of Feature Pyramid Network.  rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: Coordinate maps [batch, roi_count, height, width, num_classes, 3] """ x = PyramidROIAlign([pool_size, pool_size], image_shape, name= 'roi_align_coord')([rois] + feature_maps) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv1')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn1')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv2')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn2')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv3')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn3')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv4')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn4')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv5')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn5')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv6')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn6')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv7')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn7')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv8')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn8')(x) x = KL.Activation('relu')(x) feature_x = KL.TimeDistributed(KL.Conv2DTranspose(512, (2, 2), strides= 2, activation='relu'), name='mrcnn_coord_deconv')(x) x = KL.TimeDistributed(KL.Conv2D(3 * num_classes, (1, 1), strides=1, activation='sigmoid'), name='mrcnn_coord_reshape')(feature_x) x = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, 3]), name='mrcnn_coord')(x) mrcnn_coord_x = KL.Lambda(lambda x: x[:, :, :, :, :, (0)], name= 'mrcnn_coord_x')(x) mrcnn_coord_y = KL.Lambda(lambda x: x[:, :, :, :, :, (1)], name= 'mrcnn_coord_y')(x) mrcnn_coord_z = KL.Lambda(lambda x: x[:, :, :, :, :, (2)], name= 'mrcnn_coord_z')(x) return mrcnn_coord_x, mrcnn_coord_y, mrcnn_coord_z, feature_x 
maze|get|ant|AntMazeEnv|env|ori def get_ori(self): """Return the orientation of the ant.""" return self.wrapped_env.get_ori() 
get|arguments|input|data|learning def get_input_arguments(): parser = argparse.ArgumentParser() parser.add_argument('-m', '--mode', help= 'if 1, run under training mode, if 0 run under test mode', type=int, default=1) args = parser.parse_args() return args 
utils|get|dimensions def get_dimensions(path): with open(path) as f: first_line = f.readline() first_line = first_line.split(' ') if len(first_line) == 2: return int(first_line[1]) else: return None 
replaymemory|td|alg|not|or|batch|reward|ReplayMemory def reward_batch(self): last = self.get_size() - self._p.batch_offset return list(islice(self._r, last - self._p.batch_size, last)) 
get|size|params|vgg16|and|from|input|num|channel|VGG16|calculation def get_input_size(index): size = ori_size if not isinstance(size, int): size = size[0] if index >= 0 and index <= 1: return size elif index >= 2 and index <= 3: return size / 2 elif index >= 4 and index <= 6: return size / 4 elif index >= 7 and index <= 9: return size / 8 elif index >= 10 and index <= 12: return size / 16 elif index >= 13: return size / 32 return size 
scoring|triple|pathfinder|path|score def score_triple(h, t, r, flag): res = -10 for i in range(len(r)): if flag[i]: temp_h, temp_t = t, h else: temp_h, temp_t = h, t res = max(res, (1 + 1 - spatial.distance.cosine(r[i], temp_t - temp_h)) / 2) return res 
token|ids|data|prep|to def data_to_token_ids(data, vocab_path, tokenizer=None, normalize_digits=False ): t1 = time() print('Tokenizing data') vocab, _ = initialize_vocabulary(vocab_path) vcol = ['Age', 'Birthday'] onehotcol = ['Gender', 'Marital_State', 'City', 'Province'] onehot_size = {'Province': 29, 'City': 47, 'Gender': 2, 'Marital_State': 8} counter = 0 prep_data = [] data = data.reset_index(drop=True) bloglist = data['Blog'] blog2inds = {} for ii, uu in bloglist.items(): if uu not in blog2inds: blog2inds[uu] = [ii] else: blog2inds[uu].append(ii) for raw_blog, inds in blog2inds.items(): ablog = [[], []] blog = sentence_to_token_ids(raw_blog, vocab, tokenizer, normalize_digits) ablog[0].append([blog]) pairs = data.iloc[inds] for ii, pair in pairs.iterrows(): cmt = sentence_to_token_ids(pair['Comment'], vocab, tokenizer, normalize_digits) desc = sentence_to_token_ids(pair['Individual_Description'], vocab, tokenizer, normalize_digits) usr_feat = pair[vcol].values for feat in onehotcol: vonehot = np.eye(onehot_size[feat])[int(pair[feat])] usr_feat = np.concatenate([usr_feat, vonehot]) usr_feat = list(usr_feat) ablog[1].append([cmt, usr_feat, desc]) counter += 1 if counter % 100000 == 0: print('    processing line %d' % counter, (time() - t1) / 60) prep_data.append(ablog) return prep_data 
process|clean|data|sst|str def clean_str_sst(string): string = re.sub("[^A-Za-z0-9(),!?\\'\\`]", ' ', string) string = re.sub('\\s{2,}', ' ', string) return string.strip().lower() 
sample|samples|categorical|source|SRGANs|master|random|Regularization|GANs|miscs|Spectral def sample_categorical(n_cat, batchsize, distribution='uniform', xp=np): if distribution == 'uniform': return xp.random.randint(low=0, high=n_cat, size=batchsize).astype(xp .int32) else: raise NotImplementedError 
gen|download|wgan|progress|maybe|and|extract|celebA|utils def _progress(count, block_size, total_size): sys.stdout.write('\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0)) sys.stdout.flush() 
src|GCNModelAE|model|build def _build(self): self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim, output_dim=FLAGS.hidden1, adj=self.adj, features_nonzero=self. features_nonzero, act=tf.nn.relu, dropout=self.dropout, logging= self.logging)(self.inputs) self.embeddings = GraphConvolution(input_dim=FLAGS.hidden1, output_dim= FLAGS.hidden2, adj=self.adj, act=tf.nn.relu, dropout=self.dropout, logging=self.logging)(self.hidden1) self.attribute_decoder_layer1 = GraphConvolution(input_dim=FLAGS. hidden2, output_dim=FLAGS.hidden1, adj=self.adj, act=tf.nn.relu, dropout=self.dropout, logging=self.logging)(self.embeddings) self.attribute_decoder_layer2 = GraphConvolution(input_dim=FLAGS. hidden1, output_dim=self.input_dim, adj=self.adj, act=tf.nn.relu, dropout=self.dropout, logging=self.logging)(self. attribute_decoder_layer1) self.structure_decoder_layer1 = GraphConvolution(input_dim=FLAGS. hidden2, output_dim=FLAGS.hidden1, adj=self.adj, act=tf.nn.relu, dropout=self.dropout, logging=self.logging)(self.embeddings) self.structure_decoder_layer2 = InnerProductDecoder(input_dim=FLAGS. hidden1, act=tf.nn.sigmoid, logging=self.logging)(self. structure_decoder_layer1) self.attribute_reconstructions = self.attribute_decoder_layer2 self.structure_reconstructions = self.structure_decoder_layer2 
loader|load|data|UCR def load_UCR_data(datadir, dataset): X_train, X_test, y_train, y_test = load_txt_file(datadir, dataset) if len(X_train.shape) < 3: X_train = np.expand_dims(X_train, -1) X_test = np.expand_dims(X_test, -1) class_names = np.unique(y_train, axis=0) y_train_tmp = np.zeros(len(y_train)) y_test_tmp = np.zeros(len(y_test)) for i, class_name in enumerate(class_names): y_train_tmp[y_train == class_name] = i y_test_tmp[y_test == class_name] = i y_train = y_train_tmp y_test = y_test_tmp return X_train, X_test, y_train, y_test 
models|Vae|src|encoder|master|facenet|base|vae|generative def encoder(self, images, is_training): raise NotImplementedError 
BucketData|add|src|data|bucketdata|util def __add__(self, other): res = BucketData() res.data_list = self.data_list + other.data_list res.label_list = self.label_list + other.label_list res.max_width = max(self.max_width, other.max_width) res.max_label_len = max((self.max_label_len, other.max_label_len)) return res 
deepMOT|focal|entropy|loss|master|weighted|binary|utils def weighted_binary_focal_entropy(output, target, weights=None, gamma=2): output = torch.clamp(output, min=1e-12, max=1 - 1e-12) if weights is not None: assert weights.size(1) == 2 loss = torch.pow(output[(0), :], gamma) * target[(1), :] * weights[:, (1)].item() * torch.log(output[(1), :]) + target[(0), :] * weights[ :, (0)].item() * torch.log(output[(0), :]) * torch.pow(output[( 1), :], gamma) else: loss = target[(1), :] * torch.log(output[(1), :]) + target[(0), : ] * torch.log(output[(0), :]) return torch.neg(torch.mean(loss)) 
main def main(): if IS_TRAINING: original_imgs_path = list_images( 'D:/Database/Image_fusion_MSCOCO/original/') validatioin_imgs_path = list_images('./validation/') for ssim_weight, model_save_path in zip(SSIM_WEIGHTS, MODEL_SAVE_PATHS ): print('\nBegin to train the network ...\n') train_recons(original_imgs_path, validatioin_imgs_path, model_save_path, model_pre_path, ssim_weight, EPOCHES, BATCH_SIZE, debug=True) print('\nSuccessfully! Done training...\n') elif IS_VIDEO: ssim_weight = SSIM_WEIGHTS[0] model_path = MODEL_SAVE_PATHS[0] IR_path = list_images('video/1_IR/') VIS_path = list_images('video/1_VIS/') output_save_path = 'video/fused' + str(ssim_weight) + '/' generate(IR_path, VIS_path, model_path, model_pre_path, ssim_weight, 0, IS_VIDEO, 'addition', output_path=output_save_path) else: ssim_weight = SSIM_WEIGHTS[2] model_path = MODEL_SAVE_PATHS[2] print('\nBegin to generate pictures ...\n') path = 'images/MF_images/color/' for i in range(1): index = i + 1 infrared = path + 'lytro-2-A.jpg' visible = path + 'lytro-2-B.jpg' fusion_type = 'addition' output_save_path = 'outputs' generate(infrared, visible, model_path, model_pre_path, ssim_weight, index, IS_VIDEO, is_RGB, type=fusion_type, output_path=output_save_path) 
combo|label|no|main def main(): print( 'parsing data from log files which are generated by Atheros-CSI-TOOL\n' ) data_generator = DataLogParser(conf.n_timestamps, conf.D, conf. step_size, conf.ntx_max, conf.nrx_max, conf.nsubcarrier_max, conf. data_folder, conf.log_folder, conf.skip_frames, conf. time_offset_ratio, conf.day_conf, conf.label) data_generator.generate_image_no_label(conf.draw_date, conf.draw_label) test_data = data_generator.get_data_no_label() print('Pre-processing data\n') data_process = DataPreprocess(conf.n_timestamps, conf.D, conf.step_size, conf.ntx_max, conf.ntx, conf.nrx_max, conf.nrx, conf. nsubcarrier_max, conf.nsubcarrier, conf.data_shape_to_nn, conf. data_folder, conf.label) data_process.add_image_no_label(test_data) data_process.signal_processing(conf.do_fft, conf.fft_shape) data_process.prepare_shape() final_test_data = data_process.get_data_no_label() nn_model = NeuralNetworkModel(conf.data_shape_to_nn, conf. abs_shape_to_nn, conf.phase_shape_to_nn, conf.total_classes) print('Get test result using existing model (in test mode)\n') nn_model.load_model(conf.model_name) for key in final_test_data: plt.figure() total_test = len(final_test_data[key]) cc = 1 for idx in final_test_data[key]: result = nn_model.get_no_label_result(final_test_data[key][idx], output_label=True) plt.subplot(total_test, 1, cc) plt.plot(result) plt.title(idx) plt.ylim(0, 1.05) cc = cc + 1 plt.suptitle(key) nn_model.end() plt.show() print('Done!') 
texar|ngrams|get|bleu|evals def _get_ngrams(segment, max_order): """Extracts all n-grams up to a given maximum order from an input segment.  Args: segment: text segment from which n-grams will be extracted. max_order: maximum length in tokens of the n-grams returned by this methods.  Returns: The Counter containing all n-grams upto max_order in segment with a count of how many times each n-gram occurred. """ ngram_counts = collections.Counter() for order in range(1, max_order + 1): for i in range(0, len(segment) - order + 1): ngram = tuple(segment[i:i + order]) ngram_counts[ngram] += 1 return ngram_counts 
initialise|extensions|multi|MultiModalApplication|modal|application|weighted|sampler def initialise_weighted_sampler(self): self.sampler = [[WeightedSampler(reader=reader, window_sizes=self. data_param, batch_size=self.net_param.batch_size, windows_per_image =self.action_param.sample_per_volume, queue_length=self.net_param. queue_length) for reader in self.readers]] 
codecs|test|craystack|db|gaussian def test_gaussian_db(): bin_precision = 8 coding_precision = 12 batch_size = 5 bin_means = rng.randn() bin_stdds = np.exp(rng.randn() / 10) means = bin_means + rng.randn() / 10 stdds = bin_stdds * np.exp(rng.randn() / 10.0) data = np.array([rng.choice(1 << bin_precision) for _ in range(batch_size)] ) check_codec((batch_size,), cs.DiagGaussian_GaussianBins(means, stdds, bin_means, bin_stdds, coding_precision, bin_precision), data) 
get|xy|dien|fd|run def get_xy_fd(use_neg=False): feature_dim_dict = {'sparse': [SingleFeat('user', 3), SingleFeat( 'gender', 2), SingleFeat('item', 3 + 1), SingleFeat('item_gender', 2 + 1)], 'dense': [SingleFeat('score', 0)]} behavior_feature_list = ['item', 'item_gender'] uid = np.array([0, 1, 2]) ugender = np.array([0, 1, 0]) iid = np.array([1, 2, 3]) igender = np.array([1, 2, 1]) score = np.array([0.1, 0.2, 0.3]) hist_iid = np.array([[1, 2, 3, 0], [1, 2, 3, 0], [1, 2, 0, 0]]) hist_igender = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [2, 1, 0, 0]]) behavior_length = np.array([3, 3, 2]) feature_dict = {'user': uid, 'gender': ugender, 'item': iid, 'item_gender': igender, 'hist_item': hist_iid, 'hist_item_gender': hist_igender, 'score': score} x = [feature_dict[feat.name] for feat in feature_dim_dict['sparse']] + [ feature_dict[feat.name] for feat in feature_dim_dict['dense']] + [ feature_dict['hist_' + feat] for feat in behavior_feature_list] if use_neg: feature_dict['neg_hist_item'] = np.array([[1, 2, 3, 0], [1, 2, 3, 0 ], [1, 2, 0, 0]]) feature_dict['neg_hist_item_gender'] = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [2, 1, 0, 0]]) x += [feature_dict['neg_hist_' + feat] for feat in behavior_feature_list] x += [behavior_length] y = [1, 0, 1] return x, y, feature_dim_dict, behavior_feature_list 
beamsearch|get|model|PCGNBeamSearchDecoder|context|PCGN def get_context(self, query, keys, blog_desc_inetract, batch_size, num_units): query = tf.matmul(tf.reshape(query, (batch_size, num_units)), blog_desc_inetract) query = array_ops.expand_dims(query, 1) score = math_ops.matmul(query, keys, transpose_b=True) score = array_ops.squeeze(score, [1]) alignments = nn_ops.softmax(score) expanded_alignments = array_ops.expand_dims(alignments, 1) context = math_ops.matmul(expanded_alignments, keys) context = array_ops.squeeze(context, [1]) return context 
fisher|compute|ops|classification|factors|ConvOutputKroneckerFactor|cov|new def _compute_new_cov(self, idx=0): with _maybe_colocate_with(self._outputs_grads[idx], self. _colocate_cov_ops_with_inputs): reshaped_tensor = array_ops.reshape(self._outputs_grads[idx], [-1, self._out_channels]) return _compute_cov(reshaped_tensor) 
transition|preprocess|helper|probs|SecondOrderRandomWalker|calculation def preprocess_transition_probs(self): """ Preprocessing of transition probabilities for guiding the random walks. """ G = self.G is_directed = self.is_directed alias_nodes = {} print('') print('Preprocesing.\n') for node in tqdm(G.nodes()): unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G. neighbors(node))] norm_const = sum(unnormalized_probs) normalized_probs = [(float(u_prob) / norm_const) for u_prob in unnormalized_probs] alias_nodes[node] = alias_setup(normalized_probs) alias_edges = {} triads = {} if is_directed: for edge in G.edges(): alias_edges[edge] = self.get_alias_edge(edge[0], edge[1]) else: for edge in tqdm(G.edges()): alias_edges[edge] = self.get_alias_edge(edge[0], edge[1]) alias_edges[edge[1], edge[0]] = self.get_alias_edge(edge[1], edge[0]) self.alias_nodes = alias_nodes self.alias_edges = alias_edges return 
Conv2D|conv2d|tflib|ops def Conv2D(name, input_dim, output_dim, filter_size, inputs, he_init=True, mask_type=None, stride=1, weightnorm=None, biases=True, gain=1.0): """ inputs: tensor of shape (batch size, num channels, height, width) mask_type: one of None, 'a', 'b'  returns: tensor of shape (batch size, num channels, height, width) """ with tf.name_scope(name) as scope: if mask_type is not None: mask_type, mask_n_channels = mask_type mask = np.ones((filter_size, filter_size, input_dim, output_dim ), dtype='float32') center = filter_size // 2 mask[center + 1:, :, :, :] = 0.0 mask[(center), center + 1:, :, :] = 0.0 for i in xrange(mask_n_channels): for j in xrange(mask_n_channels): if (mask_type == 'a' and i >= j or mask_type == 'b' and i > j): mask[(center), (center), i::mask_n_channels, j:: mask_n_channels] = 0.0  def uniform(stdev, size): return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt(3), size=size).astype('float32') fan_in = input_dim * filter_size ** 2 fan_out = output_dim * filter_size ** 2 / stride ** 2 if mask_type is not None: fan_in /= 2.0 fan_out /= 2.0 if he_init: filters_stdev = np.sqrt(4.0 / (fan_in + fan_out)) else: filters_stdev = np.sqrt(2.0 / (fan_in + fan_out)) if _weights_stdev is not None: filter_values = uniform(_weights_stdev, (filter_size, filter_size, input_dim, output_dim)) else: filter_values = uniform(filters_stdev, (filter_size, filter_size, input_dim, output_dim)) filter_values *= gain filters = lib.param(name + '.Filters', filter_values) if weightnorm == None: weightnorm = _default_weightnorm if weightnorm: norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0, 1, 2))) target_norms = lib.param(name + '.g', norm_values) with tf.name_scope('weightnorm') as scope: norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0, 1, 2])) filters = filters * (target_norms / norms) if mask_type is not None: with tf.name_scope('filter_mask'): filters = filters * mask result = tf.nn.conv2d(input=inputs, filter=filters, strides=[1, 1, stride, stride], padding='SAME', data_format='NCHW') if biases: _biases = lib.param(name + '.Biases', np.zeros(output_dim, dtype='float32')) result = tf.nn.bias_add(result, _biases, data_format='NCHW') return result 
sample|generate|SRGANs|master|Regularization|GANs|evaluation|Spectral def sample_generate(gen, dst, rows=10, cols=10, seed=0): """Visualization of rows*cols images randomly generated by the generator."""  @chainer.training.make_extension() def make_image(trainer): np.random.seed(seed) n_images = rows * cols x = gen_images(gen, n_images, batchsize=n_images) _, _, h, w = x.shape x = x.reshape((rows, cols, 3, h, w)) x = x.transpose(0, 3, 1, 4, 2) x = x.reshape((rows * h, cols * w, 3)) preview_dir = '{}/preview'.format(dst) preview_path = preview_dir + '/image{:0>8}.png'.format(trainer. updater.iteration) if not os.path.exists(preview_dir): os.makedirs(preview_dir) Image.fromarray(x).save(preview_path) return make_image 
test|punctuation|tokenization|TokenizationTest|is def test_is_punctuation(self): self.assertTrue(_is_punctuation('-')) self.assertTrue(_is_punctuation('$')) self.assertTrue(_is_punctuation('`')) self.assertTrue(_is_punctuation('.')) self.assertFalse(_is_punctuation('A')) self.assertFalse(_is_punctuation(' ')) 
construct|cond|graph|delta|build|patch def cond(unused_delta, i): return tf.less(i, num_steps) 
xlnet|master|transformer|modeling|xl def transformer_xl(inp_k, n_token, n_layer, d_model, n_head, d_head, d_inner, dropout, dropatt, attn_type, bi_data, initializer, is_training, mem_len=None, inp_q=None, mems=None, same_length=False, clamp_len=-1, untie_r=False, use_tpu=True, input_mask=None, perm_mask=None, seg_id= None, reuse_len=None, ff_activation='relu', target_mapping=None, use_bfloat16=False, scope='transformer', **kwargs): """ Defines a Transformer-XL computation graph with additional support for XLNet.  Args:  inp_k: int32 Tensor in shape [len, bsz], the input token IDs. seg_id: int32 Tensor in shape [len, bsz], the input segment IDs. input_mask: float32 Tensor in shape [len, bsz], the input mask. 0 for real tokens and 1 for padding. mems: a list of float32 Tensors in shape [mem_len, bsz, d_model], memory from previous batches. The length of the list equals n_layer. If None, no memory is used. perm_mask: float32 Tensor in shape [len, len, bsz]. If perm_mask[i, j, k] = 0, i attend to j in batch k; if perm_mask[i, j, k] = 1, i does not attend to j in batch k. If None, each position attends to all the others. target_mapping: float32 Tensor in shape [num_predict, len, bsz]. If target_mapping[i, j, k] = 1, the i-th predict in batch k is on the j-th token. Only used during pretraining for partial prediction. Set to None during finetuning. inp_q: float32 Tensor in shape [len, bsz]. 1 for tokens with losses and 0 for tokens without losses. Only used during pretraining for two-stream attention. Set to None during finetuning.  n_layer: int, the number of layers. d_model: int, the hidden size. n_head: int, the number of attention heads. d_head: int, the dimension size of each attention head. d_inner: int, the hidden size in feed-forward layers. ff_activation: str, "relu" or "gelu". untie_r: bool, whether to untie the biases in attention. n_token: int, the vocab size.  is_training: bool, whether in training mode. use_tpu: bool, whether TPUs are used. use_bfloat16: bool, use bfloat16 instead of float32. dropout: float, dropout rate. dropatt: float, dropout rate on attention probabilities. init: str, the initialization scheme, either "normal" or "uniform". init_range: float, initialize the parameters with a uniform distribution in [-init_range, init_range]. Only effective when init="uniform". init_std: float, initialize the parameters with a normal distribution with mean 0 and stddev init_std. Only effective when init="normal". mem_len: int, the number of tokens to cache. reuse_len: int, the number of tokens in the currect batch to be cached and reused in the future. bi_data: bool, whether to use bidirectional input pipeline. Usually set to True during pretraining and False during finetuning. clamp_len: int, clamp all relative distances larger than clamp_len. -1 means no clamping. same_length: bool, whether to use the same attention length for each token. summary_type: str, "last", "first", "mean", or "attn". The method to pool the input to get a vector representation. initializer: A tf initializer. scope: scope name for the computation graph. """ tf.logging.info('memory input {}'.format(mems)) tf_float = tf.bfloat16 if use_bfloat16 else tf.float32 tf.logging.info('Use float type {}'.format(tf_float)) new_mems = [] with tf.variable_scope(scope): if untie_r: r_w_bias = tf.get_variable('r_w_bias', [n_layer, n_head, d_head ], dtype=tf_float, initializer=initializer) r_r_bias = tf.get_variable('r_r_bias', [n_layer, n_head, d_head ], dtype=tf_float, initializer=initializer) else: r_w_bias = tf.get_variable('r_w_bias', [n_head, d_head], dtype= tf_float, initializer=initializer) r_r_bias = tf.get_variable('r_r_bias', [n_head, d_head], dtype= tf_float, initializer=initializer) bsz = tf.shape(inp_k)[1] qlen = tf.shape(inp_k)[0] mlen = tf.shape(mems[0])[0] if mems is not None else 0 klen = mlen + qlen if attn_type == 'uni': attn_mask = _create_mask(qlen, mlen, tf_float, same_length) attn_mask = attn_mask[:, :, (None), (None)] elif attn_type == 'bi': attn_mask = None else: raise ValueError('Unsupported attention type: {}'.format(attn_type) ) if input_mask is not None and perm_mask is not None: data_mask = input_mask[None] + perm_mask elif input_mask is not None and perm_mask is None: data_mask = input_mask[None] elif input_mask is None and perm_mask is not None: data_mask = perm_mask else: data_mask = None if data_mask is not None: mems_mask = tf.zeros([tf.shape(data_mask)[0], mlen, bsz], dtype =tf_float) data_mask = tf.concat([mems_mask, data_mask], 1) if attn_mask is None: attn_mask = data_mask[:, :, :, (None)] else: attn_mask += data_mask[:, :, :, (None)] if attn_mask is not None: attn_mask = tf.cast(attn_mask > 0, dtype=tf_float) if attn_mask is not None: non_tgt_mask = -tf.eye(qlen, dtype=tf_float) non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=tf_float ), non_tgt_mask], axis=-1) non_tgt_mask = tf.cast(attn_mask + non_tgt_mask[:, :, (None), ( None)] > 0, dtype=tf_float) else: non_tgt_mask = None word_emb_k, lookup_table = embedding_lookup(x=inp_k, n_token= n_token, d_embed=d_model, initializer=initializer, use_tpu= use_tpu, dtype=tf_float, scope='word_embedding') if inp_q is not None: with tf.variable_scope('mask_emb'): mask_emb = tf.get_variable('mask_emb', [1, 1, d_model], dtype=tf_float) if target_mapping is not None: word_emb_q = tf.tile(mask_emb, [tf.shape(target_mapping )[0], bsz, 1]) else: inp_q_ext = inp_q[:, :, (None)] word_emb_q = inp_q_ext * mask_emb + (1 - inp_q_ext ) * word_emb_k output_h = tf.layers.dropout(word_emb_k, dropout, training=is_training) if inp_q is not None: output_g = tf.layers.dropout(word_emb_q, dropout, training= is_training) if seg_id is not None: if untie_r: r_s_bias = tf.get_variable('r_s_bias', [n_layer, n_head, d_head], dtype=tf_float, initializer=initializer) else: r_s_bias = tf.get_variable('r_s_bias', [n_head, d_head], dtype=tf_float, initializer=initializer) seg_embed = tf.get_variable('seg_embed', [n_layer, 2, n_head, d_head], dtype=tf_float, initializer=initializer) mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32) cat_ids = tf.concat([mem_pad, seg_id], 0) seg_mat = tf.cast(tf.logical_not(tf.equal(seg_id[:, (None)], cat_ids[(None), :])), tf.int32) seg_mat = tf.one_hot(seg_mat, 2, dtype=tf_float) else: seg_mat = None pos_emb = relative_positional_encoding(qlen, klen, d_model, clamp_len, attn_type, bi_data, bsz=bsz, dtype=tf_float) pos_emb = tf.layers.dropout(pos_emb, dropout, training=is_training) if mems is None: mems = [None] * n_layer for i in range(n_layer): new_mems.append(_cache_mem(output_h, mems[i], mem_len, reuse_len)) if seg_id is None: r_s_bias_i = None seg_embed_i = None else: r_s_bias_i = r_s_bias if not untie_r else r_s_bias[i] seg_embed_i = seg_embed[i] with tf.variable_scope('layer_{}'.format(i)): if inp_q is not None: output_h, output_g = two_stream_rel_attn(h=output_h, g= output_g, r=pos_emb, r_w_bias=r_w_bias if not untie_r else r_w_bias[i], r_r_bias=r_r_bias if not untie_r else r_r_bias[i], seg_mat=seg_mat, r_s_bias =r_s_bias_i, seg_embed=seg_embed_i, attn_mask_h= non_tgt_mask, attn_mask_g=attn_mask, mems=mems[i], target_mapping=target_mapping, d_model=d_model, n_head=n_head, d_head=d_head, dropout=dropout, dropatt=dropatt, is_training=is_training, kernel_initializer=initializer) reuse = True else: reuse = False output_h = rel_multihead_attn(h=output_h, r=pos_emb, r_w_bias=r_w_bias if not untie_r else r_w_bias[i], r_r_bias=r_r_bias if not untie_r else r_r_bias[i], seg_mat=seg_mat, r_s_bias=r_s_bias_i, seg_embed= seg_embed_i, attn_mask=non_tgt_mask, mems=mems[i], d_model=d_model, n_head=n_head, d_head=d_head, dropout=dropout, dropatt=dropatt, is_training= is_training, kernel_initializer=initializer, reuse= reuse) if inp_q is not None: output_g = positionwise_ffn(inp=output_g, d_model= d_model, d_inner=d_inner, dropout=dropout, kernel_initializer=initializer, activation_type= ff_activation, is_training=is_training) output_h = positionwise_ffn(inp=output_h, d_model=d_model, d_inner=d_inner, dropout=dropout, kernel_initializer= initializer, activation_type=ff_activation, is_training =is_training, reuse=reuse) if inp_q is not None: output = tf.layers.dropout(output_g, dropout, training=is_training) else: output = tf.layers.dropout(output_h, dropout, training=is_training) return output, new_mems, lookup_table 
BucketData|len|src|data|bucketdata|util def __len__(self): return len(self.data_list) 
texar|utils|maybe|tuple|data|dataset def maybe_tuple(data): """Returns `tuple(data)` if :attr:`data` contains more than 1 elements.  Used to wrap `map_func` inputs. """ data = tuple(data) data = data if len(data) > 1 else data[0] return data 
v1|test|plain|ResnetUtilsTest|nets|resnet def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None): """A plain ResNet without extra layers before or after the ResNet blocks.""" with tf.variable_scope(scope, values=[inputs]): with slim.arg_scope([slim.conv2d], outputs_collections='end_points'): net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride ) end_points = slim.utils.convert_collection_to_dict('end_points') return net, end_points 
enas|general|cifar1|GeneralController|init|controller def __init__(self, search_for='both', search_whole_channels=False, num_layers=4, num_branches=6, out_filters=48, lstm_size=32, lstm_num_layers=2, lstm_keep_prob=1.0, tanh_constant=None, temperature= None, lr_init=0.001, lr_dec_start=0, lr_dec_every=100, lr_dec_rate=0.9, l2_reg=0, entropy_weight=None, clip_mode=None, grad_bound=None, use_critic=False, bl_dec=0.999, optim_algo='adam', sync_replicas=False, num_aggregate=None, num_replicas=None, skip_target=0.8, skip_weight=0.5, name='controller', *args, **kwargs): print('-' * 80) print('Building ConvController') self.search_for = search_for self.search_whole_channels = search_whole_channels self.num_layers = num_layers self.num_branches = num_branches self.out_filters = out_filters self.lstm_size = lstm_size self.lstm_num_layers = lstm_num_layers self.lstm_keep_prob = lstm_keep_prob self.tanh_constant = tanh_constant self.temperature = temperature self.lr_init = lr_init self.lr_dec_start = lr_dec_start self.lr_dec_every = lr_dec_every self.lr_dec_rate = lr_dec_rate self.l2_reg = l2_reg self.entropy_weight = entropy_weight self.clip_mode = clip_mode self.grad_bound = grad_bound self.use_critic = use_critic self.bl_dec = bl_dec self.skip_target = skip_target self.skip_weight = skip_weight self.optim_algo = optim_algo self.sync_replicas = sync_replicas self.num_aggregate = num_aggregate self.num_replicas = num_replicas self.name = name self._create_params() self._build_sampler() 
texar|hparams|multi|aligned|data|dataset|default def _default_dataset_hparams(data_type=None): """Returns hyperparameters of a dataset with default values.  See :meth:`texar.data.MultiAlignedData.default_hparams` for details. """ if not data_type or _is_text_data(data_type): hparams = _default_mono_text_dataset_hparams() hparams.update({'data_type': _DataTypes.TEXT, 'vocab_share_with': None, 'embedding_init_share_with': None, 'processing_share_with': None}) elif _is_scalar_data(data_type): hparams = _default_scalar_dataset_hparams() elif _is_tfrecord_data(data_type): hparams = _default_tfrecord_dataset_hparams() hparams.update({'data_type': _DataTypes.TF_RECORD}) return hparams 
instances|vectors|WordEmbedding|datacode|to|ner def instances_to_vectors(self, instances): xs = [] for instance in instances: x = [] x.extend(instance.left_context_emb) x.append(instance.word_emb) x.extend(instance.right_context_emb) xs.append(x) xs = np.asarray(xs) return xs 
infer|dec|step|BowSeq2seq|bow|seq2seq def dec_infer_step(self, dec_bow, dec_bow_len, max_len, sess): """Single step decoder inference""" feed_dict = {self.dec_bow: dec_bow, self.dec_bow_len: dec_bow_len, self .max_len: max_len, self.drop_out: 0.0} dec_output = sess.run(self.dec_infer_output, feed_dict=feed_dict) return dec_output 
v1|MobilenetV1Test|testBuildEndPointsWithDepthMultiplierGreaterThanOne|test|nets|mobilenet def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes) endpoint_keys = [key for key in end_points.keys() if key.startswith( 'Mixed') or key.startswith('Conv')] _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(inputs, num_classes, scope='depth_multiplied_net', depth_multiplier=2.0) for key in endpoint_keys: original_depth = end_points[key].get_shape().as_list()[3] new_depth = end_points_with_multiplier[key].get_shape().as_list()[3] self.assertEqual(2.0 * original_depth, new_depth) 
dist|utils|l1 def l1_dist(x, y): return np.linalg.norm(x.flatten() - y.flatten(), ord=1) 
large|models|dfc|Vae|src|encoder|master|facenet|vae|generative def encoder(self, images, is_training): activation_fn = leaky_relu weight_decay = 0.0 with tf.variable_scope('encoder'): with slim.arg_scope([slim.batch_norm], is_training=is_training): with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=tf.truncated_normal_initializer(stddev= 0.1), weights_regularizer=slim.l2_regularizer(weight_decay), normalizer_fn=slim.batch_norm, normalizer_params=self. batch_norm_params): net = slim.conv2d(images, 32, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_1') net = slim.conv2d(net, 64, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_2') net = slim.conv2d(net, 128, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_3') net = slim.conv2d(net, 256, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_4') net = slim.conv2d(net, 512, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_5') net = slim.flatten(net) fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope='Fc_1') fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope='Fc_2') return fc1, fc2 
experiment|utils|setup def setup_experiment(logger, FLAGS, default_name): from logging import FileHandler, getLogger np.random.seed(FLAGS.seed) tf.set_random_seed(FLAGS.seed) dict_values = {k: v.value for k, v in FLAGS._flags().items()} if FLAGS.name is None: FLAGS.name = default_name % dict_values FLAGS.git_revision = get_sha() if not tf.gfile.Exists(FLAGS.data_dir) or not tf.gfile.IsDirectory(FLAGS .data_dir): raise ValueError('Could not find folder %s' % FLAGS.data_dir) FLAGS.train_dir = prepare_dir(FLAGS.train_dir, FLAGS.name) FLAGS.chks_dir = os.path.join(FLAGS.train_dir, FLAGS.chks_dir) FLAGS.samples_dir = os.path.join(FLAGS.train_dir, FLAGS.samples_dir) tf.gfile.MakeDirs(FLAGS.chks_dir) tf.gfile.MakeDirs(FLAGS.samples_dir) logger = getLogger('tensorflow') file_hndl = FileHandler(os.path.join(FLAGS.train_dir, 'tensorflow.log')) file_hndl.setLevel(tf.logging.DEBUG) logger.addHandler(file_hndl) tf.logging.set_verbosity(tf.logging.DEBUG) train_params = json.dumps({k: v.value for k, v in FLAGS._flags().items( )}, sort_keys=True) logger.info(train_params) 
LayerNormalization|init|deepctr|layers|normalization def __init__(self, axis=-1, eps=1e-09, **kwargs): self.axis = axis self.eps = eps super(LayerNormalization, self).__init__(**kwargs) 
dataset|generate|blur def blur(img, mask, target_region, value=None): if value is None: value = float(np.random.uniform(0, 2.75)) blur_effect = iaa.Sequential([iaa.GaussianBlur(value)]) cpy = target_region.copy() cpy[mask[:, :, (0)] > 0] = img[:, :, ([0, 1, 2])][mask[:, :, (0)] > 0] img = blur_effect.augment_image(cpy) return img 
netowrk|eval|relation|main def eval_relation_netowrk(eval_set, batch_size, device, model, num_choice): dataset_loader = data.DataLoader(eval_set, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_csqa_paths) cnt_correct = 0 for k, (statements, correct_labels, cpt_paths, rel_paths, qa_pairs ) in enumerate(tqdm(dataset_loader, desc='Eval Batch')): statements = statements.to(device) correct_labels = correct_labels.to(device) flat_statements = [] flat_qa_pairs = [] assert len(statements) == len(cpt_paths) for i in range(len(statements)): cur_statement = statements[i][0] cur_qa_pairs = qa_pairs[i] flat_statements.extend(cur_statement) flat_qa_pairs.extend(cur_qa_pairs) flat_statements = torch.stack(flat_statements).to(device) flat_logits = model(flat_statements, flat_qa_pairs) assert len(flat_statements) == len(statements) * num_choice for j, correct in enumerate(correct_labels): max_logit = None pred = 0 for i in range(num_choice): cur_logit = flat_logits[j * num_choice + i] if max_logit is None: max_logit = cur_logit pred = i if max_logit < cur_logit: max_logit = cur_logit pred = i if correct[0] == pred: cnt_correct += 1 acc = cnt_correct / len(eval_set) return acc 
eval|xlnet|steps|estimator|tpu|convert|master|hooks|to|TPUEstimator def _convert_eval_steps_to_hooks(self, steps): with self._ctx.with_mode(model_fn_lib.ModeKeys.EVAL) as ctx: if ctx.is_running_on_cpu(): return super(TPUEstimator, self)._convert_eval_steps_to_hooks(steps ) if steps is None: raise ValueError( 'Evaluate `steps` must be set on TPU. Cannot be `None`.') util_lib.check_positive_integer(steps, 'Eval steps') return [evaluation._StopAfterNEvalsHook(num_evals=steps), _SetEvalIterationsHook(steps)] 
utils|grad|by|value|clip def grad_clip_by_value(x, clip_magnitude=1, name=None): if isinstance(clip_magnitude, int): clip_magnitude = float(clip_magnitude) with ops.name_scope(name, 'grad_clip_by_value', [x, clip_magnitude] ) as name: identity, = py_func(lambda t, _: t, [x, clip_magnitude], [tf. float32], name=name, grad=_grad_clip_by_value_grad, stateful=False) identity.set_shape(x.get_shape()) return identity 
texar|get|fn|activation|core|layers def get_activation_fn(fn_name='identity', kwargs=None): """Returns an activation function `fn` with the signature `output = fn(input)`.  If the function specified by :attr:`fn_name` has more than one arguments without default values, then all these arguments except the input feature argument must be specified in :attr:`kwargs`. Arguments with default values can also be specified in :attr:`kwargs` to take values other than the defaults. In this case a partial function is returned with the above signature.  Args: fn_name (str or callable): An activation function, or its name or module path. The function can be:  - Built-in function defined in :tf_main:`tf < >` or             :tf_main:`tf.nn <nn>`, e.g., :tf_main:`tf.identity <identity>`. - User-defined activation functions in module :mod:`texar.custom`. - External activation functions. Must provide the full module path,              e.g., "my_module.my_activation_fn".  kwargs (optional): A `dict` or instance of :class:`~texar.HParams` containing the keyword arguments of the activation function.  Returns: An activation function. `None` if :attr:`fn_name` is `None`. """ if fn_name is None: return None fn_modules = ['tensorflow', 'tensorflow.nn', 'texar.custom', 'texar.core.layers'] activation_fn_ = utils.get_function(fn_name, fn_modules) activation_fn = activation_fn_ if kwargs is not None: if isinstance(kwargs, HParams): kwargs = kwargs.todict()  def _partial_fn(features): return activation_fn_(features, **kwargs) activation_fn = _partial_fn return activation_fn 
preprocessing|test|scripts|main|travis def main(): dataset_config = DatasetBuilder.copy_config(DatasetBuilder.KITTI_UNITTEST) dataset_config.data_split = 'trainval' unittest_dataset = DatasetBuilder.build_kitti_dataset(dataset_config) gen_label_clusters.main(unittest_dataset) gen_mini_batches.main(unittest_dataset) 
generator|NPRFKNRMPairGenerator|pair|knrm|init|nprf|utils def __init__(self, relevance_dict_path, dd_d_feature_path, sample_perquery_limit, sample_total_limit, doc_topk_term=30, nb_supervised_doc=20, kernel_size=30, batch_size=32, shuffle=True): super(NPRFKNRMPairGenerator, self).__init__(relevance_dict_path, batch_size, shuffle, sample_perquery_limit, sample_total_limit) self.doc_topk_term = doc_topk_term self.nb_supervised_doc = nb_supervised_doc self.kernel_size = kernel_size self.dd_d_feature_path = dd_d_feature_path 
avod|test|3d|anchor|Box3dEncoderTest|encoder|core|box|projected|to def test_box_3d_to_anchor_projected(self): """ Check that boxes are projected with ortho_rotate=False, and that projected boxes have the correct dimensions """ thetas = np.arange(0, 2 * np.pi, np.pi / 6) boxes_3d = [] for theta in thetas: boxes_3d.append([1, 2, 3, 4, 5, 6, theta]) boxes_3d = np.asarray(boxes_3d, dtype=np.float64) cos_thetas = np.abs(np.cos(thetas)) sin_thetas = np.abs(np.sin(thetas)) expected_dims_x = 4 * cos_thetas + 5 * sin_thetas expected_dims_z = 4 * sin_thetas + 5 * cos_thetas expected_anchors = [] for exp_x, exp_z in zip(expected_dims_x, expected_dims_z): expected_anchors.append([1, 2, 3, exp_x, 6, exp_z]) expected_anchors = np.asarray(expected_anchors, np.float64) anchors = box_3d_encoder.box_3d_to_anchor(boxes_3d, ortho_rotate=False) np.testing.assert_allclose(anchors, expected_anchors) 
tangents|fl|predict|neural|uflatten|make|flatten def fl(fx): """Flatten outputs.""" return np.reshape(fx, (-1,)) 
hidden|deepMOT|models|DHN|init|master|Munkrs def init_hidden(self, batch): if self.bidirect: if self.is_cuda: hidden = torch.zeros(2 * 2, batch, self.hidden_dim).cuda() else: hidden = torch.zeros(2, batch, self.hidden_dim), torch.zeros(2, batch, self.hidden_dim) elif self.is_cuda: hidden = torch.zeros(1, batch, self.hidden_dim).cuda(), torch.zeros( 1, batch, self.hidden_dim).cuda() else: hidden = torch.zeros(1, batch, self.hidden_dim), torch.zeros(1, batch, self.hidden_dim) return hidden 
losses|loss|gp def gp_loss(x, y, d, lm, clamp=True): batch_size = x.size()[0] gp_alpha = utils.unsqueeze(torch.rand(batch_size), ndim=x.dim()) gp_alpha = gp_alpha.cuda() interp = Variable(gp_alpha * x.data + (1 - gp_alpha) * y.data, requires_grad=True) d_interp = d(interp) grad_interp = torch.autograd.grad(outputs=d_interp, inputs=interp, grad_outputs=torch.ones(d_interp.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0] grad_interp = grad_interp.view(grad_interp.size(0), -1) diff = grad_interp.norm(2, dim=1) - 1 if clamp: diff = torch.clamp(diff, 0) return lm * torch.mean(diff ** 2) 
models|BottleneckBlock|densenet|forward def forward(self, x): out = self.conv1(self.relu(self.bn1(x))) if self.droprate > 0: out = F.dropout(out, p=self.droprate, inplace=False, training=self. training) out = self.conv2(self.relu(self.bn2(out))) if self.droprate > 0: out = F.dropout(out, p=self.droprate, inplace=False, training=self. training) return torch.cat([x, out], 1) 
mrcnn|class|loss|model|graph def mrcnn_class_loss_graph(target_class_ids, pred_class_logits, active_class_ids): """Loss for the classifier head of Mask RCNN.  target_class_ids: [batch, num_rois]. Integer class IDs. Uses zero padding to fill in the array. pred_class_logits: [batch, num_rois, num_classes] active_class_ids: [batch, num_classes]. Has a value of 1 for classes that are in the dataset of the image, and 0 for classes that are not in the dataset. """ target_class_ids = tf.cast(target_class_ids, 'int64') pred_class_ids = tf.argmax(pred_class_logits, axis=2) pred_active = tf.gather(active_class_ids[0], pred_class_ids) loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels= target_class_ids, logits=pred_class_logits) loss = loss * pred_active loss = K.switch(tf.reduce_sum(pred_active) > 0, tf.reduce_sum(loss) / tf.reduce_sum(pred_active), tf.constant(0.0)) return loss 
envs|end|project|state|hac|goal|init|Pendulum|hbaselines|to def project_state_to_end_goal(sim, state): return np.array([bound_angle(sim.data.qpos[0]), 15 if state[2] > 15 else -15 if state[2] < -15 else state[2]]) 
conlleval|uniq def uniq(iterable): seen = set() return [i for i in iterable if not (i in seen or seen.add(i))] 
PadRotateProjectDevice|init|measure|commons def __init__(self, hparams): MeasurementDevice.__init__(self, hparams) self.output_type = 'vector' 
bert|master|main|run|classifier def main(_): tf.logging.set_verbosity(tf.logging.INFO) processors = {'cola': ColaProcessor, 'mnli': MnliProcessor, 'mrpc': MrpcProcessor, 'xnli': XnliProcessor} tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case, FLAGS.init_checkpoint) if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict: raise ValueError( "At least one of `do_train`, `do_eval` or `do_predict' must be True." ) bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file) if FLAGS.max_seq_length > bert_config.max_position_embeddings: raise ValueError( 'Cannot use sequence length %d because the BERT model was only trained up to sequence length %d' % (FLAGS.max_seq_length, bert_config.max_position_embeddings)) tf.gfile.MakeDirs(FLAGS.output_dir) task_name = FLAGS.task_name.lower() if task_name not in processors: raise ValueError('Task not found: %s' % task_name) processor = processors[task_name]() label_list = processor.get_labels() tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case) tpu_cluster_resolver = None if FLAGS.use_tpu and FLAGS.tpu_name: tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver( FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project) is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2 run_config = tf.contrib.tpu.RunConfig(cluster=tpu_cluster_resolver, master=FLAGS.master, model_dir=FLAGS.output_dir, save_checkpoints_steps=FLAGS.save_checkpoints_steps, tpu_config=tf. contrib.tpu.TPUConfig(iterations_per_loop=FLAGS.iterations_per_loop, num_shards=FLAGS.num_tpu_cores, per_host_input_for_training= is_per_host)) train_examples = None num_train_steps = None num_warmup_steps = None if FLAGS.do_train: train_examples = processor.get_train_examples(FLAGS.data_dir) num_train_steps = int(len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs) num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion) model_fn = model_fn_builder(bert_config=bert_config, num_labels=len( label_list), init_checkpoint=FLAGS.init_checkpoint, learning_rate= FLAGS.learning_rate, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps, use_tpu=FLAGS.use_tpu, use_one_hot_embeddings=FLAGS.use_tpu) estimator = tf.contrib.tpu.TPUEstimator(use_tpu=FLAGS.use_tpu, model_fn =model_fn, config=run_config, train_batch_size=FLAGS. train_batch_size, eval_batch_size=FLAGS.eval_batch_size, predict_batch_size=FLAGS.predict_batch_size) if FLAGS.do_train: train_file = os.path.join(FLAGS.output_dir, 'train.tf_record') file_based_convert_examples_to_features(train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file) tf.logging.info('***** Running training *****') tf.logging.info('  Num examples = %d', len(train_examples)) tf.logging.info('  Batch size = %d', FLAGS.train_batch_size) tf.logging.info('  Num steps = %d', num_train_steps) train_input_fn = file_based_input_fn_builder(input_file=train_file, seq_length=FLAGS.max_seq_length, is_training=True, drop_remainder=True) estimator.train(input_fn=train_input_fn, max_steps=num_train_steps) if FLAGS.do_eval: eval_examples = processor.get_dev_examples(FLAGS.data_dir) num_actual_eval_examples = len(eval_examples) if FLAGS.use_tpu: while len(eval_examples) % FLAGS.eval_batch_size != 0: eval_examples.append(PaddingInputExample()) eval_file = os.path.join(FLAGS.output_dir, 'eval.tf_record') file_based_convert_examples_to_features(eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file) tf.logging.info('***** Running evaluation *****') tf.logging.info('  Num examples = %d (%d actual, %d padding)', len( eval_examples), num_actual_eval_examples, len(eval_examples) - num_actual_eval_examples) tf.logging.info('  Batch size = %d', FLAGS.eval_batch_size) eval_steps = None if FLAGS.use_tpu: assert len(eval_examples) % FLAGS.eval_batch_size == 0 eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size) eval_drop_remainder = True if FLAGS.use_tpu else False eval_input_fn = file_based_input_fn_builder(input_file=eval_file, seq_length=FLAGS.max_seq_length, is_training=False, drop_remainder=eval_drop_remainder) result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps) output_eval_file = os.path.join(FLAGS.output_dir, 'eval_results.txt') with tf.gfile.GFile(output_eval_file, 'w') as writer: tf.logging.info('***** Eval results *****') for key in sorted(result.keys()): tf.logging.info('  %s = %s', key, str(result[key])) writer.write('%s = %s\n' % (key, str(result[key]))) if FLAGS.do_predict: predict_examples = processor.get_test_examples(FLAGS.data_dir) num_actual_predict_examples = len(predict_examples) if FLAGS.use_tpu: while len(predict_examples) % FLAGS.predict_batch_size != 0: predict_examples.append(PaddingInputExample()) predict_file = os.path.join(FLAGS.output_dir, 'predict.tf_record') file_based_convert_examples_to_features(predict_examples, label_list, FLAGS.max_seq_length, tokenizer, predict_file) tf.logging.info('***** Running prediction*****') tf.logging.info('  Num examples = %d (%d actual, %d padding)', len( predict_examples), num_actual_predict_examples, len( predict_examples) - num_actual_predict_examples) tf.logging.info('  Batch size = %d', FLAGS.predict_batch_size) predict_drop_remainder = True if FLAGS.use_tpu else False predict_input_fn = file_based_input_fn_builder(input_file= predict_file, seq_length=FLAGS.max_seq_length, is_training= False, drop_remainder=predict_drop_remainder) result = estimator.predict(input_fn=predict_input_fn) output_predict_file = os.path.join(FLAGS.output_dir, 'test_results.tsv' ) with tf.gfile.GFile(output_predict_file, 'w') as writer: num_written_lines = 0 tf.logging.info('***** Predict results *****') for i, prediction in enumerate(result): probabilities = prediction['probabilities'] if i >= num_actual_predict_examples: break output_line = '\t'.join(str(class_probability) for class_probability in probabilities) + '\n' writer.write(output_line) num_written_lines += 1 assert num_written_lines == num_actual_predict_examples 
of|fisher|ops|sum|squares|classification|factors|convdiag|ConvDiagonalFactor def _convdiag_sum_of_squares(self, patches, outputs_grad): case_wise_gradients = special_math_ops.einsum('bijk,bijl->bkl', patches, outputs_grad) return math_ops.reduce_sum(math_ops.square(case_wise_gradients), axis=0) 
bert|eval|main|epoch|classifier def _eval_epoch(sess): """Evaluates on the dev set. """ iterator.restart_dataset(sess, 'eval') cum_acc = 0.0 cum_loss = 0.0 nsamples = 0 fetches = {'accu': accu, 'loss': loss, 'batch_size': batch_size} while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'eval'), tx.context.global_mode(): tf.estimator.ModeKeys.EVAL} rets = sess.run(fetches, feed_dict) cum_acc += rets['accu'] * rets['batch_size'] cum_loss += rets['loss'] * rets['batch_size'] nsamples += rets['batch_size'] except tf.errors.OutOfRangeError: break tf.logging.info('eval accu: {}; loss: {}; nsamples: {}'.format(cum_acc / nsamples, cum_loss / nsamples, nsamples)) 
sparse|src|dropout|layers def dropout_sparse(x, keep_prob, num_nonzero_elems): """Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements) """ noise_shape = [num_nonzero_elems] random_tensor = keep_prob random_tensor += tf.random_uniform(noise_shape) dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool) pre_out = tf.sparse_retain(x, dropout_mask) return pre_out * (1.0 / keep_prob) 
spectrograms|utils|get def get_spectrograms(fpath): """Returns normalized log(melspectrogram) and log(magnitude) from `sound_file`. Args: sound_file: A string. The full path of a sound file.  Returns: mel: A 2d array of shape (T, n_mels) <- Transposed mag: A 2d array of shape (T, 1+n_fft/2) <- Transposed """ y, sr = librosa.load(fpath, sr=hp.sr) y, _ = librosa.effects.trim(y) y = np.append(y[0], y[1:] - hp.preemphasis * y[:-1]) linear = librosa.stft(y=y, n_fft=hp.n_fft, hop_length=hp.hop_length, win_length=hp.win_length) mag = np.abs(linear) mel_basis = librosa.filters.mel(hp.sr, hp.n_fft, hp.n_mels) mel = np.dot(mel_basis, mag) mel = 20 * np.log10(np.maximum(1e-05, mel)) mag = 20 * np.log10(np.maximum(1e-05, mag)) mel = np.clip((mel - hp.ref_db + hp.max_db) / hp.max_db, 1e-08, 1) mag = np.clip((mag - hp.ref_db + hp.max_db) / hp.max_db, 1e-08, 1) mel = mel.T.astype(np.float32) mag = mag.T.astype(np.float32) return mel, mag 
clusters|label|avod|write|LabelClusterUtils|file|cluster|core|utils|to def _write_clusters_to_file(self, file_path, clusters, std_devs): """ Writes cluster information to a text file  Args: file_path: path to text file clusters: clusters to write std_devs: standard deviations to write """ file_dir = os.path.dirname(file_path) if not os.path.exists(file_dir): os.makedirs(file_dir) new_file = open(file_path, 'w+') all_data = np.vstack([clusters, std_devs]) np.savetxt(file_path, all_data, fmt='%.3f') new_file.close() 
bert|FullTokenizer|tokenization|tokenize def tokenize(self, text): split_tokens = [] for token in self.basic_tokenizer.tokenize(text): for sub_token in self.wordpiece_tokenizer.tokenize(token): split_tokens.append(sub_token) return split_tokens 
fisher|ops|NaiveDiagonalFB|classification|init|blocks def __init__(self, layer_collection, params): """Creates a NaiveDiagonalFB block. Args: layer_collection: The collection of all layers in the K-FAC approximate Fisher information matrix to which this FisherBlock belongs. params: The parameters of this layer (Tensor or tuple of Tensors). """ self._params = params self._batch_sizes = [] super(NaiveDiagonalFB, self).__init__(layer_collection) 
block|official|vgg|model|Model def block(self, inputs, layers): for filters in layers: inputs = self.conv2d(inputs, filters) if self.data_format == 'NHWC': kernel = [1, 2, 2, 1] elif self.data_format == 'NCHW': kernel = [1, 1, 2, 2] else: raise ValueError('Unsupported data format {}'.format(self.data_format)) return tf.nn.max_pool(inputs, kernel, kernel, padding='VALID', data_format=self.data_format) 
enas|params|model|count|utils def count_model_params(tf_variables): """ Args: tf_variables: list of all model variables """ num_vars = 0 for var in tf_variables: num_vars += np.prod([dim.value for dim in var.get_shape()]) return num_vars 
nodes|add|deepzono|dimensions def add_dimensions(man, element, offset, n): """ adds dimensions to an abstract element  Arguments --------- man : ElinaManagerPtr manager which is responsible for element element : ElinaAbstract0Ptr the element to which dimensions get added offset : int offset at which the dimensions should be added n : int n dimensions will be added to element at offset  Return ------ output : ElinaAbstract0Ptr new abstract element with the added dimensions """ dimchange_ptr = elina_dimchange_alloc(0, n) elina_dimchange_init(dimchange_ptr, 0, n) for i in range(n): dimchange_ptr.contents.dim[i] = offset output = elina_abstract0_add_dimensions(man, True, element, dimchange_ptr, False) elina_dimchange_free(dimchange_ptr) return output 
python|grpc|OpsTest|queue|ops|test|seed|rl|master @parameterized.parameters(([], False), ([1], True)) def test_queue(self, dim, batched): address = self.get_unix_address() server = ops.Server([address]) q = tf.queue.FIFOQueue(1, [tf.int32], [()])  @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): if x.shape == (1,): q.enqueue_many([x]) else: q.enqueue([x]) return x server.bind(foo, batched=batched) server.start() client = ops.Client(address) client.foo(42) self.assertAllEqual(42, q.dequeue()) server.shutdown() 
tests|test|core|PredictionLayer|layers @pytest.mark.parametrize('task,use_bias', [(task, use_bias) for task in [ 'binary', 'regression'] for use_bias in [True, False]]) def test_PredictionLayer(task, use_bias): with CustomObjectScope({'PredictionLayer': layers.PredictionLayer}): layer_test(layers.PredictionLayer, kwargs={'task': task, 'use_bias': use_bias}, input_shape=(BATCH_SIZE, 1)) 
prepare|vocabulary|custom|lambada def prepare_custom_vocabulary(lines): vocab = {} for line in lines: line_tokens = str(line).split() for word in line_tokens: if word in vocab: vocab[word] += 1 else: vocab[word] = 1 sort = sorted(vocab, key=vocab.get, reverse=True) sort = [PADDING, UNKNOWN, SELECTED, MASK_TOKEN] + sort sort = sort[:cnf.lambada_vocab_size] with open(vocab_file, 'w', encoding='utf-8') as file: for line in sort: file.write(line + '\n') vocab = {value: index for index, value in enumerate(sort)} return vocab 
bb|push|craystack|ans|BBANS def push(message, data): _, posterior_pop = posterior(data) message, latent = posterior_pop(message) likelihood_push, _ = likelihood(latent) message = likelihood_push(message, data) message = prior_push(message, latent) return message 
list|avod|test|as|BoxListTest|core|box|tensor|dict def test_as_tensor_dict(self): boxlist = box_list.BoxList(tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32)) boxlist.add_field('classes', tf.constant([0, 1])) boxlist.add_field('scores', tf.constant([0.75, 0.2])) tensor_dict = boxlist.as_tensor_dict() expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]] expected_classes = [0, 1] expected_scores = [0.75, 0.2] with self.test_session() as sess: tensor_dict_out = sess.run(tensor_dict) self.assertAllEqual(3, len(tensor_dict_out)) self.assertAllClose(expected_boxes, tensor_dict_out['boxes']) self.assertAllEqual(expected_classes, tensor_dict_out['classes']) self.assertAllClose(expected_scores, tensor_dict_out['scores']) 
house|get|R2RHouseParser|parser|name|by|pano def get_pano_by_name(self, pano_name): return self.panos[self.pano_name_map[pano_name]] 
utilities|plato|recorder|DialogueEpisodeRecorder|init|dialogue|episode def __init__(self, size=None, path=None): """ Initializes the Dialogue Episode Recorder  :param size: size of the experience (how many dialogues to store) :param path: path to save / load the experience  """ self.dialogues = [] self.size = size self.current_dialogue = None self.cumulative_reward = 0 self.path = path if path: self.load(path) 
boolean|list|avod|ops|mask|core|box def boolean_mask(boxlist, indicator, fields=None, scope=None): """Select boxes from BoxList according to indicator and return new BoxList.  `boolean_mask` returns the subset of boxes that are marked as "True" by the indicator tensor. By default, `boolean_mask` returns boxes corresponding to the input index list, as well as all additional fields stored in the boxlist (indexing into the first dimension).  However one can optionally only draw from a subset of fields.  Args: boxlist: BoxList holding N boxes indicator: a rank-1 boolean tensor fields: (optional) list of fields to also gather from.  If None (default), all fields are gathered from.  Pass an empty fields list to only gather the box coordinates. scope: name scope.  Returns: subboxlist: a BoxList corresponding to the subset of the input BoxList specified by indicator Raises: ValueError: if `indicator` is not a rank-1 boolean tensor. """ with tf.name_scope(scope, 'BooleanMask'): if indicator.shape.ndims != 1: raise ValueError('indicator should have rank 1') if indicator.dtype != tf.bool: raise ValueError('indicator should be a boolean tensor') subboxlist = box_list.BoxList(tf.boolean_mask(boxlist.get(), indicator) ) if fields is None: fields = boxlist.get_extra_fields() for field in fields: if not boxlist.has_field(field): raise ValueError('boxlist must contain all specified fields') subfieldlist = tf.boolean_mask(boxlist.get_field(field), indicator) subboxlist.add_field(field, subfieldlist) return subboxlist 
plato|component|agent|GoalGenerator|goal|init|user|simulator def __init__(self, args): """ Initializes the internal structures of the Goal Generator and does some checks.  :param args: the goal generator's arguments """ if 'ontology' not in args: raise ValueError('Goal Generator called without an ontology!') if 'database' not in args: raise ValueError('Goal Generator called without a database!') self.ontology = None if isinstance(args['ontology'], Ontology): self.ontology = args['ontology'] elif isinstance(args['ontology'], str): self.ontology = Ontology(args['ontology']) else: raise ValueError('Unacceptable ontology type %s ' % args['ontology']) self.database = None if isinstance(args['database'], DataBase): self.database = args['database'] elif isinstance(args['database'], str): if args['database'][-3:] == '.db': self.database = SQLDataBase(args['database']) elif args['database'][-5:] == '.json': self.database = JSONDataBase(args['database']) else: raise ValueError('Unacceptable database type %s ' % args[ 'database']) else: raise ValueError('Unacceptable database type %s ' % args['database']) self.goals_file = None if 'goals_file' in args: self.goals_file = args['goals_file'] self.goals = None if self.goals_file: self.load_goals(self.goals_file) cursor = self.database.SQL_connection.cursor() result = cursor.execute("select * from sqlite_master where type = 'table';" ).fetchall() if result and result[0] and result[0][1]: self.db_table_name = result[0][1] else: raise ValueError( 'Goal Generator cannot specify Table Name from database {0}'. format(self.database.db_file_name)) sql_command = 'SELECT * FROM ' + self.db_table_name + ' LIMIT 1;' cursor.execute(sql_command) self.slot_names = [i[0] for i in cursor.description] self.db_row_count = cursor.execute('SELECT COUNT(*) FROM ' + self. db_table_name + ';').fetchall()[0][0] 
tangents|taylor|empirical|expand|tayl|neural|f|utils def f_tayl(p, x): dparams = tree_multimap(lambda x, y: x - y, p, params) return taylorize_r(lambda param: f(param, x), params, dparams, degree, 0) 
MicroController|enas|trainer|cifar1|controller|micro|build def build_trainer(self, child_model): child_model.build_valid_rl() self.valid_acc = tf.to_float(child_model.valid_shuffle_acc) / tf.to_float( child_model.batch_size) self.reward = self.valid_acc if self.entropy_weight is not None: self.reward += self.entropy_weight * self.sample_entropy self.sample_log_prob = tf.reduce_sum(self.sample_log_prob) self.baseline = tf.Variable(0.0, dtype=tf.float32, trainable=False) baseline_update = tf.assign_sub(self.baseline, (1 - self.bl_dec) * ( self.baseline - self.reward)) with tf.control_dependencies([baseline_update]): self.reward = tf.identity(self.reward) self.loss = self.sample_log_prob * (self.reward - self.baseline) self.train_step = tf.Variable(0, dtype=tf.int32, trainable=False, name= 'train_step') tf_variables = [var for var in tf.trainable_variables() if var.name. startswith(self.name)] print('-' * 80) for var in tf_variables: print(var) self.train_op, self.lr, self.grad_norm, self.optimizer = get_train_ops(self .loss, tf_variables, self.train_step, clip_mode=self.clip_mode, grad_bound=self.grad_bound, l2_reg=self.l2_reg, lr_init=self. lr_init, lr_dec_start=self.lr_dec_start, lr_dec_every=self. lr_dec_every, lr_dec_rate=self.lr_dec_rate, optim_algo=self. optim_algo, sync_replicas=self.sync_replicas, num_aggregate=self. num_aggregate, num_replicas=self.num_replicas) self.skip_rate = tf.constant(0.0, dtype=tf.float32) 
init|utils|PrioritizedReplay def __init__(self, size, specs, importance_sampling_exponent, name= 'PrioritizedReplay'): super(PrioritizedReplay, self).__init__(name=name) self._priorities = tf.Variable(tf.zeros([size]), dtype=tf.float32) self._buffer = tf.nest.map_structure(lambda ts: tf.Variable(tf.zeros([ size] + ts.shape, dtype=ts.dtype)), specs) self.num_inserted = tf.Variable(0, dtype=tf.int64) self._importance_sampling_exponent = importance_sampling_exponent 
create|run|examples|MrpcProcessor|classifier def _create_examples(self, lines, set_type): """Creates examples for the training and dev sets.""" examples = [] for i, line in enumerate(lines): if i == 0: continue guid = '%s-%s' % (set_type, i) text_a = line[3] text_b = line[4] label = line[0] examples.append(InputExample(guid=guid, text_a=text_a, text_b= text_b, label=label)) return examples 
Layer|src|init|layers def __init__(self, **kwargs): allowed_kwargs = {'name', 'logging'} for kwarg in kwargs.keys(): assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg name = kwargs.get('name') if not name: layer = self.__class__.__name__.lower() name = layer + '_' + str(get_layer_uid(layer)) self.name = name self.vars = {} logging = kwargs.get('logging', False) self.logging = logging self.issparse = False 
EndToEndClassification|MSTmodel|load def load(self, path, sess): """ Loads the model variables from the specified path.  Args: path (str): folder path from where the checkpoint will be loaded. sess (tf Session): the session. """ vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model') saver = tf.train.Saver(vars) saver.restore(sess, path) 
elpips|dtype|util|to|f32 def f32_to_dtype(x, dtype): if dtype == tf.float32: return x return tf.cast(x, dtype) 
conll|init|CoNLLWriter|writer def __init__(self, i2w, i2n): self.__source_file = None self.__i2w = i2w self.__i2n = i2n 
python|grpc|OpsTest|batched|output|ops|test|seed|rl|master|foo|is @tf.function(input_signature=[tf.TensorSpec([1], tf.int32)]) def foo(unused_x): return tf.zeros([3]) 
greedy|Seq2seq|predict|seq2seq def predict_greedy(self, sess, batch_dict): """Greedy decoding, always choose the word with the highest probability""" feed_dict = {self.enc_inputs: batch_dict['enc_inputs'], self.inp_lens: batch_dict['inp_lens'], self.drop_out: 0.0} output_dict = sess.run(self.infer_output, feed_dict=feed_dict) return output_dict 
json|utils|load def load_json(path): with tf.gfile.GFile(path, 'r') as f: return json.load(f) 
get|tensorflow|padding|kaffe|type|transformer def get_padding_type(kernel_params, input_shape, output_shape): """Translates Caffe's numeric padding to one of ('SAME', 'VALID'). Caffe supports arbitrary padding values, while TensorFlow only supports 'SAME' and 'VALID' modes. So, not all Caffe paddings can be translated to TensorFlow. There are some subtleties to how the padding edge-cases are handled. These are described here: https://github.com/Yangqing/caffe2/blob/master/caffe2/proto/caffe2_legacy.proto """ k_h, k_w, s_h, s_w, p_h, p_w = kernel_params s_o_h = np.ceil(input_shape.height / float(s_h)) s_o_w = np.ceil(input_shape.width / float(s_w)) if output_shape.height == s_o_h and output_shape.width == s_o_w: return 'SAME' v_o_h = np.ceil((input_shape.height - k_h + 1.0) / float(s_h)) v_o_w = np.ceil((input_shape.width - k_w + 1.0) / float(s_w)) if output_shape.height == v_o_h and output_shape.width == v_o_w: return 'VALID' return None 
predict|seq|tag|bow|seq2seq def bow_predict_seq_tag(vocab_size, enc_batch_size, enc_outputs, enc_lens, max_len, is_gumbel=False, tau=0.5, max_src2tgt_word=3): """bow prediction as sequence tagging  Let each word from the source sentence predict its k nearest neighbors """ bow_topk_prob = tf.zeros([enc_batch_size, vocab_size]) gumbel_topk_prob = tf.zeros([enc_batch_size, vocab_size]) seq_neighbor_ind = [] seq_neighbor_prob = []  def _sample_gumbel(shape, eps=1e-20): U = tf.random_uniform(shape, minval=0, maxval=1) return -tf.log(-tf.log(U + eps) + eps) for i in range(max_src2tgt_word): bow_trans = tf.layers.Dense(500, name='bow_src2tgt_trans_%d' % i, kernel_initializer=tf.random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0)) bow_proj = tf.layers.Dense(vocab_size, name='bow_src2tgt_proj_%d' % i, kernel_initializer=tf.random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0)) bow_logits = bow_proj(bow_trans(enc_outputs)) bow_prob = tf.nn.softmax(bow_logits) pred_mask = tf.expand_dims(tf.sequence_mask(enc_lens, max_len, tf. float32), [2]) bow_prob *= pred_mask bow_topk_prob += tf.reduce_sum(bow_prob, 1) neighbor_ind = tf.argmax(bow_prob, 2) seq_neighbor_ind.append(neighbor_ind) neighbor_prob = tf.reduce_max(bow_prob, 2) seq_neighbor_prob.append(neighbor_prob) if is_gumbel: print('Using gumbel reparametrization ... ') gumbel_prob = tf.nn.softmax((bow_logits + _sample_gumbel(tf. shape(bow_logits))) / tau) gumbel_prob *= pred_mask gumbel_topk_prob += tf.reduce_sum(gumbel_prob, 1) else: print('Not using gumbel reparametrization ... ') gumbel_topk_prob += tf.reduce_sum(bow_prob, 1) seq_neighbor_ind = tf.stack(seq_neighbor_ind, 2) seq_neighbor_prob = tf.stack(seq_neighbor_prob, 2) return bow_topk_prob, gumbel_topk_prob, seq_neighbor_ind, seq_neighbor_prob 
Layer|layers|call|graphsage def _call(self, inputs): return inputs 
weight|space|examples|main def main(unused_argv): print('Loading data.') x_train, y_train, x_test, y_test = datasets.get_dataset('mnist', permute_train=True) init_fn, f, _ = stax.serial(stax.Dense(2048, 1.0, 0.05), stax.Erf(), stax.Dense(10, 1.0, 0.05)) key = random.PRNGKey(0) _, params = init_fn(key, (-1, 784)) f_lin = nt.linearize(f, params) opt_init, opt_apply, get_params = optimizers.momentum(FLAGS. learning_rate, 0.9) opt_apply = jit(opt_apply) state = opt_init(params) state_lin = opt_init(params) loss = lambda fx, y_hat: -np.mean(logsoftmax(fx) * y_hat) grad_loss = jit(grad(lambda params, x, y: loss(f(params, x), y))) grad_loss_lin = jit(grad(lambda params, x, y: loss(f_lin(params, x), y))) print('Training.') print('Epoch\tLoss\tLinearized Loss') print('------------------------------------------') epoch = 0 steps_per_epoch = 50000 // FLAGS.batch_size for i, (x, y) in enumerate(datasets.minibatch(x_train, y_train, FLAGS. batch_size, FLAGS.train_epochs)): params = get_params(state) state = opt_apply(i, grad_loss(params, x, y), state) params_lin = get_params(state_lin) state_lin = opt_apply(i, grad_loss_lin(params_lin, x, y), state_lin) if i % steps_per_epoch == 0: print('{}\t{:.4f}\t{:.4f}'.format(epoch, loss(f(params, x), y), loss(f_lin(params_lin, x), y))) epoch += 1 x, y = x_train[:10000], y_train[:10000] util.print_summary('train', y, f(params, x), f_lin(params_lin, x), loss) util.print_summary('test', y_test, f(params, x_test), f_lin(params_lin, x_test), loss) 
1|nbl|train|inception|v3|baseline|validate def validate_nbl(out, sess, epoch, nbl_validation_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, learning_rate, is_training_ph, is_validating_nbl_ph): g_step, acc_top1, acc_top5, test_loss, lr = 0, 0, 0, 0, 0 for i in range(nbl_validation_steps): out.validation_step_begin(i, nbl_validation_steps) g_step, l, acc1, acc5, lr = sess.run([global_step, loss_op, acc_top_1_op, acc_top_5_op, learning_rate], feed_dict={ is_training_ph: False, is_validating_nbl_ph: True}) acc_top1 = (acc1 + i * acc_top1) / (i + 1) acc_top5 = (acc5 + i * acc_top5) / (i + 1) test_loss = (l + i * test_loss) / (i + 1) out.validation_end(sess, epoch, g_step, True, test_loss, lr, acc_top1, acc_top5) 
kag|netowrk|train|main|epoch def train_epoch_kag_netowrk(train_set, batch_size, optimizer, device, model, num_choice, loss_func): model.train() dataset_loader = data.DataLoader(train_set, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_csqa_graphs_and_paths) bce_loss_func = nn.BCELoss() for k, (statements, correct_labels, graphs, cpt_paths, rel_paths, qa_pairs, concept_mapping_dicts) in enumerate(tqdm(dataset_loader, desc='Train Batch')): optimizer.zero_grad() statements = statements.to(device) correct_labels = correct_labels.to(device) graphs.ndata['cncpt_ids'] = graphs.ndata['cncpt_ids'].to(device) flat_statements = [] flat_qa_pairs = [] flat_cpt_paths = [] flat_rel_paths = [] assert len(statements) == len(cpt_paths) == len(rel_paths) == len( qa_pairs) for i in range(len(statements)): cur_statement = statements[i][0] cur_qa_pairs = qa_pairs[i] cur_cpt_paths = cpt_paths[i] cur_rel_paths = rel_paths[i] flat_statements.extend(cur_statement) flat_qa_pairs.extend(cur_qa_pairs) flat_cpt_paths.extend(cur_cpt_paths) flat_rel_paths.extend(cur_rel_paths) flat_statements = torch.stack(flat_statements).to(device) flat_logits = model(flat_statements, flat_qa_pairs, flat_cpt_paths, flat_rel_paths, graphs, concept_mapping_dicts) y = torch.Tensor([1] * len(statements) * (num_choice - 1)).to(device) assert len(flat_logits) == len(flat_statements) assert len(flat_statements) == len(statements) * num_choice x1 = [] x2 = [] for j, correct in enumerate(correct_labels): for i in range(num_choice): cur_logit = flat_logits[j * num_choice + i] if i != correct[0]: x2.append(cur_logit) else: for _ in range(num_choice - 1): x1.append(cur_logit) mrloss = loss_func(torch.cat(x1), torch.cat(x2), y) mrloss.backward() optimizer.step() 
pos|gym|extensions|ee|bullet|BulletManipulator|raw|qpos|to|manipulator def _ee_pos_to_qpos_raw(self, ee_pos, ee_quat=None, fing_dist=0.0, left_ee_pos=None, left_ee_quat=None, left_fing_dist=0.0): qpos = np.array(pybullet.calculateInverseKinematics(self.info.robot_id, self.info.ee_link_id, ee_pos.tolist(), None if ee_quat is None else ee_quat.tolist(), maxNumIterations=1000, residualThreshold=0.0001)) for jid in self.info.finger_jids_lst: qpos[jid] = np.clip(fing_dist / 2.0, self.info.joint_minpos[jid], self.info.joint_maxpos[jid]) if len(self.info.left_arm_jids_lst) > 0: if left_ee_pos is not None: left_qpos = np.array(pybullet.calculateInverseKinematics(self. info.robot_id, self.info.left_ee_link_id, left_ee_pos. tolist(), None if left_ee_quat is None else left_ee_quat. tolist(), maxNumIterations=1000, residualThreshold=0.0001)) else: left_qpos = self.rest_qpos qpos[self.info.left_arm_jids_lst] = left_qpos[self.info. left_arm_jids_lst] for jid in self.info.left_finger_jids_lst: qpos[jid] = np.clip(left_fing_dist / 2.0, self.info.joint_minpos[ jid], self.info.joint_maxpos[jid]) qpos = np.clip(qpos, self.info.joint_minpos, self.info.joint_maxpos) return qpos 
fasterai|VideoColorizer|visualize|video|build def _build_video(self, source_path: Path) ->Path: colorized_path = self.result_folder / source_path.name.replace('.mp4', '_no_audio.mp4') colorframes_folder = self.colorframes_root / source_path.stem colorframes_path_template = str(colorframes_folder / '%5d.jpg') colorized_path.parent.mkdir(parents=True, exist_ok=True) if colorized_path.exists(): colorized_path.unlink() fps = self._get_fps(source_path) ffmpeg.input(str(colorframes_path_template), format='image2', vcodec= 'mjpeg', framerate=fps).output(str(colorized_path), crf=17, vcodec= 'libx264').run(capture_stdout=True) result_path = self.result_folder / source_path.name if result_path.exists(): result_path.unlink() shutil.copyfile(str(colorized_path), str(result_path)) audio_file = Path(str(source_path).replace('.mp4', '.aac')) if audio_file.exists(): audio_file.unlink() os.system('ffmpeg -y -i "' + str(source_path) + '" -vn -acodec copy "' + str(audio_file) + '"') if audio_file.exists: os.system('ffmpeg -y -i "' + str(colorized_path) + '" -i "' + str( audio_file) + '" -shortest -c:v copy -c:a aac -b:a 256k "' + str(result_path) + '"') print('Video created here: ' + str(result_path)) return result_path 
get|SMILESX|config|AttentionM|utils def get_config(self): return super(AttentionM, self).get_config() 
child|enas|conv|fixed|cifar1|MicroChild|micro def _fixed_conv(self, x, f_size, out_filters, stride, is_training, stack_convs=2): """Apply fixed convolution.  Args: stacked_convs: number of separable convs to apply. """ for conv_id in range(stack_convs): inp_c = self._get_C(x) if conv_id == 0: strides = self._get_strides(stride) else: strides = [1, 1, 1, 1] with tf.variable_scope('sep_conv_{}'.format(conv_id)): w_depthwise = create_weight('w_depth', [f_size, f_size, inp_c, 1]) w_pointwise = create_weight('w_point', [1, 1, inp_c, out_filters]) x = tf.nn.relu(x) x = tf.nn.separable_conv2d(x, depthwise_filter=w_depthwise, pointwise_filter=w_pointwise, strides=strides, padding= 'SAME', data_format=self.data_format) x = batch_norm(x, is_training, data_format=self.data_format) return x 
models|attention|average|translate def average_attention(hidden_states, encoder_input_length, *args, **kwargs): lengths = tf.to_float(tf.expand_dims(encoder_input_length, axis=1)) mask = tf.sequence_mask(encoder_input_length, maxlen=tf.shape( hidden_states)[1]) weights = tf.to_float(mask) / lengths weighted_average = tf.reduce_sum(hidden_states * tf.expand_dims(weights, axis=2), axis=1) return weighted_average, weights 
TotalStats|analysis|calcOverall|resign def calcOverall(self, b, w): self.num_games = b.num_games + w.num_games self.no_resign_count = b.no_resign_count + w.no_resign_count self.correct_resign_count = b.correct_resign_count + w.correct_resign_count self.wrong_resign_count = b.wrong_resign_count + w.wrong_resign_count self.game_len_sum = b.game_len_sum + w.game_len_sum self.resigned_game_len_sum = (b.resigned_game_len_sum + w. resigned_game_len_sum) 
AverageMeter|init|util def __init__(self): self.reset() 
neural|conv|layer|style def conv_layer(layer_name, layer_input, W): conv = tf.nn.conv2d(layer_input, W, strides=[1, 1, 1, 1], padding='SAME') if args.verbose: print('--{} | shape={} | weights_shape={}'.format(layer_name, conv. get_shape(), W.get_shape())) return conv 
entropy|graphsage|metrics|masked|softmax|cross def masked_softmax_cross_entropy(preds, labels, mask): """Softmax cross-entropy loss with masking.""" loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels) mask = tf.cast(mask, dtype=tf.float32) mask /= tf.maximum(tf.reduce_sum(mask), tf.constant([1.0])) loss *= mask return tf.reduce_mean(loss) 
inverse|AffineCouplingCondYG def _inverse(self, y, yy, nlf0=None, nlf1=None, iso=None, cam=None): shift, log_scale = self._shift_and_log_scale_fn(yy, iso) tf.summary.histogram('cYG shift', shift) tf.summary.histogram('cYG log_scale', log_scale) log_scale = self.scale * tf.tanh(log_scale) x = y if log_scale is not None: x *= tf.exp(log_scale) if shift is not None: x += shift if self._last_layer: return tf.layers.flatten(x) return x 
driver|model|main def model_driver(d, data): """This function builds the computation graph for the specified model The input is the dictionary d with fields: model (model to be learnt), wform (form of the W matrices - full/diagonal/scalar/constant) K (number of states), L1 (input dimensions), L2 (output dimensions), numlayers (number of layers) """ tf.reset_default_graph() if d['wform_global'] == 'diag_to_full': d['wform'] = 'diagonal' rnn1 = rnn(model_specs=d) rnn1_handles = rnn1.build_graph() print('This model has ', rnn1.tnparams, ' parameters') config = tf.ConfigProto(log_device_placement=False) config.gpu_options.allow_growth = True config.allow_soft_placement = True sess = tf.Session(config=config) sess.run(tf.initialize_all_variables()) all_times, tr_logls, test_logls, valid_logls = rnn1.optimizer(data= data, rnn_handles=rnn1_handles, sess=sess) vars_np = rnn1.save_modelvars_np(sess) print('Switching from diagonal to full') d['wform'] = 'diag_to_full' with tf.variable_scope('rnn2'): rnn2 = rnn(model_specs=d, initializer=vars_np) rnn2_handles = rnn2.build_graph() print('The first model had ', rnn1.tnparams, ' ,the second model has ', rnn2.tnparams, ' parameters') sess.run(tf.initialize_all_variables()) all_times2, tr_logls2, test_logls2, valid_logls2 = rnn2.optimizer(data =data, rnn_handles=rnn2_handles, sess=sess, model_n=2) all_times.extend(all_times2), tr_logls.extend(tr_logls2) test_logls.extend(test_logls2), valid_logls.extend(valid_logls2) max_valid = np.max(np.array(valid_logls)) max_test = np.max(np.array(test_logls)) res_dictionary = {'valid': np.array(valid_logls), 'max_valid': max_valid, 'tst': np.array(test_logls), 'max_test': max_test, 'tr': np.array(tr_logls), 'all_times': all_times, 'tnparams': rnn1.tnparams} else: rnn1 = rnn(model_specs=d, initializer=d['init']) rnn1_handles = rnn1.build_graph() config = tf.ConfigProto(log_device_placement=False) config.gpu_options.allow_growth = True config.allow_soft_placement = True with tf.Session(config=config) as sess: sess.run(tf.initialize_all_variables()) all_times, tr_logls, test_logls, valid_logls = rnn1.optimizer(data =data, rnn_handles=rnn1_handles, sess=sess) max_valid = np.max(np.array(valid_logls)) max_test = np.max(np.array(test_logls)) res_dictionary = {'valid': np.array(valid_logls), 'max_valid': max_valid, 'tst': np.array(test_logls), 'max_test': max_test, 'tr': np.array(tr_logls), 'all_times': all_times, 'tnparams': rnn1.tnparams} res_dictionary.update(d) return res_dictionary 
python|grpc|OpsTest|of|out|ops|test|scope|variable|seed|rl|master|foo|bind @tf.function(input_signature=[tf.TensorSpec([], tf.int32)]) def foo(x): return x + a 
minibatch|negative|BalancedPositiveNegativeSamplerTest|avod|test|positive|subsample|balanced|selection|samplers|core|sampler def test_subsample_selection(self): numpy_labels = np.arange(100) numpy_indicator = numpy_labels < 90 indicator = tf.constant(numpy_indicator) numpy_labels = numpy_labels - 80 >= 0 labels = tf.constant(numpy_labels) sampler = (balanced_positive_negative_sampler. BalancedPositiveNegativeSampler()) is_sampled, _ = sampler.subsample(indicator, 64, labels) with self.test_session() as sess: is_sampled = sess.run(is_sampled) self.assertTrue(sum(is_sampled) == 64) self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 10) self.assertTrue(sum(np.logical_and(np.logical_not(numpy_labels), is_sampled)) == 54) self.assertAllEqual(is_sampled, np.logical_and(is_sampled, numpy_indicator)) 
devtools|cleverhans|NumpyDocString|tests|parse|docscrape|index def _parse_index(self, section, content): """ .. index: default :refguide: something, else, and more  """  def strip_each_in(lst): return [s.strip() for s in lst] out = {} section = section.split('::') if len(section) > 1: out['default'] = strip_each_in(section[1].split(','))[0] for line in content: line = line.split(':') if len(line) > 2: out[line[1]] = strip_each_in(line[2].split(',')) return out 
main def main(): num_patients = 229 patients_indexes = np.array([i for i in range(num_patients)]) kf = KFold(n_splits=num_folds, shuffle=False) folds_score = [] for fold, (train_patient_indexes, val_patient_indexes) in enumerate(kf. split(patients_indexes)): fold_mean_score = train(fold=fold, train_patient_indexes= train_patient_indexes, val_patient_indexes=val_patient_indexes) folds_score.append(fold_mean_score) print('Final score from ', num_folds, ' folds cross validation:') final_score = {} for key in folds_score[0].keys(): scores = [] for i in range(num_folds): scores.append(folds_score[i][key]) final_score[key] = np.mean(scores) print(key, ' score: \t', final_score[key]) df = pd.DataFrame(final_score, index=[0]) df.to_csv('x_net_final_score.csv', index=False) 
infinite|InfiniteFcnTest|fcn|test def test_infinite_fcn(self): infinite_fcn.main(None) 
domain|CreateSQLiteDB|plato|create|sqlite|db|sql|table def create_sql_table(self, sql_conn, create_table_sql): """ This function creates a table given an SQL connection  :param sql_conn: Connection object :param create_table_sql: a CREATE TABLE statement :return: """ try: sql_cursor = sql_conn.cursor() sql_cursor.execute(create_table_sql) except Error as e: print(e) 
tmp|vggverydeep19|load|master|facenet def load(filename, images): vgg19 = io.loadmat(filename) vgg19Layers = vgg19['layers']  def vbbWeights(layerNumber): W = vgg19Layers[0][layerNumber][0][0][2][0][0] W = tf.constant(W) return W  def vbbConstants(layerNumber): b = vgg19Layers[0][layerNumber][0][0][2][0][1].T b = tf.constant(np.reshape(b, b.size)) return b modelGraph = {} modelGraph['input'] = images modelGraph['conv1_1'] = tf.nn.relu(tf.nn.conv2d(modelGraph['input'], filter=vbbWeights(0), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(0)) modelGraph['conv1_2'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv1_1'], filter=vbbWeights(2), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(2)) modelGraph['avgpool1'] = tf.nn.avg_pool(modelGraph['conv1_2'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv2_1'] = tf.nn.relu(tf.nn.conv2d(modelGraph['avgpool1'], filter=vbbWeights(5), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(5)) modelGraph['conv2_2'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv2_1'], filter=vbbWeights(7), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(7)) modelGraph['avgpool2'] = tf.nn.avg_pool(modelGraph['conv2_2'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv3_1'] = tf.nn.relu(tf.nn.conv2d(modelGraph['avgpool2'], filter=vbbWeights(10), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(10)) modelGraph['conv3_2'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv3_1'], filter=vbbWeights(12), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(12)) modelGraph['conv3_3'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv3_2'], filter=vbbWeights(14), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(14)) modelGraph['conv3_4'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv3_3'], filter=vbbWeights(16), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(16)) modelGraph['avgpool3'] = tf.nn.avg_pool(modelGraph['conv3_4'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv4_1'] = tf.nn.relu(tf.nn.conv2d(modelGraph['avgpool3'], filter=vbbWeights(19), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(19)) modelGraph['conv4_2'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv4_1'], filter=vbbWeights(21), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(21)) modelGraph['conv4_3'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv4_2'], filter=vbbWeights(23), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(23)) modelGraph['conv4_4'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv4_3'], filter=vbbWeights(25), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(25)) modelGraph['avgpool4'] = tf.nn.avg_pool(modelGraph['conv4_4'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv5_1'] = tf.nn.relu(tf.nn.conv2d(modelGraph['avgpool4'], filter=vbbWeights(28), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(28)) modelGraph['conv5_2'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv5_1'], filter=vbbWeights(30), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(30)) modelGraph['conv5_3'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv5_2'], filter=vbbWeights(32), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(32)) modelGraph['conv5_4'] = tf.nn.relu(tf.nn.conv2d(modelGraph['conv5_3'], filter=vbbWeights(34), strides=[1, 1, 1, 1], padding='SAME') + vbbConstants(34)) modelGraph['avgpool5'] = tf.nn.avg_pool(modelGraph['conv5_4'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') return modelGraph 
rnn|init|rnns def __init__(self, model_specs, initializer='xavier'): """model specs is a dictionary""" self.model_specs = model_specs self.initializer = initializer 
bert|transfo|pytorch|init|modeling|xl|pretrained|TransfoXLConfig def __init__(self, vocab_size_or_config_json_file=267735, cutoffs=[20000, 40000, 200000], d_model=1024, d_embed=1024, n_head=16, d_head=64, d_inner=4096, div_val=4, pre_lnorm=False, n_layer=18, tgt_len=128, ext_len=0, mem_len=1600, clamp_len=1000, same_length=True, proj_share_all_but_first=True, attn_type=0, sample_softmax=-1, adaptive =True, tie_weight=True, dropout=0.1, dropatt=0.0, untie_r=True, init= 'normal', init_range=0.01, proj_init_std=0.01, init_std=0.02): """Constructs TransfoXLConfig.  Args: vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `TransfoXLModel` or a configuration json file. cutoffs: cutoffs for the adaptive softmax d_model: Dimensionality of the model's hidden states. d_embed: Dimensionality of the embeddings d_head: Dimensionality of the model's heads. div_val: divident value for adapative input and softmax pre_lnorm: apply LayerNorm to the input instead of the output d_inner: Inner dimension in FF n_layer: Number of hidden layers in the Transformer encoder. n_head: Number of attention heads for each attention layer in the Transformer encoder. tgt_len: number of tokens to predict ext_len: length of the extended context mem_len: length of the retained previous heads same_length: use the same attn length for all tokens proj_share_all_but_first: True to share all but first projs, False not to share. attn_type: attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al. clamp_len: use the same pos embeddings after clamp_len sample_softmax: number of samples in sampled softmax adaptive: use adaptive softmax tie_weight: tie the word embedding and softmax weights dropout: The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler. dropatt: The dropout ratio for the attention probabilities. untie_r: untie relative position biases embd_pdrop: The dropout ratio for the embeddings. init: parameter initializer to use init_range: parameters initialized by U(-init_range, init_range). proj_init_std: parameters initialized by N(0, init_std) init_std: parameters initialized by N(0, init_std) """ if isinstance(vocab_size_or_config_json_file, str) or sys.version_info[0 ] == 2 and isinstance(vocab_size_or_config_json_file, unicode): with open(vocab_size_or_config_json_file, 'r', encoding='utf-8' ) as reader: json_config = json.loads(reader.read()) for key, value in json_config.items(): self.__dict__[key] = value elif isinstance(vocab_size_or_config_json_file, int): self.n_token = vocab_size_or_config_json_file self.cutoffs = [] self.cutoffs.extend(cutoffs) self.tie_weight = tie_weight if proj_share_all_but_first: self.tie_projs = [False] + [True] * len(self.cutoffs) else: self.tie_projs = [False] + [False] * len(self.cutoffs) self.d_model = d_model self.d_embed = d_embed self.d_head = d_head self.d_inner = d_inner self.div_val = div_val self.pre_lnorm = pre_lnorm self.n_layer = n_layer self.n_head = n_head self.tgt_len = tgt_len self.ext_len = ext_len self.mem_len = mem_len self.same_length = same_length self.attn_type = attn_type self.clamp_len = clamp_len self.sample_softmax = sample_softmax self.adaptive = adaptive self.dropout = dropout self.dropatt = dropatt self.untie_r = untie_r self.init = init self.init_range = init_range self.proj_init_std = proj_init_std self.init_std = init_std else: raise ValueError( 'First argument must be either a vocabulary size (int)or the path to a pretrained model config file (str)' ) 
loops|eval|and|train def train(out, sess, epoch, training_steps, train_op, loss_op, global_step, learning_rate, is_training_ph, training_data, data_iterator, after_train_step=None): train_step, g_step = 0, 0 sess.run(data_iterator.make_initializer(training_data)) try: while True: out.train_step_begin(train_step) _, l, g_step, lr = sess.run([train_op, loss_op, global_step, learning_rate], feed_dict={is_training_ph: True}, options= out.get_run_options(), run_metadata=out.get_run_metadata()) out.train_step_end(sess, epoch, g_step, train_step, l, lr, training_steps, feed_dict={is_training_ph: True}) train_step += 1 except tf.errors.OutOfRangeError: pass out.train_end(sess, epoch, g_step) 
utils|get|ph def get_ph(tensor): """Returns a tf.placeholder that has the same size/dtype as tensor  Arguments: tensor {tf tensor} -- tensor  Returns: tf placeholder -- placeholder with same size/dtype as tensor """ return tf.placeholder(tensor.dtype, tensor.get_shape()) 
training|init|IndexGenerator|vkge|util def __init__(self): self.random_state = np.random.RandomState(0) 
plot|get|data|kld|sample def get_sample_data(path): try: df_sample = pd.read_csv(path + '/sample.txt', sep='\t') return df_sample except Exception as ex: print(str(ex)) print('error reading: %s' % path + '/sample.txt') return None 
xlnet|print|master|utils|prepro def print_(*args): new_args = [] for arg in args: if isinstance(arg, list): s = [printable_text(i) for i in arg] s = ' '.join(s) new_args.append(s) else: new_args.append(printable_text(arg)) print(*new_args) 
features|xlnet|convert|examples|master|squad|run|to def convert_examples_to_features(examples, sp_model, max_seq_length, doc_stride, max_query_length, is_training, output_fn): """Loads a data file into a list of `InputBatch`s.""" cnt_pos, cnt_neg = 0, 0 unique_id = 1000000000 max_N, max_M = 1024, 1024 f = np.zeros((max_N, max_M), dtype=np.float32) for example_index, example in enumerate(examples): if example_index % 100 == 0: tf.logging.info('Converting {}/{} pos {} neg {}'.format( example_index, len(examples), cnt_pos, cnt_neg)) query_tokens = encode_ids(sp_model, preprocess_text(example. question_text, lower=FLAGS.uncased)) if len(query_tokens) > max_query_length: query_tokens = query_tokens[0:max_query_length] paragraph_text = example.paragraph_text para_tokens = encode_pieces(sp_model, preprocess_text(example. paragraph_text, lower=FLAGS.uncased)) chartok_to_tok_index = [] tok_start_to_chartok_index = [] tok_end_to_chartok_index = [] char_cnt = 0 for i, token in enumerate(para_tokens): chartok_to_tok_index.extend([i] * len(token)) tok_start_to_chartok_index.append(char_cnt) char_cnt += len(token) tok_end_to_chartok_index.append(char_cnt - 1) tok_cat_text = ''.join(para_tokens).replace(SPIECE_UNDERLINE, ' ') N, M = len(paragraph_text), len(tok_cat_text) if N > max_N or M > max_M: max_N = max(N, max_N) max_M = max(M, max_M) f = np.zeros((max_N, max_M), dtype=np.float32) gc.collect() g = {}  def _lcs_match(max_dist): f.fill(0) g.clear() for i in range(N): for j in range(i - max_dist, i + max_dist): if j >= M or j < 0: continue if i > 0: g[i, j] = 0 f[i, j] = f[i - 1, j] if j > 0 and f[i, j - 1] > f[i, j]: g[i, j] = 1 f[i, j] = f[i, j - 1] f_prev = f[i - 1, j - 1] if i > 0 and j > 0 else 0 if preprocess_text(paragraph_text[i], lower=FLAGS. uncased, remove_space=False) == tok_cat_text[j ] and f_prev + 1 > f[i, j]: g[i, j] = 2 f[i, j] = f_prev + 1 max_dist = abs(N - M) + 5 for _ in range(2): _lcs_match(max_dist) if f[N - 1, M - 1] > 0.8 * N: break max_dist *= 2 orig_to_chartok_index = [None] * N chartok_to_orig_index = [None] * M i, j = N - 1, M - 1 while i >= 0 and j >= 0: if (i, j) not in g: break if g[i, j] == 2: orig_to_chartok_index[i] = j chartok_to_orig_index[j] = i i, j = i - 1, j - 1 elif g[i, j] == 1: j = j - 1 else: i = i - 1 if all(v is None for v in orig_to_chartok_index) or f[N - 1, M - 1 ] < 0.8 * N: print('MISMATCH DETECTED!') continue tok_start_to_orig_index = [] tok_end_to_orig_index = [] for i in range(len(para_tokens)): start_chartok_pos = tok_start_to_chartok_index[i] end_chartok_pos = tok_end_to_chartok_index[i] start_orig_pos = _convert_index(chartok_to_orig_index, start_chartok_pos, N, is_start=True) end_orig_pos = _convert_index(chartok_to_orig_index, end_chartok_pos, N, is_start=False) tok_start_to_orig_index.append(start_orig_pos) tok_end_to_orig_index.append(end_orig_pos) if not is_training: tok_start_position = tok_end_position = None if is_training and example.is_impossible: tok_start_position = -1 tok_end_position = -1 if is_training and not example.is_impossible: start_position = example.start_position end_position = start_position + len(example.orig_answer_text) - 1 start_chartok_pos = _convert_index(orig_to_chartok_index, start_position, is_start=True) tok_start_position = chartok_to_tok_index[start_chartok_pos] end_chartok_pos = _convert_index(orig_to_chartok_index, end_position, is_start=False) tok_end_position = chartok_to_tok_index[end_chartok_pos] assert tok_start_position <= tok_end_position  def _piece_to_id(x): if six.PY2 and isinstance(x, unicode): x = x.encode('utf-8') return sp_model.PieceToId(x) all_doc_tokens = list(map(_piece_to_id, para_tokens)) max_tokens_for_doc = max_seq_length - len(query_tokens) - 3 _DocSpan = collections.namedtuple('DocSpan', ['start', 'length']) doc_spans = [] start_offset = 0 while start_offset < len(all_doc_tokens): length = len(all_doc_tokens) - start_offset if length > max_tokens_for_doc: length = max_tokens_for_doc doc_spans.append(_DocSpan(start=start_offset, length=length)) if start_offset + length == len(all_doc_tokens): break start_offset += min(length, doc_stride) for doc_span_index, doc_span in enumerate(doc_spans): tokens = [] token_is_max_context = {} segment_ids = [] p_mask = [] cur_tok_start_to_orig_index = [] cur_tok_end_to_orig_index = [] for i in range(doc_span.length): split_token_index = doc_span.start + i cur_tok_start_to_orig_index.append(tok_start_to_orig_index[ split_token_index]) cur_tok_end_to_orig_index.append(tok_end_to_orig_index[ split_token_index]) is_max_context = _check_is_max_context(doc_spans, doc_span_index, split_token_index) token_is_max_context[len(tokens)] = is_max_context tokens.append(all_doc_tokens[split_token_index]) segment_ids.append(SEG_ID_P) p_mask.append(0) paragraph_len = len(tokens) tokens.append(SEP_ID) segment_ids.append(SEG_ID_P) p_mask.append(1) for token in query_tokens: tokens.append(token) segment_ids.append(SEG_ID_Q) p_mask.append(1) tokens.append(SEP_ID) segment_ids.append(SEG_ID_Q) p_mask.append(1) cls_index = len(segment_ids) tokens.append(CLS_ID) segment_ids.append(SEG_ID_CLS) p_mask.append(0) input_ids = tokens input_mask = [0] * len(input_ids) while len(input_ids) < max_seq_length: input_ids.append(0) input_mask.append(1) segment_ids.append(SEG_ID_PAD) p_mask.append(1) assert len(input_ids) == max_seq_length assert len(input_mask) == max_seq_length assert len(segment_ids) == max_seq_length assert len(p_mask) == max_seq_length span_is_impossible = example.is_impossible start_position = None end_position = None if is_training and not span_is_impossible: doc_start = doc_span.start doc_end = doc_span.start + doc_span.length - 1 out_of_span = False if not (tok_start_position >= doc_start and tok_end_position <= doc_end): out_of_span = True if out_of_span: start_position = 0 end_position = 0 span_is_impossible = True else: doc_offset = 0 start_position = (tok_start_position - doc_start + doc_offset) end_position = tok_end_position - doc_start + doc_offset if is_training and span_is_impossible: start_position = cls_index end_position = cls_index if example_index < 20: tf.logging.info('*** Example ***') tf.logging.info('unique_id: %s' % unique_id) tf.logging.info('example_index: %s' % example_index) tf.logging.info('doc_span_index: %s' % doc_span_index) tf.logging.info('tok_start_to_orig_index: %s' % ' '.join([ str(x) for x in cur_tok_start_to_orig_index])) tf.logging.info('tok_end_to_orig_index: %s' % ' '.join([str (x) for x in cur_tok_end_to_orig_index])) tf.logging.info('token_is_max_context: %s' % ' '.join([( '%d:%s' % (x, y)) for x, y in six.iteritems( token_is_max_context)])) tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids])) tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask])) tf.logging.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids])) if is_training and span_is_impossible: tf.logging.info('impossible example span') if is_training and not span_is_impossible: pieces = [sp_model.IdToPiece(token) for token in tokens [start_position:end_position + 1]] answer_text = sp_model.DecodePieces(pieces) tf.logging.info('start_position: %d' % start_position) tf.logging.info('end_position: %d' % end_position) tf.logging.info('answer: %s' % printable_text(answer_text)) if is_training: feat_example_index = None else: feat_example_index = example_index feature = InputFeatures(unique_id=unique_id, example_index= feat_example_index, doc_span_index=doc_span_index, tok_start_to_orig_index=cur_tok_start_to_orig_index, tok_end_to_orig_index=cur_tok_end_to_orig_index, token_is_max_context=token_is_max_context, input_ids= input_ids, input_mask=input_mask, p_mask=p_mask, segment_ids=segment_ids, paragraph_len=paragraph_len, cls_index=cls_index, start_position=start_position, end_position=end_position, is_impossible=span_is_impossible) output_fn(feature) unique_id += 1 if span_is_impossible: cnt_neg += 1 else: cnt_pos += 1 tf.logging.info('Total number of instances: {} = pos {} neg {}'.format( cnt_pos + cnt_neg, cnt_pos, cnt_neg)) 
collate|dataset|graphs|csqa def collate_csqa_graphs(samples): statements, correct_labels, graph_data = map(list, zip(*samples)) flat_graph_data = [] for gd in graph_data: flat_graph_data.extend(gd) batched_graph = dgl.batch(flat_graph_data) sents_vecs = torch.stack(statements) return sents_vecs, torch.Tensor([[i] for i in correct_labels] ), batched_graph 
escape|xngen def escape(line): output_parts = [] while '${' in line: start_pos = line.index('${') end_pos = line.index('}', start_pos + 2) if start_pos != 0: output_parts.append('"' + line[:start_pos].replace('"', '\\"') + '"') output_parts.append('str(' + line[start_pos + 2:end_pos] + ')') line = line[end_pos + 1:] if line: output_parts.append('"' + line.replace('"', '\\"') + '"') return ' + '.join(output_parts) 
celeba|SpectralNormalization|t|sagan|normalization|spectral def spectral_normalization(self, w): return w / self.spectral_norm(w) 
gen|plot|core|resnet11|A|for|structured|epoch def plot_resnet110_structured_A_epoch_for_epoch_core(): common.epoch_for_epoch_plot(network=common.RESNET110, is_iterative= False, prune_method=common.STRUCTURED_A, min_max_y=(-0.03, 0.01), to_ignore=['lr_lottery', 'reinit']) 
collate|dataset|paths|csqa def collate_csqa_paths(samples): (statements, correct_labels, cpt_path_data, rel_path_data, qa_pair_data, qa_text) = map(list, zip(*samples)) sents_vecs = torch.stack(statements) return sents_vecs, torch.Tensor([[i] for i in correct_labels] ), cpt_path_data, rel_path_data, qa_pair_data 
stdev|ops|deconv2d|set|tflib|weights def set_weights_stdev(weights_stdev): global _weights_stdev _weights_stdev = weights_stdev 
datacode|init|DataCreation|ner def __init__(self, input_separator=' ', padding_word_string='<none>'): self.input_separator = input_separator self.padding_word_string = padding_word_string self.word_cluster = None 
epoch|train|ner def _train_epoch(sess, epoch): start_time = time.time() loss = 0.0 corr = 0.0 num_tokens = 0.0 fetches = {'mle_loss': mle_loss, 'correct': corrects} fetches['train_op'] = train_op mode = tf.estimator.ModeKeys.TRAIN num_inst = 0 for batch in iterate_batch(data_train, config.batch_size, shuffle=True): word, char, ner, mask, length = batch feed_dict = {inputs: word, chars: char, targets: ner, masks: mask, seq_lengths: length, global_step: epoch, tx.global_mode(): mode} rets = sess.run(fetches, feed_dict) nums = np.sum(length) num_inst += len(word) loss += rets['mle_loss'] * nums corr += rets['correct'] num_tokens += nums print('train: %d (%d/%d) loss: %.4f, acc: %.2f%%' % (epoch, num_inst, len(data_train), loss / num_tokens, corr / num_tokens * 100)) print('train: %d loss: %.4f, acc: %.2f%%, time: %.2fs' % (epoch, loss / num_tokens, corr / num_tokens * 100, time.time() - start_time)) 
tICA|init|tica def __init__(self, n_components=None, lag_time=1, shrinkage=None, kinetic_mapping=False, commute_mapping=False): self.n_components = n_components self.lag_time = lag_time self.shrinkage = shrinkage self.shrinkage_ = None self.kinetic_mapping = kinetic_mapping self.commute_mapping = commute_mapping if self.kinetic_mapping and self.commute_mapping: raise ValueError( "Can't have both kinetic mapping and commute mapping. Please only use one." ) self.n_features = None self.n_observations_ = None self.n_sequences_ = None self._initialized = False self._outer_0_to_T_lagged = None self._sum_0_to_TminusTau = None self._sum_tau_to_T = None self._sum_0_to_T = None self._outer_0_to_TminusTau = None self._outer_offset_to_T = None self._components_ = None self._eigenvectors_ = None self._eigenvalues_ = None self._is_dirty = True 
get|LayerNormalization|deepctr|config|layers|normalization def get_config(self): config = {'axis': self.axis, 'eps': self.eps} base_config = super(LayerNormalization, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
models|forward|RelationNetwork def forward(self, statement_vecs, qa_pairs): pooled_qas_vecs_list = [] for index in range(len(statement_vecs)): s_vec = statement_vecs[index].to(self.device) if len(qa_pairs[index]) == 0 or False: qas_vecs = torch.cat((torch.zeros(1, self.concept_dim).to(self. device), torch.zeros(1, self.concept_dim).to(self.device), torch.stack([s_vec]).to(self.device)), dim=1).to(self.device) else: q_seq = [] a_seq = [] for qa_pair in qa_pairs[index]: q, a = qa_pair[0], qa_pair[1] q_seq.append(q) a_seq.append(a) q_seq = torch.LongTensor(q_seq).to(self.device) a_seq = torch.LongTensor(a_seq).to(self.device) q_vecs = self.concept_emd(q_seq).to(self.device) a_vecs = self.concept_emd(a_seq).to(self.device) s_vecs = torch.stack([s_vec] * len(qa_pairs[index])).to(self.device ) qas_vecs = torch.cat((q_vecs, a_vecs, s_vecs), dim=1) pooled_qas_vecs = qas_vecs.mean(dim=0).to(self.device) pooled_qas_vecs_list.append(pooled_qas_vecs) latent_rel_vecs = self.relation_extractor(torch.stack(pooled_qas_vecs_list) ) logits = self.hidden2output(latent_rel_vecs).to(self.device) return logits 
attack|Trainer|train|classification def attack(self, bim=False): self.model.load(self.sess) if self.config.dataset == 'cifar10': test_transform = transforms.Compose([transforms.ToTensor(), Transpose()]) testset = dset.CIFAR10(root=self.config.data_path, train=False, download=True, transform=test_transform) testloader = torch.utils.data.DataLoader(testset, batch_size=self. config.test_batch_size, shuffle=False, num_workers=self.config. num_workers) elif self.config.dataset == 'cifar100': test_transform = transforms.Compose([transforms.ToTensor(), Transpose()]) testset = dset.CIFAR100(root=self.config.data_path, train=False, download=True, transform=test_transform) testloader = torch.utils.data.DataLoader(testset, batch_size=self. config.test_batch_size, shuffle=False, num_workers=self.config. num_workers) print(len(testloader)) for magnitude_ in [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]: advxs_list = [] labels_list = [] if bim: for x, y in testloader: for ite in range(3): feed_dict = {self.model.inputs: x if ite == 0 else advx, self.model.targets: y, self.model.is_training: False, self.model.n_particles: 30, self.model. magnitude: magnitude_ / 3.0} [advx] = self.sess.run([self.model.inputs_adv], feed_dict=feed_dict) advxs_list.append(advx) labels_list.append(y.numpy()) else: for x, y in testloader: feed_dict = {self.model.inputs: x, self.model.targets: y, self.model.is_training: False, self.model.n_particles: 30, self.model.magnitude: magnitude_} [advx] = self.sess.run([self.model.inputs_adv], feed_dict= feed_dict) advxs_list.append(advx) labels_list.append(y.numpy()) testset_adv = torch.utils.data.TensorDataset(torch.from_numpy(np. concatenate(advxs_list, 0)), torch.from_numpy(np.concatenate( labels_list, 0))) testloader_adv = torch.utils.data.DataLoader(testset_adv, batch_size=self.config.test_batch_size, shuffle=False, num_workers=self.config.num_workers) loss_list = [] acc_list = [] ent_list = [] for x, y in testloader_adv: feed_dict = {self.model.inputs: x, self.model.targets: y, self. model.is_training: False, self.model.n_particles: 30} loss, acc, ents = self.sess.run([self.model.loss, self.model. acc, self.model.ent], feed_dict=feed_dict) loss_list.append(loss) acc_list.append(acc) ent_list.append(ents) avg_loss = np.mean(loss_list) avg_acc = np.mean(acc_list) avg_ent = np.concatenate(ent_list, 0).mean() print( '2019-09-01 10:26:52,395 Test set: epsilon: {} average loss: {:.4f}, entropy: {:.4f}, Error: {}/10000 ({:.2f}%)' .format(magnitude_, avg_loss, avg_ent, int(10000 * (1 - avg_acc )), (1 - avg_acc) * 100)) 
evaluators|init|MAPEvaluator|map def __init__(self, train_interactions, test_interactions): super(MAPEvaluator, self).__init__(train_interactions, test_interactions) 
lfw|on|parse|arguments|src|master|facenet|validate def parse_arguments(argv): parser = argparse.ArgumentParser() parser.add_argument('lfw_dir', type=str, help= 'Path to the data directory containing aligned LFW face patches.') parser.add_argument('--lfw_batch_size', type=int, help= 'Number of images to process in a batch in the LFW test set.', default=100) parser.add_argument('model', type=str, help= 'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file' ) parser.add_argument('--image_size', type=int, help= 'Image size (height, width) in pixels.', default=160) parser.add_argument('--lfw_pairs', type=str, help= 'The file containing the pairs to use for validation.', default= 'data/pairs.txt') parser.add_argument('--lfw_nrof_folds', type=int, help= 'Number of folds to use for cross validation. Mainly used for testing.' , default=10) parser.add_argument('--distance_metric', type=int, help= 'Distance metric  0:euclidian, 1:cosine similarity.', default=0) parser.add_argument('--use_flipped_images', help= 'Concatenates embeddings for the image and its horizontally flipped counterpart.' , action='store_true') parser.add_argument('--subtract_mean', help= 'Subtract feature mean before calculating distance.', action= 'store_true') parser.add_argument('--use_fixed_image_standardization', help= 'Performs fixed standardization of images.', action='store_true') return parser.parse_args(argv) 
texar|Conv1DClassifier|conv|modules|outputs|layer|classifiers @property def layer_outputs(self): """A list containing output tensors of each layer. """ return self._encoder.layer_outputs 
texar|hparams|agent|agents|base|AgentBase @property def hparams(self): """A :class:`~texar.hyperparams.HParams` instance. The hyperparameters of the module. """ return self._hparams 
with|params|tflib|name def params_with_name(name): return [p for n, p in list(_params.items()) if name in n] 
agents|init|compressor|network|D4pgPreprocessor|slac def __init__(self, name=None): super(D4pgPreprocessor, self).__init__(name=name) self.conv_layers = [tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='VALID', activation=tf.keras.activations.elu, kernel_initializer=tf.keras.initializers.glorot_uniform()), tf. keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding= 'VALID', activation=tf.keras.activations.elu, kernel_initializer=tf .keras.initializers.glorot_uniform())] spatial_size = 29 self.feature_size = spatial_size * spatial_size * 32 
init|BucketedDataIterator|rnns def __init__(self, df, num_buckets=2): df = df.sort_values('lengths').reset_index(drop=True) self.size = len(df) / num_buckets self.dfs = [] for bucket in range(num_buckets): self.dfs.append(df.ix[bucket * self.size:(bucket + 1) * self.size - 1]) self.num_buckets = num_buckets self.cursor = np.array([0] * num_buckets) self.shuffle() self.epochs = 0 
sense|init|representation|model|learning def __init__(self, s_in, s_out, sense_counts, samp_size, learning_rate, bi_sense_counts, bi_s_mat): with tf.device('/cpu:0'): selected_sense_output_indices = tf.placeholder(tf.int32, None, name ='selected_sense_output_indices') selected_sense_input_indices = tf.placeholder(tf.int32, None, name= 'selected_sense_input_indices') selected_bi_sense_output_indices = tf.placeholder(tf.int32, None, name='selected_bi_sense_output_indices') self.selected_sense_input_indices = selected_sense_input_indices self.selected_sense_output_indices = selected_sense_output_indices self.selected_bi_sense_output_indices = ( selected_bi_sense_output_indices) embedded_sense_input = tf.nn.embedding_lookup(s_in, selected_sense_input_indices) self.embedded_sense_input = embedded_sense_input self.embedded_sense_input_l2 = tf.nn.l2_normalize(embedded_sense_input, axis=1) embedded_sense_output = tf.nn.embedding_lookup(s_out, selected_sense_output_indices) self.embedded_sense_output = embedded_sense_output self.embedded_sense_output_l2 = tf.nn.l2_normalize( embedded_sense_output, axis=1) bi_embedded_sense_output = tf.nn.embedding_lookup(bi_s_mat, selected_bi_sense_output_indices) self.reward_sense_prob = tf.sigmoid(tf.reduce_sum(tf.multiply( embedded_sense_input, embedded_sense_output), 1)) self.bi_reward_sense_prob = tf.sigmoid(tf.reduce_sum(tf.multiply( embedded_sense_input, bi_embedded_sense_output), 1)) self.train = word2vec.neg_train_word2vec(s_in, s_out, selected_sense_input_indices, selected_sense_output_indices, learning_rate, vocab_count=sense_counts, num_negative_samples= samp_size) self.train_bilingual = word2vec.neg_train_word2vec(s_in, bi_s_mat, selected_sense_input_indices, selected_bi_sense_output_indices, learning_rate, vocab_count=bi_sense_counts, num_negative_samples=samp_size) 
get|sequence|KMaxPooling|deepctr|config|layers def get_config(self): config = {'k': self.k, 'axis': self.axis} base_config = super(KMaxPooling, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
learner|compute|test|LearnerTest|loss|basic def test_compute_loss_basic(self): """Basic test to exercise learner.compute_loss_and_priorities().""" batch_size = 32 num_actions = 3 unroll_length = 10 training_agent = agents.DuelingLSTMDQNNet(num_actions, OBS_SHAPE) prev_actions = tf.random.uniform([unroll_length, batch_size], maxval=2, dtype=tf.int32) tf.function(learner.compute_loss_and_priorities)(training_agent, agents .DuelingLSTMDQNNet(num_actions, OBS_SHAPE), training_agent. initial_state(batch_size), prev_actions, self._create_env_output( batch_size, unroll_length), self._create_agent_outputs(batch_size, unroll_length, num_actions), 0.99, burn_in=5) 
ManipulatorEnv|render|envs|gym|extensions|bullet|debug|env|manipulator def render_debug(self, width=600): return self.robot.render_debug(width=width) 
texar|PolicyNetBase|policies|modules|init|policy|nets def __init__(self, network=None, network_kwargs=None, hparams=None): ModuleBase.__init__(self, hparams=hparams) with tf.variable_scope(self.variable_scope): self._build_network(network, network_kwargs) 
bias|conv|no|1x3|make|cnn|helpers def make_conv_1x3_no_bias(op_name, in_tensor, filters, strides=(1, 1, 1, 1), padding='VALID', weight_decay=0.0005, stddev=0.1): return make_conv_no_bias(op_name, in_tensor, 1, 3, filters, strides, padding, weight_decay, stddev) 
bert|modeling|BertConfig|to|dict def to_dict(self): """Serializes this instance to a Python dictionary.""" output = copy.deepcopy(self.__dict__) return output 
bert|csqa|init|SwagExample|run def __init__(self, swag_id, context_sentence, start_ending, ending_0, ending_1, ending_2, ending_3, ending_4, label=None): self.swag_id = swag_id self.context_sentence = context_sentence self.start_ending = start_ending self.endings = [ending_0, ending_1, ending_2, ending_3, ending_4] self.label = label 
concat|utils|channels @add_arg_scope def concat_channels(l_inputs, data_format='NHWC', scope=None): """Concat a list of tensors on the channel axis.  Args: inputs: List Tensors; data_format: NHWC or NCHW. """ with tf.name_scope(scope, 'concat_channels', l_inputs): if data_format == 'NHWC': net = tf.concat(l_inputs, axis=3) elif data_format == 'NCHW': net = tf.concat(l_inputs, axis=1) return net 
maml2|archs|MAML|loadWeights def loadWeights(self, sess, name, step=0, modeldir='./model_checkpoint/', model_name='model.ckpt'): if self.saver == None: z = self.saving_weights self.saver = tf.train.Saver(var_list=z, max_to_keep=12) saver = self.saver checkpoint_path = modeldir + f'{name}/' + model_name + '-' + str(step) if os.path.isfile(checkpoint_path + '.marker'): saver.restore(sess, checkpoint_path) print('The checkpoint has been loaded.') else: print(checkpoint_path + '.marker not found. Starting from scratch.') 
learn|conditioned|goal|hbaselines|TD3|algorithm def learn(self, total_timesteps, log_dir=None, seed=None, log_interval=2000, eval_interval=50000, save_interval=10000, start_timesteps=50000): """Return a trained model.  Parameters ---------- total_timesteps : int the total number of samples to train on log_dir : str the directory where the training and evaluation statistics, as well as the tensorboard log, should be stored seed : int or None the initial seed for training, if None: keep current seed log_interval : int the number of training steps before logging training results eval_interval : int number of simulation steps in the training environment before an evaluation is performed save_interval : int number of simulation steps in the training environment before the model is saved start_timesteps : int, optional number of timesteps that the policy is run before training to initialize the replay buffer with samples """ self.saver = tf.compat.v1.train.Saver(self.trainable_vars, max_to_keep= total_timesteps // save_interval) ensure_dir(log_dir) ensure_dir(os.path.join(log_dir, 'checkpoints')) save_path = os.path.join(log_dir, 'tb_log') writer = tf.compat.v1.summary.FileWriter(save_path) train_filepath = os.path.join(log_dir, 'train.csv') eval_filepath = os.path.join(log_dir, 'eval.csv') random.seed(seed) np.random.seed(seed) tf.compat.v1.set_random_seed(seed) if self.verbose >= 2: print('Using agent with the following configuration:') print(str(self.__dict__.items())) eval_steps_incr = 0 save_steps_incr = 0 start_time = time.time() with self.sess.as_default(), self.graph.as_default(): self.obs = self.env.reset() self.obs = self._add_fingerprint(self.obs, self.total_steps, total_timesteps) print('Collecting pre-samples...') self._collect_samples(total_timesteps, run_steps=start_timesteps, random_actions=True) print('Done!') self.episodes = 0 self.total_steps = 0 self.episode_rewards_history = deque(maxlen=100) while True: self.epoch_episodes = 0 self.epoch_actions = [] self.epoch_q1s = [] self.epoch_q2s = [] self.epoch_actor_losses = [] self.epoch_critic_losses = [] self.epoch_episode_rewards = [] self.epoch_episode_steps = [] for _ in range(log_interval): if self.total_steps >= total_timesteps: return self._collect_samples(total_timesteps) self._train() self._log_training(train_filepath, start_time) if (self.eval_env is not None and self.total_steps - eval_steps_incr >= eval_interval): eval_steps_incr += eval_interval if isinstance(self.eval_env, list): eval_rewards = [] eval_successes = [] eval_info = [] for env in self.eval_env: rew, suc, inf = self._evaluate(total_timesteps, env) eval_rewards.append(rew) eval_successes.append(suc) eval_info.append(inf) else: eval_rewards, eval_successes, eval_info = self._evaluate( total_timesteps, self.eval_env) self._log_eval(eval_filepath, start_time, eval_rewards, eval_successes, eval_info) if writer is not None: td_map = self.policy_tf.get_td_map() if td_map: td_map.update({self.rew_ph: np.mean(self. epoch_episode_rewards), self.rew_history_ph: np. mean(self.episode_rewards_history)}) summary = self.sess.run(self.summary, td_map) writer.add_summary(summary, self.total_steps) if self.total_steps - save_steps_incr >= save_interval: save_steps_incr += save_interval self.save(os.path.join(log_dir, 'checkpoints/itr')) self.epoch += 1 
fpn|coord|unet|model|bins|graph|delta|build def build_fpn_coord_bins_delta_unet_graph(rois, feature_maps, image_shape, pool_size, num_classes, num_bins, net_name): """Builds the computation graph of the coordinate map head of Feature Pyramid Network.  rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: Coordinate maps [batch, roi_count, height, width, num_classes, num_bins] """ x = PyramidROIAlign([pool_size, pool_size], image_shape, name= 'roi_align_{}'.format(net_name))([rois] + feature_maps) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv1'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn1'.format( net_name))(x) conv1_output = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.MaxPool2D(pool_size=(2, 2)), name= 'mrcnn_{}_maxpool1'.format(net_name))(conv1_output) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv2'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn2'.format( net_name))(x) conv2_output = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.MaxPool2D(pool_size=(2, 2)), name= 'mrcnn_{}_maxpool2'.format(net_name))(conv2_output) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_{}_conv3'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn3'.format( net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv4'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn4'.format( net_name))(x) x = KL.Activation('relu')(x) deconv1_output = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation='relu'), name='mrcnn_{}_deconv1'.format(net_name) )(x) x = KL.Concatenate(axis=-1, name='mrcnn_{}_concat1'.format(net_name))([ conv2_output, deconv1_output]) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv5'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn5'.format( net_name))(x) x = KL.Activation('relu')(x) deconv2_output = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation='relu'), name='mrcnn_{}_deconv2'.format(net_name) )(x) x = KL.Concatenate(axis=-1, name='mrcnn_{}_concat2'.format(net_name))([ conv1_output, deconv2_output]) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv6'.format(net_name))(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn6'.format( net_name))(x) x = KL.Activation('relu')(x) x1 = KL.TimeDistributed(KL.Conv2D(num_bins * num_classes, (1, 1), strides=1), name='mrcnn_{}_conv_bins'.format(net_name))(x) x2 = KL.TimeDistributed(KL.Conv2D(num_bins * num_classes, (1, 1), strides=1), name='mrcnn_{}_conv_delta'.format(net_name))(x) x1 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, num_bins]), name= 'mrcnn_{}_bins_reshape'.format(net_name))(x1) x1 = KL.Activation('softmax', name='mrcnn_{}_bins'.format(net_name))(x1) x2 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, num_bins]), name= 'mrcnn_{}_delta_reshape'.format(net_name))(x2) x2 = KL.Activation('sigmoid', name='mrcnn_{}_delta_bins'.format(net_name))( x2) return x1, x2 
print|and|tab|read|printer def tab_printer(log): """ Function to print the logs in a nice tabular format. """ t = Texttable() t.add_rows([['Epoch', log['losses'][-1][0]]]) print(t.draw()) t = Texttable() t.add_rows([['Loss', round(log['losses'][-1][1], 3)]]) print(t.draw()) t = Texttable() t.add_rows([['Modularity', round(log['cluster_quality'][-1][1], 3)]]) print(t.draw()) 
commons|fn|model|auxcond|arch def model_fn_auxcond(hparams, z, x, y, generator, discriminator, mdevice): theta_ph = mdevice.get_theta_ph(hparams) theta_gen_ph = mdevice.get_theta_ph(hparams) x_gen = generator(hparams, y, z, 'gen', train=True, reuse=False) x_sample = generator(hparams, y, z, 'gen', train=False, reuse=True) x_lossy, x_gen_lossy = get_lossy(hparams, mdevice, x, theta_ph, x_gen, theta_gen_ph) _, d_logit, ac_logits = discriminator(hparams, x_lossy, y, 'discrim', train=True, reuse=False) _, d_gen_logit, ac_logits_gen = discriminator(hparams, x_gen_lossy, y, 'discrim', train=True, reuse=True) d_loss, g_loss = get_loss(hparams, d_logit, d_gen_logit, x_lossy, x_gen_lossy, discriminator, ac_logits, ac_logits_gen, y) d_update_op, g_update_op, iter_ph = utils.get_train_ops(hparams, d_loss, g_loss) return (x_lossy, x_sample, theta_ph, theta_gen_ph, d_loss, g_loss, d_update_op, g_update_op, iter_ph) 
avod|mini|preprocessor|preprocess|batch|core|MiniBatchPreprocessor def preprocess(self, indices): """Preprocesses anchor info and saves info to files  Args: indices (int array): sample indices to process. If None, processes all samples """ anchor_strides = self._anchor_strides dataset = self._dataset dataset_utils = self._dataset.kitti_utils classes_name = dataset.classes_name output_dir = self.mini_batch_utils.get_file_path(classes_name, anchor_strides, sample_name=None) os.makedirs(output_dir, exist_ok=True) all_clusters_sizes, _ = dataset.get_cluster_info() anchor_generator = grid_anchor_3d_generator.GridAnchor3dGenerator() all_samples = dataset.sample_list if indices is None: indices = np.arange(len(all_samples)) num_samples = len(indices) for sample_idx in indices: sample_name = all_samples[sample_idx].name img_idx = int(sample_name) if self._check_for_existing(classes_name, anchor_strides, sample_name): print('{} / {}: Sample already preprocessed'.format(sample_idx + 1, num_samples, sample_name)) continue ground_truth_list = obj_utils.read_labels(dataset.label_dir, img_idx) filtered_gt_list = dataset_utils.filter_labels(ground_truth_list) filtered_gt_list = np.asarray(filtered_gt_list) if len(filtered_gt_list) == 0: print('{} / {} No {}s for sample {} (Ground Truth Filter)'. format(sample_idx + 1, num_samples, classes_name, sample_name)) self._save_to_file(classes_name, anchor_strides, sample_name) continue ground_plane = obj_utils.get_road_plane(img_idx, dataset.planes_dir) image = Image.open(dataset.get_rgb_image_path(sample_name)) image_shape = [image.size[1], image.size[0]] vx_grid_2d = dataset_utils.create_sliced_voxel_grid_2d(sample_name, source=dataset.bev_source, image_shape=image_shape) all_anchor_boxes_3d = [] for class_idx in range(len(dataset.classes)): grid_anchor_boxes_3d = anchor_generator.generate(area_3d=self. _area_extents, anchor_3d_sizes=all_clusters_sizes[class_idx ], anchor_stride=self._anchor_strides[class_idx], ground_plane=ground_plane) all_anchor_boxes_3d.extend(grid_anchor_boxes_3d) all_anchor_boxes_3d = np.asarray(all_anchor_boxes_3d) anchors = box_3d_encoder.box_3d_to_anchor(all_anchor_boxes_3d) empty_anchor_filter = anchor_filter.get_empty_anchor_filter_2d(anchors, vx_grid_2d, self._density_threshold) anchors_info = self._calculate_anchors_info(all_anchor_boxes_3d, empty_anchor_filter, filtered_gt_list) anchor_ious = anchors_info[:, (self.mini_batch_utils.col_ious)] valid_iou_indices = np.where(anchor_ious > 0.0)[0] print( '{} / {}:{:>6} anchors, {:>6} iou > 0.0, for {:>3} {}(s) for sample {}' .format(sample_idx + 1, num_samples, len(anchors_info), len( valid_iou_indices), len(filtered_gt_list), classes_name, sample_name)) self._save_to_file(classes_name, anchor_strides, sample_name, anchors_info) 
ConvEncoder|extensions|hved|net|u|layer|op def layer_op(self, images):  def clip(input): output = tf.maximum(input, -50) output = tf.minimum(output, 50) return output layer_instances = [] means = dict() logvars = dict() pooling_params = {'kernel_size': 2, 'stride': 2} list_skip_flow = [{'mu': dict(), 'logvar': dict()} for k in range(len( self.skip_ind))] layer_fc_mod = dict() layer_cnn_mod = dict() for mod in MODALITIES[:4]: layer_cnn_mod[mod] = [] layer_fc_mod[mod] = [] params = self.layers[0] first_conv_layer = ConvolutionalLayer(n_output_chns=params[ 'n_features'], kernel_size=params['kernel_size'], acti_func= 'leakyrelu', with_bn=False, w_initializer=self.initializers['w' ], w_regularizer=self.regularizers['w'], name='%s_%s' % (params ['name'], mod)) layer_instances.append(first_conv_layer) layer_cnn_mod[mod].append(first_conv_layer) for i in range(1, len(self.layers)): params = self.layers[i] res_block = ResBlock(n_output_chns=params['n_features'], kernels=params['kernels'], acti_func='leakyrelu', encoding= True, w_initializer=self.initializers['w'], w_regularizer= self.regularizers['w'], name='%s_%s' % (params['name'], mod)) layer_instances.append(res_block) layer_cnn_mod[mod].append(res_block) if params['downsampling']: downsampler = Pooling(func='MAX', kernel_size=2, stride=2) layer_instances.append(downsampler) layer_cnn_mod[mod].append(downsampler) print(MODALITIES[:4]) for mod in MODALITIES[:4]: flow_mod = images[mod] print(flow_mod) for ind, cnn_mod in enumerate(layer_cnn_mod[mod]): flow_mod = cnn_mod(flow_mod) layer_cnn_mod[mod][ind] = cnn_mod if ind in self.skip_ind: pos = self.skip_ind.index(ind) list_skip_flow[pos]['mu'][mod] = flow_mod[(...), :self. hidden[pos]] list_skip_flow[pos]['logvar'][mod] = clip(flow_mod[(...), self.hidden[pos]:]) output = list_skip_flow self._print(layer_instances) return output 
cropping|replace|seg|values def replace_values_seg(img_array): for k in range(3): img_array[img_array == labels[k]] = k + 1 return img_array 
autogen|module|code|clean|name|doc def clean_module_name(name): if name.startswith('ludwig.api'): name = name.replace('ludwig.api', 'ludwig') return name 
europilot|init|ControllerState|controllerstate def __init__(self): self.state = self.__init_dict() 
prediction|compute|error|mean|UtilsNetwork def compute_mean_prediction_error(data, predicted_data, order): base = np.mean(abs(data) ** order) samples = abs(data - predicted_data) ** order return (np.mean(samples) / base) ** (1.0 / order) 
poly|deepg|normalize def normalize_poly(num_params, lexpr_cst, lexpr_weights, lexpr_dim, uexpr_cst, uexpr_weights, uexpr_dim, means, stds, dataset): if dataset == 'mnist' or dataset == 'fashion': for i in range(len(lexpr_cst)): lexpr_cst[i] = (lexpr_cst[i] - means[0]) / stds[0] uexpr_cst[i] = (uexpr_cst[i] - means[0]) / stds[0] for i in range(len(lexpr_weights)): lexpr_weights[i] /= stds[0] uexpr_weights[i] /= stds[0] else: for i in range(len(lexpr_cst)): lexpr_cst[i] = (lexpr_cst[i] - means[i % 3]) / stds[i % 3] uexpr_cst[i] = (uexpr_cst[i] - means[i % 3]) / stds[i % 3] for i in range(len(lexpr_weights)): lexpr_weights[i] /= stds[i // num_params % 3] uexpr_weights[i] /= stds[i // num_params % 3] 
readfile|setup def readfile(filename): with open(filename, 'r+') as f: return f.read() 
decoder|infer|decoding def decoding_infer(start_id, dec_cell, dec_proj, embedding_matrix, enc_state, memory, batch_size, max_dec_len, mem_lens, max_mem_len, is_attn=True, sampling_method='greedy', topk_size=3, state_size=500, multi_source=False, source_bow=None, copy=False, copy_ind=None, dec_ptr_g_proj=None, dec_ptr_k_proj=None, bow_cond=None, bow_cond_gate_proj=None): """The greedy decoding algorithm, used for inference""" dec_outputs = tf.TensorArray(tf.float32, size=max_dec_len) dec_out_index = tf.TensorArray(tf.int32, size=max_dec_len) start_id = tf.zeros([batch_size], dtype=tf.int32) + start_id dec_state = enc_state  def _dec_loop_fn(i, prev_id, dec_state, dec_outputs, dec_out_index): dec_in = tf.nn.embedding_lookup(embedding_matrix, prev_id) dec_out, dec_state = dec_cell(dec_in, dec_state) dec_outputs = dec_outputs.write(i, dec_out) dec_logits = dec_proj(dec_out) dec_index = tf.argmax(dec_logits, axis=1, output_type=tf.int32) dec_out_index = dec_out_index.write(i, dec_index) return i + 1, dec_index, dec_state, dec_outputs, dec_out_index  def _dec_loop_attn_fn(i, prev_id, dec_state, dec_outputs, dec_out_index): dec_in = tf.nn.embedding_lookup(embedding_matrix, prev_id) query = dec_state[-1].h if multi_source: context, _ = multi_source_attention(query, memory, mem_lens, max_mem_len, state_size) else: context, dist = attention(query, memory, mem_lens, max_mem_len) attn_vec = context + query bow_cond_g = 1.0 if bow_cond_gate_proj is not None: bow_cond_g = bow_cond_gate_proj(query + bow_cond) if bow_cond is not None: dec_in = dec_in + bow_cond_g * bow_cond dec_out, dec_state = dec_cell(dec_in + attn_vec, dec_state) dec_outputs = dec_outputs.write(i, dec_out) dec_logits = dec_proj(dec_out) vocab_dist = tf.nn.softmax(dec_logits) if copy: print('Using copy mechanism in inference') pointers = [] for proj_i in dec_ptr_k_proj: ptr_query = proj_i(dec_out) _, ptr = attention(ptr_query, memory[0], mem_lens[0], max_mem_len[0]) pointers.append(ptr) pointers = tf.stack(pointers) pointers = tf.reduce_mean(tf.transpose(pointers, [1, 0, 2]), 1) g = dec_ptr_g_proj(dec_out) mixed_dist, ptr_dist = _mix_dist(vocab_dist, pointers, copy_ind, g) dec_dist = mixed_dist else: print('Not using copy mechanism in inference') dec_dist = vocab_dist if sampling_method == 'greedy': dec_index = tf.argmax(dec_dist, axis=1, output_type=tf.int32) elif sampling_method == 'topk': dec_prob = tf.nn.softmax(dec_logits) dec_prob_topk, dec_id_topk = tf.nn.top_k(dec_prob, topk_size) dec_prob_topk /= tf.expand_dims(tf.reduce_sum(dec_prob_topk, 1), [1]) sample = tf.multinomial(tf.log(dec_prob_topk), 1) sample = tf.one_hot(tf.reshape(sample, [batch_size]), topk_size) dec_index = tf.reduce_sum(tf.cast(sample, tf.int32) * dec_id_topk, axis=1) elif sampling_method == 'topk_rejection': pass dec_out_index = dec_out_index.write(i, dec_index) return i + 1, dec_index, dec_state, dec_outputs, dec_out_index if is_attn: print('Attention decoding ... ') _dec_loop = _dec_loop_attn_fn else: print('Not using attention ...') _dec_loop = _dec_loop_fn print('Sampling method: %s, topk size: %d' % (sampling_method, topk_size)) loop_len = max_dec_len start_time = 0 finish_time, _, dec_state, dec_outputs, dec_out_index = tf.while_loop(cond =lambda i, _1, _2, _3, _4: tf.less(i, loop_len), body=_dec_loop, loop_vars=(start_time, start_id, dec_state, dec_outputs, dec_out_index) ) dec_outputs = tf.transpose(dec_outputs.stack(), [1, 0, 2]) dec_out_index = tf.transpose(dec_out_index.stack(), [1, 0]) return dec_outputs, dec_out_index 
nmt|utils|compute|attention def _compute_attention(attention_mechanism, cell_output, attention_state, attention_layer): """Computes the attention and alignments for a given attention_mechanism.""" alignments, next_attention_state = attention_mechanism(cell_output, state=attention_state) expanded_alignments = array_ops.expand_dims(alignments, 1) context = math_ops.matmul(expanded_alignments, attention_mechanism.values) context = array_ops.squeeze(context, [1]) if attention_layer is not None: attention = attention_layer(array_ops.concat([cell_output, context], 1) ) else: attention = context return attention, alignments, next_attention_state 
sequence|KMaxPooling|deepctr|layers|call def call(self, inputs): perm = list(range(self.dims)) perm[-1], perm[self.axis] = perm[self.axis], perm[-1] shifted_input = tf.transpose(inputs, perm) top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0] output = tf.transpose(top_k, perm) return output 
init|vgg16|prune|PruneVgg16 def __init__(self, weights_dict, **prune_args): super(PruneVgg16, self).__init__(weights_dict, **prune_args) 
ops|mat2d|params|classification|layer|utils|to def mat2d_to_layer_params(vector_template, mat2d): """Converts a canonical 2D matrix representation back to a vector. Args: vector_template: A Tensor or pair of Tensors shaped like layer parameters. mat2d: A 2D Tensor with the same shape as the value of layer_params_to_mat2d(vector_template). Returns: A Tensor or pair of Tensors with the same coefficients as mat2d and the same shape as vector_template. """ if isinstance(vector_template, (tuple, list)): w_part, b_part = mat2d[:-1], mat2d[-1] return array_ops.reshape(w_part, vector_template[0].shape), b_part else: return array_ops.reshape(mat2d, vector_template.shape) 
child|enas|general|cifar1|GeneralChild|model def _model(self, images, is_training, reuse=False): with tf.variable_scope(self.name, reuse=reuse): layers = [] out_filters = self.out_filters with tf.variable_scope('stem_conv'): w = create_weight('w', [3, 3, 3, out_filters]) x = tf.nn.conv2d(images, w, [1, 1, 1, 1], 'SAME', data_format= self.data_format) x = batch_norm(x, is_training, data_format=self.data_format) layers.append(x) if self.whole_channels: start_idx = 0 else: start_idx = self.num_branches for layer_id in range(self.num_layers): with tf.variable_scope('layer_{0}'.format(layer_id)): if self.fixed_arc is None: x = self._enas_layer(layer_id, layers, start_idx, out_filters, is_training) else: x = self._fixed_layer(layer_id, layers, start_idx, out_filters, is_training) layers.append(x) if layer_id in self.pool_layers: if self.fixed_arc is not None: out_filters *= 2 with tf.variable_scope('pool_at_{0}'.format(layer_id)): pooled_layers = [] for i, layer in enumerate(layers): with tf.variable_scope('from_{0}'.format(i)): x = self._factorized_reduction(layer, out_filters, 2, is_training) pooled_layers.append(x) layers = pooled_layers if self.whole_channels: start_idx += 1 + layer_id else: start_idx += 2 * self.num_branches + layer_id print(layers[-1]) x = global_avg_pool(x, data_format=self.data_format) if is_training: x = tf.nn.dropout(x, self.keep_prob) with tf.variable_scope('fc'): if self.data_format == 'NWHC': inp_c = x.get_shape()[3].value elif self.data_format == 'NCHW': inp_c = x.get_shape()[1].value else: raise ValueError('Unknown data_format {0}'.format(self. data_format)) w = create_weight('w', [inp_c, 10]) x = tf.matmul(x, w) return x 
linear|file|parse|input|data|index|gae def parse_index_file(filename): index = [] for line in open(filename): index.append(int(line.strip())) return index 
posterior|codec|ResNetVAE|append|resnet def posterior_append(message, latents): latents, _ = latents (post_mean, post_stdd), h_rec = rec_net_top(contexts[-1]) post_params = [(post_mean, post_stdd)] for rec_net, latent, context in reversed(list(zip(rec_nets, latents[1:], contexts[:-1]))): previous_latent, (prior_mean, prior_stdd) = latent previous_latent_val = prior_mean + codecs.std_gaussian_centres( prior_prec)[previous_latent] * prior_stdd (post_mean, post_stdd), h_rec = rec_net(h_rec, previous_latent_val, context) post_params.append((post_mean, post_stdd)) for latent, post_param in zip(latents, reversed(post_params)): latent, (prior_mean, prior_stdd) = latent post_mean, post_stdd = post_param append, _ = codecs.substack(codecs.DiagGaussian_GaussianBins( post_mean, post_stdd, prior_mean, prior_stdd, latent_prec, prior_prec), z_view) message = append(message, latent) return message 
get|DropIndependent|measure|noise|amb|shape def get_noise_shape(self): noise_shape = copy.deepcopy(self.batch_dims) noise_shape[3] = 1 return noise_shape 
gym|game|examples|make|pycolab|apprehend def make_game(): """Builds and returns an Apprehend game.""" return ascii_art.ascii_art_to_game(GAME_ART, what_lies_beneath=' ', sprites={'P': PlayerSprite, 'b': BallSprite}, update_schedule=['b', 'P']) 
Inference|utils|SMILESX def Inference(data_name, smiles_list=['CC', 'CCC', 'C=O'], data_units='', k_fold_number=8, augmentation=False, outdir='../data/'): if augmentation: p_dir_temp = 'Augm' else: p_dir_temp = 'Can' input_dir = outdir + 'Main/' + '{}/{}/'.format(data_name, p_dir_temp) save_dir = outdir + 'Inference/' + '{}/{}/'.format(data_name, p_dir_temp) os.makedirs(save_dir, exist_ok=True) print('***SMILES_X for inference starts...***\n\n') np.random.seed(seed=123) seed_list = np.random.randint(int(1000000.0), size=k_fold_number).tolist() print('***Checking the SMILES list for inference***\n') smiles_checked = list() smiles_rejected = list() for ismiles in smiles_list: mol_tmp = Chem.MolFromSmiles(ismiles) if mol_tmp != None: smiles_can = Chem.MolToSmiles(mol_tmp) smiles_checked.append(smiles_can) else: smiles_rejected.append(ismiles) if len(smiles_rejected) > 0: with open(save_dir + 'rejected_smiles.txt', 'w') as f: for ismiles in smiles_rejected: f.write('%s\n' % ismiles) if len(smiles_checked) == 0: print('***Process of inference automatically aborted!***') print( 'The provided SMILES are all incorrect and could not be verified via RDKit.' ) return smiles_x = np.array(smiles_checked) smiles_y = np.array([[np.nan] * len(smiles_checked)]).flatten() print('***Data augmentation.***\n') if augmentation == True: canonical = False rotation = True else: canonical = True rotation = False smiles_x_enum, smiles_x_enum_card, smiles_y_enum = Augmentation(smiles_x, smiles_y, canon=canonical, rotate=rotation) print('Enumerated SMILES: {}\n'.format(smiles_x_enum.shape[0])) print('***Tokenization of SMILES.***\n') smiles_x_enum_tokens = get_tokens(smiles_x_enum) smiles_y_pred_mean_array = np.empty(shape=(0, len(smiles_checked)), dtype='float') for ifold in range(k_fold_number): tokens = get_vocab(input_dir + data_name + '_tokens_set_seed' + str (seed_list[ifold]) + '.txt') vocab_size = len(tokens) tokens, vocab_size = add_extra_tokens(tokens, vocab_size) token_to_int = get_tokentoint(tokens) int_to_token = get_inttotoken(tokens) model = load_model(input_dir + 'LSTMAtt_' + data_name + '_model.best_seed_' + str(seed_list[ifold]) + '.hdf5', custom_objects={'AttentionM': AttentionM()}) if ifold == 0: max_length = model.layers[0].output_shape[-1] print('Full vocabulary: {}\nOf size: {}\n'.format(tokens, vocab_size)) print('Maximum length of tokenized SMILES: {} tokens\n'.format( max_length)) model.compile(loss='mse', optimizer='adam', metrics=[metrics.mae, metrics.mse]) smiles_x_enum_tokens_tointvec = int_vec_encode(tokenized_smiles_list =smiles_x_enum_tokens, max_length=max_length, vocab=tokens) smiles_y_pred = model.predict(smiles_x_enum_tokens_tointvec) smiles_y_pred_mean, _ = mean_median_result(smiles_x_enum_card, smiles_y_pred) smiles_y_pred_mean_array = np.append(smiles_y_pred_mean_array, smiles_y_pred_mean.reshape(1, -1), axis=0) if ifold == k_fold_number - 1: smiles_y_pred_mean_ensemble = np.mean(smiles_y_pred_mean_array, axis=0) smiles_y_pred_sd_ensemble = np.std(smiles_y_pred_mean_array, axis=0 ) pred_from_ens = pd.DataFrame(data=[smiles_x, smiles_y_pred_mean_ensemble, smiles_y_pred_sd_ensemble]).T pred_from_ens.columns = ['SMILES', 'ens_pred_mean', 'ens_pred_sd'] print('***Inference of SMILES property done.***') return pred_from_ens 
rotate|image|src|master|facenet|random def random_rotate_image(image): angle = np.random.uniform(low=-10.0, high=10.0) return misc.imrotate(image, angle, 'bicubic') 
bert|GPT2DoubleHeadsModel|gpt2|pytorch|init|modeling|pretrained def __init__(self, config): super(GPT2DoubleHeadsModel, self).__init__(config) self.transformer = GPT2Model(config) self.lm_head = GPT2LMHead(self.transformer.wte.weight, config) self.multiple_choice_head = GPT2MultipleChoiceHead(config) self.apply(self.init_weights) 
models|init|ElasticNetMethod|attacks def __init__(self, model, back='tf', sess=None): """ Note: the model parameter should be an instance of the cleverhans.model.Model abstraction provided by CleverHans. """ super(ElasticNetMethod, self).__init__(model, back, sess) import tensorflow as tf self.feedable_kwargs = {'y': tf.float32, 'y_target': tf.float32} self.structural_kwargs = ['beta', 'batch_size', 'confidence', 'targeted', 'learning_rate', 'binary_search_steps', 'max_iterations', 'abort_early', 'initial_const', 'clip_min', 'clip_max'] if not isinstance(self.model, Model): self.model = CallableModelWrapper(self.model, 'logits') 
bert|openai|checkpoint|convert|pytorch|pretrained|to def convert_openai_checkpoint_to_pytorch(openai_checkpoint_folder_path, openai_config_file, pytorch_dump_folder_path): if openai_config_file == '': config = OpenAIGPTConfig() else: config = OpenAIGPTConfig(openai_config_file) model = OpenAIGPTModel(config) load_tf_weights_in_openai_gpt(model, openai_checkpoint_folder_path) pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME print('Save PyTorch model to {}'.format(pytorch_weights_dump_path)) torch.save(model.state_dict(), pytorch_weights_dump_path) print('Save configuration file to {}'.format(pytorch_config_dump_path)) with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f: f.write(config.to_json_string()) 
tangents|stax|fn|apply|neural|Dense def apply_fn(params, inputs, **kwargs): W, b = params norm = W_std / np.sqrt(inputs.shape[-1]) return norm * np.dot(inputs, W) + b_std * b 
utils|load|mesh def load_mesh(mesh_path, is_save=False, is_normalized=False, is_flipped=False): with open(mesh_path, 'r') as f: lines = f.readlines() vertices = [] faces = [] for l in lines: l = l.strip() words = l.split(' ') if words[0] == 'v': vertices.append([float(words[1]), float(words[2]), float(words[3])] ) if words[0] == 'f': face_words = [x.split('/')[0] for x in words] faces.append([int(face_words[1]) - 1, int(face_words[2]) - 1, int(face_words[3]) - 1]) vertices = np.array(vertices, dtype=np.float64) if is_flipped: vertices[:, (2)] = -vertices[:, (2)] faces = np.array(faces, dtype=np.int32) if is_normalized: maxs = np.amax(vertices, axis=0) mins = np.amin(vertices, axis=0) diffs = maxs - mins assert diffs.shape[0] == 3 vertices = vertices / np.linalg.norm(diffs) if is_save: np.savetxt(mesh_path.replace('.obj', '_vertices.txt'), X=vertices) return vertices, faces 
colorize|fasterai|VideoColorizer|visualize|frames|raw def _colorize_raw_frames(self, source_path: Path, render_factor: int=None): colorframes_folder = self.colorframes_root / source_path.stem colorframes_folder.mkdir(parents=True, exist_ok=True) self._purge_images(colorframes_folder) bwframes_folder = self.bwframes_root / source_path.stem for img in progress_bar(os.listdir(str(bwframes_folder))): img_path = bwframes_folder / img if os.path.isfile(str(img_path)): color_image = self.vis.get_transformed_image(str(img_path), render_factor=render_factor) color_image.save(str(colorframes_folder / img)) 
gcn|init|GCN|Att|att def __init__(self, sequence_length, target_sequence_length, targets_num_max, num_classes, word_embedding, l2_reg_lambda=0.0, num_hidden=100): rand_base = 0.01 self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name= 'input_x') self.input_target = tf.placeholder(tf.int32, [None, target_sequence_length], name='input_x') self.input_targets_all = tf.placeholder(tf.int32, [None, targets_num_max, target_sequence_length], name='input_x') self.sen_len = tf.placeholder(tf.int32, None, name='sen_len') self.target_len = tf.placeholder(tf.int32, None, name='target_len') with tf.name_scope('targets_all_len'): self.targets_all_len_a = tf.placeholder(tf.int32, [None, targets_num_max], name='targets_all_len') batch_size = tf.shape(self.input_x)[0] self.targets_all_len = [] for i in range(targets_num_max): targets_i_len = tf.slice(self.targets_all_len_a, [0, i], [ batch_size, 1]) self.targets_all_len.append(tf.squeeze(targets_i_len)) self.targets_num = tf.placeholder(tf.int32, None, name='targets_num') self.relate_cross = tf.placeholder(tf.float32, [None, targets_num_max, targets_num_max], name='relate_cross') self.relate_self = tf.placeholder(tf.float32, [None, targets_num_max, targets_num_max], name='relate_self') self.target_which = tf.placeholder(tf.float32, [None, targets_num_max], name='which_position') self.target_position = tf.placeholder(tf.float32, [None, sequence_length], name='target_position') with tf.name_scope('targets_all_position'): self.targets_all_position_a = tf.placeholder(tf.float32, [None, targets_num_max, sequence_length], name='targets_all_position') self.targets_all_position = [] for i in range(targets_num_max): targets_i_len = self.targets_all_position_a[:, (i), :] self.targets_all_position.append(tf.squeeze(targets_i_len)) self.input_y = tf.placeholder(tf.float32, [None, num_classes], name= 'input_y') self.dropout_keep_prob = tf.placeholder(tf.float32, name= 'dropout_keep_prob') l2_loss = tf.constant(0.0) with tf.name_scope('embedding'): self.word_embedding = tf.constant(word_embedding, name='word_embedding' ) with tf.name_scope('embedded_sen'): self.embedded_sen = tf.nn.embedding_lookup(self.word_embedding, self.input_x) self.embedded_sen = tf.cast(self.embedded_sen, tf.float32) self.embedded_sen = tf.nn.dropout(self.embedded_sen, keep_prob=self .dropout_keep_prob) embedding_size = word_embedding.shape[1] print('embedding_size {}'.format(embedding_size)) num_hidden = embedding_size with tf.name_scope('embedding_target'): self.embedded_target = tf.nn.embedding_lookup(self.word_embedding, self.input_target) self.embedded_target = tf.cast(self.embedded_target, tf.float32) self.embedded_target = tf.nn.dropout(self.embedded_target, keep_prob=self.dropout_keep_prob) with tf.name_scope('embedding_targets'): self.embedded_targets_all = list(range(targets_num_max)) for i in range(targets_num_max): self.input_targets_i = self.input_targets_all[:, (i), :] self.embedded_target_i = tf.nn.embedding_lookup(self. word_embedding, self.input_targets_i) self.embedded_target_i = tf.cast(self.embedded_target_i, tf.float32 ) self.embedded_target_i = tf.nn.dropout(self.embedded_target_i, keep_prob=self.dropout_keep_prob) self.embedded_targets_all[i] = self.embedded_target_i with tf.name_scope('Bi-LSTM_sentence'): cell = tf.nn.rnn_cell.LSTMCell self.LSTM_Hiddens_sen = bi_dynamic_rnn(cell, self.embedded_sen, num_hidden, self.sen_len, sequence_length, 'bi-lstm-sentence', 'all', dropout=True, dropout_prob=self.dropout_keep_prob) pool_sen = reduce_mean_with_len(self.LSTM_Hiddens_sen, self.sen_len) with tf.variable_scope('Bi-LSTM_targets') as scope: self.LSTM_targets_all = list(range(targets_num_max)) poor_targets_all = list(range(targets_num_max)) for i in range(targets_num_max): cell = tf.nn.rnn_cell.LSTMCell self.LSTM_targets_all[i] = bi_dynamic_rnn(cell, self. embedded_targets_all[i], num_hidden, self.targets_all_len[i ], target_sequence_length, 'bi-lstm-targets', 'all', dropout=True, dropout_prob=self.dropout_keep_prob) poor_targets_all[i] = reduce_mean_with_len(self. LSTM_targets_all[i], self.targets_all_len[i]) scope.reuse_variables() with tf.variable_scope('Attention-targets_all2sentence') as scope: self.outputs_ts = list(range(targets_num_max)) for i in range(targets_num_max): target_position_i = tf.expand_dims(self.targets_all_position[i], 2) LSTM_Hiddens_sen_p_i = tf.multiply(self.LSTM_Hiddens_sen, target_position_i) att_s_i = bilinear_attention_layer(LSTM_Hiddens_sen_p_i, poor_targets_all[i], self.sen_len, 2 * num_hidden, l2_reg_lambda, random_base=rand_base, layer_id='sen') self.outputs_ts[i] = tf.squeeze(tf.matmul(att_s_i, self. LSTM_Hiddens_sen), axis=1) scope.reuse_variables() with tf.name_scope('targets_gather'): self.targets_concat = tf.concat([tf.expand_dims(i, axis=2) for i in self.outputs_ts], axis=2) with tf.name_scope('GCN_layer1'): W_cross = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_cross') b_cross = tf.Variable(tf.random.uniform([2 * num_hidden], - rand_base, rand_base), name='b_cross') W_self = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_self') b_self = tf.Variable(tf.random.uniform([2 * num_hidden], -rand_base, rand_base), name='b_self') GCN1_cross = WXbA_Relu(self.targets_concat, self.relate_cross, W_cross, b_cross) GCN1_self = WXbA_Relu(self.targets_concat, self.relate_self, W_self, b_self) GCN1_out = GCN1_cross + GCN1_self with tf.name_scope('GCN_layer2'): W_cross = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_cross') b_cross = tf.Variable(tf.random.uniform([2 * num_hidden], - rand_base, rand_base), name='b_cross') W_self = tf.Variable(tf.random.uniform([2 * num_hidden, 2 * num_hidden], -rand_base, rand_base), name='W_self') b_self = tf.Variable(tf.random.uniform([2 * num_hidden], -rand_base, rand_base), name='b_self') GCN2_cross = WXbA_Relu(GCN1_out, self.relate_cross, W_cross, b_cross) GCN2_self = WXbA_Relu(GCN1_out, self.relate_self, W_self, b_self) GCN2_out = GCN2_cross + GCN2_self target_which = tf.expand_dims(self.target_which, 1) self.GCN2_out = tf.multiply(GCN2_out, target_which) self.targets_representation = tf.reduce_sum(self.GCN2_out, 2) with tf.name_scope('output'): W = tf.Variable(tf.random_normal([2 * num_hidden, num_classes])) b = tf.Variable(tf.random_normal([num_classes])) self.scores = tf.nn.xw_plus_b(self.targets_representation, W, b, name='scores') l2_loss += tf.nn.l2_loss(W) l2_loss += tf.nn.l2_loss(b) self.predictions = tf.argmax(self.scores, 1, name='predictions') self.true_y = tf.argmax(self.input_y, 1, name='true_y') self.softmax = tf.nn.softmax(self.scores, name='softmax') with tf.name_scope('loss'): self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=self. scores, labels=self.input_y) self.loss = tf.reduce_mean(self.losses, name='loss' ) + l2_reg_lambda * l2_loss with tf.name_scope('accuracy'): self.correct_pred = tf.equal(self.predictions, self.true_y) self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, 'float'), name='accuracy') print('LOADED Att-GCN!') 
revop|configdataset|utils def configdataset(dataset, dir_main): dataset = dataset.lower() if dataset == 'roxford5k' or dataset == 'rparis6k' or dataset == 'instre': cfg = {'ext': '.jpg', 'qext': '.jpg', 'dir_data': os.path.join( dir_main, dataset)} cfg['gnd_fname'] = os.path.join(cfg['dir_data'], 'gnd_' + dataset + '.mat') print(cfg['gnd_fname']) gt = loadmat(cfg['gnd_fname']) cfg['imlist'] = [str(''.join(im)) for iml in np.squeeze(gt['imlist' ]) for im in iml] cfg['qimlist'] = [str(''.join(im)) for iml in np.squeeze(gt[ 'qimlist']) for im in iml] cfg['gnd'] = gnd_mat2py(gt['gnd']) cfg['n'] = len(cfg['imlist']) cfg['nq'] = len(cfg['qimlist']) elif dataset == 'revisitop1m': cfg = {'ext': '.jpg', 'dir_data': os.path.join(dir_main, dataset)} cfg['imlist_fname'] = os.path.join(cfg['dir_data'], '{}.txt'.format (dataset)) cfg['imlist'] = read_imlist(cfg['imlist_fname']) cfg['n'] = len(cfg['imlist']) else: raise ValueError('Unknown dataset: %s!' % dataset) cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg') cfg['im_fname'] = config_imname cfg['qim_fname'] = config_qimname cfg['dataset'] = dataset return cfg 
pc|point|cloud|draw|util def draw_point_cloud(input_points, canvasSize=500, space=200, diameter=25, xrot=0, yrot=0, zrot=0, switch_xyz=[0, 1, 2], normalize=True): """ Render point cloud to image with alpha channel. Input: points: Nx3 numpy array (+y is up direction) Output: gray image as numpy array of size canvasSizexcanvasSize """ image = np.zeros((canvasSize, canvasSize)) if input_points is None or input_points.shape[0] == 0: return image points = input_points[:, (switch_xyz)] M = euler2mat(zrot, yrot, xrot) points = np.dot(M, points.transpose()).transpose() if normalize: centroid = np.mean(points, axis=0) points -= centroid furthest_distance = np.max(np.sqrt(np.sum(abs(points) ** 2, axis=-1))) points /= furthest_distance radius = (diameter - 1) / 2.0 disk = np.zeros((diameter, diameter)) for i in range(diameter): for j in range(diameter): if (i - radius) * (i - radius) + (j - radius) * (j - radius ) <= radius * radius: disk[i, j] = np.exp((-(i - radius) ** 2 - (j - radius) ** 2 ) / radius ** 2) mask = np.argwhere(disk > 0) dx = mask[:, (0)] dy = mask[:, (1)] dv = disk[disk > 0] zorder = np.argsort(points[:, (2)]) points = points[(zorder), :] points[:, (2)] = (points[:, (2)] - np.min(points[:, (2)])) / np.max( points[:, (2)] - np.min(points[:, (2)])) max_depth = np.max(points[:, (2)]) for i in range(points.shape[0]): j = points.shape[0] - i - 1 x = points[j, 0] y = points[j, 1] xc = canvasSize / 2 + x * space yc = canvasSize / 2 + y * space xc = int(np.round(xc)) yc = int(np.round(yc)) px = dx + xc py = dy + yc image[px, py] = image[px, py] * 0.7 + dv * (max_depth - points[j, 2] ) * 0.3 image = image / np.max(image) return image 
delFixed|main def main(): parser = argparse.ArgumentParser() parser.add_argument('--delete_prop', '-p', help='Deleting proportion') args = parser.parse_args() fp = float(args.delete_prop) npz_num = int(args.npz_num) albumSize_max = 400 hm_path = '../dataset/' locations_path = hm_path + 'Flickr_album/' locations = os.listdir(locations_path) ct = 0 ct_whole = 0 for location in locations: location_path = locations_path + location albums_path = glob.glob(location_path + '/' + 'iter_*.npz') num_winner_list = [] for album_path in albums_path: ct_whole += 1 f = np.load(album_path) album_this = f['albumFea'] true_winner = f['label'] album_id = f['album_id'] album_size, _ = np.shape(album_this) if album_size < 16: continue p = np.round(fp * album_size) diff = np.transpose(np.transpose(album_this) - album_this[:, ( true_winner)]) diff = np.delete(diff, true_winner, axis=1) num_winners, num_deletion, ind_delete = IP(diff, p, albumSize_max) if math.isnan(num_deletion): ct += 1 continue rank_true_winner = get_winner_rank(album_this, ind_delete, true_winner) sys.stdout.write('winner_rank=%d\n' % rank_true_winner) sys.stdout.flush() fail_rate = ct / ct_whole sys.stdout.write('fail_rate=%.4f\n' % fail_rate) sys.stdout.flush() 
envs|gym|game|grid|2Env|PycolabGridWorldsLevel1|pycolab|make|worlds|env def _make_game(self): self._setup() return grid_worlds.make_game(positive_rewards=1, swap_actions=True) 
record|add|utils|hooks|to|thumt def _add_to_record(records, record, max_to_keep): added = None removed = None models = {} for name, score in records: models[name] = score if len(records) < max_to_keep: if record[0] not in models: added = record[0] records.append(record) else: sorted_records = sorted(records, key=lambda x: -x[1]) worst_score = sorted_records[-1][1] current_score = record[1] if current_score >= worst_score: if record[0] not in models: added = record[0] removed = sorted_records[-1][0] records = sorted_records[:-1] + [record] records = sorted(records, key=lambda x: -x[1]) return added, removed, records 
of|get|params|and|prune|computation|for|whole|model|PruneMobileNetForImagenet|mobilenet|imagenet def _get_params_and_computation_of_whole_model(self, weights_dict): all_computation = 0 all_params = 0 for weight_name in weights_dict: if 'kernel' in weight_name: output_size = self._get_output_size_from_layer_name(weight_name) computation = get_computation(weights_dict, weight_name, {}, '', output_size) params = get_parameters(weights_dict, weight_name, {}, '') all_computation += computation all_params += params return all_params, all_computation 
tf|models|attacks|init|CarliniWagnerL def __init__(self, sess, model, confidence, targeted, learning_rate, max_iterations, abort_early, initial_const, largest_const, const_factor, clip_min, clip_max, num_labels, shape): """ Return a tensor that constructs adversarial examples for the given input. Generate uses tf.py_func in order to operate over tensors. :param x: (required) A tensor with the inputs. :param y: (optional) A tensor with the true labels for an untargeted attack. If None (and y_target is None) then use the original labels the classifier assigns. :param y_target: (optional) A tensor with the target labels for a targeted attack. :param confidence: Confidence of adversarial examples: higher produces examples with larger l2 distortion, but more strongly classified as adversarial. :param learning_rate: The learning rate for the attack algorithm. Smaller values produce better results but are slower to converge. :param max_iterations: The maximum number of iterations. Setting this to a larger value will produce lower distortion results. Using only a few iterations requires a larger learning rate, and will produce larger distortion results. :param abort_early: If true, allows early aborts if gradient descent is unable to make progress (i.e., gets stuck in a local minimum). :param initial_const: The initial tradeoff-constant to use to tune the relative importance of size of the pururbation and confidence of classification. A smaller value of this constant gives lower distortion results. :param largest_const: When the tradeoff-constant exceeds this value, the attack terminates. Larger values gives lower distortion results. :param const_factor: How much to increase the tradeoff-constant by on each iteration of the attack if the prior iteration failed. :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value """ self.initial_const = initial_const self.largest_const = largest_const self.const_factor = const_factor self.max_pixel_change = 0.0001 self.pixel_change_fraction = 0.3 self.l2_attack = CarliniWagnerL2(sess, model, 1, confidence, targeted, learning_rate, 1, max_iterations, abort_early, initial_const, clip_min, clip_max, num_labels, shape, extension=0) 
Extension|cpplint|FileInfo def Extension(self): """File extension - text following the final period.""" return self.Split()[2] 
bert|modeling|master|layer|norm def layer_norm(input_tensor, name=None): """Run layer normalization on the last dimension of the tensor.""" return tf.contrib.layers.layer_norm(inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name) 
decode|compute|pose|src|coords|resized def compute_resized_coords(coords, resizeFactor): """ Given the index/coordinates of a cell in some input array (e.g. image), provides the new coordinates if that array was resized by making it resizeFactor times bigger. E.g.: image of size 3x3 is resized to 6x6 (resizeFactor=2), we'd like to know the new coordinates of cell [1,2] -> Function would return [2.5,4.5] :param coords: Coordinates (indices) of a cell in some input array :param resizeFactor: Resize coefficient = shape_dest/shape_source. E.g.: resizeFactor=2 means the destination array is twice as big as the original one :return: Coordinates in an array of size shape_dest=resizeFactor*shape_source, expressing the array indices of the closest point to 'coords' if an image of size shape_source was resized to shape_dest """ return (np.array(coords, dtype=float) + 0.5) * resizeFactor - 0.5 
get|optimizer|problem|R2RProblem|r2r def get_optimizer(self, learning_rate): return tf.keras.optimizers.Adam(learning_rate=learning_rate) 
contributed|facial|clustering|encodings|master|facenet|cluster def cluster_facial_encodings(facial_encodings): """ Cluster facial encodings  Intended to be an optional switch for different clustering algorithms, as of right now only chinese whispers is available.  Input: facial_encodings: (image_path, facial_encoding) dictionary of facial encodings  Output: sorted_clusters: a list of clusters, a cluster being a list of imagepaths, sorted by largest cluster to smallest  """ if len(facial_encodings) <= 1: print( "Number of facial encodings must be greater than one, can't cluster" ) return [] sorted_clusters = _chinese_whispers(facial_encodings.items()) return sorted_clusters 
val|plot|save|samples|utils def save_val_samples(samples_dir, gen_imgs, step, N_samples=2, N_ims=2): row = N_samples col = N_ims titles = ['Generated', 'Original'] fig, axs = plt.subplots(row, col) cnt = 0 for j in range(col): for i in range(row): axs[i, j].imshow(gen_imgs[cnt]) axs[i, j].set_title(titles[j]) axs[i, j].axis('off') cnt += 1 fig.savefig(os.path.join(samples_dir, '%d.png' % step)) plt.close() 
GradientCheckpointedOptimizer|init|optimizer def __init__(self, optimizer_name, lr, hparams, use_tpu=False): if optimizer_name == 'Adam' and use_tpu: optimizer_name = 'TrueAdam' tf.logging.info('Using optimizer %s', optimizer_name) if optimizer_name == 'Adam': self._opt = tf.contrib.opt.LazyAdamOptimizer(lr / 500.0, beta1= hparams.optimizer_adam_beta1, beta2=hparams. optimizer_adam_beta2, epsilon=hparams.optimizer_adam_epsilon) elif optimizer_name == 'Momentum': self._opt = tf.train.MomentumOptimizer(lr, momentum=hparams. optimizer_momentum_momentum, use_nesterov=hparams. optimizer_momentum_nesterov) elif optimizer_name == 'YellowFin': self._opt = yellowfin.YellowFinOptimizer(learning_rate=lr, momentum =hparams.optimizer_momentum_momentum) elif optimizer_name == 'TrueAdam': self._opt = tf.train.AdamOptimizer(lr / 500.0, beta1=hparams. optimizer_adam_beta1, beta2=hparams.optimizer_adam_beta2, epsilon=hparams.optimizer_adam_epsilon) elif optimizer_name == 'Adafactor': self._opt = t2t_opt.AdafactorOptimizer(lr / 500.0) elif optimizer_name == 'Adagrad': self._opt = tf.train.AdagradOptimizer(lr / 500.0) else: self._opt = tf.contrib.layers.OPTIMIZER_CLS_NAMES[optimizer_name](lr) 
tensorflow|rmsnorm|norm|rms def rms_norm(x, eps=1e-08, p=-1.0, bias=False, scope=None): """ Root Mean Square Layer Normalization :param x: input tensor, with shape [batch, ..., dimension] :param eps: epsilon value, default 1e-8 :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled) :param bias: whether use bias term for RMSNorm, disabled by default because RMSNorm doesn't enforce re-centering invariance. :param scope: the variable scope :return: a normalized tensor, with shape as `x` """ with tf.variable_scope(scope or 'rms_norm'): layer_size = x.get_shape().as_list()[-1] scale = tf.get_variable('scale', [layer_size], initializer=tf. ones_initializer()) if bias: offset = tf.get_variable('offset', [layer_size], initializer=tf .zeros_initializer()) else: offset = 0.0 if p < 0.0 or p > 1.0: ms = tf.reduce_mean(x ** 2, -1, keep_dims=True) else: partial_size = int(layer_size * p) partial_x, _ = tf.split(x, [partial_size, layer_size - partial_size], axis=-1) ms = tf.reduce_mean(partial_x ** 2, -1, keep_dims=True) return scale * x * tf.rsqrt(ms + eps) + offset 
house|compute|angle|pitch|utils def compute_pitch_angle(source_coordinates, target_coordinates, radians=True): """Computes pitch (elevation) angle given two xyz coordinates.  Args: source_coordinates: Source triplet with xyz coordinates. target_coordinates: Target triplet with xyz coordinates. radians: Whether to output in radians or degrees.  Returns: Pitch angle. 0 means neutral elevation, angles increase towards positive z coordinates. In radians, pi means looking at the ceiling and -pi means looking at the floor. """ source_x, source_y, source_z = source_coordinates target_x, target_y, target_z = target_coordinates delta_x = target_x - source_x delta_y = target_y - source_y delta_z = target_z - source_z delta_xy = math.sqrt(delta_x ** 2 + delta_y ** 2) pitch = math.atan2(delta_z, delta_xy) if radians: return pitch return math.degrees(pitch) 
features|based|file|convert|examples|data|utils|to def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file): """Convert a set of `InputExample`s to a TFRecord file.""" writer = tf.python_io.TFRecordWriter(output_file) for ex_index, example in enumerate(examples): feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)  def create_int_feature(values): return tf.train.Feature(int64_list=tf.train.Int64List(value= list(values))) features = collections.OrderedDict() features['input_ids'] = create_int_feature(feature.input_ids) features['input_mask'] = create_int_feature(feature.input_mask) features['segment_ids'] = create_int_feature(feature.segment_ids) features['label_ids'] = create_int_feature([feature.label_id]) tf_example = tf.train.Example(features=tf.train.Features(feature= features)) writer.write(tf_example.SerializeToString()) 
CRPM|restore|Net|model def model_restore(self): config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) saver = tf.train.Saver() sess.run(tf.global_variables_initializer()) variables_names = [v.name for v in tf.trainable_variables()] print(variables_names) saver.restore(sess, self.model_path) print('model restored...') return sess 
CIN|get|interaction|deepctr|config|layers def get_config(self): config = {'layer_size': self.layer_size, 'split_half': self.split_half, 'activation': self.activation, 'seed': self.seed} base_config = super(CIN, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
InceptionV3Test|test|v3|inception|nets|testBuildEndPoints def testBuildEndPoints(self): batch_size = 5 height, width = 299, 299 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = inception.inception_v3(inputs, num_classes) self.assertTrue('Logits' in end_points) logits = end_points['Logits'] self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue('AuxLogits' in end_points) aux_logits = end_points['AuxLogits'] self.assertListEqual(aux_logits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue('Mixed_7c' in end_points) pre_pool = end_points['Mixed_7c'] self.assertListEqual(pre_pool.get_shape().as_list(), [batch_size, 8, 8, 2048]) self.assertTrue('PreLogits' in end_points) pre_logits = end_points['PreLogits'] self.assertListEqual(pre_logits.get_shape().as_list(), [batch_size, 1, 1, 2048]) 
pos|gym|get|extensions|ee|vel|bullet|ori|BulletManipulator|manipulator def get_ee_pos_ori_vel(self): ee_state = self.sim.getLinkState(self.info.robot_id, self.info. ee_link_id, computeLinkVelocity=1) return np.array(ee_state[0]), np.array(ee_state[1]), np.array(ee_state[6] ), np.array(ee_state[7]) 
darkflow|walk|walker|utils|weights|loader def walk(self, size): if self.eof: return None end_point = self.offset + 4 * size assert end_point <= self.size, 'Over-read {}'.format(self.path) float32_1D_array = np.memmap(self.path, shape=(), mode='r', offset=self .offset, dtype='({})float32,'.format(size)) self.offset = end_point if end_point == self.size: self.eof = True return float32_1D_array 
init|LorentzVector|utilFunctions def __init__(self, *args): if len(args) > 0: self.x = args[0] self.y = args[1] self.z = args[2] self.t = args[3] 
bert|field|csqa|extract|select def select_field(features, field): return [[choice[field] for choice in feature.choices_features] for feature in features] 
cvusa|train def train(start_epoch=0): input_data = InputData(polar) if polar: sat_x = tf.placeholder(tf.float32, [None, 112, 616, 3], name='sat_x') else: sat_x = tf.placeholder(tf.float32, [None, 256, 256, 3], name='sat_x') grd_x = tf.placeholder(tf.float32, [None, 112, 616, 3], name='grd_x') keep_prob = tf.placeholder(tf.float32) learning_rate = tf.placeholder(tf.float32) dimension = int(network_type[-1]) sat_global, grd_global = SAFA(sat_x, grd_x, keep_prob, dimension, is_training) out_channel = sat_global.get_shape().as_list()[-1] sat_global_descriptor = np.zeros([input_data.get_test_dataset_size(), out_channel]) grd_global_descriptor = np.zeros([input_data.get_test_dataset_size(), out_channel]) loss = compute_loss(sat_global, grd_global) global_step = tf.Variable(0, trainable=False) with tf.device('/gpu:0'): with tf.name_scope('train'): train_step = tf.train.AdamOptimizer(learning_rate, 0.9, 0.999 ).minimize(loss, global_step=global_step) print('setting saver...') saver = tf.train.Saver(tf.global_variables(), max_to_keep=None) print('setting saver done...') print('run model...') config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True) config.gpu_options.allow_growth = True config.gpu_options.per_process_gpu_memory_fraction = 0.9 print('open session ...') with tf.Session(config=config) as sess: print('initialize...') sess.run(tf.global_variables_initializer()) print('load model...') if start_epoch == 0: load_model_path = '../Model/Initialize/initial_model.ckpt' saver.restore(sess, load_model_path) else: load_model_path = ('./Model/' + data_type + '/' + network_type + '/polar_' + str(polar) + '/' + str(start_epoch - 1) + '/model.ckpt') saver.restore(sess, load_model_path) print('   Model loaded from: %s' % load_model_path) print('load model...FINISHED') for epoch in range(start_epoch, start_epoch + number_of_epoch): iter = 0 while True: batch_sat, batch_grd = input_data.next_pair_batch(batch_size) if batch_sat is None: break global_step_val = tf.train.global_step(sess, global_step) feed_dict = {sat_x: batch_sat, grd_x: batch_grd, learning_rate: learning_rate_val, keep_prob: keep_prob_val} if iter % 20 == 0: _, loss_val = sess.run([train_step, loss], feed_dict= feed_dict) print('global %d, epoch %d, iter %d: loss : %.4f ' % ( global_step_val, epoch, iter, loss_val)) else: sess.run(train_step, feed_dict=feed_dict) iter += 1 model_dir = ('../Model/' + data_type + '/' + network_type + '/polar_' + str(polar) + '/' + str(epoch) + '/') if not os.path.exists(model_dir): os.makedirs(model_dir) save_path = saver.save(sess, model_dir + 'model.ckpt') print('Model saved in file: %s' % save_path) print('validate...') print('   compute global descriptors') input_data.reset_scan() val_i = 0 while True: batch_sat, batch_grd = input_data.next_batch_scan(batch_size) if batch_sat is None: break feed_dict = {sat_x: batch_sat, grd_x: batch_grd, keep_prob: 1.0 } sat_global_val, grd_global_val = sess.run([sat_global, grd_global], feed_dict=feed_dict) sat_global_descriptor[val_i:val_i + sat_global_val.shape[0], : ] = sat_global_val grd_global_descriptor[val_i:val_i + grd_global_val.shape[0], : ] = grd_global_val val_i += sat_global_val.shape[0] print('   compute accuracy') val_accuracy = validate(grd_global_descriptor, sat_global_descriptor) print('   %d: top-1 accuracy = %.1f' % (epoch, val_accuracy * 100.0)) file = '../Result/' + data_type + '/' + str(network_type ) + '_polar_' + str(polar) + '_accuracy.txt' with open(file, 'a') as file: file.write(str(epoch) + ' ' + str(iter) + ' : ' + str( val_accuracy) + '\n') 
HVEDApplication|extensions|U|hved|and|application|u|connect|network|data def connect_data_and_network(self, outputs_collector=None, gradients_collector=None):  def switch_sampler(for_training): with tf.name_scope('train' if for_training else 'validation'): sampler = self.get_sampler()[0][0 if for_training else -1] return sampler.pop_batch_op() self.var = tf.placeholder_with_default(0, [], 'var') self.choices = tf.placeholder_with_default([True, True, True, True], [4 ], 'choices') if self.is_training: self.lr = tf.placeholder_with_default(self.action_param.lr, [], 'learning_rate') if self.action_param.validation_every_n > 0: data_dict = tf.cond(tf.logical_not(self.is_validation), lambda : switch_sampler(for_training=True), lambda : switch_sampler( for_training=False)) else: data_dict = switch_sampler(for_training=True) image = tf.cast(data_dict['image'], tf.float32) image_unstack = tf.unstack(image, axis=-1) print('hellllo') print(image) net_img, post_param = self.net({MODALITIES_img[k]: tf.expand_dims( image_unstack[k], -1) for k in range(4)}, self.choices, is_training=self.is_training) net_seg = net_img['seg'] net_img = tf.concat([net_img[mod] for mod in MODALITIES_img], axis=-1) with tf.name_scope('Optimiser'): optimiser_class = OptimiserFactory.create(name=self. action_param.optimiser) self.optimiser = optimiser_class.get_instance(learning_rate=self.lr ) print('seeg') gt = data_dict['label'] cross = LossFunction(n_class=4, loss_type='CrossEntropy') dice = LossFunction(n_class=4, loss_type='Dice', softmax=True) gt = data_dict['label'] loss_cross = cross(prediction=net_seg, ground_truth=gt, weight_map=None ) loss_dice = dice(prediction=net_seg, ground_truth=gt) loss_seg = loss_cross + loss_dice print('output') print(net_img) loss_reconstruction = tf.reduce_mean(tf.square(net_img - image)) print('output_seg') print(net_seg) print('gt') print(gt) sum_inter_KLD = 0.0 sum_prior_KLD = 0.0 nb_skip = len(post_param) for k in range(nb_skip): inter_KLD, prior_KLD = compute_KLD(post_param[k]['mu'], post_param[k]['logvar'], self.choices) sum_inter_KLD += inter_KLD sum_prior_KLD += prior_KLD KLD = 1 / nb_skip * sum_inter_KLD + 1 / nb_skip * sum_prior_KLD data_loss = loss_seg + 0.1 * KLD + 0.1 * loss_reconstruction reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) reg_loss = tf.reduce_mean([tf.reduce_mean(reg_loss) for reg_loss in reg_losses]) loss = data_loss + reg_loss grads = self.optimiser.compute_gradients(loss) gradients_collector.add_to_collection([grads]) outputs_collector.add_to_collection(var=loss, name='loss', average_over_devices=False, collection=CONSOLE) outputs_collector.add_to_collection(var=loss, name='loss', average_over_devices=True, summary_type='scalar', collection= TF_SUMMARIES) outputs_collector.add_to_collection(var=KLD, name='KLD', average_over_devices=False, collection=CONSOLE) outputs_collector.add_to_collection(var=loss_reconstruction, name= 'loss_reconstruction', average_over_devices=False, collection= CONSOLE) outputs_collector.add_to_collection(var=self.choices, name= 'choices', average_over_devices=False, collection=CONSOLE) outputs_collector.add_to_collection(var=self.lr, name='lr', average_over_devices=False, collection=CONSOLE) elif self.is_inference: data_dict = switch_sampler(for_training=False) image = tf.cast(data_dict['image'], tf.float32) image = tf.unstack(image, axis=-1) choices = self.segmentation_param.choices print(self.segmentation_param.choices) choices = [str2bool(k) for k in choices] print('salut') print(choices) output_mod = self.segmentation_param.output_mod[0] post_process_layer = PostProcessingLayer('ARGMAX', num_classes=4) if output_mod == 'seg': net_img, _ = self.net({MODALITIES_img[k]: tf.expand_dims(image[ k], -1) for k in range(4)}, choices, is_training=True, is_inference=False) net_out = post_process_layer(net_img['seg']) else: net_img, _ = self.net({MODALITIES_img[k]: tf.expand_dims(image[ k], -1) for k in range(4)}, choices, is_training=True, is_inference=True) net_out = net_img[output_mod] outputs_collector.add_to_collection(var=net_out, name='window', average_over_devices=False, collection=NETWORK_OUTPUT) outputs_collector.add_to_collection(var=data_dict['image_location'], name='location', average_over_devices=False, collection= NETWORK_OUTPUT) self.initialise_aggregator() 
gcn|models|loss|master|MLP|text def _loss(self): for var in self.layers[0].vars.values(): self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var) self.loss += masked_softmax_cross_entropy(self.outputs, self. placeholders['labels'], self.placeholders['labels_mask']) 
car|clustering|main def main(): """ Calculates clusters for each class  Returns: all_clusters: list of clusters for each class all_std_devs: list of cluster standard deviations for each class """ dataset = DatasetBuilder.build_kitti_dataset(DatasetBuilder.KITTI_TRAIN) sample_list = dataset.load_sample_names(dataset.cluster_split) all_dims = [] num_samples = len(sample_list) for sample_idx in range(num_samples): sys.stdout.write('\rClustering labels {} / {}'.format(sample_idx + 1, num_samples)) sys.stdout.flush() sample_name = sample_list[sample_idx] img_idx = int(sample_name) obj_labels = obj_utils.read_labels(dataset.label_dir, img_idx) filtered_lwh = LabelClusterUtils._filter_labels_by_class(obj_labels, dataset.classes) if filtered_lwh[0]: all_dims.extend(filtered_lwh[0]) all_dims = np.array(all_dims) print('\nFinished reading labels, clustering data...\n') np.set_printoptions(formatter={'float': lambda x: '{0:0.3f}'.format(x)}) k_means = KMeans(n_clusters=1, random_state=0).fit(all_dims) cluster_centre = k_means.cluster_centers_[0] std_dev = np.std(all_dims, axis=0) two_sigma_length_lo = cluster_centre[0] - 2 * std_dev[0] three_sigma_length_lo = cluster_centre[0] - 3 * std_dev[0] small_mask_2 = all_dims[:, (0)] < two_sigma_length_lo small_dims_2 = all_dims[small_mask_2] small_mask_3 = all_dims[:, (0)] < three_sigma_length_lo small_dims_3 = all_dims[small_mask_3] small_k_means_2 = KMeans(n_clusters=1, random_state=0).fit(small_dims_2) small_k_means_3 = KMeans(n_clusters=1, random_state=0).fit(small_dims_3) small_std_dev_2 = np.std(small_dims_2, axis=0) small_std_dev_3 = np.std(small_dims_3, axis=0) print('small_k_means_2:', small_k_means_2.cluster_centers_) print('small_k_means_3:', small_k_means_3.cluster_centers_) print('small_std_dev_2:', small_std_dev_2) print('small_std_dev_3:', small_std_dev_3) two_sigma_length_hi = cluster_centre[0] + 2 * std_dev[0] three_sigma_length_hi = cluster_centre[0] + 3 * std_dev[0] large_mask_2 = all_dims[:, (0)] > two_sigma_length_hi large_dims_2 = all_dims[large_mask_2] large_mask_3 = all_dims[:, (0)] > three_sigma_length_hi large_dims_3 = all_dims[large_mask_3] large_k_means_2 = KMeans(n_clusters=1, random_state=0).fit(large_dims_2) large_k_means_3 = KMeans(n_clusters=1, random_state=0).fit(large_dims_3) large_std_dev_2 = np.std(large_dims_2, axis=0) large_std_dev_3 = np.std(large_dims_3, axis=0) print('large_k_means_2:', large_k_means_2.cluster_centers_) print('large_k_means_3:', large_k_means_3.cluster_centers_) print('large_std_dev_2:', large_std_dev_2) print('large_std_dev_3:', large_std_dev_3) 
regression|standardize|data|misc|loader def standardize(data_train, *args): """ Standardize a dataset to have zero mean and unit standard deviation.  :param data_train: 2-D Numpy array. Training data. :param data_test: 2-D Numpy array. Test data.  :return: (train_set, test_set, mean, std), The standardized dataset and their mean and standard deviation before processing. """ std = np.std(data_train, 0, keepdims=True) std[std == 0] = 1 mean = np.mean(data_train, 0, keepdims=True) data_train_standardized = (data_train - mean) / std output = [data_train_standardized] for d in args: dd = (d - mean) / std output.append(dd) output.append(mean) output.append(std) return output 
td|alg|not|Main|step|or|train|main|run|condition def step_condition(): return t_n < self.p.batch_size 
test|testBuildClassificationNetwork|InceptionV2Test|inception|nets|v2 def testBuildClassificationNetwork(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) logits, end_points = inception.inception_v2(inputs, num_classes) self.assertTrue(logits.op.name.startswith( 'InceptionV2/Logits/SpatialSqueeze')) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue('Predictions' in end_points) self.assertListEqual(end_points['Predictions'].get_shape().as_list(), [ batch_size, num_classes]) 
texar|get|models|fn|input|Seq2seqBase|base|seq2seq def get_input_fn(self, mode, hparams=None): """Creates an input function `input_fn` that provides input data for the model in an :tf_main:`Estimator <estimator/Estimator>`. See, e.g., :tf_main:`tf.estimator.train_and_evaluate <estimator/train_and_evaluate>`.  Args: mode: One of members in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`. hparams: A `dict` or an :class:`~texar.HParams` instance containing the hyperparameters of :class:`~texar.data.PairedTextData`. See :meth:`~texar.data.PairedTextData.default_hparams` for the the structure and default values of the hyperparameters.  Returns: An input function that returns a tuple `(features, labels)` when called. `features` contains data fields that are related to source text, and `labels` contains data fields related to target text. See :class:`~texar.data.PairedTextData` for all data fields. """  def _input_fn(): data = PairedTextData(hparams) iterator = data.dataset.make_initializable_iterator() tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator. initializer) batch = iterator.get_next() features, labels = {}, {} for key, value in batch.items(): if key.startswith('source_'): features[key] = value else: labels[key] = value return features, labels return _input_fn 
texar|DataBase|hparams|base|data @property def hparams(self): """A :class:`~texar.HParams` instance of the data hyperparameters. """ return self._hparams 
texar|multi|MultiAlignedData|aligned|name|data @property def data_name(self, name_or_id): """The name of the data tensor of scalar dataset by its name or id.. If the dataset is not a scalar data, returns `None`. """ i = self._maybe_name_to_id(name_or_id) if not _is_scalar_data(self._hparams.datasets[i]['data_type']): return None name = dsutils._connect_name(self._data_spec.name_prefix[i], self. _data_spec.decoder[i].data_tensor_name) return name 
checker|avod|test|3d|FormatCheckerTest|check|core|format|box def test_check_box_3d_format(self): test_var = [0, 0, 0, 0, 0, 0, 0] np.testing.assert_raises(TypeError, fc.check_box_3d_format, test_var) test_var = np.ones([1, 5]) np.testing.assert_raises(TypeError, fc.check_box_3d_format, test_var) test_var = np.ones([5, 6]) np.testing.assert_raises(TypeError, fc.check_box_3d_format, test_var) test_var = np.ones([1, 7]) fc.check_box_3d_format(test_var) test_var = np.ones([10, 7]) fc.check_box_3d_format(test_var) test_var = tf.ones([5, 7]) fc.check_box_3d_format(test_var) test_var = tf.ones([5, 3]) np.testing.assert_raises(TypeError, fc.check_box_3d_format, test_var) 
small|walk|helper|RandomWalker|calculation def small_walk(self, start_node): """ Generate a node sequence from a start node. """ walk = [start_node] while len(walk) != self.length: end_point = walk[-1] neighbors = nx.neighbors(self.graph, end_point) if len(neighbors) > 0: walk = walk + random.sample(neighbors, 1) else: break return walk 
eval|get|test|ckpt|actor|manager def _get_ckpt_manager(ckpt_dir, **kwargs): checkpoint_prefix = os.path.join(ckpt_dir, 'model.ckpt') ckpt = tf.train.Checkpoint(**kwargs) manager = tf.train.CheckpointManager(ckpt, checkpoint_prefix, max_to_keep=5, keep_checkpoint_every_n_hours=6) return manager 
official|io|file|data|register|utils|GarbageCollector def register(self, filepath): self.temp_buffers.append(filepath) 
gen|get|saliency|voc|images def get_images(data_dir): images = [] for f in glob.iglob(data_dir.rstrip('\\/') + '/*.jpg'): images.append(f) return images 
xlnet|labels|has|estimator|tpu|master|InputsStructureRecorder|InputPipeline def has_labels(self): return 'labels' in self._feature_structure 
xlnet|pieces|encode|master|utils|prepro def encode_pieces(sp_model, text, return_unicode=True, sample=False): if six.PY2 and isinstance(text, unicode): text = text.encode('utf-8') if not sample: pieces = sp_model.EncodeAsPieces(text) else: pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1) new_pieces = [] for piece in pieces: if len(piece) > 1 and piece[-1] == ',' and piece[-2].isdigit(): cur_pieces = sp_model.EncodeAsPieces(piece[:-1].replace( SPIECE_UNDERLINE, '')) if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0 ] == SPIECE_UNDERLINE: if len(cur_pieces[0]) == 1: cur_pieces = cur_pieces[1:] else: cur_pieces[0] = cur_pieces[0][1:] cur_pieces.append(piece[-1]) new_pieces.extend(cur_pieces) else: new_pieces.append(piece) if six.PY2 and return_unicode: ret_pieces = [] for piece in new_pieces: if isinstance(piece, str): piece = piece.decode('utf-8') ret_pieces.append(piece) new_pieces = ret_pieces return new_pieces 
sparse|gen|lth|plot|resnet2|oneshot def plot_resnet20_sparse_oneshot_lth(): common.lth_plot(network=common.RESNET20, is_iterative=False, prune_method=common.UNSTRUCTURED, min_max_y=(-0.03, 0.01)) 
xlnet|estimator|tpu|master|is|iterable def _is_iterable(obj): """A Python 2 and 3 compatible util to check whether `obj` is iterable.""" try: iter(obj) return True except TypeError: return False 
commons|theta|measure|sample|KeepPatch def sample_theta(self, hparams): return 1 - self.patch_mask(hparams) 
deepctr|core|PredictionLayer|layers|call def call(self, inputs, **kwargs): x = inputs if self.use_bias: x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC') if self.task == 'binary': x = tf.sigmoid(x) output = tf.reshape(x, (-1, 1)) return output 
test|vs|MonteCarloTest|monte|carlo|analytic|sample|nngp @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '[batch_size={}, device_count={} store_on_device={} ]'.format( batch_size, device_count, store_on_device), 'batch_size': batch_size, 'device_count': device_count, 'store_on_device': store_on_device} for batch_size in BATCH_SIZES for device_count in DEVICE_COUNTS for store_on_device in STORE_ON_DEVICE)) def test_sample_vs_analytic_nngp(self, batch_size, device_count, store_on_device): utils.stub_out_pmap(batch, device_count) x1, x2, init_fn, apply_fn, stax_kernel_fn, key = _get_inputs_and_model( 1024, 256, xla_bridge.get_backend().platform == 'tpu') sample = monte_carlo.monte_carlo_kernel_fn(init_fn, apply_fn, key, 200, batch_size, device_count, store_on_device) ker_empirical = sample(x1, x2, 'nngp') ker_analytic = stax_kernel_fn(x1, x2, 'nngp') utils.assert_close_matrices(self, ker_analytic, ker_empirical, 0.02) 
process|xlnet|feature|create|FeatureWriter|float|master|squad|run def create_float_feature(values): f = tf.train.Feature(float_list=tf.train.FloatList(value=list(values))) return f 
ensemble|elpips|flipY|apply def flipY(X): return X[:, ::-1, :, :] 
plato|component|nlg|NLG|agent|init def __init__(self): """ Initialize the internal structures of the nlg """ super(NLG, self).__init__() 
pos|attr|bim|load|metrics|neg def load_pos_neg_attr(model_pos, data_pos, model_neg, data_neg): """Load two sets of attributions from model_pos-data_pos and model_neg-data_neg.  Filter and only return attributions of correctly classified inputs.  Args: model_pos: the model name for which objects have positive attributions. data_pos: the data name for which objects have positive attributions. model_neg: the model name for which objects have negative attributions. data_neg: the data name for which objects have negative attributions.  Returns: arrays of attributions corresponding to model_pos-data_pos and model_neg-data_neg pairs. """ dir_pos = os.path.join(BASE_DIR, ATTR_DIR[FLAGS.scratch], model_pos + '-' + data_pos) pool = multiprocessing.Pool(FLAGS.num_threads) attr_pos = np.array(pool.map(lambda f: np.load(tf.gfile.GFile(os.path. join(dir_pos, f), 'rb')), sorted(tf.gfile.ListDirectory(dir_pos)))) dir_neg = os.path.join(BASE_DIR, ATTR_DIR[FLAGS.scratch], model_neg + '-' + data_neg) attr_neg = np.array(pool.map(lambda f: np.load(tf.gfile.GFile(os.path. join(dir_neg, f), 'rb')), sorted(tf.gfile.ListDirectory(dir_neg)))) if FLAGS.scratch: attr_pos = attr_pos[:FLAGS.num_imgs] attr_neg = attr_neg[:FLAGS.num_imgs] return attr_pos, attr_neg 
tests|gym|update|test|things|pycolab|post def post_update(engine, character, thing_to_do): """Make the test entity for `character` do something after updating itself.  Assuming the pycolab game `engine` has the character `character` handled by one of the `Sprite`s or `Drape`s handled in this module, then on the next game iteration, that entity will execute `thing_to_do` after performing all of its own update tasks.  This code injection works only for the next game iteration, after which it is cleared.  Args: engine: a pycolab game. character: a character handled in the game by an instance of one of the `Sprite`s or `Drape`s defined in this module. thing_to_do: a callable that takes all of the arguments to the `Sprite` or `Drape` `update` method. """ engine.the_plot.setdefault('test_post_update', {})[character] = thing_to_do 
space|test|manager|TestMisc|utils|ac def test_manager_ac_space(self): params = dict(ob_space=None, relative_goals=False, use_fingerprints= False, fingerprint_dim=1) rel_params = params.copy() rel_params.update({'relative_goals': True}) ac_space = get_manager_ac_space(env_name='AntMaze', **params) test_space(ac_space, expected_min=np.array([-10, -10, -0.5, -1, -1, -1, -1, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3]), expected_max= np.array([10, 10, 0.5, 1, 1, 1, 1, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3]), expected_size=15) ac_space = get_manager_ac_space(env_name='AntGather', **params) test_space(ac_space, expected_min=np.array([-10, -10, -0.5, -1, -1, -1, -1, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3]), expected_max= np.array([10, 10, 0.5, 1, 1, 1, 1, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3]), expected_size=15) ac_space = get_manager_ac_space(env_name='AntPush', **params) test_space(ac_space, expected_min=np.array([-10, -10, -0.5, -1, -1, -1, -1, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3]), expected_max= np.array([10, 10, 0.5, 1, 1, 1, 1, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3]), expected_size=15) ac_space = get_manager_ac_space(env_name='AntFall', **params) test_space(ac_space, expected_min=np.array([-10, -10, -0.5, -1, -1, -1, -1, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3]), expected_max= np.array([10, 10, 0.5, 1, 1, 1, 1, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3, 0.5, 0.3]), expected_size=15) ac_space = get_manager_ac_space(env_name='UR5', **params) test_space(ac_space, expected_min=np.array([-2 * np.pi, -2 * np.pi, -2 * np.pi, -4, -4, -4]), expected_max=np.array([2 * np.pi, 2 * np.pi, 2 * np.pi, 4, 4, 4]), expected_size=6) ac_space = get_manager_ac_space(env_name='Pendulum', **params) test_space(ac_space, expected_min=np.array([-np.pi, -15]), expected_max =np.array([np.pi, 15]), expected_size=2) ac_space = get_manager_ac_space(env_name='ring0', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(1)]), expected_max=np.array([(1) for _ in range(1)]), expected_size=1) ac_space = get_manager_ac_space(env_name='ring0', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(1)]), expected_max=np.array([(0.5) for _ in range(1)]), expected_size=1) ac_space = get_manager_ac_space(env_name='ring1', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(1)]), expected_max=np.array([(1) for _ in range(1)]), expected_size=1) ac_space = get_manager_ac_space(env_name='ring1', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(1)]), expected_max=np.array([(0.5) for _ in range(1)]), expected_size=1) ac_space = get_manager_ac_space(env_name='merge0', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(5)]), expected_max=np.array([(1) for _ in range(5)]), expected_size=5) ac_space = get_manager_ac_space(env_name='merge0', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(5)]), expected_max=np.array([(0.5) for _ in range(5)]), expected_size=5) ac_space = get_manager_ac_space(env_name='merge1', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(13)]), expected_max=np.array([(1) for _ in range(13)]), expected_size=13) ac_space = get_manager_ac_space(env_name='merge1', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(13)]), expected_max=np.array([(0.5) for _ in range(13)]), expected_size=13) ac_space = get_manager_ac_space(env_name='merge2', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(17)]), expected_max=np.array([(1) for _ in range(17)]), expected_size=17) ac_space = get_manager_ac_space(env_name='merge2', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(17)]), expected_max=np.array([(0.5) for _ in range(17)]), expected_size=17) ac_space = get_manager_ac_space(env_name='figureeight0', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(1)]), expected_max=np.array([(1) for _ in range(1)]), expected_size=1) ac_space = get_manager_ac_space(env_name='figureeight0', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(1)]), expected_max=np.array([(0.5) for _ in range(1)]), expected_size=1) ac_space = get_manager_ac_space(env_name='figureeight1', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(7)]), expected_max=np.array([(1) for _ in range(7)]), expected_size=7) ac_space = get_manager_ac_space(env_name='figureeight1', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(7)]), expected_max=np.array([(0.5) for _ in range(7)]), expected_size=7) ac_space = get_manager_ac_space(env_name='figureeight2', **params) test_space(ac_space, expected_min=np.array([(0) for _ in range(14)]), expected_max=np.array([(1) for _ in range(14)]), expected_size=14) ac_space = get_manager_ac_space(env_name='figureeight2', **rel_params) test_space(ac_space, expected_min=np.array([(-0.5) for _ in range(14)]), expected_max=np.array([(0.5) for _ in range(14)]), expected_size=14) ac_space = get_manager_ac_space(env_name='grid0', **params) del ac_space ac_space = get_manager_ac_space(env_name='grid1', **params) del ac_space ac_space = get_manager_ac_space(env_name='bottleneck0', **params) del ac_space ac_space = get_manager_ac_space(env_name='bottleneck1', **params) del ac_space ac_space = get_manager_ac_space(env_name='bottleneck2', **params) del ac_space 
normalize|space|xlnet|fix|white|master|squad|answer|utils def white_space_fix(text): return ' '.join(text.split()) 
prefab|gym|sprites|east|parts|pycolab|MazeWalker def _east(self, board, the_plot): """Try moving one cell rightward. Returns `None` on success.""" return self._move(board, the_plot, self._EAST) 
MultiStepHook|end|utils|hooks|thumt def end(self, session): self._hook.end(session) 
fn|ae|speller|model|embedding|text def embedding_fn(ids): if hparams.embedding_size != 0: target_embedding = tf.get_variable('target_embedding', [hparams. target_vocab_size, hparams.embedding_size], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) return tf.nn.embedding_lookup(target_embedding, ids) else: return tf.one_hot(ids, hparams.target_vocab_size) 
gan|cifar|OptimizedResBlockDisc1|resnet def OptimizedResBlockDisc1(inputs): conv_1 = functools.partial(lib.ops.conv2d.Conv2D, input_dim=3, output_dim=DIM_D) conv_2 = functools.partial(ConvMeanPool, input_dim=DIM_D, output_dim=DIM_D) conv_shortcut = MeanPoolConv shortcut = conv_shortcut('Discriminator.1.Shortcut', input_dim=3, output_dim=DIM_D, filter_size=1, he_init=False, biases=True, inputs =inputs) output = inputs output = conv_1('Discriminator.1.Conv1', filter_size=3, inputs=output) output = nonlinearity(output) output = conv_2('Discriminator.1.Conv2', filter_size=3, inputs=output) return shortcut + output 
codecs|mixture|buckets|craystack|create|logistic def _create_logistic_mixture_buckets(means, log_scales, logit_probs, coding_prec, bin_prec, bin_lb, bin_ub): inv_stdv = np.exp(-log_scales) buckets = np.linspace(bin_lb, bin_ub, (1 << bin_prec) + 1) buckets = np.broadcast_to(buckets, means.shape + ((1 << bin_prec) + 1,)) cdfs = inv_stdv[..., np.newaxis] * (buckets - means[..., np.newaxis]) cdfs[..., 0] = -np.inf cdfs[..., -1] = np.inf cdfs = sigmoid(cdfs) prob_cpts = cdfs[(...), 1:] - cdfs[(...), :-1] mixture_probs = util.softmax(logit_probs, axis=1) probs = np.sum(prob_cpts * mixture_probs[..., np.newaxis], axis=1) return _cumulative_buckets_from_probs(probs, coding_prec) 
generate|anchors|main def main(): """ Visualization of 3D grid anchor generation, showing 2D projections in BEV and image space, and a 3D display of the anchors """ dataset_config = DatasetBuilder.copy_config(DatasetBuilder.KITTI_TRAIN) dataset_config.num_clusters[0] = 1 dataset = DatasetBuilder.build_kitti_dataset(dataset_config) label_cluster_utils = LabelClusterUtils(dataset) clusters, _ = label_cluster_utils.get_clusters() img_idx = 1 fake_clusters = np.array([[4, 2, 3]]) fake_anchor_stride = [5.0, 5.0] ground_plane = [0, -1, 0, 1.72] anchor_3d_generator = grid_anchor_3d_generator.GridAnchor3dGenerator() area_extents = np.array([[-40, 40], [-5, 5], [0, 70]]) start_time = time.time() anchor_boxes_3d = anchor_3d_generator.generate(area_3d=dataset. kitti_utils.area_extents, anchor_3d_sizes=fake_clusters, anchor_stride=fake_anchor_stride, ground_plane=ground_plane) all_anchors = box_3d_encoder.box_3d_to_anchor(anchor_boxes_3d) end_time = time.time() print('Anchors generated in {} s'.format(end_time - start_time)) bev_boxes, bev_normalized_boxes = anchor_projector.project_to_bev( all_anchors, area_extents[[0, 2]]) bev_fig, (bev_axes, bev_normalized_axes) = plt.subplots(1, 2, figsize=( 16, 7)) bev_axes.set_xlim(0, 80) bev_axes.set_ylim(70, 0) bev_normalized_axes.set_xlim(0, 1.0) bev_normalized_axes.set_ylim(1, 0.0) plt.show(block=False) for box in bev_boxes: box_w = box[2] - box[0] box_h = box[3] - box[1] rect = patches.Rectangle((box[0], box[1]), box_w, box_h, linewidth= 2, edgecolor='b', facecolor='none') bev_axes.add_patch(rect) for normalized_box in bev_normalized_boxes: box_w = normalized_box[2] - normalized_box[0] box_h = normalized_box[3] - normalized_box[1] rect = patches.Rectangle((normalized_box[0], normalized_box[1]), box_w, box_h, linewidth=2, edgecolor='b', facecolor='none') bev_normalized_axes.add_patch(rect) rgb_fig, rgb_2d_axes, rgb_3d_axes = vis_utils.visualization(dataset. rgb_image_dir, img_idx) plt.show(block=False) image_path = dataset.get_rgb_image_path(dataset.sample_names[img_idx]) image_shape = np.array(Image.open(image_path)).shape stereo_calib_p2 = calib_utils.read_calibration(dataset.calib_dir, img_idx ).p2 start_time = time.time() rgb_boxes, rgb_normalized_boxes = anchor_projector.project_to_image_space( all_anchors, stereo_calib_p2, image_shape) end_time = time.time() print('Anchors projected in {} s'.format(end_time - start_time)) stereo_calib = calib_utils.read_calibration(dataset.calib_dir, 0) p = stereo_calib.p2 for anchor_idx in range(len(anchor_boxes_3d)): anchor_box_3d = anchor_boxes_3d[anchor_idx] obj_label = box_3d_encoder.box_3d_to_object_label(anchor_box_3d) vis_utils.draw_box_3d(rgb_3d_axes, obj_label, p) rgb_box_2d = rgb_boxes[anchor_idx] box_x1 = rgb_box_2d[0] box_y1 = rgb_box_2d[1] box_w = rgb_box_2d[2] - box_x1 box_h = rgb_box_2d[3] - box_y1 rect = patches.Rectangle((box_x1, box_y1), box_w, box_h, linewidth= 2, edgecolor='b', facecolor='none') rgb_2d_axes.add_patch(rect) if anchor_idx % 32 == 0: rgb_fig.canvas.draw() plt.show(block=True) 
compute|output|interaction|deepctr|layers|InteractingLayer|shape def compute_output_shape(self, input_shape): return None, input_shape[1], self.att_embedding_size * self.head_num 
utils|disperse|data|padding def disperse_padding(inp, max_length, target): assert len(inp) == len(target) desired_length = np.random.randint(len(inp), max_length + 1) rand_floats = np.random.random(size=desired_length) cur_symbol = 0 res_in = [] res_out = [] for i in range(desired_length): remaining_symbols = len(inp) - cur_symbol if rand_floats[i] * (desired_length - i) >= remaining_symbols: res_in.append(0) res_out.append(0) else: res_in.append(inp[cur_symbol]) res_out.append(target[cur_symbol]) cur_symbol += 1 remaining_symbols = len(inp) - cur_symbol assert remaining_symbols == 0 assert len(res_in) == desired_length assert len(res_out) == desired_length return res_in, res_out 
texar|and|evaluate|Executor|executor|train|run def train_and_evaluate(self, max_train_steps=None, eval_steps=None): """Trains and evaluates the model. See :tf_main:`tf.estimator.train_and_evaluate <estimator/train_and_evaluate>` for more details.  Args: max_train_steps (int, optional): Total number of steps for which to train model. If `None`, train forever or until the train data generates the OutOfRange exception. If OutOfRange occurs in the middle, training stops before :attr:`max_steps` steps. eval_steps (int, optional): Number of steps for which to evaluate model. If `None`, evaluates until the eval data raises an OutOfRange exception. """ train_spec = self._get_train_spec(max_steps=max_train_steps) eval_spec = self._get_eval_spec(steps=eval_steps) tf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec) 
FeedForwardPolicy|conditioned|goal|policy|hbaselines|value def value(self, obs, context, action): """See parent class.""" obs = self._get_obs(obs, context, axis=1) return self.sess.run(self.critic_tf, feed_dict={self.obs_ph: obs, self. action_ph: action}) 
DataProcessing|v2|collectAllData def collectAllData_v2(path): df = pd.read_csv(path) df = df.set_index('Filename') return df 
TestGoalConditionedPolicy|setUp|policy|test def setUp(self): self.policy_params = {'sess': tf.compat.v1.Session(), 'ac_space': Box( low=-1, high=1, shape=(1,), dtype=np.float32), 'ob_space': Box(low= -2, high=2, shape=(2,), dtype=np.float32), 'co_space': Box(low=-3, high=3, shape=(3,), dtype=np.float32), 'layers': None, 'verbose': 2} self.policy_params.update(GOAL_CONDITIONED_PARAMS.copy()) 
BiLSTM|sequence|init|deepctr|layers def __init__(self, units, layers=2, res_layers=0, dropout_rate=0.2, merge_mode='ave', **kwargs): if merge_mode not in ['fw', 'bw', 'sum', 'mul', 'ave', 'concat', None]: raise ValueError( 'Invalid merge mode. Merge mode should be one of {"fw","bw","sum", "mul", "ave", "concat", None}' ) self.units = units self.layers = layers self.res_layers = res_layers self.dropout_rate = dropout_rate self.merge_mode = merge_mode super(BiLSTM, self).__init__(**kwargs) self.supports_masking = True 
texar|decoders|init|data|VarUttTextDataDecoder def __init__(self, split_level='word', delimiter=' ', sentence_delimiter= '|||', bos_token=None, eos_token=None, max_seq_length=None, max_utterance_cnt=None, token_to_id_map=None, text_tensor_name='text', length_tensor_name='length', text_id_tensor_name='text_ids', utterance_cnt_tensor_name='utterance_cnt'): self._split_level = split_level self._delimiter = delimiter self._bos_token = bos_token self._eos_token = eos_token self._max_seq_length = max_seq_length self._token_to_id_map = token_to_id_map self._text_tensor_name = text_tensor_name self._text_id_tensor_name = text_id_tensor_name self._length_tensor_name = length_tensor_name self._utterance_cnt_tensor_name = utterance_cnt_tensor_name self._sentence_delimiter = sentence_delimiter self._max_utterance_cnt = max_utterance_cnt self._added_length = 0 
concat|envs|gym|PycolabEnv|obs|pycolab|env def _concat_obs(self, state): self.state = self.state[:, :, 1:] return np.concatenate((self.state, state), axis=-1) 
output|Output|begin|step|train def train_step_begin(self, step): if self.pctens is not None and step % self.pctens == 0: self.run_options = tf.RunOptions(report_tensor_allocations_upon_oom =True, trace_level=tf.RunOptions.FULL_TRACE) self.run_metadata = tf.RunMetadata() 
bert|write|predictions|master|squad|run def write_predictions(all_examples, all_features, all_results, n_best_size, max_answer_length, do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file): """Write final predictions to the json file and log-odds of null if needed.""" tf.logging.info('Writing predictions to: %s' % output_prediction_file) tf.logging.info('Writing nbest to: %s' % output_nbest_file) example_index_to_features = collections.defaultdict(list) for feature in all_features: example_index_to_features[feature.example_index].append(feature) unique_id_to_result = {} for result in all_results: unique_id_to_result[result.unique_id] = result _PrelimPrediction = collections.namedtuple('PrelimPrediction', [ 'feature_index', 'start_index', 'end_index', 'start_logit', 'end_logit']) all_predictions = collections.OrderedDict() all_nbest_json = collections.OrderedDict() scores_diff_json = collections.OrderedDict() for example_index, example in enumerate(all_examples): features = example_index_to_features[example_index] prelim_predictions = [] score_null = 1000000 min_null_feature_index = 0 null_start_logit = 0 null_end_logit = 0 for feature_index, feature in enumerate(features): result = unique_id_to_result[feature.unique_id] start_indexes = _get_best_indexes(result.start_logits, n_best_size) end_indexes = _get_best_indexes(result.end_logits, n_best_size) if FLAGS.version_2_with_negative: feature_null_score = result.start_logits[0 ] + result.end_logits[0] if feature_null_score < score_null: score_null = feature_null_score min_null_feature_index = feature_index null_start_logit = result.start_logits[0] null_end_logit = result.end_logits[0] for start_index in start_indexes: for end_index in end_indexes: if start_index >= len(feature.tokens): continue if end_index >= len(feature.tokens): continue if start_index not in feature.token_to_orig_map: continue if end_index not in feature.token_to_orig_map: continue if not feature.token_is_max_context.get(start_index, False ): continue if end_index < start_index: continue length = end_index - start_index + 1 if length > max_answer_length: continue prelim_predictions.append(_PrelimPrediction( feature_index=feature_index, start_index= start_index, end_index=end_index, start_logit= result.start_logits[start_index], end_logit=result. end_logits[end_index])) if FLAGS.version_2_with_negative: prelim_predictions.append(_PrelimPrediction(feature_index= min_null_feature_index, start_index=0, end_index=0, start_logit=null_start_logit, end_logit=null_end_logit)) prelim_predictions = sorted(prelim_predictions, key=lambda x: x. start_logit + x.end_logit, reverse=True) _NbestPrediction = collections.namedtuple('NbestPrediction', [ 'text', 'start_logit', 'end_logit']) seen_predictions = {} nbest = [] for pred in prelim_predictions: if len(nbest) >= n_best_size: break feature = features[pred.feature_index] if pred.start_index > 0: tok_tokens = feature.tokens[pred.start_index:pred.end_index + 1 ] orig_doc_start = feature.token_to_orig_map[pred.start_index] orig_doc_end = feature.token_to_orig_map[pred.end_index] orig_tokens = example.doc_tokens[orig_doc_start: orig_doc_end + 1] tok_text = ' '.join(tok_tokens) tok_text = tok_text.replace(' ##', '') tok_text = tok_text.replace('##', '') tok_text = tok_text.strip() tok_text = ' '.join(tok_text.split()) orig_text = ' '.join(orig_tokens) final_text = get_final_text(tok_text, orig_text, do_lower_case) if final_text in seen_predictions: continue seen_predictions[final_text] = True else: final_text = '' seen_predictions[final_text] = True nbest.append(_NbestPrediction(text=final_text, start_logit=pred .start_logit, end_logit=pred.end_logit)) if FLAGS.version_2_with_negative: if '' not in seen_predictions: nbest.append(_NbestPrediction(text='', start_logit= null_start_logit, end_logit=null_end_logit)) if not nbest: nbest.append(_NbestPrediction(text='empty', start_logit=0.0, end_logit=0.0)) assert len(nbest) >= 1 total_scores = [] best_non_null_entry = None for entry in nbest: total_scores.append(entry.start_logit + entry.end_logit) if not best_non_null_entry: if entry.text: best_non_null_entry = entry probs = _compute_softmax(total_scores) nbest_json = [] for i, entry in enumerate(nbest): output = collections.OrderedDict() output['text'] = entry.text output['probability'] = probs[i] output['start_logit'] = entry.start_logit output['end_logit'] = entry.end_logit nbest_json.append(output) assert len(nbest_json) >= 1 if not FLAGS.version_2_with_negative: all_predictions[example.qas_id] = nbest_json[0]['text'] else: score_diff = (score_null - best_non_null_entry.start_logit - best_non_null_entry.end_logit) scores_diff_json[example.qas_id] = score_diff if score_diff > FLAGS.null_score_diff_threshold: all_predictions[example.qas_id] = '' else: all_predictions[example.qas_id] = best_non_null_entry.text all_nbest_json[example.qas_id] = nbest_json with tf.gfile.GFile(output_prediction_file, 'w') as writer: writer.write(json.dumps(all_predictions, indent=4) + '\n') with tf.gfile.GFile(output_nbest_file, 'w') as writer: writer.write(json.dumps(all_nbest_json, indent=4) + '\n') if FLAGS.version_2_with_negative: with tf.gfile.GFile(output_null_log_odds_file, 'w') as writer: writer.write(json.dumps(scores_diff_json, indent=4) + '\n') 
get|output|Output|options|run def get_run_options(self): return self.run_options 
tests|gym|test|scrolling|pycolab|main def main(argv=()): del argv unittest.main() 
envs|gym|grid|setup|PycolabConnectNEnv|pycolab|worlds|env def _setup(self): self.action_space = spaces.Discrete(self._shape[1]) self._observation_shape = [self._shape[0], self._shape[1], 1] self.observation_space = spaces.Box(0.0, 1.0, self._observation_shape) 
vector|make|caps|cnn|helpers|homogeneous def make_homogeneous_vector_caps(op_name, in_tensor, out_caps, cap_dims, weight_decay=0.0005): with tf.device('/device:CPU:0'), tf.variable_scope('vars/caps', reuse= tf.AUTO_REUSE): in_caps_sz = in_tensor.get_shape().as_list()[2] w_out_cap = tf.get_variable('w_' + op_name, shape=[out_caps, in_caps_sz, cap_dims], regularizer=l2_regularizer(weight_decay), initializer=tf.glorot_uniform_initializer()) with tf.name_scope(op_name): return tf.reduce_sum(tf.multiply(in_tensor, w_out_cap), 2) 
CONTENT|load|train|data def load_data_CONTENT(path): im = ~cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (256, 256)) return np.expand_dims(im, -1) / 127.5 - 1.0 
SRDRM|gen|residual|block|models|nets def residual_block(self, layer_input, filters): """Residual block described in paper""" d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(layer_input) d = BatchNormalization(momentum=0.5)(d) d = Activation('relu')(d) d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d) d = BatchNormalization(momentum=0.5)(d) d = Add()([d, layer_input]) return d 
partial|nets|v2|wrapped|mobilenet def wrapped_partial(func, *args, **kwargs): partial_func = functools.partial(func, *args, **kwargs) functools.update_wrapper(partial_func, func) return partial_func 
string|scannet|log|evaluate|withoverlap def log_string(out_str): LOG_FOUT.write(out_str + '\n') LOG_FOUT.flush() print(out_str) 
add|SummaryWriter|src|summary|new def __new__(cls, logdir): assert logdir is not None and logdir != '', 'need model_dir to initialize SummaryWriter' if SummaryWriter.__instance is None: SummaryWriter.__instance = super(SummaryWriter, cls).__new__(cls) fw = FileWriter(logdir, graph=ops.get_default_graph()) setattr(SummaryWriter.__instance, '_summary_writer', fw) setattr(SummaryWriter.__instance, 'add_graph', fw.add_graph) setattr(SummaryWriter.__instance, 'add_meta_graph', fw.add_meta_graph) setattr(SummaryWriter.__instance, 'add_session_log', fw.add_session_log ) return SummaryWriter.__instance 
hparams|inf|mnist|basic|read|utils def read_hparams(pkl_filepath): with open(pkl_filepath, 'rb') as f: hparams = pickle.load(f) return hparams 
network|resnet18|ResNet18 def network(self, inputs, num_classes, scope, is_training, kargs): print('Use ResNet18') use_bias = kargs.use_bias block_sizes = kargs.block_sizes strides = kargs.strides initializer = kargs.initializer regularizer = kargs.regularizer weights_dict = kargs.get('weights_dict') or {} weight_decay = kargs.weight_decay channel_num = kargs.channels_num if isinstance(channel_num, dict): channel_num = list(channel_num.values()) print('Use set channels', channel_num) else: print('Use ori channels', channel_num) with tf.variable_scope(scope + '/conv0') as nsc: net = tf.layers.conv2d(inputs, channel_num[0], [7, 7], strides=[2, 2], padding='same', use_bias=use_bias, kernel_initializer= initializer(), kernel_regularizer=regularizer(weight_decay)) self.restore_weights(nsc, 'conv', weights_dict) net = tf.layers.batch_normalization(net, axis=-1, training= is_training, epsilon=1e-05, momentum=0.997, beta_regularizer= regularizer(weight_decay), gamma_regularizer=regularizer( weight_decay)) self.restore_weights(nsc, 'bn', weights_dict) net = tf.nn.relu(net) net = tf.layers.max_pooling2d(net, [3, 3], strides=[2, 2], padding= 'SAME') for i, num_block in enumerate(block_sizes): with tf.variable_scope(scope + '/block%d' % (i + 1)) as nsc: with tf.variable_scope('sub_block0') as nsc: net = self.basic_block(net, channel_num[2 * i * num_block + 1:2 * i * num_block + 3], strides[i], use_bias, is_training, initializer, regularizer, weights_dict, weight_decay) for j in range(1, num_block): with tf.variable_scope('sub_block%d' % j) as nsc: net = self.basic_block(net, channel_num[2 * (i * num_block + j) + 1:2 * (i * num_block + j) + 3], 1, use_bias, is_training, initializer, regularizer, weights_dict, weight_decay) with tf.variable_scope(scope + '/dense') as nsc: assert net.get_shape().as_list()[2] == 7 net = tf.reduce_mean(net, [1, 2], keepdims=False) net = tf.layers.dense(net, num_classes, use_bias=True, kernel_regularizer=regularizer(weight_decay), kernel_initializer=initializer()) self.restore_weights(nsc, 'dense', weights_dict) return net 
renorm|update|var|training|vkge|constraints def renorm_update_var(log_var_matrix, norm=1.0, axis=0): var_matrix = tf.exp(log_var_matrix) row_norms = tf.reduce_sum(var_matrix, axis=axis) scaled = var_matrix * tf.expand_dims(norm / row_norms, axis=axis) scaled = tf.log(scaled) return tf.assign(log_var_matrix, scaled) 
bert|openai|heads|split|Attention|pytorch|modeling|pretrained def split_heads(self, x, k=False): new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head) x = x.view(*new_x_shape) if k: return x.permute(0, 2, 3, 1) else: return x.permute(0, 2, 1, 3) 
one|record|parse|imagenet def _parse_one_record(record, is_training, data_augmentation_args): feature_map = {'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''), 'image/class/label': tf.FixedLenFeature([], dtype=tf.int64, default_value=-1), 'image/class/text': tf. FixedLenFeature([], dtype=tf.string, default_value=''), 'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32), 'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32), 'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32), 'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32)} sparse_float32 = tf.VarLenFeature(dtype=tf.float32) features = tf.parse_single_example(record, feature_map) image = tf.image.decode_image(features['image/encoded'], channels=3) label = tf.cast(features['image/class/label'], dtype=tf.int32) xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0) ymin = tf.expand_dims(features['image/object/bbox/ymin'].values, 0) xmax = tf.expand_dims(features['image/object/bbox/xmax'].values, 0) ymax = tf.expand_dims(features['image/object/bbox/ymax'].values, 0) bbox = tf.concat([ymin, xmin, ymax, xmax], 0) bbox = tf.expand_dims(bbox, 0) bbox = tf.transpose(bbox, [0, 2, 1]) if is_training: image, label, bbox = _data_augmentation(image, label, bbox, data_augmentation_args) else: image, label, bbox = _process_for_eval(image, label, bbox, data_augmentation_args) return image, label 
gen|stdev|wganlib|set|mnist|conv2d|weights def set_weights_stdev(weights_stdev): global _weights_stdev _weights_stdev = weights_stdev 
get|models|fn|Seq2Seq|func|evaluation|seq2seq|thumt def evaluation_fn(features, params=None): if params is None: params = copy.copy(self.parameters) else: params = copy.copy(params) params.dropout = 0.0 params.use_variational_dropout = False params.label_smoothing = 0.0 with tf.variable_scope(self._scope): score = model_graph(features, 'eval', params) return score 
nmt|step|Decoder|decoder @abc.abstractmethod def step(self, time, inputs, state, name=None): """Called per step of decoding (but only once for dynamic decoding).  Args: time: Scalar `int32` tensor. Current step number. inputs: RNNCell input (possibly nested tuple of) tensor[s] for this time step. state: RNNCell state (possibly nested tuple of) tensor[s] from previous time step. name: Name scope for any created operations.  Returns: `(outputs, next_state, next_inputs, finished)`: `outputs` is an object containing the decoder output, `next_state` is a (structure of) state tensors and TensorArrays, `next_inputs` is the tensor that should be used as input for the next step, `finished` is a boolean tensor telling whether the sequence is complete, for each sequence in the batch. """ raise NotImplementedError 
image|from|latent|model|layerwise|shape def latent_from_image_shape(hps): return lambda s: (s[0], hps.z_size, s[2] // 2, s[3] // 2) 
HybridKMeans|utils|fit def fit(self, X): kmeans_list = [] inertias_list = [] for n in range(self.n_init_): kmeans = HybridKmeans_implementation(self.k_, self.delta_, self. max_iter_).run(X) kmeans_list.append(kmeans) inertias_list.append(kmeans.inertia_) if self.plot_: kmeans.plot(X, title=str(n)) return kmeans_list[int(np.argmin(np.asarray(inertias_list)))] 
rates|decay|train|learning|Trainer def decay_learning_rates(self, decay_rate): """Decay learning rates  Arguments: decay_rate {float} -- factor to decay """ if not isinstance(decay_rate, list): decay_rate = [decay_rate] * self.network.n_layers assert len(decay_rate) == self.network.n_layers lrs = self.get_learning_rates() lrs = [(lr * dr if lr else None) for lr, dr in zip(lrs, decay_rate)] self.set_learning_rates(lrs) 
maze|reset|ant|AntMazeEnv|env def reset(self): """Reset the environment.""" self.t = 0 self.trajectory = [] self.wrapped_env.reset() if len(self._init_positions) > 1: xy = random.choice(self._init_positions) self.wrapped_env.set_xy(xy) return self._get_obs() 
tripletloss|src|master|train|facenet def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch, batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file, embedding_size, anchor, positive, negative, triplet_loss): batch_number = 0 if args.learning_rate > 0.0: lr = args.learning_rate else: lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch) while batch_number < args.epoch_size: image_paths, num_per_class = sample_people(dataset, args. people_per_batch, args.images_per_person) print('Running forward pass on sampled images: ', end='') start_time = time.time() nrof_examples = args.people_per_batch * args.images_per_person labels_array = np.reshape(np.arange(nrof_examples), (-1, 3)) image_paths_array = np.reshape(np.expand_dims(np.array(image_paths), 1), (-1, 3)) sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array}) emb_array = np.zeros((nrof_examples, embedding_size)) nrof_batches = int(np.ceil(nrof_examples / args.batch_size)) for i in range(nrof_batches): batch_size = min(nrof_examples - i * args.batch_size, args. batch_size) emb, lab = sess.run([embeddings, labels_batch], feed_dict={ batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}) emb_array[(lab), :] = emb print('%.3f' % (time.time() - start_time)) print('Selecting suitable triplets for training') triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, image_paths, args.people_per_batch, args.alpha) selection_time = time.time() - start_time print( '(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds' % (nrof_random_negs, nrof_triplets, selection_time)) nrof_batches = int(np.ceil(nrof_triplets * 3 / args.batch_size)) triplet_paths = list(itertools.chain(*triplets)) labels_array = np.reshape(np.arange(len(triplet_paths)), (-1, 3)) triplet_paths_array = np.reshape(np.expand_dims(np.array( triplet_paths), 1), (-1, 3)) sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array}) nrof_examples = len(triplet_paths) train_time = 0 i = 0 emb_array = np.zeros((nrof_examples, embedding_size)) loss_array = np.zeros((nrof_triplets,)) summary = tf.Summary() step = 0 while i < nrof_batches: start_time = time.time() batch_size = min(nrof_examples - i * args.batch_size, args. batch_size) feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True} err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict) emb_array[(lab), :] = emb loss_array[i] = err duration = time.time() - start_time print('Epoch: [%d][%d/%d]\tTime %.3f\tLoss %2.3f' % (epoch, batch_number + 1, args.epoch_size, duration, err)) batch_number += 1 i += 1 train_time += duration summary.value.add(tag='loss', simple_value=err) summary.value.add(tag='time/selection', simple_value=selection_time) summary_writer.add_summary(summary, step) return step 
get|size|params|and|from|input|resnet32|num|channel|ResNet32|calculation def get_input_size(index): size = ori_size if not isinstance(size, int): size = size[0] if index >= 0 and index <= 10: return size elif index >= 11 and index <= 20: return size / 2 elif index >= 21 and index <= 30: return size / 4 else: raise ValueError('unknown layer index') return size 
plato|component|agent|training|manager|dialogue|is|DialogueManager def is_training(self): """ Assess whether there are any trainable components in this dialogue Manager.  :return: True or False """ return self.TRAIN_DST or self.TRAIN_POLICY 
gym|extensions|print|bullet|ManipulatorInfo|manipulator def print(self): print('ManipulatorInfo: robot_id', self.robot_id, '\n joint_ids', self. joint_ids, '\n joint_names', self.joint_names, '\n joint_minpos', self.joint_minpos, '\n joint_maxpos', self.joint_maxpos, '\n joint_maxforce', self.joint_maxforce, '\n joint_maxvel', self. joint_maxvel, '\n ee_link_id', self.ee_link_id, """ right_arm_jids_lst""", self.arm_jids_lst, '\n ee_jid', self. ee_jid, """ finger_jids_lst""", self.finger_jids_lst, """ left_ee_link_id""", self.left_ee_link_id, """ left_arm_jids_lst""", self.left_arm_jids_lst, '\n left_ee_jid', self.left_ee_jid, """ left_finger_jids_lst""", self. left_finger_jids_lst) 
supply|validation|feeder|data|DefaultSupplier def supply_validation_data(self, length, batch_size) ->tuple: return self.__gen_training_data(False) 
logger|cleverhans|utils|create def create_logger(name): """ Create a logger object with the given name.  If this is the first time that we call this method, then initialize the formatter. """ base = logging.getLogger('cleverhans') if len(base.handlers) == 0: ch = logging.StreamHandler() formatter = logging.Formatter( '[%(levelname)s %(asctime)s %(name)s] ' + '%(message)s') ch.setFormatter(formatter) base.addHandler(ch) return base 
TranslationModel|sentence|translate|model|decode|translation def decode_sentence(self, sentence_tuple, remove_unk=False): return next(self.decode_batch([sentence_tuple], remove_unk)) 
imagenet64|pvae|load|chunked|tflib def load_chunked(batch_size, datapath, rs=np.random, chunk=None): return make_generator_from_chunked(os.path.join(datapath, 'imagenet64/train_64_chunks'), 1281149, batch_size, rs, chunk ), make_generator_from_chunked(os.path.join(datapath, 'imagenet64/valid_64_chunks'), 49999, batch_size, rs, chunk) 
grads|autoencoder|dp|clip|sgd def clip_grads(w_ps_grads, max_bound, layerwise_clip): quad_sums = [tf.reduce_sum(w ** 2, axis=[1, 2]) for w in w_ps_grads] if layerwise_clip: grad_norms = [tf.sqrt(s) for s in quad_sums] norm_factors = [tf.minimum(max_bound / n, tf.ones(n.get_shape())) for n in grad_norms] w_ps_grads = [(w * f[:, (None), (None)]) for w, f in zip(w_ps_grads, norm_factors)] else: grad_norms = tf.sqrt(tf.add_n(quad_sums)) norm_factors = tf.minimum(max_bound / grad_norms, tf.ones( grad_norms.get_shape())) w_ps_grads = [(w * norm_factors[:, (None), (None)]) for w in w_ps_grads ] return w_ps_grads 
init|envs|AntGatherEnv def __init__(self, n_apples=8, n_bombs=8, activity_range=10.0, robot_object_spacing=2.0, catch_range=1.0, n_bins=10, sensor_range=6.0, sensor_span=2.0 * math.pi, coef_inner_rew=0.0, dying_cost=0, *args, ** kwargs): """Instantiate the environment class.  In order to match the environment presented in the article, we modify the following default values:  * activity_range: 6 -> 10 * sensor_span: pi -> 2*pi * dying_cost: 0 -> -10  We also add a horizon attribute which is set to 500.  Parameters ---------- n_apples : int Number of apples in each episode n_bombs : int Number of bombs in each episode activity_range : float The span for generating objects (x, y in [-range, range]) robot_object_spacing : float Number of objects in each episode catch_range : float Minimum distance range to catch an object n_bins : float Number of objects in each episode sensor_range : float Maximum sensor range (how far it can go) sensor_span : float Maximum sensor span (how wide it can span), in radians coef_inner_rew : float inner (AntEnv) reward weighting coefficient dying_cost : float reward assigned with dying in the environment """ super(AntGatherEnv, self).__init__(*args, n_apples=n_apples, n_bombs= n_bombs, activity_range=activity_range, robot_object_spacing= robot_object_spacing, catch_range=catch_range, n_bins=n_bins, sensor_range=sensor_range, sensor_span=sensor_span, coef_inner_rew= coef_inner_rew, dying_cost=dying_cost, **kwargs) self.step_number = 0 
generate|handler|l1 def _handler_l1(ir_path, vis_path, model_path, model_pre_path, ssim_weight, index, output_path=None): ir_img = get_train_images(ir_path, flag=False) vis_img = get_train_images(vis_path, flag=False) dimension = ir_img.shape ir_img = ir_img.reshape([1, dimension[0], dimension[1], dimension[2]]) vis_img = vis_img.reshape([1, dimension[0], dimension[1], dimension[2]]) ir_img = np.transpose(ir_img, (0, 2, 1, 3)) vis_img = np.transpose(vis_img, (0, 2, 1, 3)) print('img shape final:', ir_img.shape) with tf.Graph().as_default(), tf.Session() as sess: infrared_field = tf.placeholder(tf.float32, shape=ir_img.shape, name='content') visible_field = tf.placeholder(tf.float32, shape=ir_img.shape, name ='style') dfn = DenseFuseNet(model_pre_path) enc_ir = dfn.transform_encoder(infrared_field) enc_vis = dfn.transform_encoder(visible_field) target = tf.placeholder(tf.float32, shape=enc_ir.shape, name='target') output_image = dfn.transform_decoder(target) saver = tf.train.Saver() saver.restore(sess, model_path) enc_ir_temp, enc_vis_temp = sess.run([enc_ir, enc_vis], feed_dict={ infrared_field: ir_img, visible_field: vis_img}) feature = L1_norm(enc_ir_temp, enc_vis_temp) output = sess.run(output_image, feed_dict={target: feature}) save_images(ir_path, output, output_path, prefix='fused' + str( index), suffix='_densefuse_l1norm_' + str(ssim_weight)) 
get|test|ndh|id|env|pano|NDHEnvTest def _get_pano_id(self, pano_name, scan_id): return self._env._scan_info[scan_id].pano_name_to_id[pano_name] 
initialize|bar|progress|utils def initialize_progress_bar(data_loader_list): num_examples = sum([len(tensor) for tensor in data_loader_list.values()]) return set_progress_bar(num_examples) 
learner|device|fn|specific|agent|inference|make|main @tf.function def agent_inference(*args): return agent(*decode(args)) 
utils|images|save def save_images(images, size, image_path): return imsave(inverse_transform(images), size, image_path) 
texar|utils|get|class def get_class(class_name, module_paths=None): """Returns the class based on class name.  Args: class_name (str): Name or full path to the class. module_paths (list): Paths to candidate modules to search for the class. This is used if the class cannot be located solely based on `class_name`. The first module in the list that contains the class is used.  Returns: The target class.  Raises: ValueError: If class is not found based on :attr:`class_name` and :attr:`module_paths`. """ class_ = locate(class_name) if class_ is None and module_paths is not None: for module_path in module_paths: class_ = locate('.'.join([module_path, class_name])) if class_ is not None: break if class_ is None: raise ValueError('Class not found in {}: {}'.format(module_paths, class_name)) return class_ 
embed|get|sum|int|aux|dstn|att|wgt def get_wgt_sum_embed_aux(data_embed_ori, data_embed_ctxt, W1_ctxt, b1_ctxt, W2_ctxt, b2_ctxt, max_num_ctxt, total_embed_dim_ctxt): data_embed_ori_exp = tf.expand_dims(data_embed_ori, 1) data_embed_ori_tile = tf.tile(data_embed_ori_exp, [1, max_num_ctxt, 1]) data_concat = tf.concat([data_embed_ori_tile, data_embed_ctxt], 2) data_concat = tf.reshape(data_concat, [-1, total_embed_dim_ctxt]) hidden = tf.matmul(data_concat, W1_ctxt) + b1_ctxt hidden = tf.nn.relu(hidden) hidden = tf.nn.dropout(hidden, keep_prob) wgt_ctxt = tf.exp(tf.matmul(hidden, W2_ctxt) + b2_ctxt) wgt_ctxt = tf.reshape(wgt_ctxt, [-1, max_num_ctxt, 1]) temp = wgt_ctxt * data_embed_ctxt output = tf.reduce_sum(temp, 1) return output 
init|FileDataSrc|parse def __init__(self, chunks): self.chunks = [] self.done = chunks 
tangents|taylor|empirical|expand|jvp|neural|r|f|utils|taylorize def f_jvp(p): _, val_jvp = jvp(f, (p,), (dparams,)) return val_jvp 
contributed|embeddings|master|facenet|main|export def main(args): train_set = facenet.get_dataset(args.data_dir) image_list, label_list = facenet.get_image_paths_and_labels(train_set) path_exp = os.path.expanduser(args.data_dir) classes = [path for path in os.listdir(path_exp) if os.path.isdir(os. path.join(path_exp, path))] classes.sort() label_strings = [name for name in classes if os.path.isdir(os.path.join (path_exp, name))] with tf.Graph().as_default(): with tf.Session() as sess: facenet.load_model(args.model_dir) images_placeholder = tf.get_default_graph().get_tensor_by_name( 'input:0') embeddings = tf.get_default_graph().get_tensor_by_name( 'embeddings:0') phase_train_placeholder = tf.get_default_graph( ).get_tensor_by_name('phase_train:0') nrof_images = len(image_list) print('Number of images: ', nrof_images) batch_size = args.image_batch if nrof_images % batch_size == 0: nrof_batches = nrof_images // batch_size else: nrof_batches = nrof_images // batch_size + 1 print('Number of batches: ', nrof_batches) embedding_size = embeddings.get_shape()[1] emb_array = np.zeros((nrof_images, embedding_size)) start_time = time.time() for i in range(nrof_batches): if i == nrof_batches - 1: n = nrof_images else: n = i * batch_size + batch_size if args.is_aligned is True: images = facenet.load_data(image_list[i * batch_size:n], False, False, args.image_size) else: images = load_and_align_data(image_list[i * batch_size: n], args.image_size, args.margin, args. gpu_memory_fraction) feed_dict = {images_placeholder: images, phase_train_placeholder: False} embed = sess.run(embeddings, feed_dict=feed_dict) emb_array[i * batch_size:n, :] = embed print('Completed batch', i + 1, 'of', nrof_batches) run_time = time.time() - start_time print('Run time: ', run_time) label_list = np.array(label_list) np.save(args.embeddings_name, emb_array) np.save(args.labels_name, label_list) label_strings = np.array(label_strings) np.save(args.labels_strings_name, label_strings[label_list]) 
bert|features|convert|examples|master|squad|run|to def convert_examples_to_features(examples, tokenizer, max_seq_length, doc_stride, max_query_length, is_training, output_fn): """Loads a data file into a list of `InputBatch`s.""" unique_id = 1000000000 for example_index, example in enumerate(examples): query_tokens = tokenizer.tokenize(example.question_text) if len(query_tokens) > max_query_length: query_tokens = query_tokens[0:max_query_length] tok_to_orig_index = [] orig_to_tok_index = [] all_doc_tokens = [] for i, token in enumerate(example.doc_tokens): orig_to_tok_index.append(len(all_doc_tokens)) sub_tokens = tokenizer.tokenize(token) for sub_token in sub_tokens: tok_to_orig_index.append(i) all_doc_tokens.append(sub_token) tok_start_position = None tok_end_position = None if is_training and example.is_impossible: tok_start_position = -1 tok_end_position = -1 if is_training and not example.is_impossible: tok_start_position = orig_to_tok_index[example.start_position] if example.end_position < len(example.doc_tokens) - 1: tok_end_position = orig_to_tok_index[example.end_position + 1 ] - 1 else: tok_end_position = len(all_doc_tokens) - 1 tok_start_position, tok_end_position = _improve_answer_span( all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.orig_answer_text) max_tokens_for_doc = max_seq_length - len(query_tokens) - 3 _DocSpan = collections.namedtuple('DocSpan', ['start', 'length']) doc_spans = [] start_offset = 0 while start_offset < len(all_doc_tokens): length = len(all_doc_tokens) - start_offset if length > max_tokens_for_doc: length = max_tokens_for_doc doc_spans.append(_DocSpan(start=start_offset, length=length)) if start_offset + length == len(all_doc_tokens): break start_offset += min(length, doc_stride) for doc_span_index, doc_span in enumerate(doc_spans): tokens = [] token_to_orig_map = {} token_is_max_context = {} segment_ids = [] tokens.append('[CLS]') segment_ids.append(0) for token in query_tokens: tokens.append(token) segment_ids.append(0) tokens.append('[SEP]') segment_ids.append(0) for i in range(doc_span.length): split_token_index = doc_span.start + i token_to_orig_map[len(tokens)] = tok_to_orig_index[ split_token_index] is_max_context = _check_is_max_context(doc_spans, doc_span_index, split_token_index) token_is_max_context[len(tokens)] = is_max_context tokens.append(all_doc_tokens[split_token_index]) segment_ids.append(1) tokens.append('[SEP]') segment_ids.append(1) input_ids = tokenizer.convert_tokens_to_ids(tokens) input_mask = [1] * len(input_ids) while len(input_ids) < max_seq_length: input_ids.append(0) input_mask.append(0) segment_ids.append(0) assert len(input_ids) == max_seq_length assert len(input_mask) == max_seq_length assert len(segment_ids) == max_seq_length start_position = None end_position = None if is_training and not example.is_impossible: doc_start = doc_span.start doc_end = doc_span.start + doc_span.length - 1 out_of_span = False if not (tok_start_position >= doc_start and tok_end_position <= doc_end): out_of_span = True if out_of_span: start_position = 0 end_position = 0 else: doc_offset = len(query_tokens) + 2 start_position = (tok_start_position - doc_start + doc_offset) end_position = tok_end_position - doc_start + doc_offset if is_training and example.is_impossible: start_position = 0 end_position = 0 if example_index < 20: tf.logging.info('*** Example ***') tf.logging.info('unique_id: %s' % unique_id) tf.logging.info('example_index: %s' % example_index) tf.logging.info('doc_span_index: %s' % doc_span_index) tf.logging.info('tokens: %s' % ' '.join([tokenization. printable_text(x) for x in tokens])) tf.logging.info('token_to_orig_map: %s' % ' '.join([( '%d:%d' % (x, y)) for x, y in six.iteritems( token_to_orig_map)])) tf.logging.info('token_is_max_context: %s' % ' '.join([( '%d:%s' % (x, y)) for x, y in six.iteritems( token_is_max_context)])) tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids])) tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask])) tf.logging.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids])) if is_training and example.is_impossible: tf.logging.info('impossible example') if is_training and not example.is_impossible: answer_text = ' '.join(tokens[start_position: end_position + 1]) tf.logging.info('start_position: %d' % start_position) tf.logging.info('end_position: %d' % end_position) tf.logging.info('answer: %s' % tokenization. printable_text(answer_text)) feature = InputFeatures(unique_id=unique_id, example_index= example_index, doc_span_index=doc_span_index, tokens=tokens, token_to_orig_map=token_to_orig_map, token_is_max_context= token_is_max_context, input_ids=input_ids, input_mask= input_mask, segment_ids=segment_ids, start_position= start_position, end_position=end_position, is_impossible= example.is_impossible) output_fn(feature) unique_id += 1 
plyfile|PlyData|getitem def __getitem__(self, name): return self._element_lookup[name] 
NASNetTest|test|nasnet|nets|testAllEndPointsShapesLargeModel def testAllEndPointsShapesLargeModel(self): batch_size = 5 height, width = 331, 331 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) tf.train.create_global_step() with slim.arg_scope(nasnet.nasnet_large_arg_scope()): _, end_points = nasnet.build_nasnet_large(inputs, num_classes) endpoints_shapes = {'Stem': [batch_size, 42, 42, 336], 'Cell_0': [ batch_size, 42, 42, 1008], 'Cell_1': [batch_size, 42, 42, 1008], 'Cell_2': [batch_size, 42, 42, 1008], 'Cell_3': [batch_size, 42, 42, 1008], 'Cell_4': [batch_size, 42, 42, 1008], 'Cell_5': [batch_size, 42, 42, 1008], 'Cell_6': [batch_size, 21, 21, 2016], 'Cell_7': [ batch_size, 21, 21, 2016], 'Cell_8': [batch_size, 21, 21, 2016], 'Cell_9': [batch_size, 21, 21, 2016], 'Cell_10': [batch_size, 21, 21, 2016], 'Cell_11': [batch_size, 21, 21, 2016], 'Cell_12': [ batch_size, 11, 11, 4032], 'Cell_13': [batch_size, 11, 11, 4032], 'Cell_14': [batch_size, 11, 11, 4032], 'Cell_15': [batch_size, 11, 11, 4032], 'Cell_16': [batch_size, 11, 11, 4032], 'Cell_17': [ batch_size, 11, 11, 4032], 'Reduction_Cell_0': [batch_size, 21, 21, 1344], 'Reduction_Cell_1': [batch_size, 11, 11, 2688], 'global_pool': [batch_size, 4032], 'AuxLogits': [batch_size, num_classes], 'Logits': [batch_size, num_classes], 'Predictions': [ batch_size, num_classes]} self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys()) for endpoint_name in endpoints_shapes: tf.logging.info('Endpoint name: {}'.format(endpoint_name)) expected_shape = endpoints_shapes[endpoint_name] self.assertTrue(endpoint_name in end_points) self.assertListEqual(end_points[endpoint_name].get_shape().as_list( ), expected_shape) 
RawPiczakCNN|init|RawPiczak|EndToEndClassification|Models|EnvClassification def __init__(self, model_name, num_classes=50, weights_initializer= initializers.xavier_initializer(), biases_initializer=init_ops. zeros_initializer(), weights_regularizer=None, biases_regularizer=None, dropout=False): """ Initializes the RawPiczakCNN model class.  Args: model_name (str): model name. num_classes (int): number of the classes (i.e. size of the output layer of the classifier). weights_initializer (func): how to initialize the weights of all layers. biases_initializer (func): how to initialize the biases of all layers. weights_regularizer (func): regularization of the weights of all layers. biases_regularizer (func): regularization of the biases of all layers. dropout (bool): whether or not to use dropout. For training with random initialization (so without loading the pretrained MSTmodel layers dropout prevents overfitting. """ self.model_name = model_name self.num_classes = num_classes self.W_init = weights_initializer self.b_init = biases_initializer self.W_reg = weights_regularizer self.b_reg = biases_regularizer self.dropout = dropout 
pad|table2text|batch|data|utils|build def _pad(s, maxlen): s_ = copy.deepcopy(s) s_ = s_[:maxlen] slen = len(s_) if slen < maxlen: s_ += [pad_id] * (maxlen - slen) return s_ 
texar|Conv1DClassifier|build|conv|modules|classifiers def _build(self, inputs, sequence_length=None, dtype=None, mode=None): """Feeds the inputs through the network and makes classification.  The arguments are the same as in :class:`~texar.modules.Conv1DEncoder`.  The predictions of binary classification ("num_classes"=1) and multi-way classification ("num_classes">1) are different, as explained below.  Args: inputs: The inputs to the network, which is a 3D tensor. See :class:`~texar.modules.Conv1DEncoder` for more details. sequence_length (optional): An int tensor of shape `[batch_size]` containing the length of each element in :attr:`inputs`. If given, time steps beyond the length will first be masked out before feeding to the layers. dtype (optional): Type of the inputs. If not provided, infers from inputs automatically. mode (optional): A tensor taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including `TRAIN`, `EVAL`, and `PREDICT`. If `None`, :func:`texar.global_mode` is used.  Returns: A tuple `(logits, pred)`, where  - **`logits`** is a Tensor of shape `[batch_size, num_classes]`            for `num_classes` >1, and `[batch_size]` for `num_classes` =1             (i.e., binary classification). - **`pred`** is the prediction, a Tensor of shape `[batch_size]`             and type `tf.int64`. For binary classification, the standard             sigmoid function is used for prediction, and the class labels are             `{0, 1}`. """ logits = self._encoder(inputs, sequence_length, dtype, mode) num_classes = self._hparams.num_classes is_binary = num_classes == 1 is_binary = is_binary or num_classes <= 0 and logits.shape[1] == 1 if is_binary: pred = tf.greater(logits, 0) logits = tf.reshape(logits, [-1]) else: pred = tf.argmax(logits, 1) pred = tf.to_int64(tf.reshape(pred, [-1])) self._built = True return logits, pred 
master|get|xlnet|initializer def _get_initializer(FLAGS): """Get variable intializer.""" if FLAGS.init == 'uniform': initializer = tf.initializers.random_uniform(minval=-FLAGS. init_range, maxval=FLAGS.init_range, seed=None) elif FLAGS.init == 'normal': initializer = tf.initializers.random_normal(stddev=FLAGS.init_std, seed=None) else: raise ValueError('Initializer {} not supported'.format(FLAGS.init)) return initializer 
generator|256|pvae|mnist|tflib|binarized def binarized_generator(generator, include_targets=False, n_labelled=None):  def get_epoch(): for data in generator(): if n_labelled is not None: images, targets, labelled = data else: images, targets = data images = images.reshape((-1, 1, 28, 28)) images = discretize(images) if include_targets: if n_labelled is not None: yield images, targets, labelled else: yield images, targets else: yield images, return get_epoch 
darc|readMetaData|DataArchive def _readMetaData(self): signature = self.file.read(4) if signature != SIGNATURE: raise DarcException('Not a DataArchive.') version, = struct.unpack('<I', self.file.read(4)) if version > VERSION: raise DarcException('Unsupported version.') self.version = version directory_address, = struct.unpack('<Q', self.file.read(8)) if version >= 2: name_to_index_address, = struct.unpack('<Q', self.file.read(8)) self.file.seek(directory_address, 0) directory_size, = struct.unpack('<Q', self.file.read(8)) offsets = struct.unpack('<{}Q'.format(directory_size), self.file.read(8 * directory_size)) self.directory = list(offsets) if version >= 2: self.file.seek(name_to_index_address, 0) self.name_to_index = np.load(self.file).item() 
texar|test|tfrecord|setting|data|TFRecordDataTest|default def test_default_setting(self): """Tests the logics of TFRecordData. """ self._run_and_test(self._hparams) 
tICA|correlation|tica|offset @property def offset_correlation_(self): two_N = 2 * (self.n_observations_ - self.lag_time * self.n_sequences_) term = (self._outer_0_to_T_lagged + self._outer_0_to_T_lagged.T) / two_N means = self.means_ value = term - np.outer(means, means) return value 
get|avod|core|evaluator|cls|accuracy|Evaluator def get_cls_accuracy(self, predictions, eval_avod_losses, eval_rpn_losses, global_step): """Updates the calculated accuracies for rpn and avod losses.  Args: predictions: A dictionary containing the model outputs. eval_avod_losses: A dictionary containing all the avod averaged losses. eval_rpn_losses: A dictionary containing all the rpn averaged losses. global_step: Current global step that is being evaluated. """ objectness_pred = predictions[RpnModel.PRED_MB_OBJECTNESS] objectness_gt = predictions[RpnModel.PRED_MB_OBJECTNESS_GT] objectness_accuracy = self.calculate_cls_accuracy(objectness_pred, objectness_gt) sum_rpn_obj_accuracy = eval_rpn_losses[KEY_SUM_RPN_OBJ_ACC] sum_rpn_obj_accuracy += objectness_accuracy eval_rpn_losses.update({KEY_SUM_RPN_OBJ_ACC: sum_rpn_obj_accuracy}) print('Step {}: RPN Objectness Accuracy: {}'.format(global_step, objectness_accuracy)) if self.full_model: classification_pred = predictions[AvodModel. PRED_MB_CLASSIFICATION_SOFTMAX] classification_gt = predictions[AvodModel.PRED_MB_CLASSIFICATIONS_GT] classification_accuracy = self.calculate_cls_accuracy( classification_pred, classification_gt) sum_avod_cls_accuracy = eval_avod_losses[KEY_SUM_AVOD_CLS_ACC] sum_avod_cls_accuracy += classification_accuracy eval_avod_losses.update({KEY_SUM_AVOD_CLS_ACC: sum_avod_cls_accuracy}) print('Step {}: AVOD Classification Accuracy: {}'.format( global_step, classification_accuracy)) 
imagenet|official|resnet|set|input|shapes|ImageNetTFExampleInput def set_shapes(self, batch_size, images, labels): """Statically set the batch_size dimension.""" if self.transpose_input: images.set_shape(images.get_shape().merge_with(tf.TensorShape([None, None, None, batch_size]))) images = tf.reshape(images, [-1]) labels.set_shape(labels.get_shape().merge_with(tf.TensorShape([ batch_size]))) else: images.set_shape(images.get_shape().merge_with(tf.TensorShape([ batch_size, None, None, None]))) labels.set_shape(labels.get_shape().merge_with(tf.TensorShape([ batch_size]))) return images, labels 
frequent|ensemble|most def most_frequent(List, qid=0): res = max(sorted(list(set(List))), key=List.count) agree = 0 for i in List: if i == res: agree += 1 if agree == 2 or agree == 5: print(qid + ',' + res) agree_res[agree] += 1 return res 
bert|tokenization|tokenize|master|whitespace def whitespace_tokenize(text): """Runs basic whitespace cleaning and splitting on a piece of text.""" text = text.strip() if not text: return [] tokens = text.split() return tokens 
models|CarliniWagnerL2|generate|attacks def generate(self, x, **kwargs): """ Return a tensor that constructs adversarial examples for the given input. Generate uses tf.py_func in order to operate over tensors. :param x: (required) A tensor with the inputs. :param y: (optional) A tensor with the true labels for an untargeted attack. If None (and y_target is None) then use the original labels the classifier assigns. :param y_target: (optional) A tensor with the target labels for a targeted attack. :param confidence: Confidence of adversarial examples: higher produces examples with larger l2 distortion, but more strongly classified as adversarial. :param batch_size: Number of attacks to run simultaneously. :param learning_rate: The learning rate for the attack algorithm. Smaller values produce better results but are slower to converge. :param binary_search_steps: The number of times we perform binary search to find the optimal tradeoff- constant between norm of the purturbation and confidence of the classification. :param max_iterations: The maximum number of iterations. Setting this to a larger value will produce lower distortion results. Using only a few iterations requires a larger learning rate, and will produce larger distortion results. :param abort_early: If true, allows early aborts if gradient descent is unable to make progress (i.e., gets stuck in a local minimum). :param initial_const: The initial tradeoff-constant to use to tune the relative importance of size of the pururbation and confidence of classification. If binary_search_steps is large, the initial constant is not important. A smaller value of this constant gives lower distortion results. :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value """ import tensorflow as tf from .attacks_tf import CarliniWagnerL2 as CWL2 self.parse_params(**kwargs) labels, nb_classes = self.get_or_guess_labels(x, kwargs) print(x.get_shape().as_list()[1:]) attack = CWL2(self.sess, self.model, self.batch_size, self.confidence, 'y_target' in kwargs, self.learning_rate, self.binary_search_steps, self.max_iterations, self.abort_early, self.initial_const, self. clip_min, self.clip_max, nb_classes, 3)  def cw_wrap(x_val, y_val): return np.array(attack.attack(x_val, y_val), dtype=np.float32) wrap = tf.py_func(cw_wrap, [x, labels], tf.float32) return wrap 
assignment|get|checkpoint|from|map|model|utils def _get_assignment_map_from_checkpoint(tvars, init_checkpoint): """ Compute the union of the current variables and checkpoint variables. Because the variable scope of the original BERT and Texar implementation, we need to build a assignment map to match the variables. """ assignment_map = {} initialized_variable_names = {} name_to_variable = collections.OrderedDict() for var in tvars: name = var.name m = re.match('^(.*):\\d+$', name) if m is not None: name = m.group(1) name_to_variable[name] = var init_vars = tf.train.list_variables(init_checkpoint) assignment_map = {'bert/embeddings/word_embeddings': 'bert/word_embeddings/w', 'bert/embeddings/token_type_embeddings': 'bert/token_type_embeddings/w', 'bert/embeddings/position_embeddings': 'bert/position_embeddings/w', 'bert/embeddings/LayerNorm/beta': 'bert/encoder/LayerNorm/beta', 'bert/embeddings/LayerNorm/gamma': 'bert/encoder/LayerNorm/gamma'} for check_name, model_name in assignment_map.items(): initialized_variable_names[model_name] = 1 initialized_variable_names[model_name + ':0'] = 1 for check_name, shape in init_vars: if check_name.startswith('bert'): if check_name.startswith('bert/embeddings'): continue model_name = re.sub('layer_\\d+/output/dense', lambda x: x. group(0).replace('output/dense', 'ffn/output'), check_name) if model_name == check_name: model_name = re.sub('layer_\\d+/output/LayerNorm', lambda x: x.group(0).replace('output/LayerNorm', 'ffn/LayerNorm'), check_name) if model_name == check_name: model_name = re.sub('layer_\\d+/intermediate/dense', lambda x: x.group(0).replace('intermediate/dense', 'ffn/intermediate'), check_name) if model_name == check_name: model_name = re.sub('attention/output/dense', 'attention/self/output', check_name) if model_name == check_name: model_name = check_name.replace('attention/output/LayerNorm', 'output/LayerNorm') assert model_name in name_to_variable.keys( ), 'model name:{} not exists!'.format(model_name) assignment_map[check_name] = model_name initialized_variable_names[model_name] = 1 initialized_variable_names[model_name + ':0'] = 1 return assignment_map, initialized_variable_names 
Aggregator|utils|read @tf.Module.with_name_scope def read(self, actor_ids): """Reads the values corresponding to a list of actors.  Args: actor_ids: 1D tensor with the list of actor IDs we want to read.  Returns: A structure of tensors with the same shapes as the input specs. A dimension is added in front of each tensor, with size equal to the number of actor_ids provided. """ return tf.nest.map_structure(lambda s: s.sparse_read(actor_ids), self. _state) 
test|pix2pix|no|padding|DiscriminatorTest|nets|layers|four def test_four_layers_no_padding(self): batch_size = 2 input_size = 256 output_size = self._layer_output_size(input_size, pad=0) output_size = self._layer_output_size(output_size, pad=0) output_size = self._layer_output_size(output_size, pad=0) output_size = self._layer_output_size(output_size, stride=1, pad=0) output_size = self._layer_output_size(output_size, stride=1, pad=0) images = tf.ones((batch_size, input_size, input_size, 3)) with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()): logits, end_points = pix2pix.pix2pix_discriminator(images, num_filters=[64, 128, 256, 512], padding=0) self.assertListEqual([batch_size, output_size, output_size, 1], logits. shape.as_list()) self.assertListEqual([batch_size, output_size, output_size, 1], end_points['predictions'].shape.as_list()) 
utils|softmax def softmax(x): e_x = np.exp(x - np.max(x)) return e_x / e_x.sum(axis=0) 
dnn|get|hot|mul|masked def get_masked_mul_hot(x_input_mul_hot): data_mask = tf.cast(tf.greater(x_input_mul_hot, 0), tf.float32) data_mask = tf.expand_dims(data_mask, axis=3) data_mask = tf.tile(data_mask, (1, 1, 1, k)) data_embed_mul_hot = tf.nn.embedding_lookup(emb_mat, x_input_mul_hot) data_embed_mul_hot_masked = tf.multiply(data_embed_mul_hot, data_mask) return data_embed_mul_hot_masked 
testBuildLogitsLargeModel|pnasnet|test|nasnet|PNASNetTest|nets def testBuildLogitsLargeModel(self): batch_size = 5 height, width = 331, 331 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) tf.train.create_global_step() with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()): logits, end_points = pnasnet.build_pnasnet_large(inputs, num_classes) auxlogits = end_points['AuxLogits'] predictions = end_points['Predictions'] self.assertListEqual(auxlogits.get_shape().as_list(), [batch_size, num_classes]) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertListEqual(predictions.get_shape().as_list(), [batch_size, num_classes]) 
build|PCGNModel|model|PCGN def build_model(self): print('building model... ...') with tf.variable_scope('seq2seq_placeholder'): self.encoder_inputs = tf.placeholder(tf.int32, [None, None], name= 'encoder_inputs') self.decoder_inputs = tf.placeholder(tf.int32, [None, None], name= 'decoder_inputs') self.decoder_targets = tf.placeholder(tf.int32, [None, None], name= 'decoder_targets') self.decoder_targets_masks = tf.placeholder(tf.bool, [None, None], name='mask') self.encoder_length = tf.placeholder(tf.int32, [None], name= 'encoder_length') self.decoder_length = tf.placeholder(tf.int32, [None], name= 'decoder_length') self.user_feat = tf.placeholder(tf.float32, [None, self.feat_dim], name='user_feat') self.user_desc = tf.placeholder(tf.int32, [None, None], name= 'user_desc') self.desc_length = tf.placeholder(tf.int32, [None], name= 'user_desc_length') self.max_target_sequence_length = tf.constant(value=self. target_max_length, name='max_target_len') with tf.variable_scope('seq2seq_embedding'): self.embedding = self.init_embedding(self.vocab_size, self. embedding_size) with tf.variable_scope('seq2seq_encoder'): encoder_outputs, encoder_states = build_encoder(self.embedding, self.encoder_inputs, self.encoder_length, self. encode_num_layers, self.encode_num_units, self.encode_cell_type, bidir=self.encode_bidir) if self.use_user_desc or self.use_user_feat: with tf.variable_scope('user_profile_encoder'): desc_initializer = tf.contrib.layers.xavier_initializer() self.user_feat_mem_embedding = tf.layers.Dense(self. user_feat_mem_unit, use_bias=False, activation=tf.nn.relu, kernel_initializer=desc_initializer, name='user_feat_mem_layer' ) self.user_feats, self.user_embs, self.user_desc_encode = (self. build_user_embedding(self.user_feat, self.user_desc, self. desc_length, self.user_feat_unit, self.desc_rnn_unit, self. embedding, self.use_user_desc, self.use_user_feat)) if self.use_external_desc_express: dim2 = self.desc_rnn_unit dim1 = self.decode_num_units if self.use_blog_user_coattn: dim1 = dim1 * 2 self.blog_desc_inetract = tf.Variable(desc_initializer( shape=(dim1, dim2)), name='blog_desc_inetraction_layer', dtype=tf.float32) if self.use_external_feat_express: dim2 = dim2 + self.user_feat_unit self.user_map_layer = tf.Variable(desc_initializer(shape=( dim2, self.user_map_unit)), name='user_map_layer', dtype=tf.float32) with tf.variable_scope('seq2seq_decoder'): encoder_length = self.encoder_length if self.use_user_desc or self.use_user_feat: user_feats = self.user_feats user_embs = self.user_embs if self.use_user_desc: desc_length = self.desc_length user_desc_encode = self.user_desc_encode if self.beam_search: print('use beamsearch decoding..') encoder_outputs = tile_batch(encoder_outputs, multiplier=self. beam_size) encoder_states = tile_batch(encoder_states, multiplier=self. beam_size) encoder_length = tile_batch(encoder_length, multiplier=self. beam_size) if self.use_user_desc or self.use_user_feat: user_feats = tile_batch(user_feats, multiplier=self.beam_size) user_embs = tile_batch(user_embs, multiplier=self.beam_size) if self.use_user_desc: desc_length = tile_batch(desc_length, multiplier=self. beam_size) user_desc_encode = tile_batch(user_desc_encode, multiplier=self.beam_size) attention_mechanism = BahdanauAttention(num_units=self. attn_num_units, memory=encoder_outputs, memory_sequence_length= encoder_length) if self.use_blog_user_coattn: attention_mechanism_desc = BahdanauAttention(num_units=self. desc_attn_num_units, memory=user_desc_encode, memory_sequence_length=desc_length) decoder_cell = create_rnn_cell(self.decode_num_layers, self. decode_num_units, self.decode_cell_type) if self.use_blog_user_coattn: _attention_mechanism = (attention_mechanism, attention_mechanism_desc) _attention_layer_size = [self.decode_num_units, self. decode_num_units] else: _attention_mechanism = attention_mechanism _attention_layer_size = self.decode_num_units if self.use_user_feat: if self.use_gate_memory: _read_g = tf.layers.Dense(self.user_feat_mem_unit, use_bias =False, name='internal_read_gate') _write_g = tf.layers.Dense(self.user_feat_mem_unit, use_bias=False, name='internal_write_gate') if self.use_blog_user_coattn: _read_atten_gate = tf.layers.Dense(2 * self. desc_attn_num_units, use_bias=False, name= 'internal_read_attn_gate') else: _read_atten_gate = None else: _read_g = None _write_g = None _read_atten_gate = None decoder_cell = PCGNWrapper(cell=decoder_cell, attention_mechanism=_attention_mechanism, user_feats= user_feats, user_embs=user_embs, user_feat_mem_units=self. user_feat_mem_unit, user_feat_mem_embedding=self. user_feat_mem_embedding, read_gate=_read_g, write_gate= _write_g, use_gate_memory=self.use_gate_memory, attention_layer_size=_attention_layer_size, read_atten_gate =_read_atten_gate, name='PCGNWrapper') else: decoder_cell = AttentionWrapper(cell=decoder_cell, attention_mechanism=_attention_mechanism, attention_layer_size=_attention_layer_size, name= 'Attention_Wrapper') batch_size = (self.batch_size if not self.beam_search else self. batch_size * self.beam_size) decoder_initial_state = decoder_cell.zero_state(batch_size= batch_size, dtype=tf.float32).clone(cell_state=encoder_states) output_layer = tf.layers.Dense(self.vocab_size, use_bias=False, name='output_projection') if self.mode == 'train': decoder_inputs_embedded = tf.nn.embedding_lookup(self.embedding, self.decoder_inputs) training_helper = TrainingHelper(inputs=decoder_inputs_embedded, sequence_length=self.decoder_length, name='training_helper') training_decoder = BasicDecoder(cell=decoder_cell, helper= training_helper, initial_state=decoder_initial_state) (self.decoder_outputs, self.final_state, self.final_sequence_length ) = (dynamic_decode(decoder=training_decoder, impute_finished=True, maximum_iterations=self. max_target_sequence_length)) self.decoder_logits_train = tf.identity(self.decoder_outputs. rnn_output) if self.use_external_desc_express: if self.use_external_feat_express: _user_feats = user_embs else: _user_feats = None self.decoder_logits_train = self.external_personality_express( self.decoder_logits_train, user_desc_encode, self. blog_desc_inetract, user_feats=_user_feats, use_external_feat_express=self. use_external_feat_express, user_map=self.user_map_layer) with tf.variable_scope('decoder'): self.generic_logits = output_layer(self.decoder_logits_train) if self.use_gate_memory: self.feat_mem = self.final_state.user_feat_mem with tf.variable_scope('loss'): g_probs = tf.nn.softmax(self.generic_logits) train_log_probs = tf.log(g_probs) self.g_losses = tf.nn.sparse_softmax_cross_entropy_with_logits( logits=self.generic_logits, labels=self.decoder_targets) losses = tf.boolean_mask(self.g_losses, self. decoder_targets_masks) self.loss = tf.reduce_mean(losses) if self.use_gate_memory: self.int_mem_reg = tf.reduce_mean(tf.norm(self.feat_mem + 1e-07, axis=1)) self.loss += self.int_mem_reg CE = tf.nn.sparse_softmax_cross_entropy_with_logits(logits= train_log_probs, labels=self.decoder_targets) CE = tf.boolean_mask(CE, tf.cast(self.decoder_targets_masks, tf .bool)) self.CE = tf.reduce_mean(CE) optimizer = tf.train.AdamOptimizer(self.learning_rate) trainable_params = tf.trainable_variables() gradients = tf.gradients(self.loss, trainable_params) clip_gradients, _ = tf.clip_by_global_norm(gradients, self. max_gradient_norm) self.train_op = optimizer.apply_gradients(zip(clip_gradients, trainable_params)) elif self.mode == 'infer': start_tokens = tf.ones([self.batch_size], tf.int32) * SOS_ID end_token = EOS_ID if self.use_user_feat or self.use_user_desc: if self.use_external_desc_express: _embed_desc = user_desc_encode _blog_desc_inetract = self.blog_desc_inetract _user_map = self.user_map_layer if self.use_external_feat_express: _feat_embed = user_embs else: _feat_embed = None else: _embed_desc = None _blog_desc_inetract = None _user_map = None _feat_embed = None inference_decoder = PCGNBeamSearchDecoder(cell=decoder_cell, embedding=self.embedding, start_tokens=start_tokens, end_token=end_token, initial_state= decoder_initial_state, beam_width=self.beam_size, output_layer=output_layer, use_external_desc_express= self.use_external_desc_express, embed_desc=_embed_desc, blog_desc_inetract=_blog_desc_inetract, feat_embed= _feat_embed, use_external_feat_express=self. use_external_feat_express, user_map=_user_map) else: inference_decoder = BeamSearchDecoder(cell=decoder_cell, embedding=self.embedding, start_tokens=start_tokens, end_token=end_token, initial_state= decoder_initial_state, beam_width=self.beam_size, output_layer=output_layer) decoder_outputs, _, _ = dynamic_decode(decoder= inference_decoder, maximum_iterations=self.infer_max_iter) infer_outputs = decoder_outputs.predicted_ids self.infer_outputs = tf.transpose(infer_outputs, [0, 2, 1], name='infer_outputs') self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=self. max_to_keep) 
xnncommon|name|target|parse def parse_target_name(target_name): arch = list() isa = None for target_part in target_name.split('_'): if target_part in _ARCH_TO_MACRO_MAP: arch = [target_part] elif target_part in _ISA_TO_ARCH_MAP: isa = target_part if isa and not arch: arch = _ISA_TO_ARCH_MAP[isa] return arch, isa 
bert|forward|BertPreTrainingHeads|pytorch|modeling|pretrained def forward(self, sequence_output, pooled_output): prediction_scores = self.predictions(sequence_output) seq_relationship_score = self.seq_relationship(pooled_output) return prediction_scores, seq_relationship_score 
get|test|input|cvusa|data|id|InputData def get_test_id(self): return self.__cur_test_id 
darkflow|TFNet|net|init|build def __init__(self, FLAGS, darknet=None): self.ntrain = 0 if isinstance(FLAGS, dict): from ..defaults import argHandler newFLAGS = argHandler() newFLAGS.setDefaults() newFLAGS.update(FLAGS) FLAGS = newFLAGS self.FLAGS = FLAGS if self.FLAGS.pbLoad and self.FLAGS.metaLoad: self.say('\nLoading from .pb and .meta') self.graph = tf.Graph() device_name = FLAGS.gpuName if FLAGS.gpu > 0.0 else None with tf.device(device_name): with self.graph.as_default() as g: self.build_from_pb() return if darknet is None: darknet = Darknet(FLAGS) self.ntrain = len(darknet.layers) self.darknet = darknet args = [darknet.meta, FLAGS] self.num_layer = len(darknet.layers) self.framework = create_framework(*args) self.meta = darknet.meta self.say('\nBuilding net ...') start = time.time() self.graph = tf.Graph() device_name = FLAGS.gpuName if FLAGS.gpu > 0.0 else None with tf.device(device_name): with self.graph.as_default() as g: self.build_forward() self.setup_meta_ops() self.say('Finished in {}s\n'.format(time.time() - start)) 
gym|MarauderDrape|marauders|init|examples|extraterrestrial|pycolab def __init__(self, curtain, character): super(MarauderDrape, self).__init__(curtain, character) self._dx = -1 
fpn|mask|graph|deeper|model|coords|build def build_fpn_mask_coords_deeper_graph(rois, feature_maps, image_shape, pool_size, num_classes, use_bn): """Builds the computation graph of the coordinate map head of Feature Pyramid Network.  rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: Coordinate maps [batch, roi_count, height, width, num_classes, 3] """ x = PyramidROIAlign([pool_size, pool_size], image_shape, name= 'roi_align_coord')([rois] + feature_maps) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv1')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn1')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv2')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn2')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv3')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn3')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv4')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn4')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv5')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn5')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv6')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn6')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv7')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn7')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_conv8')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn8')(x) x = KL.Activation('relu')(x) feature_x = KL.TimeDistributed(KL.Conv2DTranspose(512, (2, 2), strides= 2, activation='relu'), name='mrcnn_coord_deconv')(x) x = KL.TimeDistributed(KL.Conv2D(512, (1, 1), strides=1, activation= 'relu'), name='mrcnn_coord_deeper')(feature_x) x = KL.TimeDistributed(KL.Conv2D(4 * num_classes, (1, 1), strides=1, activation='sigmoid'), name='mrcnn_coord_reshape')(x) x = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, 4]), name='mrcnn_coord')(x) mrcnn_mask = KL.Lambda(lambda x: x[:, :, :, :, :, (0)], name='mrcnn_mask')( x) mrcnn_coord_x = KL.Lambda(lambda x: x[:, :, :, :, :, (1)], name= 'mrcnn_coord_x')(x) mrcnn_coord_y = KL.Lambda(lambda x: x[:, :, :, :, :, (2)], name= 'mrcnn_coord_y')(x) mrcnn_coord_z = KL.Lambda(lambda x: x[:, :, :, :, :, (3)], name= 'mrcnn_coord_z')(x) return mrcnn_mask, mrcnn_coord_x, mrcnn_coord_y, mrcnn_coord_z, feature_x 
curriculum|test|check|paths|CurriculumEnvTest|env|order def _check_paths_order(self): for path, sorted_path in zip(self._env._paths, self._env._sorted_paths): self.assertEqual(path[constants.PATH_ID], sorted_path[constants. PATH_ID]) 
bert|BertForMultipleChoiceExtraction|pytorch|init|modeling|pretrained def __init__(self, config, num_choices, mlp_hidden_dim, mlp_dropout): super(BertForMultipleChoiceExtraction, self).__init__(config) self.num_choices = num_choices self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, 1) self.mlp_classifier = nn.Sequential(nn.Linear(config.hidden_size, mlp_hidden_dim * 2), nn.BatchNorm1d(mlp_hidden_dim * 2), nn. LeakyReLU(), nn.Dropout(mlp_dropout), nn.Linear(mlp_hidden_dim * 2, 1), nn.Sigmoid()) self.apply(self.init_bert_weights) 
Model|init|model|classification def __init__(self, config, input_dim, n_data, attack): """ Initialize a class Model. :param config: Configuration Bundle. :param input_dim: int :param n_data: int """ super().__init__(config) if config.optimizer == 'ekfac': print('[!] Optimizer: EKFAC') self.layer_collection = lc.LayerCollection(mode='ekfac') elif config.optimizer == 'kfac': print('[!] Optimizer: KFAC') self.layer_collection = lc.LayerCollection(mode='kfac') elif config.optimizer == 'diag': print('[!] Optimizer: Diagonal Fisher') self.layer_collection = lc.LayerCollection(mode='diag') else: print('[!] Optimizer: {}'.format(config.optimizer)) self.layer_collection = None self.input_dim = input_dim self.n_data = n_data self.attack = attack self.re_init_kfac_scale_op = None self.cov_update_op = None self.inv_update_op = None self.eigen_basis_update_op = None self.scale_update_op = None self.var_update_op = None self.var_scale_update_op = None self.train_op = None self.inputs = None self.targets = None self.is_training = None self.n_particles = None self.sampler = None self.acc = None self.loss = None self.total_loss = None self.optim = None self.saver = None if self.attack: self.build_attack_model() else: self.build_model() self.init_optim() self.init_saver() for v in tf.trainable_variables(): print(v.get_shape().as_list()) print('Number of trainable variables: ', np.sum([np.prod(v.get_shape(). as_list()) for v in tf.trainable_variables()])) 
sequence|generate|sampler|TopKSampler def generate_sequence(self, batch, model, data_loader, start_idx, end_len): XMB = batch['sequences'][:, :start_idx] MMB = batch['attention_mask'][:, :start_idx] XMB = model_utils.prepare_position_embeddings(self.opt, data_loader. vocab_encoder, XMB.unsqueeze(-1)) lm_probs = F.log_softmax(model(XMB.unsqueeze(1), sequence_mask=MMB), dim=-1 ) values, indices = lm_probs[:, (-1), :].topk(self.opt.eval.k) seqs = indices.t().clone() losses = -values.view(-1, 1) ended = (seqs == self.end_token).float() counts = 1 - ended XMB = XMB.repeat(self.opt.eval.k, 1, 1) MMB = MMB.repeat(self.opt.eval.k, 1) next_pos = XMB[:, -1:, (1)] + 1 next_x = torch.cat((indices.view(self.opt.eval.k, -1), next_pos), -1 ).unsqueeze(1) XMB = torch.cat((XMB, next_x), 1) MMB = torch.cat([MMB, torch.ones(XMB.size(0), 1, device=MMB.device)], 1) for _ in range(end_len): lm_probs = F.log_softmax(model(XMB.unsqueeze(1), sequence_mask=MMB), dim=-1) values, indices = lm_probs[:, (-1), :].topk(self.opt.eval.k) choice = torch.multinomial(values.exp(), 1) next_idx = indices.gather(-1, choice) ended = ended + (next_idx == self.end_token).float() * (1 - ended) next_idx = next_idx * (1 - ended).long() + ended.long( ) * self.end_token counts += 1 - ended seqs = torch.cat([seqs, next_idx], 1) if ended.sum().item() == self.opt.eval.k: break losses -= values.gather(-1, choice) * (1 - ended) XMB, MMB = self.append_batch(XMB, next_idx, MMB) beams = [] for beam in seqs: beams.append(' '.join(''.join([data_loader.vocab_decoder[tok.item() ].replace('</w>', ' ').replace('\n', '') for tok in beam if tok != self.end_token]).split())) sampling_result = {'sequence': beams[0], 'beams': beams, 'beam_losses': losses.squeeze().tolist(), 'loss': losses[0].item(), 'beam_lengths': counts.long().squeeze().tolist(), 'length': counts[0].long().item()} return sampling_result 
nn|ops|conv|from|caps def caps_from_conv(op_name, in_tensor, cap_dims): with tf.name_scope(op_name): shape = in_tensor.get_shape().as_list() cap_count = shape[1] * shape[2] * shape[3] // cap_dims return tf.reshape(in_tensor, [-1, 1, cap_count, cap_dims], name= 'caps_shape_' + op_name) 
def|RESCAL|predict|models|embeddings|OpenKE def predict_def(self): config = self.get_config() predict_h, predict_t, predict_r = self.get_predict_instance() predict_h_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, predict_h), [-1, config.hidden_size, 1]) predict_t_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, predict_t), [-1, config.hidden_size, 1]) predict_r_e = tf.reshape(tf.nn.embedding_lookup(self.rel_matrices, predict_r), [-1, config.hidden_size, config.hidden_size]) self.predict = -tf.reduce_sum(self._calc(predict_h_e, predict_t_e, predict_r_e), 1, keep_dims=False) 
gan|DSpritesInceptionScore|metric|save def save(self): self.shape_network.save(self.sess, self.shape_network_path) 
envs|TestPendulum|reset|test def test_reset(self): """Ensure the state initialization is within the expected range.""" state = self.env.reset() num_obj = len(state) // 3 for i in range(num_obj): self.assertTrue(np.arccos(state[i]) >= self.env.initial_state_space [i][0]) self.assertTrue(np.arccos(state[i]) <= self.env.initial_state_space [i][1]) self.assertTrue(state[i + 2 * num_obj] >= self.env. initial_state_space[i + num_obj][0]) self.assertTrue(state[i + 2 * num_obj] <= self.env. initial_state_space[i + num_obj][1]) 
demo|dc|svae|main|env def main(args): env, max_episode_steps = make_env_from_args(args.env_name, args.seed) print('max_episode_steps', max_episode_steps) policy = make_policy_from_args(env, args.controller_class, max_episode_steps) scaled_policy_params_lst = None rnd_params_lst = None if args.policy_file is not None: policy_files = sorted(glob(os.path.expanduser(args.policy_file))) assert len(policy_files) > 0 scaled_policy_params_lst = [] rnd_params_lst = [] for fl in policy_files: print('Loading', fl) data = np.load(fl) if 'x_all' in data.keys(): x_all = data['x_all'][0:args.num_episodes] y_all = data['y_all'][0:args.num_episodes] rnd_params_all = None if 'rnd_params_all' in data.keys(): rnd_params_all = data['rnd_params_all'] best_id = y_all.argmax() print('y_all', y_all.reshape(-1)) print('x_all shape', x_all.shape, 'y_all shape', y_all.shape) if rnd_params_all is not None: print('rnd_params_all', rnd_params_all.shape) if x_all.shape[0] >= 10: scaled_policy_params_lst.append(x_all[(best_id), :]) if rnd_params_all is not None: rnd_params_lst.append(rnd_params_all[(best_id), :]) print('best_y', y_all[best_id], 'best_id', best_id) else: for i in range(x_all.shape[0]): scaled_policy_params_lst.append(x_all[(i), :]) if rnd_params_all is not None: rnd_params_lst.append(rnd_params_all[(i), :]) elif 'x_buf' in data.keys(): x = data['x_buf'] rwd = data['rwd_buf'] rnd_params = None if 'rnd_params_buf' in data.keys(): rnd_params = data['rnd_params_buf'] goodness = 1 - data['bads_1toT_buf'].mean(axis=1) num_to_load = args.num_episodes if x.shape[0] < num_to_load: num_to_load = x.shape[0] for i in range(num_to_load): scaled_policy_params_lst.append(x[(i), :]) if rnd_params is not None: rnd_params_lst.append(rnd_params[(i), :]) print('loaded {:d} policies from {:s}'.format(num_to_load, fl)) print('rwd', rwd[0:num_to_load]) print('goodness', goodness[0:num_to_load]) print('rnd_params_lst', rnd_params_lst) else: print('x_all or x_buf in data should specify controllers') assert False assert len(scaled_policy_params_lst) > 0 if len(rnd_params_lst) > 0: assert len(scaled_policy_params_lst) == len(rnd_params_lst) args.num_episodes = len(scaled_policy_params_lst) print('scaled_policy_params_lst', scaled_policy_params_lst) play(env, policy, args.num_episodes, args.num_randomized, scaled_policy_params_lst, rnd_params_lst, resample_policy_params=True) env.close() 
devtools|cleverhans|tests|get|errors|NumpyModuleDocString|docscrape def get_errors(self): errors = NumpyDocString.get_errors(self, check_order=False) return errors 
get|PCGNModel|model|context|PCGN def get_context(self, query, keys, blog_desc_inetract, batch_size, num_units): query = tf.matmul(tf.reshape(query, (batch_size, num_units)), blog_desc_inetract) query = array_ops.expand_dims(query, 1) score = math_ops.matmul(query, keys, transpose_b=True) score = array_ops.squeeze(score, [1]) alignments = nn_ops.softmax(score) expanded_alignments = array_ops.expand_dims(alignments, 1) context = math_ops.matmul(expanded_alignments, keys) context = array_ops.squeeze(context, [1]) return context 
texar|tf|modules|decoders|TrainingHelper|sequence|length|helpers @property def sequence_length(self): return self._sequence_length 
networks|pool|elpips|vgg16|op def op(X): return tf.nn.max_pool(X, ksize=ksize, strides=strides, padding=padding, name=name, data_format=data_format) 
AccuracyFormatter|pct|get|formatter|accuracy|format|common def format_pct(self, x, display_range): x = self.convert_to_pct(x) if self.decimals is None: scaled_range = self.convert_to_pct(display_range) if scaled_range <= 0: decimals = 0 else: decimals = math.ceil(2.0 - math.log10(2.0 * scaled_range)) if decimals > 5: decimals = 5 elif decimals < 0: decimals = 0 else: decimals = self.decimals s = '{{x:{}0.{{decimals}}f}}'.format('+' if is_delta and abs(x) > 0 else '' ).format(x=x, decimals=int(decimals)) return s + self.symbol 
upsample|sobolev|conv|cifar1 def upsample_conv(x, **kwargs): return apply_conv(upsample(x), **kwargs) 
contributed|face|setup|mtcnn|Detection|master|facenet def _setup_mtcnn(self): with tf.Graph().as_default(): gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction= gpu_memory_fraction) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)) with sess.as_default(): return align.detect_face.create_mtcnn(sess, None) 
2|go|train|inception|v3|baseline def go(start_epoch, end_epoch, run_name, weights_file, profile_compute_time_every_n_steps, save_summary_info_every_n_steps, log_annotated_images, image_size, batch_size, num_gpus, data_dir, black_list_file, log_dir, do_validate_all, do_validate_nbl): tf.reset_default_graph() out = Output(log_dir, run_name, profile_compute_time_every_n_steps, save_summary_info_every_n_steps) out.log_msg('Setting up data feeds...') training_dataset = DataSet('train', image_size, batch_size, num_gpus, data_dir, None) validation_dataset = DataSet('validation', image_size, batch_size, num_gpus, data_dir, black_list_file) training_data = train_inputs(training_dataset, log_annotated_images) validation_data = eval_inputs(validation_dataset, log_annotated_images) nbl_val_data = non_blacklisted_eval_inputs(validation_dataset, log_annotated_images) training_steps = training_dataset.training_batches_per_epoch() validation_steps = validation_dataset.validation_batches_per_epoch() nbl_val_steps = validation_dataset.nbl_validation_batches_per_epoch() with tf.device('/device:CPU:0'): with tf.name_scope('input/placeholders'): is_training_ph = tf.placeholder(tf.bool) is_validating_nbl_ph = tf.placeholder(tf.bool) global_step = tf.train.get_or_create_global_step() decay_steps = int(training_steps * 30.0) learning_rate_op = tf.train.exponential_decay(0.1, global_step, decay_steps, 0.16, staircase=True) opt = tf.train.RMSPropOptimizer(learning_rate_op, decay=0.9, momentum=0.9, epsilon=1.0) train_op, loss_op, acc_top_1_op, acc_top_5_op = run_towers(opt, global_step, is_training_ph, is_validating_nbl_ph, training_data, validation_data, nbl_val_data, DataSet. num_classes(), num_gpus) out.log_msg('Starting Session...') with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess: out.set_session_graph(sess.graph) if weights_file is not None: out.log_msg('Restoring weights file: {}'.format(weights_file)) tf.train.Saver().restore(sess, weights_file) else: tf.global_variables_initializer().run() coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) try: for e in range(start_epoch, end_epoch + 1): train(out, sess, e, training_steps, train_op, loss_op, global_step, learning_rate_op, is_training_ph, is_validating_nbl_ph) if do_validate_all: validate(out, sess, e, validation_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, learning_rate_op, is_training_ph, is_validating_nbl_ph) if do_validate_nbl: validate_nbl(out, sess, e, nbl_val_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, learning_rate_op, is_training_ph, is_validating_nbl_ph) except tf.errors.OutOfRangeError: out.log_msg('Finished.') finally: coord.request_stop() coord.join(threads) sess.close() out.close_files() 
set|ent|embeddings|OpenKE|Config|config|rate|neg def set_ent_neg_rate(self, rate): self.negative_ent = rate 
parameters|common|get|prune def _get_parameters(kernel_size, input_channel, output_channel): params = 0 params += kernel_size[0] * kernel_size[1 ] * input_channel * output_channel + output_channel return params 
rnn|texar|GumbelSoftmaxEmbeddingHelper|decoder|modules|decoders|sample|helpers def sample(self, time, outputs, state, name=None): """Returns `sample_id` of shape `[batch_size, vocab_size]`. If `straight_through` is False, this is gumbel softmax distributions over vocabulary with temperature `tau`. If `straight_through` is True, this is one-hot vectors of the greedy samples. """ sample_ids = tf.nn.softmax(outputs / self._tau) sample_ids = GumbelSoftmax(self._tau, logits=outputs).sample() if self._straight_through: size = tf.shape(sample_ids)[-1] sample_ids_hard = tf.cast(tf.one_hot(tf.argmax(sample_ids, -1), size), sample_ids.dtype) sample_ids = tf.stop_gradient(sample_ids_hard - sample_ids ) + sample_ids return sample_ids 
utils|uicm|uqim def _uicm(x): R = x[:, :, (0)].flatten() G = x[:, :, (1)].flatten() B = x[:, :, (2)].flatten() RG = R - G YB = (R + G) / 2 - B mu_a_RG = mu_a(RG) mu_a_YB = mu_a(YB) s_a_RG = s_a(RG, mu_a_RG) s_a_YB = s_a(YB, mu_a_YB) l = math.sqrt(math.pow(mu_a_RG, 2) + math.pow(mu_a_YB, 2)) r = math.sqrt(s_a_RG + s_a_YB) return -0.0268 * l + 0.1586 * r 
prepare|embeddings|utils def prepare_embeddings(): """ Prepares fastText embeddings (available at https://fasttext.cc/docs/en/english-vectors.html) for use in the model. Function expects unarchived fastText embedding file. """ file_in = io.open(cnf.embedding_file, 'r', encoding='utf-8', newline= '\n', errors='ignore') n, d = list(map(int, file_in.readline().split(' '))) print('Preparing embedding with dimensions:', n, d) word_to_id = {'C_PAD': 0, 'C_UNK': 1} word_to_vec = [['0' for _ in range(d)], [str(random.random()) for _ in range(d)]] word_id = 2 with open(cnf.emb_vector_file, 'w') as vector_file: for line in file_in: tokens = line.rstrip().split() word_to_vec.append(tokens[1:]) word_to_id[tokens[0]] = word_id if word_id % 100000 == 0: vector_file.writelines([(' '.join(word) + '\n') for word in word_to_vec]) word_to_vec = [] print('Done with', word_id, 'word') word_id += 1 vector_file.writelines([(' '.join(word) + '\n') for word in word_to_vec]) with open(cnf.emb_word_dictionary, 'wb') as id_out: pickle.dump(word_to_id, id_out) print('Pickled word dictionary') vector_file = np.loadtxt(cnf.emb_vector_file, dtype=np.float) with open(cnf.emb_vector_file, 'wb') as emb_file_bin: pickle.dump(vector_file, emb_file_bin) print('Pickled embedding') 
bert|printable|tokenization|master|text def printable_text(text): """Returns text encoded in a way suitable for print or `tf.logging`.""" if six.PY3: if isinstance(text, str): return text elif isinstance(text, bytes): return text.decode('utf-8', 'ignore') else: raise ValueError('Unsupported string type: %s' % type(text)) elif six.PY2: if isinstance(text, str): return text elif isinstance(text, unicode): return text.encode('utf-8') else: raise ValueError('Unsupported string type: %s' % type(text)) else: raise ValueError('Not running on Python2 or Python 3?') 
tangents|empirical|fn|ntk|neural|vjp|jvp|delta|utils|implicit def delta_vjp_jvp(delta):  def delta_vjp(delta): return vjp(lambda p: f(p, x2), params)[1](delta) return jvp(lambda p: f(p, x1), (params,), delta_vjp(delta))[1] 
pool|nasnet|avg|nets|utils|global @tf.contrib.framework.add_arg_scope def global_avg_pool(x, data_format=INVALID): """Average pool away the height and width spatial dimensions of x.""" assert data_format != INVALID assert data_format in ['NHWC', 'NCHW'] assert x.shape.ndims == 4 if data_format == 'NHWC': return tf.reduce_mean(x, [1, 2]) else: return tf.reduce_mean(x, [2, 3]) 
fc|fusion|avod|core|layers|build def build(fc_layers_config, input_rois, input_weights, num_final_classes, box_rep, is_training, end_points_collection): """Builds fusion layers  Args: fc_layers_config: Fully connected layers config object input_rois: List of input roi feature maps input_weights: List of weights for each input e.g. [1.0, 1.0] num_final_classes: Final number of output classes, including 'Background' box_rep: Box representation (e.g. 'box_3d', 'box_8c', 'box_4c') is_training: Whether the network is training or evaluating end_points_collection: End points collection to add entries to  Returns: cls_logits: Output classification logits offsets: Output offsets angle_vectors: Output angle vectors (or None) end_points: End points dict """ fusion_type = fc_layers_config.fusion_type fusion_method = fc_layers_config.fusion_method num_layers = fc_layers_config.num_layers layer_sizes = fc_layers_config.layer_sizes l2_weight_decay = fc_layers_config.l2_weight_decay keep_prob = fc_layers_config.keep_prob if not len(input_weights) == len(input_rois): raise ValueError( 'Length of input_weights does not match length of input_rois') if not len(layer_sizes) == num_layers: raise ValueError('Length of layer_sizes does not match num_layers') if fusion_type == 'early': cls_logits, offsets, angle_vectors = _early_fusion_fc_layers(num_layers =num_layers, layer_sizes=layer_sizes, input_rois=input_rois, input_weights=input_weights, fusion_method=fusion_method, l2_weight_decay=l2_weight_decay, keep_prob=keep_prob, num_final_classes=num_final_classes, box_rep=box_rep, is_training=is_training) elif fusion_type == 'late': cls_logits, offsets, angle_vectors = _late_fusion_fc_layers(num_layers =num_layers, layer_sizes=layer_sizes, input_rois=input_rois, input_weights=input_weights, fusion_method=fusion_method, l2_weight_decay=l2_weight_decay, keep_prob=keep_prob, num_final_classes=num_final_classes, box_rep=box_rep, is_training=is_training) elif fusion_type == 'deep': cls_logits, offsets, angle_vectors = _deep_fusion_fc_layers(num_layers =num_layers, layer_sizes=layer_sizes, input_rois=input_rois, input_weights=input_weights, fusion_method=fusion_method, l2_weight_decay=l2_weight_decay, keep_prob=keep_prob, num_final_classes=num_final_classes, box_rep=box_rep, is_training=is_training) else: raise ValueError('Invalid fusion type {}'.format(fusion_type)) end_points = slim.utils.convert_collection_to_dict(end_points_collection) return cls_logits, offsets, angle_vectors, end_points 
texar|scalar|test|and|ScalarDataTest|data|run def _run_and_test(self, hparams): scalar_data = tx.data.ScalarData(hparams) self.assertEqual(scalar_data.list_items()[0], hparams['dataset'][ 'data_name']) iterator = scalar_data.dataset.make_initializable_iterator() data_batch = iterator.get_next() with self.test_session() as sess: sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer()) sess.run(iterator.initializer) i = 0 while True: try: data_batch_ = sess.run(data_batch) self.assertEqual(set(data_batch_.keys()), set(scalar_data. list_items())) value = data_batch_[scalar_data.data_name][0] self.assertEqual(i, value) i += 1 if hparams['dataset']['data_type'] == 'int': self.assertTrue(isinstance(value, np.int32)) else: self.assertTrue(isinstance(value, np.float32)) except tf.errors.OutOfRangeError: print('Done -- epoch limit reached') break 
block|1|test|official|BaseTest|layer|resnet def test_block_1(self): self._resnet_block_ops(test=True, batch_size=BATCH_SIZE, **BLOCK_TESTS[1]) 
bayesian|buildnet|regression|init|controller|learning|BayesianLearning @zs.reuse('buildnet') def buildnet(observed): """ Get the BayesianNet instance and output of the bayesian neural network with some nodes observed. :param observed: dict of (str, tensor). Representing the mapping from node name to value. :return: BayesianNet instance. """ with zs.BayesianNet(observed=observed) as model: y_pred, h_pred = self._net.predict(self.x, self.n_particles) return model, y_pred, h_pred 
change|words|colors|closest|to|colour def closest_colour(requested_colour): min_colours = {} for key, name in webcolors.css3_hex_to_names.items(): r_c, g_c, b_c = webcolors.hex_to_rgb(key) rd = (r_c - requested_colour[0]) ** 2 gd = (g_c - requested_colour[1]) ** 2 bd = (b_c - requested_colour[2]) ** 2 min_colours[rd + gd + bd] = name return min_colours[min(min_colours.keys())] 
process|str|clean|data def clean_str(string): string = re.sub("[^A-Za-z0-9(),!?\\'`]", ' ', string) string = re.sub("\\'s", " 's", string) string = re.sub("\\'ve", " 've", string) string = re.sub("n\\'t", " n't", string) string = re.sub("\\'re", " 're", string) string = re.sub("\\'d", " 'd", string) string = re.sub("\\'ll", " 'll", string) string = re.sub(',', ' , ', string) string = re.sub('!', ' ! ', string) string = re.sub('\\(', ' \\( ', string) string = re.sub('\\)', ' \\) ', string) string = re.sub('\\?', ' \\? ', string) string = re.sub('\\s{2,}', ' ', string) return string.strip().lower() 
contributed|align|master|facenet|cluster|data def align_data(image_list, image_size, margin, pnet, rnet, onet): minsize = 20 threshold = [0.6, 0.7, 0.7] factor = 0.709 img_list = [] for x in xrange(len(image_list)): img_size = np.asarray(image_list[x].shape)[0:2] bounding_boxes, _ = align.detect_face.detect_face(image_list[x], minsize, pnet, rnet, onet, threshold, factor) nrof_samples = len(bounding_boxes) if nrof_samples > 0: for i in xrange(nrof_samples): if bounding_boxes[i][4] > 0.95: det = np.squeeze(bounding_boxes[(i), 0:4]) bb = np.zeros(4, dtype=np.int32) bb[0] = np.maximum(det[0] - margin / 2, 0) bb[1] = np.maximum(det[1] - margin / 2, 0) bb[2] = np.minimum(det[2] + margin / 2, img_size[1]) bb[3] = np.minimum(det[3] + margin / 2, img_size[0]) cropped = image_list[x][bb[1]:bb[3], bb[0]:bb[2], :] aligned = misc.imresize(cropped, (image_size, image_size), interp='bilinear') prewhitened = facenet.prewhiten(aligned) img_list.append(prewhitened) if len(img_list) > 0: images = np.stack(img_list) return images else: return None 
speak|darkflow|ops|net|simple|connected def speak(self): layer = self.lay args = [layer.inp, layer.out] args += [layer.activation] msg = 'full {} x {}  {}' return msg.format(*args) 
tangents|get|stax|neural|covariance def _get_covariance(x1, x2, marginal_type): """Computes uncentred covariance (nngp) between two sets of inputs  Args: x1: a (2+k)D (k = 0, 2) `np.ndarray` of shape `[n1, <k inner dimensions>, n_features]`. x2: an optional `np.ndarray` that has the same shape as `a` apart from possibly different leading (`n2`) dimension. `None` means `x2 == x1`. marginal_type: an instance of `Marginalisation` specifying between which dimensions should the covariances be computed.  Returns: an `np.ndarray` with uncentred batch covariance with shape `[n1, n2]` `+ [<k inner dimensions>]` (if `covar_type` is `OVER_PIXELS`) `+ [<k inner dimensions>, <k spatial dimensions>]` (if `covar_type` is `OVER_POINTS` or `NO`) """ x2 = x1 if x2 is None else x2 if marginal_type in (M.OVER_ALL, M.OVER_PIXELS): ret = np.matmul(np.moveaxis(x1, 0, -2), np.moveaxis(x2, 0, -1)) ret = np.moveaxis(ret, (-2, -1), (0, 1)) elif marginal_type in (M.OVER_POINTS, M.NO): ret = np.squeeze(np.dot(x1, x2[..., None]), -1) ret = np.transpose(ret, (0, 3, 1, 4, 2, 5)) else: raise NotImplementedError( 'Only implemented for `OVER_ALL`, `OVER_PIXELS`, `OVER_POINTS` and `NO`; supplied {}' .format(marginal_type)) return ret / x1.shape[-1] 
house|parser|Region|init def __init__(self, region_line): parts = region_line.strip().split() assert 'R' == parts[0] assert self.NUM_TOKENS == len(parts) self.index = int(parts[self.INDEX_LOC]) self.level_index = int(parts[self.LEVEL_LOC]) self.label = parts[self.LABEL_LOC] self.center = float(parts[self.PX_LOC]), float(parts[self.PY_LOC]), float( parts[self.PZ_LOC]) 
add|mylogger|log|root|logging|level|to def log_to_root(message, *args, **kwargs): logging.log(levelNum, message, *args, **kwargs) 
fisher|ops|var|scope|classification|factors|ConvInputEigenBasisKroneckerFactor @property def _var_scope(self): return 'ff_convin_eigen_basis/' + scope_string_from_params([self. _inputs, self._filter_shape, self._strides, self._padding, self. _has_bias]) 
Bytewurst|europilot|joystick|hexLE @property def hexLE(self): return hexlify(self.raw) 
evaluate|onehot|shapenet def evaluate(): testset = input_fn(testlist, BATCH_SIZE, 10000) test_iterator = testset.make_initializable_iterator() next_test_element = test_iterator.get_next() with tf.device('/gpu:0'): xyz_pl, label_pl, cls_label_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT) training_pl = tf.placeholder(tf.bool, shape=()) pred, end_points = MODEL.get_model(xyz_pl, cls_label_pl, NUM_CLASSES, training_pl, config=net_config) MODEL.get_loss(pred, label_pl, end_points) if net_config.weight_decay is not None: reg_loss = tf.multiply(tf.losses.get_regularization_loss(), net_config.weight_decay, name='reg_loss') tf.add_to_collection('losses', reg_loss) losses = tf.get_collection('losses') total_loss = tf.add_n(losses, name='total_loss') saver = tf.train.Saver() n = len([n.name for n in tf.get_default_graph().as_graph_def().node]) print('*****************The Graph has %d nodes*****************' % n) config = tf.ConfigProto() config.gpu_options.allow_growth = True config.allow_soft_placement = True config.log_device_placement = False with tf.Session(config=config) as sess: ops = {'xyz_pl': xyz_pl, 'label_pl': label_pl, 'cls_label_pl': cls_label_pl, 'training_pl': training_pl, 'pred': pred, 'loss': total_loss} saver.restore(sess, os.path.join(LOG_DIR, FLAGS.model_name)) sess.run(test_iterator.initializer) eval_one_epoch(sess, ops, next_test_element) 
get|avod|indices|models|AvodModel|model|core|box|build def get_box_indices(boxes): proposals_shape = boxes.get_shape().as_list() if any(dim is None for dim in proposals_shape): proposals_shape = tf.shape(boxes) ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32) multiplier = tf.expand_dims(tf.range(start=0, limit=proposals_shape[0]), 1) return tf.reshape(ones_mat * multiplier, [-1]) 
v1|test|InceptionV1Test|inception|nets|testUnknowBatchSize def testUnknowBatchSize(self): batch_size = 1 height, width = 224, 224 num_classes = 1000 inputs = tf.placeholder(tf.float32, (None, height, width, 3)) logits, _ = inception.inception_v1(inputs, num_classes) self.assertTrue(logits.op.name.startswith('InceptionV1/Logits')) self.assertListEqual(logits.get_shape().as_list(), [None, num_classes]) images = tf.random_uniform((batch_size, height, width, 3)) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) output = sess.run(logits, {inputs: images.eval()}) self.assertEquals(output.shape, (batch_size, num_classes)) 
CRPM|init|Net def __init__(self, batch_size, image_size, raw_path, num_label, regularation_rate, learning_rate, logger_path, decay_rate, is_training, data_file, label_file, dev_data_file, clip, num_epochs, model_path): """ standard:   training standard. 1: Cs-CNN 2: CRPM-Net channel:   Input data channel. 18: L,P,C-band complex_cross-value [C]. 27: L,P,C-band real-value [C]. filter_size: Base convlution kernel size features_root: the channel number of first convolution layer image_size: Sliding window size for CRPM-Net stride: Slideing wondow size for CRPM-Net """ self.data_path = data_file self.label_path = label_file self.dev_data_path = dev_data_file self.batch_size = batch_size self.image_size = image_size self.num_label = num_label self.model_path = model_path self.clip = clip self.standard = 2 self.layers = 3 self.filter_size = 3 self.channels = 18 self.features_root = 12 self.num_epochs = num_epochs self.learning_rate = learning_rate self.decay_rate = decay_rate self.is_training = is_training self.stride = 64 self.raw_path = raw_path self.image_size = 128 self.logger = get_logger(logger_path) self.regularizer = tf.contrib.layers.l2_regularizer(0.0001) 
embed|instances|WordEmbedding|datacode|ner def embed_instances(self, instances): for instance in instances: self.embed_instance(instance) 
StochasticBlock|init|nn|stochastic def __init__(self, nChannels, growthRate, nLayers, drop_rate, after_norm_type, droppath_rate): super(StochasticBlock, self).__init__() self.growthRate = growthRate self.drop_rate = drop_rate self.droppath_rate = droppath_rate self.nLayers = nLayers self.preprocess = BNReLUConv(nChannels, growthRate * nLayers, 1, 1, 0, affine=True, groups=nLayers, after_norm_type=after_norm_type) layers = [] for i in range(1, int(nLayers)): layers.append(Layer(growthRate, i, drop_rate, after_norm_type, droppath_rate)) self.layers = nn.ModuleList(layers) 
init|vnet|ConvBlock def __init__(self, n_stages, n_filters_in, n_filters_out, normalization='none' ): super(ConvBlock, self).__init__() ops = [] for i in range(n_stages): if i == 0: input_channel = n_filters_in else: input_channel = n_filters_out ops.append(nn.Conv3d(input_channel, n_filters_out, 3, padding=1)) if normalization == 'batchnorm': ops.append(nn.BatchNorm3d(n_filters_out)) elif normalization == 'groupnorm': ops.append(nn.GroupNorm(num_groups=16, num_channels=n_filters_out)) elif normalization == 'instancenorm': ops.append(nn.InstanceNorm3d(n_filters_out)) elif normalization != 'none': assert False ops.append(nn.ReLU(inplace=True)) self.conv = nn.Sequential(*ops) 
blur|amb|measure|utils def blur(hparams, x): size = hparams.blur_filter_size gaussian_filter = get_gaussian_filter(hparams.blur_radius, size) gaussian_filter = np.reshape(gaussian_filter, [size, size, 1, 1]) x_blurred_list = [] for i in range(hparams.image_dims[-1]): x_blurred = tf.nn.conv2d(x[:, :, :, i:i + 1], gaussian_filter, strides=[1, 1, 1, 1], padding='SAME') x_blurred_list.append(x_blurred) x_blurred = tf.concat(x_blurred_list, axis=3) return x_blurred 
auxiliary|models|dien|net|deepctr def auxiliary_net(in_, stag='auxiliary_net'): bn1 = tf.layers.batch_normalization(inputs=in_, name='bn1' + stag, reuse=tf.AUTO_REUSE) dnn1 = tf.layers.dense(bn1, 100, activation=None, name='f1' + stag, reuse=tf.AUTO_REUSE) dnn1 = tf.nn.sigmoid(dnn1) dnn2 = tf.layers.dense(dnn1, 50, activation=None, name='f2' + stag, reuse=tf.AUTO_REUSE) dnn2 = tf.nn.sigmoid(dnn2) dnn3 = tf.layers.dense(dnn2, 1, activation=None, name='f3' + stag, reuse=tf.AUTO_REUSE) y_hat = tf.nn.sigmoid(dnn3) return y_hat 
TanhLayer|forward|layers def forward(self, x): """Forward propagation  Arguments: x {tf tensor} -- x_{t}  Returns: x {tf tensor} -- x_{t+1} """ return tf.nn.tanh(x) 
tmp|visstd|master|facenet|main|deepdream def visstd(a, s=0.1): """Normalize the image range for visualization""" return (a - a.mean()) / max(a.std(), 0.0001) * s + 0.5 
at|preprocessing|size|vgg|smallest|least def _smallest_size_at_least(height, width, smallest_side): """Computes new shape with the smallest side equal to `smallest_side`.  Computes new shape with the smallest side equal to `smallest_side` while preserving the original aspect ratio.  Args: height: an int32 scalar tensor indicating the current height. width: an int32 scalar tensor indicating the current width. smallest_side: A python integer or scalar `Tensor` indicating the size of the smallest side after resize.  Returns: new_height: an int32 scalar tensor indicating the new height. new_width: and int32 scalar tensor indicating the new width. """ smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32) height = tf.to_float(height) width = tf.to_float(width) smallest_side = tf.to_float(smallest_side) scale = tf.cond(tf.greater(height, width), lambda : smallest_side / width, lambda : smallest_side / height) new_height = tf.to_int32(tf.rint(height * scale)) new_width = tf.to_int32(tf.rint(width * scale)) return new_height, new_width 
getter|tangents|get|decorator|namedtuple|neural|utils def getter_decorator(fn): try: argspec = _argspec(fn) get_index = argspec.args.index('get') defaults = argspec.defaults except: raise ValueError( '`get_namedtuple` functions must have a `get` argument.')  @wraps(fn) def getter_fn(*args, **kwargs): canonicalized_args = list(args) if 'get' in kwargs: get_is_not_tuple, get = canonicalize_get(kwargs['get']) kwargs['get'] = get elif get_index < len(args): get_is_not_tuple, get = canonicalize_get(args[get_index]) canonicalized_args[get_index] = get elif defaults is None: raise ValueError( '`get_namedtuple` function must have a `get` argument provided orset by default.' ) else: get_is_not_tuple, get = canonicalize_get(defaults[get_index - len(args)]) fn_out = fn(*canonicalized_args, **kwargs) if get is None: if isinstance(fn_out, dict): ReturnType = named_tuple_factory(name, tuple(fn_out.keys())) fn_out = ReturnType(*fn_out.values()) return fn_out fn_out = _output_to_dict(fn_out) if get_is_not_tuple: if isinstance(fn_out, types.GeneratorType): return (output[get[0]] for output in fn_out) else: return fn_out[get[0]] ReturnType = named_tuple_factory(name, get) if isinstance(fn_out, types.GeneratorType): return (ReturnType(*tuple(output[g] for g in get)) for output in fn_out) else: return ReturnType(*tuple(fn_out[g] for g in get)) return getter_fn 
ce|training|loss|masked|utils def masked_ce_loss(y_pred, y_true, void_class=11.0, weight=None, reduce= True, return_mask=False): """ y_pred: output predictded probabilities Y_true: true labels void_class: background/void class label weight: if weights are use reduce: if reduction is appleid to final loss  masked version of crossentropy loss ported over from the masked version in https://github.com/SimJeg/FC-DenseNet/blob/master/metrics.py """ el = torch.ones_like(y_true) * void_class mask = torch.ne(y_true, el).long() y_true_tmp = y_true * mask loss = F.cross_entropy(y_pred, y_true_tmp, weight=weight, reduction='none') loss = mask.float() * loss if reduce: if return_mask: return loss.sum() / mask.sum(), mask.sum() else: return loss.sum() / mask.sum() else: return loss, mask 
dstc2|dstc|utilities|plato|parser|parse|Parser|encode|action def encode_action_dstc(self, actions, system=True): """ Endoce the dialogue actions - specific for DSTC2  :param actions: :param system: :return: """ if not actions: print( 'WARNING: Parse DSTC2 action encoding called with empty actions list (returning 0).' ) return 0 action = actions[0] if self.dstc2_acts and action.intent in self.dstc2_acts: return self.dstc2_acts.index(action.intent) if action.intent == 'request': if system and action.params[0].slot in self.system_requestable_slots: return len(self.dstc2_acts) + self.system_requestable_slots.index( action.params[0].slot) elif action.params[0].slot in self.requestable_slots: return len(self.dstc2_acts) + self.requestable_slots.index(action .params[0].slot) if action.intent == 'inform' and action.params[0 ].slot in self.requestable_slots: if system: return len(self.dstc2_acts) + len(self.system_requestable_slots ) + self.requestable_slots.index(action.params[0].slot) else: return len(self.dstc2_acts) + len(self.requestable_slots ) + self.requestable_slots.index(action.params[0].slot) print( 'Parse DSTC2 action encoder warning: Selecting default action (unable to encode: {0})!' .format(action)) return 0 
attention|nmt|LuongMonotonicAttention|utils|call def __call__(self, query, state): """Score the query based on the keys and values.  Args: query: Tensor of dtype matching `self.values` and shape `[batch_size, query_depth]`. state: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`).  Returns: alignments: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). """ with variable_scope.variable_scope(None, 'luong_monotonic_attention', [ query]): score = _luong_score(query, self._keys, self._scale) score_bias = variable_scope.get_variable('attention_score_bias', dtype=query.dtype, initializer=self._score_bias_init) score += score_bias alignments = self._probability_fn(score, state) next_state = alignments return alignments, next_state 
DifferentialEvolutionSolver|rand2|models|differential|evolution def _rand2(self, samples): """ rand2bin, rand2exp """ r0, r1, r2, r3, r4 = samples bprime = self.population[r0] + self.scale * (self.population[r1] + self .population[r2] - self.population[r3] - self.population[r4]) return bprime 
generator|pair|pairs|nprf|drmm|count|utils|NPRFDRMMPairGenerator def count_pairs(self, qid_list, sample_size):  def count_on_topic(neg_len, pos_len, sample_size): sample_size = min(neg_len, sample_size) if sample_size == 0: return 0 else: return pos_len * sample_size total = 0 for qid in qid_list: relevance = self.relevance_dict.get(qid) relevance_posting = relevance.get_judged_docid_list() res = relevance.get_supervised_docid_list() if len(res) < self.nb_supervised_doc: pass else: rel_0, rel_1, rel_2 = relevance_posting[0], relevance_posting[1 ], relevance_posting[2] count_01 = count_on_topic(len(rel_0), len(rel_1), sample_size) count_12 = count_on_topic(len(rel_1), len(rel_2), sample_size) count = min(self.sample_perquery_limit, count_01 + count_12) total += count total = min(self.sample_total_limit, total) return total 
dimensionality|PCA|reduction def PCA(matrix, n): pca = decomposition.PCA(n_components=n) X_train = matrix - np.mean(matrix) return pca.fit_transform(X_train) 
texar|size|tf|CustomHelper|modules|decoders|batch|helpers @property def batch_size(self): if self._batch_size is None: raise ValueError('batch_size accessed before initialize was called') return self._batch_size 
texar|utils|combined|fn|make|data|filter|dataset def _combined_fn(data): outputs = [] for fn in filter_fns: if fn: outputs.append(fn(data)) if mode == 'and': return tf.reduce_all(outputs) elif mode == 'or': return tf.reduce_any(outputs) else: raise ValueError('Unknown mode: {}'.format(mode)) 
models|DeepFM|deepfm|deepctr def DeepFM(feature_dim_dict, embedding_size=8, use_fm=True, dnn_hidden_units=(128, 128), l2_reg_linear=1e-05, l2_reg_embedding= 1e-05, l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task='binary'): """Instantiates the DeepFM Network architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param use_fm: bool,use FM part or not :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN :param l2_reg_linear: float. L2 regularizer strength applied to linear part :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_dnn: float. L2 regularizer strength applied to DNN :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param dnn_activation: Activation function to use in DNN :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) deep_emb_list, linear_emb_list, dense_input_dict, inputs_list = ( preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True)) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) fm_input = concat_fun(deep_emb_list, axis=1) deep_input = tf.keras.layers.Flatten()(fm_input) fm_out = FM()(fm_input) deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed)(deep_input) deep_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)( deep_out) if len(dnn_hidden_units) == 0 and use_fm == False: final_logit = linear_logit elif len(dnn_hidden_units) == 0 and use_fm == True: final_logit = tf.keras.layers.add([linear_logit, fm_out]) elif len(dnn_hidden_units) > 0 and use_fm == False: final_logit = tf.keras.layers.add([linear_logit, deep_logit]) elif len(dnn_hidden_units) > 0 and use_fm == True: final_logit = tf.keras.layers.add([linear_logit, fm_out, deep_logit]) else: raise NotImplementedError output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
rnn|time|transpose|deepctr|batch|contrib def _transpose_batch_time(x): """Transpose the batch and time dimensions of a Tensor.    Retains as much of the static shape information as possible.    Args:  x: A tensor of rank 2 or higher.    Returns:  x transposed along the first two dimensions.    Raises:  ValueError: if `x` is rank 1 or lower.  """ x_static_shape = x.get_shape() if x_static_shape.ndims is not None and x_static_shape.ndims < 2: raise ValueError( 'Expected input tensor %s to have rank at least 2, but saw shape: %s' % (x, x_static_shape)) x_rank = array_ops.rank(x) x_t = array_ops.transpose(x, array_ops.concat(([1, 0], math_ops.range(2, x_rank)), axis=0)) x_t.set_shape(tensor_shape.TensorShape([x_static_shape[1].value, x_static_shape[0].value]).concatenate(x_static_shape[2:])) return x_t 
Model|conv|model def _conv(self, name, x, filter_size, in_filters, out_filters, strides): """Convolution.""" with tf.variable_scope(name): n = filter_size * filter_size * out_filters kernel = tf.get_variable('DW', [filter_size, filter_size, in_filters, out_filters], tf.float32, initializer=tf. random_normal_initializer(stddev=np.sqrt(2.0 / n))) return tf.nn.conv2d(x, kernel, strides, padding='SAME') 
texar|multi|MultiAlignedData|fn|aligned|length|make|data|bucket def _make_bucket_length_fn(self): length_fn = self._hparams.bucket_length_fn if not length_fn: i = -1 for i, hparams_i in enumerate(self._hparams.datasets): if _is_text_data(hparams_i['data_type']): break if i < 0: raise ValueError('Undefined `length_fn`.') length_fn = lambda x: x[self.length_name(i)] elif not is_callable(length_fn): length_fn = utils.get_function(length_fn, ['texar.custom']) return length_fn 
tests|test|sequence|layers|Transformer def test_Transformer(): with CustomObjectScope({'Transformer': sequence.Transformer}): layer_test(sequence.Transformer, kwargs={'att_embedding_size': 1, 'head_num': 8, 'use_layer_norm': True, 'supports_masking': False, 'dropout_rate': 0.5}, input_shape=[(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, 1), (BATCH_SIZE, 1)]) 
compute|output|sequence|deepctr|layers|Transformer|shape def compute_output_shape(self, input_shape): return None, 1, self.att_embedding_size * self.head_num 
strip|get|final|spaces|squad|run|text def _strip_spaces(text): ns_chars = [] ns_to_s_map = collections.OrderedDict() for i, c in enumerate(text): if c == ' ': continue ns_to_s_map[len(ns_chars)] = i ns_chars.append(c) ns_text = ''.join(ns_chars) return ns_text, ns_to_s_map 
BasicRNNDecoder|rnn|texar|size|output|modules|decoders @property def output_size(self): """Output size of one step. """ return BasicRNNDecoderOutput(logits=self._rnn_output_size(), sample_id= self._helper.sample_ids_shape, cell_output=self._cell.output_size) 
get|list|inputs|input|deepctr|embedding def get_inputs_list(inputs): return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs))))) 
batches|per|validation|input|DataSet|sieve|epoch def validation_batches_per_epoch(self): return int(math.ceil(VALIDATION_IMAGE_COUNT / self.batch_size)) 
predict|print|seq2paraphrase|data|Dataset|utils def print_predict_seq2paraphrase(self, output_dict, batch_dict, num_cases=3): """Print the predicted sentences, sequence to k sequence model (given a sentence, predict all k possible paraphrases)""" inputs = batch_dict['enc_inputs'][:3] references = batch_dict['references'][:3] for i in range(num_cases): print('inputs:') print('    ' + self.decode_sent(inputs[i])) pred_para = output_dict['enc_infer_pred'][i] print('paraphrase outputs:') for p in pred_para: print('    ' + self.decode_sent(p)) print('references:') for r in references[i]: print('    ' + self.decode_sent(r)) print('') return 
embed|models|decoder|attention|translate def embed(input_): embedded_input = tf.nn.embedding_lookup(embedding, input_) if decoder.use_dropout and decoder.word_keep_prob is not None: noise_shape = [1, 1] if decoder.pervasive_dropout else [tf.shape( input_)[0], 1] embedded_input = tf.nn.dropout(embedded_input, keep_prob=decoder. word_keep_prob, noise_shape=noise_shape) if decoder.use_dropout and decoder.embedding_keep_prob is not None: size = tf.shape(embedded_input)[1] noise_shape = [1, size] if decoder.pervasive_dropout else [tf.shape (input_)[0], size] embedded_input = tf.nn.dropout(embedded_input, keep_prob=decoder. embedding_keep_prob, noise_shape=noise_shape) return embedded_input 
triples|create|related|dict|find def create_triples_dict(): triples_dict = {} triple_to_id, id_to_triple = read_triple2id() for triple, triple_id in triple_to_id.items(): head_id = triple[0] tail_id = triple[1] if head_id not in triples_dict: triples_dict[head_id] = [] if tail_id not in triples_dict: triples_dict[tail_id] = [] triples_dict[head_id].append(triple_id) triples_dict[tail_id].append(triple_id) return triples_dict 
crossvalid|ml def crossvalid(train_df, test_df, clfs, run_index, fold_index): features_cols = train_df.columns.difference(['Drug1', 'Drug2', 'Class', 'Drug_x', 'Drug_y']) X = train_df[features_cols].values print(type(X)) y = train_df['Class'].values.ravel() X_new = test_df[features_cols].values y_new = test_df['Class'].values.ravel() results = pd.DataFrame() for name, clf in clfs: clf.fit(X, y) scores = get_scores(clf, X_new, y_new) scores['method'] = name scores['fold'] = fold_index scores['run'] = run_index results = results.append(scores, ignore_index=True) return results, X, y, X_new, y_new 
darkflow|postprocess|predict|net|yolo def postprocess(self, net_out, im, save=True): """ Takes net output, draw predictions, save to disk """ meta, FLAGS = self.meta, self.FLAGS threshold = FLAGS.threshold colors, labels = meta['colors'], meta['labels'] boxes = self.findboxes(net_out) if type(im) is not np.ndarray: imgcv = cv2.imread(im) else: imgcv = im h, w, _ = imgcv.shape resultsForJSON = [] for b in boxes: boxResults = self.process_box(b, h, w, threshold) if boxResults is None: continue left, right, top, bot, mess, max_indx, confidence = boxResults thick = int((h + w) // 300) if self.FLAGS.json: resultsForJSON.append({'label': mess, 'confidence': float( '%.2f' % confidence), 'topleft': {'x': left, 'y': top}, 'bottomright': {'x': right, 'y': bot}}) continue cv2.rectangle(imgcv, (left, top), (right, bot), self.meta['colors'] [max_indx], thick) cv2.putText(imgcv, mess, (left, top - 12), 0, 0.001 * h, self.meta[ 'colors'][max_indx], thick // 3) if not save: return imgcv outfolder = os.path.join(self.FLAGS.imgdir, 'out') img_name = os.path.join(outfolder, os.path.basename(im)) if self.FLAGS.json: textJSON = json.dumps(resultsForJSON) textFile = os.path.splitext(img_name)[0] + '.json' with open(textFile, 'w') as f: f.write(textJSON) return cv2.imwrite(img_name, imgcv) 
acwgangp|classifier|ACWGANGP def classifier(self, x, is_training=True, reuse=False): with tf.variable_scope('classifier', reuse=reuse): net = lrelu(bn(linear(x, 128, scope='c_fc1'), is_training= is_training, scope='c_bn1')) out_logit = linear(net, self.y_dim, scope='c_fc2') out = tf.nn.softmax(out_logit) return out, out_logit 
darkflow|ckpt|net|flow|save def _save_ckpt(self, step, loss_profile): file = '{}-{}{}' model = self.meta['name'] profile = file.format(model, step, '.profile') profile = os.path.join(self.FLAGS.backup, profile) with open(profile, 'wb') as profile_ckpt: pickle.dump(loss_profile, profile_ckpt) ckpt = file.format(model, step, '') ckpt = os.path.join(self.FLAGS.backup, ckpt) self.say('Checkpoint at step {}'.format(step)) self.saver.save(self.sess, ckpt) 
ProcessLine|cpplint def ProcessLine(filename, file_extension, clean_lines, line, include_state, function_state, nesting_state, error, extra_check_functions=[]): """Processes a single line in the file.  Args: filename: Filename of the file that is being processed. file_extension: The extension (dot not included) of the file. clean_lines: An array of strings, each representing a line of the file, with comments stripped. line: Number of line being processed. include_state: An _IncludeState instance in which the headers are inserted. function_state: A _FunctionState instance which counts function lines, etc. nesting_state: A NestingState instance which maintains information about the current stack of nested blocks being parsed. error: A callable to which errors are reported, which takes 4 arguments: filename, line number, error level, and message extra_check_functions: An array of additional check functions that will be run on each source line. Each function takes 4 arguments: filename, clean_lines, line, error """ raw_lines = clean_lines.raw_lines ParseNolintSuppressions(filename, raw_lines[line], line, error) nesting_state.Update(filename, clean_lines, line, error) CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line, error) if nesting_state.InAsmBlock(): return CheckForFunctionLengths(filename, clean_lines, line, function_state, error) CheckForMultilineCommentsAndStrings(filename, clean_lines, line, error) CheckStyle(filename, clean_lines, line, file_extension, nesting_state, error) CheckLanguage(filename, clean_lines, line, file_extension, include_state, nesting_state, error) CheckForNonConstReference(filename, clean_lines, line, nesting_state, error ) CheckForNonStandardConstructs(filename, clean_lines, line, nesting_state, error) CheckVlogArguments(filename, clean_lines, line, error) CheckPosixThreading(filename, clean_lines, line, error) CheckInvalidIncrement(filename, clean_lines, line, error) CheckMakePairUsesDeduction(filename, clean_lines, line, error) CheckDefaultLambdaCaptures(filename, clean_lines, line, error) CheckRedundantVirtual(filename, clean_lines, line, error) CheckRedundantOverrideOrFinal(filename, clean_lines, line, error) for check_fn in extra_check_functions: check_fn(filename, clean_lines, line, error) 
source|SRGANs|Mixed|master|score|inception|Regularization|GANs|call|Spectral def __call__(self, x): hs = [] for name, _ in self.trunk: h = getattr(self, name)(x) hs.append(h) return F.concat(hs) 
download|speech|corpus|DataArchive|file|move def _move_file(self, src, dest): """ Moves one file. """ if dest.exists() and self.user_option.force: dest.unlink() shutil.move(str(src), str(dest)) 
texar|get|core|optimizer|fn|opt|optimization def _get_opt(learning_rate=None): opt_kwargs = hparams['kwargs'].todict() fn_args = set(utils.get_args(opt_class.__init__)) if 'learning_rate' in fn_args and learning_rate is not None: opt_kwargs['learning_rate'] = learning_rate return opt_class(**opt_kwargs) 
with|len|and|graphs|csqa|paths|data|dataset def __len__(self): return self.n_samples 
fuc|src|img|aug def img_aug_fuc(img, kps, bboxs=None): """ function that used for data augmentation :param img: ori_image that was prepared to run aug, shape must be [h, w, c] :param kps: a ndarray that contains keypoints on this image. shape must be [person_num, joints_num, 3]. person_num means that there contains 'person_num' person on this image joints_num means that we want to detect how many joints. e.g. like 1 for single head point or 14 for all body points in ai-challenger format. 3 means [x, y, v], v is the visable attribute for one joint point. :param bboxs:  a list of lists. [[xmin, ymin, xmax, ymax], ...] :return: img , kps, bboxs after augmentation. """ kps_ori = np.copy(kps) p_flip = np.random.randint(0, 2, 2) seq = iaa.Sequential([iaa.Fliplr(p_flip[0]), iaa.Flipud(p_flip[1]), iaa .Multiply((0.7, 1.5)), iaa.Affine(rotate=(-45, 45), scale=(0.5, 1.5 ), mode='constant'), iaa.Grayscale((0.0, 1.0))]) seq_det = seq.to_deterministic() keypoints = ia.KeypointsOnImage([], shape=img.shape) person_num, joints_num, _ = kps.shape for person in range(person_num): for joint in range(joints_num): point = kps[person][joint] keypoints.keypoints.append(ia.Keypoint(x=point[0], y=point[1])) if bboxs: assert type(bboxs) == type([]) bbs = ia.BoundingBoxesOnImage([], shape=img.shape) for value in bboxs: bbs.bounding_boxes.append(ia.BoundingBox(x1=value[0], y1=value[ 1], x2=value[2], y2=value[3])) img_aug = seq_det.augment_image(img).copy() kps_aug = seq_det.augment_keypoints([keypoints])[0] bboxs_ret = [] if bboxs: bbs_aug = seq_det.augment_bounding_boxes([bbs])[0] for i in range(len(bbs_aug.bounding_boxes)): boxaug = bbs_aug.bounding_boxes[i] box = [boxaug.x1, boxaug.y1, boxaug.x2, boxaug.y2] bboxs_ret.append(box) ret_kps = [] for i in range(len(keypoints.keypoints)): point_after = kps_aug.keypoints[i] ret_kps.append([point_after.x, point_after.y, 1]) ret_kps = np.reshape(np.asarray(ret_kps), newshape=(-1, joints_num, 3)) assert img_aug.shape == img.shape assert ret_kps.shape == kps_ori.shape for person in range(ret_kps.shape[0]): for joint in range(ret_kps.shape[1]): ret_kps[person][joint][2] = int(kps_ori[person][joint][2]) if joints_num > 2: import copy change_index = [[0, 3], [1, 4], [2, 5], [6, 9], [7, 10], [8, 11]] for p in p_flip: if p == 1: for person in range(ret_kps.shape[0]): for index in change_index: left_point = copy.copy(ret_kps[person][index[0]]) ret_kps[person][index[0]] = ret_kps[person][index[1]] ret_kps[person][index[1]] = left_point return img_aug, ret_kps, bboxs_ret 
get|target|updater|agents|agent|SlacAgent|slac def _get_target_updater(self, tau=1.0, period=1): """Performs a soft update of the target network parameters.  For each weight w_s in the original network, and its corresponding weight w_t in the target network, a soft update is: w_t = (1- tau) x w_t + tau x ws  Args: tau: A float scalar in [0, 1]. Default `tau=1.0` means hard update. period: Step interval at which the target network is updated.  Returns: A callable that performs a soft update of the target network parameters. """ with tf.name_scope('update_target'):  def update(): """Update target network.""" critic_update_1 = common.soft_variables_update(self. _critic_network1.variables, self._target_critic_network1. variables, tau) critic_update_2 = common.soft_variables_update(self. _critic_network2.variables, self._target_critic_network2. variables, tau) return tf.group(critic_update_1, critic_update_2) return common.Periodically(update, period, 'update_targets') 
compute|output|sequence|BiasEncoding|deepctr|layers|shape def compute_output_shape(self, input_shape): return input_shape 
CategoricalLogitsNegativeLogProbLoss|ops|classification|loss|functions|dist @property def dist(self): return categorical.Categorical(logits=self._logits) 
def|models|embeddings|OpenKE|embedding|HolE def embedding_def(self): config = self.get_config() self.ent_embeddings = tf.get_variable(name='ent_embeddings', shape=[ config.entTotal, config.hidden_size], initializer=tf.contrib.layers .xavier_initializer(uniform=False)) self.rel_embeddings = tf.get_variable(name='rel_embeddings', shape=[ config.relTotal, config.hidden_size], initializer=tf.contrib.layers .xavier_initializer(uniform=False)) self.parameter_lists = {'ent_embeddings': self.ent_embeddings, 'rel_embeddings': self.rel_embeddings} 
path|pathfinder|scoring def path_scoring(path, context): global concept_embs path_concepts = concept_embs[path] cosine_dist = np.apply_along_axis(spatial.distance.cosine, 1, path_concepts, context) cosine_sim = 1 - cosine_dist if len(path) > 2: return min(cosine_sim[1:-1]) else: return 1.0 
texar|core|get|test|optimizer|OptimizationTest|optimization def test_get_optimizer(self): """Tests get_optimizer. """ default_optimizer_fn, optimizer_class = opt.get_optimizer_fn(opt. default_optimization_hparams()['optimizer']) default_optimizer = default_optimizer_fn(1.0) self.assertTrue(optimizer_class, tf.train.Optimizer) self.assertIsInstance(default_optimizer, tf.train.AdamOptimizer) hparams = {'type': 'MomentumOptimizer', 'kwargs': {'learning_rate': 0.001, 'momentum': 0.9, 'use_nesterov': True}} momentum_optimizer_fn, _ = opt.get_optimizer_fn(hparams) momentum_optimizer = momentum_optimizer_fn() self.assertIsInstance(momentum_optimizer, tf.train.MomentumOptimizer) hparams = {'type': tf.train.MomentumOptimizer, 'kwargs': {'momentum': 0.9, 'use_nesterov': True}} momentum_optimizer_fn, _ = opt.get_optimizer_fn(hparams) momentum_optimizer = momentum_optimizer_fn(0.001) self.assertIsInstance(momentum_optimizer, tf.train.MomentumOptimizer) hparams = {'type': tf.train.MomentumOptimizer(0.001, 0.9)} momentum_optimizer, _ = opt.get_optimizer_fn(hparams) self.assertIsInstance(momentum_optimizer, tf.train.MomentumOptimizer) 
texar|dtype|agent|agents|utils|Space @property def dtype(self): """Data type of the element. """ return self._dtype 
size|utils|data|Dataset|dataset def dataset_size(self, setname): return len(self._dataset[setname]) 
del|loss|WassFeatureLoss|fasterai def __del__(self): self.hooks.remove() 
ptb|producer|enas|input|data|utils def ptb_input_producer(raw_data, batch_size, num_steps, shuffle=False, randomize=False): """ Args: raw_data: np tensor of size [num_words]. batch_size: self-explained. num_steps: number of BPTT steps. """ num_batches_per_epoch = (np.size(raw_data) // batch_size - 1) // num_steps raw_data = tf.convert_to_tensor(raw_data, name='raw_data', dtype=tf.int32) data_len = tf.size(raw_data) batch_len = data_len // batch_size data = tf.reshape(raw_data[0:batch_size * batch_len], [batch_size, batch_len]) epoch_size = (batch_len - 1) // num_steps with tf.device('/cpu:0'): epoch_size = tf.identity(epoch_size, name='epoch_size') if randomize: i = tf.random_uniform([1], minval=0, maxval=batch_len - num_steps, dtype=tf.int32) i = tf.reduce_sum(i) x = tf.strided_slice(data, [0, i], [batch_size, i + num_steps]) y = tf.strided_slice(data, [0, i + 1], [batch_size, i + num_steps + 1]) else: i = tf.train.range_input_producer(epoch_size, shuffle=shuffle ).dequeue() x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps]) y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1]) x.set_shape([batch_size, num_steps]) y.set_shape([batch_size, num_steps]) return x, y, num_batches_per_epoch 
RDF2Vec|randomWalkUniform def randomWalkUniform(triples, startNode, max_depth=5): next_node = startNode path = 'n' + str(startNode) + '->' for i in range(max_depth): neighs = getLinks(triples, next_node) if len(neighs) == 0: break weights = [] queue = [] for neigh in neighs: for edge in neighs[neigh]: queue.append((edge, neigh)) weights.append(1.0) edge, next_node = random.choice(queue) path = path + 'e' + str(edge) + '->' path = path + 'n' + str(next_node) + '->' return path 
get|filenames|cifar1|Cifar1|DataSet def get_filenames(self): if self.subset in ['train', 'validation', 'eval']: return [os.path.join(self.data_dir, self.subset + '.tfrecords')] else: raise ValueError('Invalid data subset "%s"' % self.subset) 
in|download|speech|corpus|DataArchive|audio|move|dir def _move_audio_in_dir(self, src, dest): """ Moves audio files in one directory.  Parameters ---------- src : Path the path of the directory audio files are in. dest : Path the path of the directory audio files are moved to. """ if self.user_option.verbose: print('Move:', src.name) os.makedirs(str(dest), exist_ok=True) for wav_file in self.global_config.extensions.itemize_in_directory(src): dest_path = dest / wav_file.name self._move_file(wav_file, dest_path) 
IsDeletedOrDefault|cpplint def IsDeletedOrDefault(clean_lines, linenum): """Check if current constructor or operator is deleted or default.  Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. Returns: True if this is a deleted or default constructor. """ open_paren = clean_lines.elided[linenum].find('(') if open_paren < 0: return False close_line, _, close_paren = CloseExpression(clean_lines, linenum, open_paren) if close_paren < 0: return False return Match('\\s*=\\s*(?:delete|default)\\b', close_line[close_paren:]) 
MadryEtAl|attacks|models|params|parse def parse_params(self, eps=0.3, eps_iter=0.01, nb_iter=40, y=None, ord=np. inf, clip_min=None, clip_max=None, y_target=None, **kwargs): """ Take in a dictionary of parameters and applies attack-specific checks before saving them as attributes.  Attack-specific parameters: :param eps: (required float) maximum distortion of adversarial example compared to original input :param eps_iter: (required float) step size for each attack iteration :param nb_iter: (required int) Number of attack iterations. :param y: (optional) A tensor with the model labels. :param y_target: (optional) A tensor with the labels to target. Leave y_target=None if y is also set. Labels should be one-hot-encoded. :param ord: (optional) Order of the norm (mimics Numpy). Possible values: np.inf, 1 or 2. :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value """ self.eps = eps self.eps_iter = eps_iter self.nb_iter = nb_iter self.y = y self.y_target = y_target self.ord = ord self.clip_min = clip_min self.clip_max = clip_max if self.y is not None and self.y_target is not None: raise ValueError('Must not set both y and y_target') if self.ord not in [np.inf, 1, 2]: raise ValueError('Norm order must be either np.inf, 1, or 2.') return True 
forward|ECELoss|train|classification def forward(self, logits, labels, title): softmaxes = F.softmax(logits, dim=1) confidences, predictions = torch.max(softmaxes, 1) accuracies = predictions.eq(labels) ece = torch.zeros(1, device=logits.device) for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers): in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper .item()) prop_in_bin = in_bin.float().mean() if prop_in_bin.item() > 0: accuracy_in_bin = accuracies[in_bin].float().mean() avg_confidence_in_bin = confidences[in_bin].mean() ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin ) * prop_in_bin accuracy_in_bin_list = [] for bin_lower, bin_upper in zip(self.bin_lowers_plot, self.bin_uppers_plot ): in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper .item()) prop_in_bin = in_bin.float().mean() accuracy_in_bin = 0 if prop_in_bin.item() > 0: accuracy_in_bin = accuracies[in_bin].float().mean().item() accuracy_in_bin_list.append(accuracy_in_bin) p1 = plt.bar(np.arange(10) / 10.0, accuracy_in_bin_list, 0.1, align= 'edge', edgecolor='black') p2 = plt.plot([0, 1], [0, 1], '--', color='gray') plt.ylabel('Accuracy') plt.xlabel('Confidence') plt.xticks(np.arange(0, 1.01, 0.2)) plt.yticks(np.arange(0, 1.01, 0.2)) plt.xlim(left=0, right=1) plt.ylim(bottom=0, top=1) plt.grid(True) plt.savefig(title + '/cal.pdf') return ece 
fisher|ops|classification|init|factors|ScaleFactor def __init__(self): """ Initialize class ScaleFactor. """ self._input_factor_eigen_basis = None self._output_factor_eigen_basis = None self._input_factor_eigen_value = None self._output_factor_eigen_value = None super(ScaleFactor, self).__init__() 
ConvScaleFactor|fisher|sources|ops|classification|factors|num @property def _num_sources(self): return len(self._outputs_grads) 
FindHeader|cpplint|IncludeState def FindHeader(self, header): """Check if a header has already been included.  Args: header: header to check. Returns: Line number of previous occurrence, or -1 if the header has not been seen before. """ for section_list in self.include_list: for f in section_list: if f[0] == header: return f[1] return -1 
gen|bbans|net1|pixelvae|pvae|layer|two def gen_net1(z1_partial, z2): return session.run([mu1_prior, sig1_prior], feed_dict={latents1: z1_partial, latents2: z2, bn_is_training: False, bn_stats_iter: 0, total_iters: 99999}) 
classes|StructuredPolicy|params|dc|svae|policy|unscale|utils def unscale_params(self, scaled_params): return scaled_params 
14|train|ruemonge2 def train(): trainset = input_fn(trainlist, BATCH_SIZE, 10000) train_iterator = trainset.make_initializable_iterator() next_train_element = train_iterator.get_next() testset = input_fn(testlist, BATCH_SIZE, 10000) test_iterator = testset.make_initializable_iterator() next_test_element = test_iterator.get_next() with tf.device('/gpu:0'): input_pl, label_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT) training_pl = tf.placeholder(tf.bool, shape=()) global_step = tf.Variable(0, trainable=False, name='global_step') pred, end_points = MODEL.get_model(input_pl, training_pl, config= net_config) MODEL.get_loss(pred, label_pl, end_points) if net_config.weight_decay is not None: reg_loss = tf.multiply(tf.losses.get_regularization_loss(), net_config.weight_decay, name='reg_loss') tf.add_to_collection('losses', reg_loss) losses = tf.get_collection('losses') total_loss = tf.add_n(losses, name='total_loss') tf.summary.scalar('total_loss', total_loss) for l in (losses + [total_loss]): tf.summary.scalar(l.op.name, l) correct = tf.equal(tf.argmax(pred, 2, output_type=tf.int32), tf. cast(label_pl, tf.int32)) accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float( BATCH_SIZE) tf.summary.scalar('accuracy', accuracy) print('--- Get training operator') learning_rate = get_learning_rate(global_step) tf.summary.scalar('learning_rate', learning_rate) if OPTIMIZER == 'momentum': optimizer = tf.train.MomentumOptimizer(learning_rate, momentum= MOMENTUM, use_nesterov=True) elif OPTIMIZER == 'adam': optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=0.0001) update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) with tf.control_dependencies(update_ops): train_op = optimizer.minimize(total_loss, global_step=global_step) saver = tf.train.Saver(max_to_keep=500) n = len([n.name for n in tf.get_default_graph().as_graph_def().node]) print('*****************The Graph has %d nodes*****************' % n) config = tf.ConfigProto() config.gpu_options.allow_growth = True config.allow_soft_placement = True config.log_device_placement = False init = tf.group(tf.global_variables_initializer(), tf. local_variables_initializer()) if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR) with tf.Session(config=config) as sess: merged = tf.summary.merge_all() train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph) test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph) sess.run(init) latest_ckpt = tf.train.latest_checkpoint(LOG_DIR) print(FLAGS.load_ckpt) if FLAGS.load_ckpt is not None: saver.restore(sess, FLAGS.load_ckpt) print('{}-Checkpoint loaded from {}!'.format(datetime.now(), FLAGS.load_ckpt)) elif latest_ckpt: print('{}-Found checkpoint {}'.format(datetime.now(), latest_ckpt)) saver.restore(sess, latest_ckpt) print('{}-Checkpoint loaded from {} (Iter {})'.format(datetime. now(), latest_ckpt, sess.run(global_step))) ops = {'input_pl': input_pl, 'label_pl': label_pl, 'training_pl': training_pl, 'pred': pred, 'loss': total_loss, 'train_op': train_op, 'merged': merged, 'global_step': global_step, 'end_points': end_points} if latest_ckpt: checkpoint_epoch = int(latest_ckpt.split('-')[-1]) + 1 elif FLAGS.load_ckpt is not None: checkpoint_epoch = int(FLAGS.load_ckpt.split('-')[-1]) + 1 else: checkpoint_epoch = 0 for epoch in range(checkpoint_epoch, MAX_EPOCH): log_string('**** EPOCH %03d ****' % epoch) sys.stdout.flush() sess.run(train_iterator.initializer) train_one_epoch(sess, ops, next_train_element, train_writer) log_string(str(datetime.now())) log_string('---- EPOCH %03d EVALUATION ----' % epoch) sess.run(test_iterator.initializer) eval_one_epoch(sess, ops, next_test_element, test_writer) save_path = saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt' ), global_step=epoch) log_string('Model saved in file: %s' % save_path) 
nmt|utils|load|vocab def load_vocab(vocab_file): vocab = [] with codecs.getreader('utf-8')(tf.gfile.GFile(vocab_file, 'rb')) as f: vocab_size = 0 for word in f: vocab_size += 1 vocab.append(word.strip()) return vocab, vocab_size 
tests|test|interaction|CrossNet|layers|invalid def test_CrossNet_invalid(): with pytest.raises(ValueError): with CustomObjectScope({'CrossNet': layers.CrossNet}): layer_test(layers.CrossNet, kwargs={'layer_num': 1, 'l2_reg': 0 }, input_shape=(2, 3, 4)) 
TPUEncodeTest|utils|setUp|test def setUp(self): super(TPUEncodeTest, self).setUp() self.data = tf.random.uniform([128], maxval=100000, dtype=tf.int32 ), tf.cast(tf.random.uniform([128], maxval=65535, dtype=tf.int32), tf.uint16), tf.cast(tf.random.uniform([64, 84, 84, 4], maxval=256, dtype=tf.int32), tf.uint8), tf.cast(tf.random.uniform([1], maxval= 256, dtype=tf.int32), tf.uint8), tf.cast(tf.random.uniform([100, 128, 1, 1, 1], maxval=256, dtype=tf.int32), tf.uint8), tf.cast(tf. random.uniform([128, 100, 1, 1, 1], maxval=256, dtype=tf.int32), tf .uint8) 
dump|features|combined|esc5|Dataset|EndToEndClassification|processor|processed def _dump_features_processed_esc50_combined(load_parsed_esc50, save_folder_path, save_folder_path_raw, augmentations=4, frames=101, seed=41, batch_size=50): """ Generates ESC50 features from the 'processed' dataset. It does so according to the specifications in the paper. Each of the 2000 5sec clips is cut into 50% overlapping segments. 4 augmentations are made of each. Largely the same in implementation as the original Piczak code.  Args: load_parsed_esc50 (str): folder containing the esc50_meta.pkl and esc50_audio.dat files. save_folder_path (str): folder for saving logscaled mel features. save_folder_path_raw (str): folder for saving raw waveform features. augmentations (int): number of augmentations of each segment. frames (int): nr of frames of the mel features. seed (int): seed for pseudo RNG. batch_size (int): batch size for multiprocessing (note, this has nothing to do with the minibatch size). """ np.random.seed(seed) if isinstance(load_parsed_esc50, str): meta, audio = load_processed_esc50(load_parsed_esc50) else: raise ValueError('load_parsed_esc50 should be a to folder') if not (os.path.isdir(save_folder_path) and os.path.isdir( save_folder_path_raw)): raise ValueError('please provide valid folders for saving the features' ) segments = [] segments_raw = [] for b in range(len(audio) // batch_size + 1): print('b:{}'.format(b)) start = b * batch_size end = (b + 1) * batch_size if end > len(audio): end = len(audio) seg_combined = Parallel(n_jobs=CPU_COUNT)(delayed( _extract_segments_combined)((audio[(i), :], meta.loc[i, 'filename'], meta.loc[i, 'fold'], meta.loc[i, 'category'], meta .loc[i, 'category_name'], 0, frames)) for i in range(start, end)) segments_batch = [seg[0] for seg in seg_combined] segments_raw_batch = [seg[1] for seg in seg_combined] segments.extend(segments_batch) segments_raw.extend(segments_raw_batch) for _ in range(augmentations): seg_combined = Parallel(n_jobs=CPU_COUNT)(delayed( _extract_segments_combined)((_augment_esc50(audio[(i), :]), meta.loc[i, 'filename'], meta.loc[i, 'fold'], meta.loc[i, 'category'], meta.loc[i, 'category_name'], 1, frames)) for i in range(start, end)) segments_batch = [seg[0] for seg in seg_combined] segments_raw_batch = [seg[1] for seg in seg_combined] segments.extend(segments_batch) segments_raw.extend(segments_raw_batch) segments = [pd.concat(segments, ignore_index=True)] segments_raw = [pd.concat(segments_raw, ignore_index=True)] print('{} / {}'.format(end, len(audio))) segments[0][0:30000].to_pickle(os.path.join(save_folder_path, 'esc50_features_long_logspec0.pkl')) segments[0][30000:60000].to_pickle(os.path.join(save_folder_path, 'esc50_features_long_logspec1.pkl')) segments[0][60000:].to_pickle(os.path.join(save_folder_path, 'esc50_features_long_logspec2.pkl')) segments_raw[0][0:30000].to_pickle(os.path.join(save_folder_path_raw, 'esc50_features_long_raw0.pkl')) segments_raw[0][30000:60000].to_pickle(os.path.join( save_folder_path_raw, 'esc50_features_long_raw1.pkl')) segments_raw[0][60000:].to_pickle(os.path.join(save_folder_path_raw, 'esc50_features_long_raw2.pkl')) 
gradients|tfprocess|TFProcess|average def average_gradients(self, tower_grads): average_grads = [] for grad_and_vars in zip(*tower_grads): grads = [] for g, _ in grad_and_vars: expanded_g = tf.expand_dims(g, dim=0) grads.append(expanded_g) grad = tf.concat(grads, axis=0) grad = tf.reduce_mean(grad, reduction_indices=0) v = grad_and_vars[0][1] grad_and_var = grad, v average_grads.append(grad_and_var) return average_grads 
texar|get|core|layers|initializer def get_initializer(hparams=None): """Returns an initializer instance.  .. role:: python(code) :language: python  Args: hparams (dict or HParams, optional): Hyperparameters with the structure  .. code-block:: python  { "type": "initializer_class_or_function", "kwargs": { #... } }  The "type" field can be a initializer class, its name or module path, or class instance. If class name is provided, the class must be from one the following modules: :tf_main:`tf.initializers <initializers>`, :tf_main:`tf.keras.initializers <keras/initializers>`, :tf_main:`tf < >`, and :mod:`texar.custom`. The class is created by :python:`initializer_class(**kwargs)`. If a class instance is given, "kwargs" is ignored and can be omitted.  Besides, the "type" field can also be an initialization function called with :python:`initialization_fn(**kwargs)`. In this case "type" can be the function, or its name or module path. If function name is provided, the function must be from one of the above modules or module `tf.contrib.layers`. If no keyword argument is required, "kwargs" can be omitted.  Returns: An initializer instance. `None` if :attr:`hparams` is `None`. """ if hparams is None: return None kwargs = hparams.get('kwargs', {}) if isinstance(kwargs, HParams): kwargs = kwargs.todict() modules = ['tensorflow.initializers', 'tensorflow.keras.initializers', 'tensorflow', 'texar.custom'] try: initializer = utils.check_or_get_instance(hparams['type'], kwargs, modules) except TypeError: modules += ['tensorflow.contrib.layers'] initializer_fn = utils.get_function(hparams['type'], modules) initializer = initializer_fn(**kwargs) return initializer 
call|operations|norm|batch def __call__(self, x, train=True): return tf.contrib.layers.batch_norm(x, decay=self.momentum, updates_collections=None, epsilon=self.epsilon, scale=True, is_training=train, scope=self.name) 
get|autoencoder|z|Attentive|layer def get_z_layer(model, X_train): Z = model.predict(X_train, verbose=0) return Z 
spectrogram|melSpectrogram|mel def melSpectrogram(signal, sample_rate, frame_size=0.025, frame_stride=0.01, nfilt=70, NFFT=1024, normalized=True): """ Computes the mel-spectrogram of an input signal  :param signal: 1d-array, signal of the sound extract :param sample_rate: int, sample rate of the sound extract :param frame_size: float [0.025], size of the Fourier-transform window :param frame_stride: float [0.01], size of the stride during Fourier-transform :param nfilt: int [70], number of frequency bins :param NFFT: int [1024], number of points used for the Fourier-transform :param normalized: bool [True], indicates whether to apply mean-removal or not  :return: 2d-array, the spectrogram """ pre_emphasis = 0.97 emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1]) frame_length, frame_step = (frame_size * sample_rate, frame_stride * sample_rate) signal_length = len(emphasized_signal) frame_length = int(round(frame_length)) frame_step = int(round(frame_step)) num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step)) pad_signal_length = num_frames * frame_step + frame_length z = np.zeros(pad_signal_length - signal_length) pad_signal = np.append(emphasized_signal, z) indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np .arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T frames = pad_signal[indices.astype(np.int32, copy=False)] frames *= np.hamming(frame_length) mag_frames = np.absolute(np.fft.rfft(frames, NFFT)) pow_frames = 1.0 / NFFT * mag_frames ** 2 low_freq_mel = 0 high_freq_mel = 2595 * np.log10(1 + sample_rate / 2 / 700) mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2) hz_points = 700 * (10 ** (mel_points / 2595) - 1) bin = np.floor((NFFT + 1) * hz_points / sample_rate) fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1)))) for m in range(1, nfilt + 1): f_m_minus = int(bin[m - 1]) f_m = int(bin[m]) f_m_plus = int(bin[m + 1]) for k in range(f_m_minus, f_m): fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1]) for k in range(f_m, f_m_plus): fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m]) filter_banks = np.dot(pow_frames, fbank.T) filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks) filter_banks = 20 * np.log10(filter_banks) if normalized: filter_banks -= np.mean(filter_banks, axis=0) + 1e-08 return filter_banks 
testBuildLogits|test|inception|nets|v4|InceptionTest def testBuildLogits(self): batch_size = 5 height, width = 299, 299 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) logits, end_points = inception.inception_v4(inputs, num_classes) auxlogits = end_points['AuxLogits'] predictions = end_points['Predictions'] self.assertTrue(auxlogits.op.name.startswith('InceptionV4/AuxLogits')) self.assertListEqual(auxlogits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue(logits.op.name.startswith('InceptionV4/Logits')) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue(predictions.op.name.startswith( 'InceptionV4/Logits/Predictions')) self.assertListEqual(predictions.get_shape().as_list(), [batch_size, num_classes]) 
python|grpc|OpsTest|ops|test|not|seed|rl|master|outputs2|fully|specified def test_not_fully_specified_outputs2(self): address = self.get_unix_address() server = ops.Server([address])  @tf.function(input_signature=[tf.TensorSpec([1], tf.int32)]) def foo(x): result, = tf.py_function(lambda x: x, [x], [tf.int32]) result.set_shape([None]) return result server.bind(foo, batched=True) server.start() client = ops.Client(address) self.assertAllEqual(42, client.foo(42)) server.shutdown() 
variables|xlnet|ops|estimator|tpu|sync|master def _sync_variables_ops(ctx): """Create varriables synchronization ops.  Gets the variables back from TPU nodes. This means the variables updated by TPU will now be *synced* to host memory. In BROADCAST mode, we skip this sync since the variables are ususally too big to transmit via RPC.  Args: ctx: A `_InternalTPUContext` instance with mode.  Returns: A list of sync ops. """ if not ctx.is_input_broadcast_with_iterators(): return [array_ops.check_numerics(v.read_value(), 'Gradient for %s is NaN' % v.name).op for v in variables. trainable_variables()] else: return [control_flow_ops.no_op()] 
BinaryFullyConnectedLayer|layers|sign|func def _sign_func(self, x): """Modified sign function randomize if 0, else sign  Arguments: x {tf tensor} -- input tensor  Returns: tf tensor -- sign(x) """ cond = x == tf.zeros_like(x) return tf.where(cond, tf.ones_like(x), tf.sign(x)) 
loadInitial|model def loadInitial(self, fname, sess): wts = np.load(fname) for k in wts.keys(): if not re.match('^fc8', k): wvar = self.weights[k] wk = wts[k].reshape(wvar.get_shape()) wvar.load(wk, sess) 
xlnet|on|estimator|tpu|master|train|system def _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn): """Executes `model_fn_wrapper` multiple times on all TPU shards.""" iterations_per_loop_var = _create_or_get_iterations_per_loop() (single_tpu_train_step, host_call, captured_scaffold_fn, captured_training_hooks ) = model_fn_wrapper.convert_to_single_tpu_train_step(dequeue_fn)  def multi_tpu_train_steps_on_single_shard(): loop_vars = [_INITIAL_LOSS] if model_fn_wrapper._train_cache_fn is not None: batch_size = ctx.global_batch_size num_shards = ctx._config._tpu_config.num_shards loop_vars += model_fn_wrapper._train_cache_fn(batch_size // num_shards) return training_loop.repeat(iterations_per_loop_var, single_tpu_train_step, loop_vars) compile_op, ret = tpu.split_compile_and_shard( multi_tpu_train_steps_on_single_shard, inputs=[], num_shards=ctx. num_replicas, outputs_from_all_shards=False, device_assignment=ctx. device_assignment) loss = ret[0] scaffold = _get_scaffold(captured_scaffold_fn) return compile_op, loss, host_call, scaffold, captured_training_hooks.get() 
bert|gpt2|set|pytorch|GPT2LMHead|embeddings|modeling|pretrained|weights def set_embeddings_weights(self, model_embeddings_weights): embed_shape = model_embeddings_weights.shape self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False) self.decoder.weight = model_embeddings_weights 
gradients|optimizers|utils|apply|LossScalingOptimizer|thumt def apply_gradients(self, grads_and_vars, global_step=None, name=None): grads, var_list = list(zip(*grads_and_vars)) grad_norm = tf.global_norm(grads) new_grads = [] is_overflow = tf.logical_not(tf.is_finite(grad_norm)) for grad in grads: if grad is not None: grad = tf.cond(is_overflow, lambda : tf.zeros_like(grad), lambda : grad) new_grads.append(grad) grads_and_vars = list(zip(new_grads, var_list)) update_op = self._optimizer.apply_gradients(grads_and_vars, global_step, name) first_var = min(var_list, key=lambda x: x.name) iter_var = self._create_non_slot_variable(initial_value=0, name='iter', colocate_with=first_var) rescale_iter = self._create_non_slot_variable(initial_value=0, name= 'rescale_iter', colocate_with=first_var) overflow_iter = self._create_non_slot_variable(initial_value=-1, name= 'overflow_iter', colocate_with=first_var) overflow_var = self._create_non_slot_variable(initial_value=0, name= 'overflow', colocate_with=first_var) scale_var = self._get_non_slot_variable('scale', tf.get_default_graph()) iter_op = tf.assign_add(iter_var, 1) overflow_count_op = tf.cond(is_overflow, lambda : tf.assign_add( overflow_var, 1), lambda : overflow_var) overflow_iter_op = tf.cond(is_overflow, lambda : tf.assign( overflow_iter, iter_var), lambda : iter_var)  def increase_scale(): scale_op = tf.assign(scale_var, scale_var * self._scale_factor, use_locking=self._use_locking) iter_op = tf.assign(rescale_iter, iter_var, use_locking=self. _use_locking) return tf.group(*[scale_op, iter_op])  def decrease_scale(): scale_op = tf.assign(scale_var, scale_var / self._scale_factor, use_locking=self._use_locking) if self._threshold is not None: scale_op = tf.assign(scale_op, tf.maximum(scale_var, self. _threshold)) iter_op = tf.assign(rescale_iter, iter_var, use_locking=self. _use_locking) overflow_op = tf.assign(overflow_var, 0) return tf.group(*[scale_op, iter_op, overflow_op]) with tf.control_dependencies([overflow_count_op, overflow_iter_op]): percentage = tf.div(tf.cast(overflow_var, tf.float32), tf.cast( iter_var - rescale_iter, tf.float32)) decrease_scale_op = tf.cond(tf.logical_and(is_overflow, tf.greater( percentage, self._tolerance)), decrease_scale, lambda : tf.no_op()) increase_scale_op = tf.cond(tf.logical_and(tf.logical_not( is_overflow), tf.equal((iter_var - overflow_iter) % self. _scale_window, 0)), increase_scale, lambda : tf.no_op()) ops = [update_op, iter_op, overflow_count_op, overflow_iter_op, increase_scale_op, decrease_scale_op] return tf.group(*ops) 
sobolev|discriminator|cifar1 def discriminator(x, reuse): with tf.variable_scope('discriminator', reuse=reuse): with tf.name_scope('pre_process'): x = resblock_optimized(x, filters=128) with tf.name_scope('x1'): x = resblock(x, filters=128, resample='down') x = resblock(x, filters=128) x = resblock(x, filters=128) with tf.name_scope('post_process'): x = activation(x) x = tf.reduce_mean(x, axis=[1, 2]) flat = tf.contrib.layers.flatten(x) flat = tf.layers.dense(flat, 1) return flat 
preparation|within|bounds|data|sample def sample_within_bounds(signal, x, y, bounds): xmin, xmax, ymin, ymax = bounds idxs = (xmin <= x) & (x < xmax) & (ymin <= y) & (y < ymax) sample = np.zeros((x.shape[0], x.shape[1], signal.shape[-1])) sample[(idxs), :] = signal[(x[idxs]), (y[idxs]), :] return sample 
init|nodes|deepzono|DeepzonoMatmul def __init__(self, matrix, input_names, output_name, output_shape): """ Arguments --------- matrix : numpy.ndarray 2D matrix for the matrix multiplication input_names : iterable iterable with the name of the vector for the matrix multiplication output_name : str name of this node's output output_shape : iterable iterable of ints with the shape of the output of this node """ add_input_output_information(self, input_names, output_name, output_shape) self.matrix = np.ascontiguousarray(matrix, dtype=np.double) 
tf|conditioned|std|goal|reduce|hbaselines|util def reduce_std(tensor, axis=None, keepdims=False): """Get the standard deviation of a Tensor.  Parameters ---------- tensor : tf.Tensor or tf.Variable the input tensor axis : int or list of int the axis to itterate the std over keepdims : bool keep the other dimensions the same  Returns ------- tf.Tensor the std of the tensor """ return tf.sqrt(reduce_var(tensor, axis=axis, keepdims=keepdims)) 
deepwalk|DeepWalk|models|ge|init def __init__(self, graph, walk_length, num_walks, workers=1): self.graph = graph self.w2v_model = None self._embeddings = {} self.walker = RandomWalker(graph, p=1, q=1) self.sentences = self.walker.simulate_walks(num_walks=num_walks, walk_length=walk_length, workers=workers, verbose=1) 
static|utils|test|MakeTimeMajorTest def test_static(self): x = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) self.assertAllEqual(utils.make_time_major(x), tf.constant([[[1, 2], [5, 6]], [[3, 4], [7, 8]]])) 
gen|update|wgan|var|with|mean|celebA|batch|utils|norm def mean_var_with_update(): ema_apply_op = ema.apply([batch_mean, batch_var]) with tf.control_dependencies([ema_apply_op]): return tf.identity(batch_mean), tf.identity(batch_var) 
gan|CDVAECLSGAN|cls|mcc|init|cdvae def __init__(self, arch, normalizers=None): """ Cross Domain Variational Auto Encoding Generative Adversarial Net with Adversarial classifier (CDVAE-CLS-GAN) Arguments: `arch`: network architecture (`dict`) """ self.arch = arch self.normalizers = normalizers with tf.name_scope('SpeakerCode'): self.y_emb = self._l2_regularized_embedding(self.arch['y_dim'], self.arch['z_dim'], 'y_embedding') self.sp_enc = tf.make_template('SP_Encoder', self.sp_encoder) self.mcc_enc = tf.make_template('MCC_Encoder', self.mcc_encoder) self.sp_dec = tf.make_template('SP_Decoder', self.sp_decoder) self.mcc_dec = tf.make_template('MCC_Decoder', self.mcc_decoder) self.mcc_dis = tf.make_template('MCC_Discriminator', self.mcc_discriminator ) self.latent_cls = tf.make_template('Latent_Classifier', self. latent_classifier) 
sparse|gen|plot|gnmt|for|oneshot|epoch def plot_gnmt_sparse_oneshot_epoch_for_epoch(): common.epoch_for_epoch_plot(network=common.GNMT, is_iterative=False, prune_method=common.UNSTRUCTURED, min_max_y=(-6, 1.5)) 
generator|DeepWalk|feed|model|dict def feed_dict_generator(self, a_random_walk, step, gamma): """ Method to generate random walk features, gamma and proper time index. """ batch_inputs = batch_input_generator(a_random_walk, self.args. random_walk_length, self.args.window_size) batch_labels = batch_label_generator(a_random_walk, self.args. random_walk_length, self.args.window_size) feed_dict = {self.walker_layer.train_labels: batch_labels, self. walker_layer.train_inputs: batch_inputs, self.gamma: gamma, self. step: float(step)} return feed_dict 
bert|InputFeatures|init|master|run|classifier def __init__(self, input_ids, input_mask, segment_ids, label_id, is_real_example=True): self.input_ids = input_ids self.input_mask = input_mask self.segment_ids = segment_ids self.label_id = label_id self.is_real_example = is_real_example 
npy|embeddings|glove|to def glove_to_npy(): config = configparser.ConfigParser() config.read('paths.cfg') glove_embeddings = parse_glove(config['paths']['glove']) words = [] vectors = [] for key, vec in glove_embeddings.items(): words.append(key) vectors.append(vec) print('Writing glove vectors.', flush=True) np.save(config['paths']['glove_vec_npy'], vectors) print('Finished writing glove vectors.', flush=True) vocab_text = '\n'.join(words) print('Writing glove vocab.', flush=True) with open(config['paths']['glove_vocab'], 'wb') as f: f.write(vocab_text.encode('utf-8')) print('Finished writing glove vocab.', flush=True) 
texar|utils|ceildiv def ceildiv(a, b): """Divides with ceil.  E.g., `5 / 2 = 2.5`, `ceildiv(5, 2) = 3`.  Args: a (int): Dividend integer. b (int): Divisor integer.  Returns: int: Ceil quotient. """ return -(-a // b) 
corrupt|training|init|SimpleCorruptor|vkge def __init__(self, index_generator=None, candidate_indices=None, corr_obj=False ): self.index_generator, self.candidate_indices = (index_generator, candidate_indices) self.corr_obj = corr_obj 
forward|normalizer|process|StandardScaler def forward_process(self, x): return (x - self.mu) / (self.std + EPSILON) 
tangents|prediction|predict|neural|mean def _mean_prediction(op, g_td, y_train): """Compute the mean prediction of a Gaussian process.  Args: op: Some vector operator that projects the data along the relevant directions, op(vec, dt) = M^{-1} @ (I - E^(-M dt)) @ vec g_td: A kernel relating training data with test data. The kernel should be an `np.ndarray` of shape [n_test * output_dim, n_train * output_dim] or [n_test, n_train]. y_train: An `np.ndarray` of shape [n_train, output_dim] of targets for the training data.  Returns: The mean prediction of the GP. `g_td @ op @ y_train`. """ fl, ufl = _make_flatten_uflatten(g_td, y_train) mean_pred = op(fl(y_train)) mean_pred = np.dot(g_td, mean_pred) return ufl(mean_pred) 
homog|fisher|ops|classification|append|factors def _append_homog(tensor): """Appends a homogeneous coordinate to the last dimension of a Tensor. Args: tensor: A Tensor. Returns: A Tensor identical to the input but one larger in the last dimension.  The new entries are filled with ones. """ rank = len(tensor.shape.as_list()) shape = array_ops.concat([array_ops.shape(tensor)[:-1], [1]], axis=0) ones = array_ops.ones(shape, dtype=tensor.dtype) return array_ops.concat([tensor, ones], axis=rank - 1) 
SepConv|models|init|layers|sto def __init__(self, C_in, C_out, kernel_size=3, stride=1, padding=1, groups= 1, affine=True, after_norm_type='bn'): super(SepConv, self).__init__() self.op = nn.Sequential(nn.BatchNorm2d(C_in, affine=affine, track_running_stats=False), nn.ReLU(inplace=True), nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False), nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False, groups=groups), nn.BatchNorm2d(C_in, affine= True, track_running_stats=False), nn.ReLU(inplace=True), nn.Conv2d( C_in, C_in, kernel_size=kernel_size, stride=1, padding=padding, groups=C_in, bias=False), nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False, groups=groups), NORMS[after_norm_type](C_out, groups)) 
neg|get|Model def get_neg(ILL, output_layer, k): neg = [] t = len(ILL) ILL_vec = np.array([output_layer[e1] for e1 in ILL]) KG_vec = np.array(output_layer) sim = scipy.spatial.distance.cdist(ILL_vec, KG_vec, metric='cityblock') for i in range(t): rank = sim[(i), :].argsort() neg.append(rank[0:k]) neg = np.array(neg) neg = neg.reshape((t * k,)) return neg 
plyfile|header|PlyData @property def header(self): """ Provide PLY-formatted metadata for the instance.  """ lines = ['ply'] if self.text: lines.append('format ascii 1.0') else: lines.append('format ' + _byte_order_reverse[self.byte_order] + ' 1.0') for c in self.comments: lines.append('comment ' + c) for c in self.obj_info: lines.append('obj_info ' + c) lines.extend(elt.header for elt in self.elements) lines.append('end_header') return '\r\n'.join(lines) 
CamRestDST|applications|camrest|load|dst|restaurants|cambridge def load(self, model_path): """ Loads the Ludwig model from the given path.  :param model_path: path to the model :return: """ super(CamRestDST, self).load(model_path) 
FindStartOfExpressionInLine|cpplint def FindStartOfExpressionInLine(line, endpos, stack): """Find position at the matching start of current expression.  This is almost the reverse of FindEndOfExpressionInLine, but note that the input position and returned position differs by 1.  Args: line: a CleansedLines line. endpos: start searching at this position. stack: nesting stack at endpos.  Returns: On finding matching start: (index at matching start, None) On finding an unclosed expression: (-1, None) Otherwise: (-1, new stack at beginning of this line) """ i = endpos while i >= 0: char = line[i] if char in ')]}': stack.append(char) elif char == '>': if i > 0 and (line[i - 1] == '-' or Match('\\s>=\\s', line[i - 1:]) or Search('\\boperator\\s*$', line[0:i])): i -= 1 else: stack.append('>') elif char == '<': if i > 0 and line[i - 1] == '<': i -= 1 elif stack and stack[-1] == '>': stack.pop() if not stack: return i, None elif char in '([{': while stack and stack[-1] == '>': stack.pop() if not stack: return -1, None if char == '(' and stack[-1] == ')' or char == '[' and stack[-1 ] == ']' or char == '{' and stack[-1] == '}': stack.pop() if not stack: return i, None else: return -1, None elif char == ';': while stack and stack[-1] == '>': stack.pop() if not stack: return -1, None i -= 1 return -1, stack 
get|pair|learn|bpe|statistics def get_pair_statistics(vocab): """Count frequency of all symbol pairs, and create index""" stats = defaultdict(int) indices = defaultdict(lambda : defaultdict(int)) for i, (word, freq) in enumerate(vocab): prev_char = word[0] for char in word[1:]: stats[prev_char, char] += freq indices[prev_char, char][i] += 1 prev_char = char return stats, indices 
bias|conv|no|make|cnn|helpers def make_conv_no_bias(op_name, in_tensor, filter_size_h, filter_size_w, filters, strides=(1, 1, 1, 1), padding='VALID', weight_decay=0.0005, stddev=0.1): with tf.device('/device:CPU:0'), tf.variable_scope('vars/convs', reuse= tf.AUTO_REUSE): input_size = in_tensor.get_shape().as_list()[3] shape = filter_size_h, filter_size_w, input_size, filters w = tf.get_variable('W_' + op_name, shape=shape, regularizer= l2_regularizer(weight_decay), initializer=tf. truncated_normal_initializer(stddev=stddev)) with tf.name_scope(op_name): return tf.nn.conv2d(in_tensor, w, strides=strides, padding=padding, name=op_name) 
eval|cleverhans|tf|batch|utils def batch_eval(sess, tf_inputs, tf_outputs, numpy_inputs, feed=None, args=None ): """ A helper function that computes a tensor on numpy inputs by batches.  :param sess: :param tf_inputs: :param tf_outputs: :param numpy_inputs: :param feed: An optional dictionary that is appended to the feeding dictionary before the session runs. Can be used to feed the learning phase of a Keras model for instance. :param args: dict or argparse `Namespace` object. Should contain `batch_size` """ args = _ArgsWrapper(args or {}) assert args.batch_size, 'Batch size was not given in args dict' n = len(numpy_inputs) assert n > 0 assert n == len(tf_inputs) m = numpy_inputs[0].shape[0] for i in xrange(1, n): assert numpy_inputs[i].shape[0] == m out = [] for _ in tf_outputs: out.append([]) with sess.as_default(): for start in xrange(0, m, args.batch_size): batch = start // args.batch_size if batch % 100 == 0 and batch > 0: _logger.debug('Batch ' + str(batch)) start = batch * args.batch_size end = start + args.batch_size numpy_input_batches = [numpy_input[start:end] for numpy_input in numpy_inputs] cur_batch_size = numpy_input_batches[0].shape[0] assert cur_batch_size <= args.batch_size for e in numpy_input_batches: assert e.shape[0] == cur_batch_size feed_dict = dict(zip(tf_inputs, numpy_input_batches)) if feed is not None: feed_dict.update(feed) numpy_output_batches = sess.run(tf_outputs, feed_dict=feed_dict) for e in numpy_output_batches: assert e.shape[0] == cur_batch_size, e.shape for out_elem, numpy_output_batch in zip(out, numpy_output_batches): out_elem.append(numpy_output_batch) out = [np.concatenate(x, axis=0) for x in out] for e in out: assert e.shape[0] == m, e.shape return out 
bert|openai|pytorch|modeling|pretrained|swish def swish(x): return x * torch.sigmoid(x) 
strip|texar|eos|recur|utils def _recur_strip(s): if is_str(s): s_tokens = s.split() if eos_token in s_tokens: return ' '.join(s_tokens[:s_tokens.index(eos_token)]) else: return s else: s_ = [_recur_strip(si) for si in s] return _maybe_list_to_array(s_, s) 
EigenMultivariateNormal|distributions|regression|init|controller def __init__(self, mean, u_b=None, v_b=None, r=None, group_event_ndims=0, is_reparameterized=True, check_numerics=False): mean = tf.convert_to_tensor(mean) _assert_rank_op = tf.assert_greater_equal(tf.rank(mean), 2, message= 'mean should be at least a 2-D tensor.') with tf.control_dependencies([_assert_rank_op]): self._mean = mean u_b = tf.convert_to_tensor(u_b) self._u_b = u_b v_b = tf.convert_to_tensor(v_b) self._v_b = v_b r = tf.convert_to_tensor(r) self._r = r dtype = assert_same_float_dtype([(self._mean, 'MatrixVariateNormal.mean'), (self._u_b, 'MatrixVariateNormal.u_b'), (self._v_b, 'MatrixVariateNormal.v_b'), (self._r, 'MatrixVariateNormal.r')]) self.log_std = 0.5 * tf.log(self._r) self.std = tf.exp(self.log_std) super(EigenMultivariateNormal, self).__init__(dtype=dtype, param_dtype= dtype, is_continuous=True, is_reparameterized=is_reparameterized, group_ndims=group_event_ndims) 
models|loss|MLP|graphsage def _loss(self): for var in self.layers[0].vars.values(): self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var) if self.categorical: self.loss += metrics.masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask']) else: diff = self.labels - self.outputs self.loss += tf.reduce_sum(tf.sqrt(tf.reduce_sum(diff * diff, axis=1))) 
cupy|utils|asnumpy def asnumpy(x): if cupy is not None: return cupy.asnumpy(x) else: return numpy.asarray(x) 
init|LayerNorm|gpt def __init__(self, n_state, e=1e-05): super(LayerNorm, self).__init__() self.g = nn.Parameter(torch.ones(n_state)) self.b = nn.Parameter(torch.zeros(n_state)) self.e = e 
samplerate|utils|espeakng|ESpeakNG @property def samplerate(self): return self._samplerate 
clean|chatbots|CornellChatbotBasic|line|cornell def clean_line(self, line): """ Params: :line: Line to be processed and returned. """  def replace(matchobj): return re.sub("'", " '", str(matchobj.group(0)))  def replace_null(matchobj): return re.sub("'", '', str(matchobj.group(0))) line = re.sub("[^a-z .?!'0-9]", '', line) line = re.sub('[.]', ' . ', line) line = re.sub('[?]', ' ? ', line) line = re.sub('[!]', ' ! ', line) line = re.sub("[ ]'[ ]", ' ', line) line = re.sub(" '[a-z]", replace_null, line) line = re.sub("n't", " n't", line) line = re.sub("[^ n]'[^ t]", replace, line) return line 
lrelu|ops def lrelu(x, leak=0.2, name='lrelu'): return tf.maximum(x, leak * x) 
inttotoken|utils|get|SMILESX def get_inttotoken(tokens): return dict((i, c) for i, c in enumerate(tokens)) 
1|networks|get|elpips|squeezenet1|slice3 def get_slice3(self, input): with tf.name_scope('slice3'): net = self._pool(input, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name='max_pool_slice3', data_format='NHWC') net = self.fire_module(input=net, index=6, ch_in=128, ch_out_squeeze=32, ch_out_expand=128) return self.fire_module(input=net, index=7, ch_in=256, ch_out_squeeze=32, ch_out_expand=128) 
texar|multihead|modules|attention|MultiheadAttentionEncoder|build|encoders def _build(self, queries, memory, memory_attention_bias, cache=None, mode=None ): """Encodes the inputs.  Args: queries: A 3d tensor with shape of [batch, length_query, depth_query]. memory: A 3d tensor with shape of [batch, length_key, depth_key]. memory_attention_bias: A 3d tensor with shape of [batch, length_key, num_units]. cache: Memory cache only when inferencing the sentence from sractch. mode (optional): A tensor taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including `TRAIN`, `EVAL` and `PREDICT`. Controls dropout mode. If `None` (default), :func:`texar.global_mode` is used.  Returns: A Tensor of shape `[batch_size, max_time, dim]` containing the encoded vectors. """ with tf.variable_scope(self.variable_scope): num_heads = self._hparams.num_heads num_units = self._hparams.num_units if num_units % num_heads: raise ValueError( 'Value depth (%d) must be divisible by the number of attention heads (%d).' % (num_units, num_heads))  def _update_and_return(layer, key): if memory is None: out = layer(queries) if cache is not None: key = 'self_{}'.format(key) res = cache[key] if isinstance(res, tf.TensorArray): res = res.write(res.size(), tf.squeeze(out, axis=[1])) out = transpose_batch_time(res.stack()) else: res = tf.concat([res, out], axis=1) out = res cache[key] = res elif cache is not None: key = 'memory_{}'.format(key) res = cache[key] if isinstance(res, tf.TensorArray): size = res.size() false_fn = lambda : transpose_batch_time(res.stack()) else: size = tf.shape(res)[1] false_fn = lambda : res out = tf.cond(tf.equal(size, 0), true_fn=lambda : layer( memory), false_fn=false_fn) else: out = layer(memory) return out Q = self.Q_dense(queries) K = _update_and_return(self.K_dense, 'keys') V = _update_and_return(self.V_dense, 'values') Q_ = self._split_heads(Q) K_ = self._split_heads(K) V_ = self._split_heads(V) key_depth_per_head = num_units // num_heads Q_ *= key_depth_per_head ** -0.5 logits = tf.matmul(Q_, K_, transpose_b=True) if memory_attention_bias is not None: logits += memory_attention_bias weights = tf.nn.softmax(logits, name='attention_weights') weights = tf.layers.dropout(weights, rate=self._hparams. dropout_rate, training=is_train_mode(mode)) outputs = tf.matmul(weights, V_) outputs = self._combine_heads(outputs) outputs = self.O_dense(outputs) if not self._built: self._add_internal_trainable_variables() self._built = True return outputs 
GaussianMixture|sum|models|log|exp def _sum_log_exp(self, X, weights, mu, log_var): diff = tf.expand_dims(X, 0) - tf.expand_dims(mu, 1) diff_times_inv_cov = diff * tf.expand_dims(1.0 / tf.exp(log_var), 1) sum_sq_dist_times_inv_cov = tf.reduce_sum(diff_times_inv_cov * diff, axis=2 ) ln2piD = tf.log(2 * np.pi) * self.dim lnD = tf.reduce_sum(log_var, axis=1) log_coefficients = tf.expand_dims(ln2piD + lnD, 1) log_components = -0.5 * (log_coefficients + sum_sq_dist_times_inv_cov) log_weighted = log_components + tf.expand_dims(tf.log(weights), 1) log_shift = tf.expand_dims(tf.reduce_max(log_weighted, 0), 0) return log_weighted, log_shift 
output|Output|model|save|latest def save_model_latest(self, session, epoch, g_step): self.latest_weights_file = self.tf_saver_latest.save(session, '{}-{}-latest'.format(self.model_file_base, epoch), global_step=g_step) 
ConfigType|europilot|train|getattr def __getattr__(self, attr): raise TrainException('Invalid configuration: %s' % attr) 
localization|get|main def main(data_dir=DATA_DIRECTORY, data_list=DATA_LIST_PATH, restore_from= RESTORE_FROM, save_dir=SAVE_DIR, n_classes=NUM_CLASSES, adapt=False): """Create the model and obtain the localization cues.""" graph = tf.Graph() with graph.as_default(): coord = tf.train.Coordinator() with tf.name_scope('create_inputs'): reader = ImageReader_Classfc(data_dir, data_list, None, 1234, False, False, n_classes, coord) image, catg_with_bcgd, catg_wo_bcgd = (reader.image, reader. catg_with_bcgd, reader.catg_wo_bcgd) image_batch, catg_batch_with_bcgd, catg_batch_wo_bcgd = tf.expand_dims( image, dim=0), tf.expand_dims(catg_with_bcgd, dim=0 ), tf.expand_dims(catg_wo_bcgd, dim=0) h_orig, w_orig = tf.to_float(tf.shape(image_batch)[1]), tf.to_float(tf .shape(image_batch)[2]) image_batch075 = tf.image.resize_images(image_batch, tf.stack([tf. to_int32(tf.multiply(h_orig, 0.75)), tf.to_int32(tf.multiply( w_orig, 0.75))])) image_batch05 = tf.image.resize_images(image_batch, tf.stack([tf. to_int32(tf.multiply(h_orig, 0.5)), tf.to_int32(tf.multiply( w_orig, 0.5))])) with tf.variable_scope('', reuse=False): net = DeepLabResNetModel({'data': image_batch}, is_training=False) with tf.variable_scope('', reuse=True): net075 = DeepLabResNetModel({'data': image_batch075}, is_training=False) with tf.variable_scope('', reuse=True): net05 = DeepLabResNetModel({'data': image_batch05}, is_training =False) restore_var = tf.global_variables() raw_output100_init = net.layers['fc1_voc12_d0'] raw_output075_init = tf.image.resize_images(net075.layers[ 'fc1_voc12_d0'], tf.shape(raw_output100_init)[1:3,]) raw_output05_init = tf.image.resize_images(net05.layers[ 'fc1_voc12_d0'], tf.shape(raw_output100_init)[1:3,]) raw_output_init = tf.reduce_max(tf.stack([raw_output100_init, raw_output075_init, raw_output05_init]), axis=0) raw_output100_adapt = net.layers['fc1_voc12'] raw_output075_adapt = tf.image.resize_images(net075.layers[ 'fc1_voc12'], tf.shape(raw_output100_adapt)[1:3,]) raw_output05_adapt = tf.image.resize_images(net05.layers[ 'fc1_voc12'], tf.shape(raw_output100_adapt)[1:3,]) raw_output_adapt = tf.reduce_max(tf.stack([raw_output100_adapt, raw_output075_adapt, raw_output05_adapt]), axis=0) catg_vec_with_bcgd = tf.expand_dims(tf.expand_dims( catg_batch_with_bcgd, dim=1), dim=2) catg_vec_wo_bcgd = tf.expand_dims(tf.expand_dims(catg_batch_wo_bcgd, dim=1), dim=2) raw_output_up_init = tf.image.resize_bilinear(raw_output_init, tf. shape(image_batch)[1:3,]) raw_output_up_adapt = tf.image.resize_bilinear(raw_output_adapt, tf .shape(image_batch)[1:3,]) raw_output_up_init = raw_output_up_init - tf.reduce_min(tf. reduce_min(raw_output_up_init, axis=1, keep_dims=True), axis=2, keep_dims=True) + EPSILON raw_output_up_init = raw_output_up_init / tf.reduce_max(tf. reduce_max(raw_output_up_init, axis=1, keep_dims=True), axis=2, keep_dims=True) attention_init = raw_output_up_init * catg_vec_wo_bcgd local_cues_init = tf.squeeze(attention_init, axis=0) attention_adapt = raw_output_up_adapt * catg_vec_with_bcgd attention_adapt = tf.argmax(attention_adapt, axis=3) local_cues_adapt = tf.expand_dims(attention_adapt, dim=3) local_cues_adapt = tf.squeeze(local_cues_adapt, axis=0) indices_with_bcgd = tf.cast(tf.squeeze(tf.where(tf.greater( catg_with_bcgd, 0.0)), 1), tf.int32) indices_wo_bcgd = tf.cast(tf.squeeze(tf.where(tf.greater( catg_wo_bcgd, 0.0)), 1), tf.int32) config = tf.ConfigProto() config.gpu_options.allow_growth = True with tf.Session(config=config, graph=graph) as sess: tf.global_variables_initializer().run() loader = tf.train.Saver(var_list=restore_var) if restore_from is not None: load(loader, sess, restore_from) threads = tf.train.start_queue_runners(coord=coord, sess=sess) images_list = get_images_list(data_list) for i in range(len(images_list)): if adapt: attentions, catgs = sess.run([local_cues_adapt, indices_with_bcgd]) final_attentions = attentions else: attentions, catgs = sess.run([local_cues_init, indices_wo_bcgd] ) final_attentions = attentions[:, :, (catgs)] base_fname = images_list[i].strip('\n').rsplit('/', 1)[1].replace( 'jpg', 'npz') f_name = save_dir + '/' + base_fname np.savez(f_name, actv=final_attentions) if i % 1000 == 0: print('Processed {}/{}'.format(i, len(images_list))) coord.request_stop() coord.join(threads) 
run|after|utils|hooks|EvaluationHook|thumt def after_run(self, run_context, run_values): stale_global_step = run_values.results if self._timer.should_trigger_for_step(stale_global_step + 1): global_step = run_context.session.run(self._global_step) if self._timer.should_trigger_for_step(global_step): self._timer.update_last_triggered_step(global_step) save_path = os.path.join(self._base_dir, 'model.ckpt') saver = _get_saver() tf.logging.info('Saving checkpoints for %d into %s.' % ( global_step, save_path)) saver.save(run_context.session, save_path, global_step=global_step) tf.logging.info('Validating model at step %d' % global_step) score = _evaluate(self._eval_fn, self._eval_input_fn, self. _eval_decode_fn, self._base_dir, self._session_config) tf.logging.info('%s at step %d: %f' % (self._metric, global_step, score)) _save_log(self._log_name, (self._metric, global_step, score)) checkpoint_filename = os.path.join(self._base_dir, 'checkpoint') all_checkpoints = _read_checkpoint_def(checkpoint_filename) records = _read_score_record(self._record_name) latest_checkpoint = all_checkpoints[-1] record = [latest_checkpoint, score] added, removed, records = _add_to_record(records, record, self. _max_to_keep) if added is not None: old_path = os.path.join(self._base_dir, added) new_path = os.path.join(self._save_path, added) old_files = tf.gfile.Glob(old_path + '*') tf.logging.info('Copying %s to %s' % (old_path, new_path)) for o_file in old_files: n_file = o_file.replace(old_path, new_path) tf.gfile.Copy(o_file, n_file, overwrite=True) if removed is not None: filename = os.path.join(self._save_path, removed) tf.logging.info('Removing %s' % filename) files = tf.gfile.Glob(filename + '*') for name in files: tf.gfile.Remove(name) _save_score_record(self._record_name, records) checkpoint_filename = checkpoint_filename.replace(self. _base_dir, self._save_path) _save_checkpoint_def(checkpoint_filename, [item[0] for item in records]) best_score = records[0][1] tf.logging.info('Best score at step %d: %f' % (global_step, best_score)) 
fisher|ops|classification|factors|ConvOutputKroneckerFactor|cov|shape @property def _cov_shape(self): size = self._out_channels return [size, size] 
filter|filters|ColorizerFilter|fasterai def filter(self, orig_image: PilImage, filtered_image: PilImage, render_factor: int) ->PilImage: render_sz = render_factor * self.render_base model_image = self._model_process(orig=filtered_image, sz=render_sz) if self.map_to_orig: return self._post_process(model_image, orig_image) else: return self._post_process(model_image, filtered_image) 
test|testGetIteratorWithSkipCount|iterator|nmt|IteratorUtilsTest|utils def testGetIteratorWithSkipCount(self): tf.set_random_seed(1) tgt_vocab_table = src_vocab_table = lookup_ops.index_table_from_tensor(tf .constant(['a', 'b', 'c', 'eos', 'sos'])) src_dataset = tf.data.Dataset.from_tensor_slices(tf.constant(['c a', 'c c a', 'd', 'f e a g'])) tgt_dataset = tf.data.Dataset.from_tensor_slices(tf.constant(['b c', 'a b', '', 'c c'])) hparams = tf.contrib.training.HParams(random_seed=3, num_buckets=5, eos ='eos', sos='sos') batch_size = 2 src_max_len = 3 skip_count = tf.placeholder(shape=(), dtype=tf.int64) dataset = iterator_utils.get_iterator(src_dataset=src_dataset, tgt_dataset=tgt_dataset, src_vocab_table=src_vocab_table, tgt_vocab_table=tgt_vocab_table, batch_size=batch_size, sos=hparams .sos, eos=hparams.eos, random_seed=hparams.random_seed, num_buckets =hparams.num_buckets, src_max_len=src_max_len, skip_count= skip_count, reshuffle_each_iteration=False) table_initializer = tf.tables_initializer() iterator = dataset.make_initializable_iterator() get_next = iterator.get_next() with self.test_session() as sess: sess.run(table_initializer) sess.run(iterator.initializer, feed_dict={skip_count: 1}) features = sess.run(get_next) self.assertAllEqual([[-1, -1, 0], [2, 2, 0]], features['source']) self.assertAllEqual([3, 3], features['source_sequence_length']) self.assertAllEqual([[4, 2, 2], [4, 0, 1]], features['target_input']) self.assertAllEqual([[2, 2, 3], [0, 1, 3]], features['target_output']) self.assertAllEqual([3, 3], features['target_sequence_length']) sess.run(iterator.initializer, feed_dict={skip_count: 0}) features = sess.run(get_next) self.assertAllEqual([[-1, -1, 0], [2, 2, 0]], features['source']) self.assertAllEqual([3, 3], features['source_sequence_length']) self.assertAllEqual([[4, 2, 2], [4, 0, 1]], features['target_input']) self.assertAllEqual([[2, 2, 3], [0, 1, 3]], features['target_output']) self.assertAllEqual([3, 3], features['target_sequence_length']) 
tests|test|FM|interaction|layers def test_FM(): with CustomObjectScope({'FM': layers.FM}): layer_test(layers.FM, kwargs={}, input_shape=(BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE)) 
devtools|cleverhans|tests|handle|module|docscrape def handle_module(val, name): module_errors = [] docstring = val if docstring is None: module_errors.append((name, '**missing** module-level docstring')) else: module_errors = [(name, e) for e in NumpyModuleDocString(docstring) .get_errors()] return module_errors 
BiInteractionPooling|interaction|deepctr|layers|build def build(self, input_shape): if len(input_shape) != 3: raise ValueError( 'Unexpected inputs dimensions %d, expect to be 3 dimensions' % len(input_shape)) super(BiInteractionPooling, self).build(input_shape) 
sparse|gen|plot|for|resnet5|iterative|epoch def plot_resnet50_sparse_iterative_epoch_for_epoch(): common.epoch_for_epoch_plot(network=common.RESNET50, is_iterative=True, prune_method=common.UNSTRUCTURED, min_max_y=(-0.05, 0.01)) 
bert|openai|get|lr|OpenAIAdam|pytorch|optimization|pretrained def get_lr(self): lr = [] for group in self.param_groups: for p in group['params']: state = self.state[p] if len(state) == 0: return [0] if group['t_total'] != -1: schedule_fct = SCHEDULES[group['schedule']] lr_scheduled = group['lr'] * schedule_fct(state['step'] / group['t_total'], group['warmup']) else: lr_scheduled = group['lr'] lr.append(lr_scheduled) return lr 
mac|training|train|batch def train_batch(optims, draw_pert_op, train_batch_count, sess, args): """ perform z-steps, w-steps, noise-draws and clipping according to the chosen optimizers :param sess: session :param optims: optimizer tuple :param draw_pert_op: drawing new noise :param train_batch_count: :param args: run arguments :return: """ for _ in range(args.num_z_steps): sess.run(optims.z) sess.run(draw_pert_op) if args.log_w_opt: w_step_logging(sess, train_batch_count) sess.run(optims.w) 
VocabEntry|vocab|id2word def id2word(self, wid): return self.id2word[wid] 
nodes|tmp|rename|master|facenet|main|deepdream def rename_nodes(graph_def, rename_func): res_def = tf.GraphDef() for n0 in graph_def.node: n = res_def.node.add() n.MergeFrom(n0) n.name = rename_func(n.name) for i, s in enumerate(n.input): n.input[i] = rename_func(s) if s[0] != '^' else '^' + rename_func(s [1:]) return res_def 
RDF2Vec|init|MySentences def __init__(self, dirname, filename): self.dirname = dirname self.filename = filename 
ABReluTest|test|abs|stax def test_abs(self, same_inputs): key = random.PRNGKey(1) X0_1 = random.normal(key, (5, 7)) fc = stax.Dense(10, 1, 0) X0_2 = None if same_inputs else random.normal(key, (9, 7)) init_fn, apply_leaky_relu, kernel_fn_abs = stax.serial(fc, stax.Abs()) _, apply_ab_relu, kernel_fn_ab_relu = stax.serial(fc, stax.ABRelu(-1, 1)) params = init_fn(key, input_shape=(-1, 7)) X1_1_abs = apply_leaky_relu(params, X0_1) X1_1_ab_relu = apply_ab_relu(params, X0_1) self.assertAllClose(X1_1_abs, X1_1_ab_relu, True) kernels_abs = kernel_fn_abs(X0_1, X0_2, ('nngp', 'ntk')) kernels_ab_relu = kernel_fn_ab_relu(X0_1, X0_2, ('nngp', 'ntk')) self.assertAllClose(kernels_abs, kernels_ab_relu, True) 
encoder|model|build def build_encoder(embeddings, encoder_inputs, encoder_length, num_layers, num_units, cell_type, bidir=False, dtype=tf.float32, name='encoder'): """ encoder: build rnn encoder for Seq2seq source_ids: [batch_size, max_time] bidir: bidirectional or unidirectional  Returns: encoder_outputs: [batch_size, max_time, num_units] encoder_states: (StateTuple(shape=(batch_size, num_units)), ...) """ embed_inputs = tf.nn.embedding_lookup(embeddings, encoder_inputs) if bidir: encoder_states = [] layer_inputs = embed_inputs for i in range(num_layers): with tf.variable_scope(name + '_layer_%d' % (i + 1)): fw_cell = create_rnn_cell(1, num_units, cell_type) bw_cell = create_rnn_cell(1, num_units, cell_type) dyn_rnn = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, layer_inputs, sequence_length=encoder_length, dtype= dtype, swap_memory=True) bi_outputs, (state_fw, state_bw) = dyn_rnn if cell_type == 'LSTM': state_c = state_fw.c + state_bw.c state_h = state_fw.h + state_bw.h encoder_states.append(LSTMStateTuple(state_c, state_h)) else: encoder_states.append(state_fw + state_bw) layer_inputs = tf.layers.dense(tf.concat(bi_outputs, -1), num_units) encoder_outputs = layer_inputs encoder_states = tuple(encoder_states) else: rnn_cell = create_rnn_cell(num_layers, num_units, cell_type) encoder_outputs, encoder_states = tf.nn.dynamic_rnn(rnn_cell, embed_inputs, sequence_length=encoder_length, dtype=dtype, swap_memory=True) return encoder_outputs, encoder_states 
unet|main def main(_): if not os.path.exists(FLAGS.checkpoint_dir): os.makedirs(FLAGS.checkpoint_dir) if not os.path.exists(FLAGS.results_dir): os.makedirs(FLAGS.results_dir) if not os.path.exists(FLAGS.best_checkpoint_dir): os.makedirs(FLAGS.best_checkpoint_dir) gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_frac) patch_shape = 32, 32, 32 extraction_step = 8, 8, 8 testing_extraction_shape = 8, 8, 8 if FLAGS.training: with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options) ) as sess: network = UNET(sess, patch_shape, extraction_step) network.build_model() network.train() if FLAGS.testing: test(patch_shape, testing_extraction_shape) 
ExamplesPerSecondHook|cifar1|before|utils|run def before_run(self, run_context): return basic_session_run_hooks.SessionRunArgs(self._global_step_tensor) 
SNResNetConcatDiscriminator|call|snresnet def __call__(self, x, y=None): h = x h = self.block1(h) h = self.block2(h) h = self.block3(h) if y is not None: emb = self.l_y(y) H, W = h.shape[2], h.shape[3] emb = F.broadcast_to(F.reshape(emb, (emb.shape[0], emb.shape[1], 1, 1)), (emb.shape[0], emb.shape[1], H, W)) h = F.concat([h, emb], axis=1) h = self.block4(h) h = self.block5(h) h = self.block6(h) h = self.activation(h) h = F.sum(h, axis=(2, 3)) output = self.l7(h) return output 
compute|avod|losses|loss|core|WeightedSmoothL1Loss def _compute_loss(self, prediction_tensor, target_tensor, weight): """Compute loss function. Args: prediction_tensor: A float tensor of shape [num_anchors, code_size] representing the (encoded) predicted locations of objects. target_tensor: A float tensor of shape [num_anchors, code_size] representing the regression targets Returns: loss: an anchorwise tensor of shape [num_anchors] representing the value of the loss function """ diff = prediction_tensor - target_tensor abs_diff = tf.abs(diff) abs_diff_lt_1 = tf.less(abs_diff, 1) anchorwise_smooth_l1norm = tf.reduce_sum(tf.where(abs_diff_lt_1, 0.5 * tf.square(abs_diff), abs_diff - 0.5), axis=1) * weight return anchorwise_smooth_l1norm 
texar|unmerge|dim|search|beam|utils def _unmerge_beam_dim(tensor, batch_size, beam_size): """Reshapes first dimension back to [batch_size, beam_size].  Args: tensor: Tensor to reshape of shape [batch_size*beam_size, ...] batch_size: Tensor, original batch size. beam_size: int, original beam size.  Returns: Reshaped tensor of shape [batch_size, beam_size, ...] """ shape = shape_list(tensor) new_shape = [batch_size] + [beam_size] + shape[1:] return tf.reshape(tensor, new_shape) 
mars|init|Mars|train def __init__(self, dataset_dir, num_validation_y=0.1, seed=1234): self._dataset_dir = dataset_dir self._num_validation_y = num_validation_y self._seed = seed 
R2RProblem|init|problem|r2r def __init__(self, runtime_config, mode, data_sources, curriculum=''): self._runtime_config = runtime_config self._mode = mode self._data_sources = data_sources self._curriculum = curriculum self._agent = agent.R2RAgent(agent_config.get_r2r_agent_config()) self._prob_ac = 0.5 self._env = None self._loss_type = None self._eval_dict = self._get_eval_dict() 
texar|multi|MultiAlignedData|aligned|init|data|embedding|value def embedding_init_value(self, name_or_id): """Returns the `Tensor` of embedding init value of the dataset by its name or id. `None` if the dataset is not of text type. """ i = self._maybe_name_to_id(name_or_id) return self._embedding[i] 
sph3gcn|conv3d|separable|util def separable_conv3d(inputs, num_out_channels, kernel_size, depth_multiplier, scope, nn_index, nn_count, filt_index, use_xavier= True, stddev=0.001, weight_decay=None, activation_fn=tf.nn.elu, with_bn =False, with_bias=False, reuse=None, is_training=None): """ 3D separable convolution with non-linear operation.  Args: inputs: 3-D tensor variable BxNxC num_out_channels: int kernel_size: int depth_multiplier: int scope: string nn_index: int32 array, neighbor indices nn_count: int32 array, number of neighbors filt_index: int32 array, filter bin indices use_xavier: bool, use xavier_initializer if true stddev: float, stddev for truncated_normal init weight_decay: float activation_fn: function with_bn: bool, whether to use batch norm is_training: bool Tensor variable  Returns: Variable tensor """ with tf.variable_scope(scope, reuse=reuse) as sc: num_in_channels = inputs.get_shape().as_list()[-1] depthwise_kernel_shape = [kernel_size, num_in_channels, depth_multiplier] depthwise_kernel = _variable_with_weight_decay('depthwise_weights', shape=depthwise_kernel_shape, use_xavier=use_xavier, stddev= stddev, with_decay=weight_decay) outputs = tf_conv3d.depthwise_conv3d(inputs, depthwise_kernel, nn_index, nn_count, filt_index) batch_size = outputs.get_shape().as_list()[0] num_in_channels = outputs.get_shape().as_list()[-1] kernel_shape = [num_in_channels, num_out_channels] kernel = _variable_with_weight_decay('weights', shape=kernel_shape, use_xavier=use_xavier, stddev=stddev, with_decay=weight_decay) outputs = tf.reshape(outputs, [-1, num_in_channels]) outputs = tf.matmul(outputs, kernel) outputs = tf.reshape(outputs, [batch_size, -1, num_out_channels]) if with_bias: biases = tf.get_variable('biases', [num_out_channels], dtype=tf .float32, initializer=tf.constant_initializer(0.0)) outputs = tf.nn.bias_add(outputs, biases) if activation_fn is not None: outputs = activation_fn(outputs) if with_bn: outputs = batch_normalization(outputs, is_training, name='bn', reuse=reuse) return outputs 
rnn|size|output|PLSTM|translate @property def output_size(self): if self._proj_size is not None: return self._proj_size else: return self._num_units 
space|avod|project|image|projector|3d|core|box|to def project_to_image_space(box_3d, calib_p2, truncate=False, image_size= None, discard_before_truncation=True): """ Projects a box_3d into image space  Args: box_3d: single box_3d to project calib_p2: stereo calibration p2 matrix truncate: if True, 2D projections are truncated to be inside the image image_size: [w, h] must be provided if truncate is True, used for truncation discard_before_truncation: If True, discard boxes that are larger than 80% of the image in width OR height BEFORE truncation. If False, discard boxes that are larger than 80% of the width AND height AFTER truncation.  Returns: Projected box in image space [x1, y1, x2, y2] Returns None if box is not inside the image """ format_checker.check_box_3d_format(box_3d) obj_label = box_3d_encoder.box_3d_to_object_label(box_3d) corners_3d = obj_utils.compute_box_corners_3d(obj_label) projected = calib_utils.project_to_image(corners_3d, calib_p2) x1 = np.amin(projected[0]) y1 = np.amin(projected[1]) x2 = np.amax(projected[0]) y2 = np.amax(projected[1]) img_box = np.array([x1, y1, x2, y2]) if truncate: if not image_size: raise ValueError('Image size must be provided') image_w = image_size[0] image_h = image_size[1] if img_box[0] > image_w or img_box[1] > image_h or img_box[2 ] < 0 or img_box[3] < 0: return None if discard_before_truncation: img_box_w = img_box[2] - img_box[0] img_box_h = img_box[3] - img_box[1] if img_box_w > image_w * 0.8 or img_box_h > image_h * 0.8: return None if img_box[0] < 0: img_box[0] = 0 if img_box[1] < 0: img_box[1] = 0 if img_box[2] > image_w: img_box[2] = image_w if img_box[3] > image_h: img_box[3] = image_h if not discard_before_truncation: img_box_w = img_box[2] - img_box[0] img_box_h = img_box[3] - img_box[1] if img_box_w > image_w * 0.8 and img_box_h > image_h * 0.8: return None return img_box 
examples|get|datasets|dataset def get_dataset(name, n_train=None, n_test=None, permute_train=False, do_flatten_and_normalize=True): """Download, parse and process a dataset to unit scale and one-hot labels.""" ds_builder = tfds.builder(name) use_s3_api = ds_builder.version.implements(tfds.core.Experiment.S3) if use_s3_api: ds_train, ds_test = tfds.as_numpy(tfds.load(name + ':3.*.*', split= ['train' + ('[:%d]' % n_train if n_train is not None else ''), 'test' + ('[:%d]' % n_test if n_test is not None else '')], batch_size=-1, as_dataset_kwargs={'shuffle_files': False})) else: ds_train, ds_test = tfds.as_numpy(tfds.load(name, split=['train', 'test'], batch_size=-1, as_dataset_kwargs={'shuffle_files': False}) ) train_images, train_labels, test_images, test_labels = ds_train['image' ], ds_train['label'], ds_test['image'], ds_test['label'] if not use_s3_api: if n_train is not None: train_images = train_images[:n_train] train_labels = train_labels[:n_train] if n_test is not None: test_images = test_images[:n_test] test_labels = test_labels[:n_test] if do_flatten_and_normalize: train_images = _partial_flatten_and_normalize(train_images) test_images = _partial_flatten_and_normalize(test_images) num_classes = ds_builder.info.features['label'].num_classes train_labels = _one_hot(train_labels, num_classes) test_labels = _one_hot(test_labels, num_classes) if permute_train: perm = np.random.RandomState(0).permutation(train_images.shape[0]) train_images = train_images[perm] train_labels = train_labels[perm] return train_images, train_labels, test_images, test_labels 
ProcessFile|cpplint def ProcessFile(filename, vlevel, extra_check_functions=[]): """Does google-lint on a single file.  Args: filename: The name of the file to parse.  vlevel: The level of errors to report.  Every error of confidence >= verbose_level will be reported.  0 is a good default.  extra_check_functions: An array of additional check functions that will be run on each source line. Each function takes 4 arguments: filename, clean_lines, line, error """ _SetVerboseLevel(vlevel) _BackupFilters() if not ProcessConfigOverrides(filename): _RestoreFilters() return lf_lines = [] crlf_lines = [] try: if filename == '-': lines = codecs.StreamReaderWriter(sys.stdin, codecs.getreader( 'utf8'), codecs.getwriter('utf8'), 'replace').read().split('\n' ) else: lines = codecs.open(filename, 'r', 'utf8', 'replace').read().split( '\n') for linenum in range(len(lines) - 1): if lines[linenum].endswith('\r'): lines[linenum] = lines[linenum].rstrip('\r') crlf_lines.append(linenum + 1) else: lf_lines.append(linenum + 1) except IOError: sys.stderr.write("Skipping input '%s': Can't open for reading\n" % filename) _RestoreFilters() return file_extension = filename[filename.rfind('.') + 1:] if filename != '-' and file_extension not in _valid_extensions: sys.stderr.write('Ignoring %s; not a valid file name (%s)\n' % ( filename, ', '.join(_valid_extensions))) else: ProcessFileData(filename, file_extension, lines, Error, extra_check_functions) if lf_lines and crlf_lines: for linenum in crlf_lines: Error(filename, linenum, 'whitespace/newline', 1, 'Unexpected \\r (^M) found; better to use only \\n') sys.stderr.write('Done processing %s\n' % filename) _RestoreFilters() 
gan|critic|fasterai|custom|critics def custom_gan_critic(n_channels: int=3, nf: int=256, n_blocks: int=3, p: int=0.15): """Critic to train a `GAN`.""" layers = [_conv(n_channels, nf, ks=4, stride=2), nn.Dropout2d(p / 2)] for i in range(n_blocks): layers += [_conv(nf, nf, ks=3, stride=1), nn.Dropout2d(p), _conv(nf, nf * 2, ks=4, stride=2, self_attention=i == 0)] nf *= 2 layers += [_conv(nf, nf, ks=3, stride=1), _conv(nf, 1, ks=4, bias=False, padding=0, use_activ=False), Flatten()] return nn.Sequential(*layers) 
convert|minigo|main def main(): if len(sys.argv) < 2: print('Model filename without extension needed as an argument.') exit() model = sys.argv[1] print('loading ', model) print() weights = getMinigoWeightsV2(model) if 0: for name, variables in [('load_checkpoint', var_names.keys())]: print(name, len(variables)) print(deduped(variables)) print() save_leelaz_weights(model + '_converted.txt.gz', merge_gammas(weights)) 
get|deepctr|DNN|core|config|layers def get_config(self): config = {'activation': self.activation, 'hidden_units': self. hidden_units, 'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate, 'seed': self.seed} base_config = super(DNN, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
CpgController|plot|gym|control|cpg|oscillator|daisy|custom def plot(self): for y in self.y_data: plt.plot(self.x_data, y, linewidth=2.5) plt.xlabel('Time [s]', fontsize=12) plt.ylabel('Oscillator output', fontsize=12) plt.show() 
optimizers|get def get_optimizers(w_list, z_list, losses, args): """ create a tuple of optimizer operations depending on settings. also return global steps to track learning rate decay and w-learning rate for logging :param w_list: weights per layer :param z_list: zs per layer :param losses: tuple of all losses :param args: run arguments :return: """ w_lr, z_lr, global_w_step = get_learning_rates(args) w_optim = get_w_optim_sgd(losses, w_list, w_lr, global_w_step, args) z_optim = get_z_optim_sgd(losses, z_lr, z_list, args) optim_collect = namedtuple('optim', ['w', 'z']) optims = optim_collect(w_optim, z_optim) return optims, global_w_step, w_lr 
NatSR|generate|ready|data|SR def data_ready_SR(data_path, label_path, tfrecord_file, patch_h, patch_w, stride): label_list = np.sort(np.asarray(glob.glob(os.path.join(label_path, '/*.png')))) img_list = np.sort(np.asarray(glob.glob(os.path.join(data_path, 'X' + str(scale) + '/*.png')))) offset = 0 fileNum = len(label_list) patches = [] labels = [] for n in range(fileNum): img = imread(img_list[n]) label = imread(label_list[n]) assert os.path.basename(img_list[n])[:-6] == os.path.basename( label_list[n])[:-4] x, y, ch = label.shape for m in range(8): for i in range(0 + offset, x - patch_h + 1, stride): for j in range(0 + offset, y - patch_w + 1, stride): patch_d = img[i // scale:i // scale + patch_h // scale, j // scale:j // scale + patch_w // scale] patch_l = label[i:i + patch_h, j:j + patch_w] if gradients(patch_l.astype(np.float64) / 255.0 ) >= 0.005 and np.var(patch_l.astype(np.float64) / 255.0) >= 0.03: patches.append(augmentation(patch_d, m).tobytes()) labels.append(augmentation(patch_l, m).tobytes()) np.random.seed(36) np.random.shuffle(patches) np.random.seed(36) np.random.shuffle(labels) print(len(patches)) print('Shape: [%d, %d, %d]' % (patch_h, patch_w, ch)) writer = tf.python_io.TFRecordWriter(tfrecord_file) for i in range(len(patches)): write_to_tfrecord(writer, labels[i], patches[i]) writer.close() 
DatasetTable2text|predict|table2text|print|data|utils def print_predict(self, model_name, output_dict=None, batch_dict=None, output_fd=None, num_cases=0): """print the output prediction""" batch_size = batch_dict['enc_keys'].shape[0] for i in range(batch_size): str_out = 'enc_keys    | ' elen = batch_dict['enc_lens'][i] str_out += ' '.join([self.id2key[k] for k in batch_dict['enc_keys'] [i][:elen]]) + '\n' str_out += 'enc_locs    | ' str_out += ' '.join([str(l) for l in batch_dict['enc_locs'][i][:elen]] ) + '\n' str_out += 'enc_vals    | ' str_out += ' '.join([self.id2word[w] for w in batch_dict['enc_vals' ][i][:elen]]) + '\n' dblen = batch_dict['dec_bow_len'][i] str_out += 'dec_bow     | ' str_out += ' '.join([self.id2word[w] for w in batch_dict['dec_bow'] [i][:dblen]]) + '\n' str_out += 'reference   | ' str_out += ' '.join([self.id2word[w] for w in batch_dict[ 'references'][i][0]]) + '\n' if output_dict is not None: str_out += 'dec_predict | ' olen = _get_sent_len(output_dict['dec_predict'][i], self.end_id) str_out += ' '.join([self.id2word[w] for w in output_dict[ 'dec_predict'][i][:olen]]) + '\n' if i < num_cases: print(str_out) return 
polynomials|gcn|chebyshev|master|recurrence|utils|text def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap): s_lap = sp.csr_matrix(scaled_lap, copy=True) return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two 
